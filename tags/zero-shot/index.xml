<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zero-Shot on Ayush Subedi</title>
    <link>https://ayushsubedi.github.io/tags/zero-shot/</link>
    <description>Recent content in Zero-Shot on Ayush Subedi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 05 Nov 2024 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ayushsubedi.github.io/tags/zero-shot/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>[Paper Exploration] In-Depth Analysis of the Segment Anything Model (SAM)</title>
      <link>https://ayushsubedi.github.io/posts/segment_anything/</link>
      <pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/segment_anything/</guid>
      <description>&lt;h1 id=&#34;paper-exploration-in-depth-analysis-of-the-segment-anything-model-sam&#34;&gt;[Paper Exploration] In-Depth Analysis of the Segment Anything Model (SAM)&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Authors: Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Published on 2023&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;strong&gt;Segment Anything Model (SAM)&lt;/strong&gt; was developed by Meta AI as a foundation model for image segmentation tasks. The goal of SAM is to create a universal model that can efficiently handle various segmentation tasks with minimal &lt;strong&gt;prompting&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://uploads-ssl.webflow.com/641bc1f4cda8707686f07277/643d06676d8d2025ce33b806_ezgif.com-video-to-gif-2.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive &amp;ndash; often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at this https URL to foster research into foundation models for computer vision.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;iframe width=&#34;100%&#34; height =&#34;1024&#34; src=&#34;https://ayushsubedi.github.io/pdfs/sam.pdf#toolbar=0&#34;&gt;&lt;/iframe&gt;
&lt;h2 id=&#34;1-problem-formulation&#34;&gt;1. Problem Formulation&lt;/h2&gt;
&lt;p&gt;In image segmentation, the task is to partition an image into different regions or objects. Mathematically, given an image $I \in \mathbb{R}^{H \times W \times 3}$, where $H$ and $W$ are the height and width, the goal is to generate a mask $M \in {0,1}^{H \times W}$ for each object or region.&lt;/p&gt;
&lt;h3 id=&#34;segment-anything-objective&#34;&gt;Segment Anything Objective:&lt;/h3&gt;
&lt;p&gt;The objective of SAM is to generalize across diverse segmentation tasks, where the input can be various forms of &lt;strong&gt;prompts&lt;/strong&gt;: text, points, bounding boxes, or even free-form scribbles. The task then is to predict the segmentation mask based on these prompts.&lt;/p&gt;
&lt;p&gt;Let the input image be $I$, and the prompt $P$, the segmentation mask is predicted by:&lt;/p&gt;
&lt;p&gt;$$
M = f(I, P; \theta)
$$&lt;/p&gt;
&lt;p&gt;Where $f$ is the SAM model, parameterized by $\theta$, that predicts the mask $M$ given the image $I$ and prompt $P$.&lt;/p&gt;
&lt;h2 id=&#34;2-sam-architecture&#34;&gt;2. SAM Architecture&lt;/h2&gt;
&lt;h3 id=&#34;encoder&#34;&gt;Encoder:&lt;/h3&gt;
&lt;p&gt;SAM&amp;rsquo;s encoder is a deep neural network that takes the input image $I$ and processes it into a feature map representation $F$. This can be expressed as:&lt;/p&gt;
&lt;p&gt;$$
F = \text{Encoder}(I; \theta_{enc})
$$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The encoder uses a &lt;strong&gt;Vision Transformer (ViT)&lt;/strong&gt;, which is particularly well-suited for handling large and diverse datasets because of its attention-based mechanism. The ViT splits the image into patches and applies self-attention:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Patch embedding&lt;/strong&gt;:
Divide the input image $I$ into $N$ patches, each of size $P \times P$:&lt;/p&gt;
&lt;p&gt;$$
I \rightarrow {P_1, P_2, &amp;hellip;, P_N}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://www.researchgate.net/publication/366199197/figure/fig5/AS:11431281106824020@1670850470249/Patch-embedding-process.ppm&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-attention mechanism&lt;/strong&gt;:
Each patch is embedded into a fixed-dimensional latent space $Z \in \mathbb{R}^{N \times D}$, where $D$ is the embedding dimension:&lt;/p&gt;
&lt;p&gt;$$
Z = \text{softmax}\left(\frac{Q K^T}{\sqrt{d}}\right) V
$$&lt;/p&gt;
&lt;p&gt;where $Q, K, V$ are the query, key, and value matrices computed from the patch embeddings, and $d$ is a scaling factor to normalize the dot products.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;prompt-interaction&#34;&gt;Prompt Interaction:&lt;/h3&gt;
&lt;p&gt;Given the feature map $F$, the prompt $P$ is projected into the same latent space. Let $P_{\text{latent}}$ be the prompt embedding:&lt;/p&gt;
&lt;p&gt;$$
P_{\text{latent}} = \text{Embed}(P; \theta_{P})
$$&lt;/p&gt;
&lt;p&gt;The interaction between $F$ and $P_{\text{latent}}$ is learned via cross-attention mechanisms. The cross-attention operation can be written as:&lt;/p&gt;
&lt;p&gt;$$
\text{CrossAttn}(F, P_{\text{latent}}) = \text{softmax}\left(\frac{F P_{\text{latent}}^T}{\sqrt{d}}\right) P_{\text{latent}}
$$&lt;/p&gt;
&lt;h3 id=&#34;decoder&#34;&gt;Decoder:&lt;/h3&gt;
&lt;p&gt;The decoder takes the refined feature map $F&amp;rsquo;$ from the encoder and the prompt interaction to produce the final segmentation mask. This is done by upscaling the latent features back to the image resolution. Mathematically:&lt;/p&gt;
&lt;p&gt;$$
M = \text{Decoder}(F&amp;rsquo;, P_{\text{latent}}; \theta_{dec})
$$&lt;/p&gt;
&lt;h2 id=&#34;3-loss-function&#34;&gt;3. Loss Function&lt;/h2&gt;
&lt;p&gt;SAM is trained using a combination of loss functions to ensure both pixel-wise accuracy and boundary precision. The typical losses used are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross-Entropy Loss&lt;/strong&gt;:&lt;br&gt;
Used for pixel-wise classification:&lt;/p&gt;
&lt;p&gt;$$
L_{\text{CE}} = - \sum_{i,j} M_{i,j} \log(\hat{M}_{i,j})
$$&lt;/p&gt;
&lt;p&gt;where $\hat{M}_{i,j}$ is the predicted mask probability for pixel $(i, j)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dice Loss&lt;/strong&gt;:&lt;br&gt;
Focuses on the overlap between the predicted and ground-truth masks:&lt;/p&gt;
&lt;p&gt;$$
L_{\text{Dice}} = 1 - \frac{2 \sum M \cdot \hat{M}}{\sum M + \sum \hat{M}}
$$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.determined.ai/assets/images/blogs/brain-mri-demo/dicevsiou2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The relationship between IoU and the Dice coefficient can be expressed as:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\text{Dice} = \frac{2 \times \text{IoU}}{1 + \text{IoU}}
$$&lt;/p&gt;
&lt;p&gt;where,&lt;/p&gt;
&lt;p&gt;$$\text{IoU} = \frac{|A \cap B|}{|A \cup B|}$$&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Total Loss&lt;/strong&gt;:&lt;br&gt;
The total loss function combines both:&lt;/p&gt;
&lt;p&gt;$$
L = L_{\text{CE}} + \lambda L_{\text{Dice}}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;task-model-and-data-engine&#34;&gt;Task, Model, and Data Engine&lt;/h2&gt;
&lt;p&gt;The successful implementation of the Segment Anything Model (SAM) relies on a well-defined framework encompassing the task, the model architecture, and the underlying data engine. Each component plays a crucial role in the overall performance and applicability of SAM across various segmentation tasks.&lt;/p&gt;
&lt;h3 id=&#34;1-task&#34;&gt;1. Task&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://andlukyane.com/images/paper_reviews/sam/2023-04-07_08-33-02.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The primary task of SAM is to perform image segmentation, which involves partitioning an image into distinct segments or regions that correspond to different objects or parts within the image. The versatility of SAM allows it to tackle a wide range of segmentation tasks, including but not limited to:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://images.prismic.io/encord/9411df89-27df-4931-8f40-3b47e3422269_Panoptic+Segmentation+vs+Semantic+Segmentation.png?auto=compress,format&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Semantic Segmentation&lt;/strong&gt;: Classifying each pixel in the image into a predefined category, providing a global understanding of the scene.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Instance Segmentation&lt;/strong&gt;: Distinguishing between different instances of the same object category, enabling the model to identify and segment individual objects separately.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Panoptic Segmentation&lt;/strong&gt;: A combination of semantic and instance segmentation that provides a comprehensive representation of the scene, identifying both object categories and individual instances.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SAM&amp;rsquo;s design allows it to adapt to these tasks with minimal prompting, making it a powerful tool for various applications.&lt;/p&gt;
&lt;h3 id=&#34;2-model&#34;&gt;2. Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://andlukyane.com/images/paper_reviews/sam/2023-04-07_08-44-16.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The SAM architecture is a sophisticated model built on the principles of foundation models and Vision Transformers (ViTs). Key characteristics of the SAM model include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pre-trained Architecture&lt;/strong&gt;: SAM is trained on a large dataset (SA-1B) that encompasses diverse images and segmentation tasks, allowing it to generalize effectively across various domains.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prompt-based Segmentation&lt;/strong&gt;: SAM leverages user-provided prompts (such as points, bounding boxes, or text) to guide the segmentation process, facilitating a more interactive and flexible user experience.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: The model&amp;rsquo;s architecture is designed to scale efficiently, accommodating various image sizes and complexities without significant degradation in performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The combination of these features makes SAM an efficient and adaptable model for image segmentation tasks.&lt;/p&gt;
&lt;h3 id=&#34;3-data-engine&#34;&gt;3. Data Engine&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://andlukyane.com/images/paper_reviews/sam/2023-04-07_15-16-35.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The data engine plays a critical role in the training and evaluation of SAM. It encompasses the following aspects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dataset Quality&lt;/strong&gt;: The performance of SAM is significantly influenced by the quality and diversity of the training dataset. The SA-1B dataset, consisting of over a billion segmented masks, provides a rich source of information for training the model on various objects and scenes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Preprocessing&lt;/strong&gt;: Effective data preprocessing techniques (e.g., normalization, augmentation) are essential to enhance the robustness of the model. These techniques help improve the model&amp;rsquo;s performance by ensuring it can handle variations in input data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evaluation Metrics&lt;/strong&gt;: Establishing clear evaluation metrics is vital for assessing the performance of SAM across different segmentation tasks. Common metrics include Intersection over Union (IoU), pixel accuracy, and F1 score, which provide insights into the model&amp;rsquo;s effectiveness and areas for improvement.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Together, the task, model, and data engine form a cohesive framework that enhances the functionality and effectiveness of the Segment Anything Model, allowing it to address a wide array of segmentation challenges.&lt;/p&gt;
&lt;h2 id=&#34;appendix-extended-terminologies&#34;&gt;Appendix: Extended Terminologies&lt;/h2&gt;
&lt;h3 id=&#34;1-foundation-models&#34;&gt;1. Foundation Models&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Foundation models&lt;/strong&gt; are large, pre-trained models that can be fine-tuned for various downstream tasks. These models are typically trained on massive datasets, allowing them to generalize across tasks without being explicitly trained on each.&lt;/p&gt;
&lt;p&gt;Let $D_{\text{pretrain}}$ represent the large pre-training dataset and $\theta$ the model parameters. A foundation model is trained by minimizing the loss function $L$:&lt;/p&gt;
&lt;p&gt;$$
\theta^* = \arg \min_\theta E_{(x,y) \sim D_{\text{pretrain}}} [L(f(x; \theta), y)]
$$&lt;/p&gt;
&lt;p&gt;Where $f(x; \theta)$ is the model&amp;rsquo;s prediction for input $x$.&lt;/p&gt;
&lt;p&gt;Foundation models allow for &lt;strong&gt;transfer learning&lt;/strong&gt;, which means they can be adapted to new tasks by fine-tuning on specific datasets.&lt;/p&gt;
&lt;h3 id=&#34;2-model-finetuning&#34;&gt;2. Model Finetuning&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Model fine-tuning&lt;/strong&gt; is the process of adapting a pre-trained model to a specific task or dataset. In the context of SAM, fine-tuning refers to using a pre-trained segmentation model and adapting it for specific segmentation tasks.&lt;/p&gt;
&lt;p&gt;For a fine-tuning dataset $D_{\text{fine}}$, the fine-tuned parameters $\theta_{\text{fine}}$ are obtained as:&lt;/p&gt;
&lt;p&gt;$$
\theta_{\text{fine}} = \arg \min_\theta E_{(x,y) \sim D_{\text{fine}}} [L(f(x; \theta), y)]
$$&lt;/p&gt;
&lt;p&gt;This method allows SAM to adapt its general segmentation ability to specialized tasks like medical imaging or satellite image segmentation.&lt;/p&gt;
&lt;h3 id=&#34;3-human-in-the-loop-hitl-relevance&#34;&gt;3. Human in the Loop (HITL) Relevance&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.prod.website-files.com/5fb24a974499e90dae242d98/63492e0f10678129c531cf83_Human-in-the-Loop%20in%20Machine%20Learning_%20What%20is%20it%20and%20How%20Does%20it%20Work_.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Human-in-the-Loop (HITL)&lt;/strong&gt; refers to involving humans at various stages of a machine learning system’s lifecycle to improve the model’s performance. In SAM, humans provide critical inputs in three main stages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Data Annotation&lt;/strong&gt;: Human annotators provide ground-truth masks for training data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interactive Segmentation&lt;/strong&gt;: Users provide prompts (points, bounding boxes, scribbles) to guide SAM’s segmentation process.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Feedback and Fine-tuning&lt;/strong&gt;: Corrections made by users on the generated masks are fed back into the system to further fine-tune the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The model with human input can be represented as:&lt;/p&gt;
&lt;p&gt;$$
M = f(I, P_{\text{human}}; \theta)
$$&lt;/p&gt;
&lt;p&gt;Where $P_{\text{human}}$ is the human prompt, $I$ is the input image, and $\theta$ are the model parameters.&lt;/p&gt;
&lt;h3 id=&#34;4-coco-and-other-datasets&#34;&gt;4. COCO and Other Datasets&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://production-media.paperswithcode.com/datasets/0daad4f0-886b-44ed-9b96-80d99e037f16.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;COCO (Common Objects in Context)&lt;/strong&gt; is a large-scale dataset commonly used for segmentation, detection, and captioning tasks. It includes annotations for objects in complex scenes.&lt;/p&gt;
&lt;p&gt;Let $\mathcal{D}_{\text{COCO}} = {(I_1, M_1), (I_2, M_2), \dots}$ represent the COCO dataset, where $I_i$ is an image and $M_i$ is the corresponding mask.&lt;/p&gt;
&lt;p&gt;Other important datasets used in segmentation tasks include &lt;strong&gt;ADE20K&lt;/strong&gt;, &lt;strong&gt;LVIS&lt;/strong&gt;, and &lt;strong&gt;ImageNet&lt;/strong&gt;. These datasets help train models like SAM across diverse object categories.&lt;/p&gt;
&lt;h3 id=&#34;5-zero-shot-and-few-shot-learning&#34;&gt;5. Zero-shot and Few-shot Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Zero-shot learning&lt;/strong&gt; refers to the model&amp;rsquo;s ability to generalize to unseen objects or tasks without any specific training examples. SAM is capable of performing zero-shot segmentation due to its extensive pre-training on diverse datasets.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a new object $O_{\text{new}}$, the model generates a mask $M$ without specific training examples:&lt;/p&gt;
&lt;p&gt;$$
M = f(I, O_{\text{new}}; \theta)
$$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://amitness.com/posts/images/zero-shot-vs-transfer.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Few-shot learning&lt;/strong&gt; involves training a model on a small number of labeled examples. SAM can fine-tune itself on a few labeled samples and still produce accurate masks for new data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/v2/resize:fit:1400/1*Wufqbrmtdt49Cmez5NnUaA.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;6-flops-floating-point-operations&#34;&gt;6. FLOPs (Floating Point Operations)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;FLOPs&lt;/strong&gt; refer to the number of floating-point operations required to perform inference through the model. This gives a measure of the computational complexity of a neural network.&lt;/p&gt;
&lt;p&gt;For a convolutional layer with input dimensions $H \times W \times C_{\text{in}}$, kernel size $K \times K$, and output channels $C_{\text{out}}$, the FLOPs can be computed as:&lt;/p&gt;
&lt;p&gt;$$
\text{FLOPs} = H \times W \times C_{\text{in}} \times K^2 \times C_{\text{out}}
$$&lt;/p&gt;
&lt;p&gt;In a transformer model (like SAM), the self-attention mechanism contributes significantly to FLOPs. For a sequence length $N$ and embedding size $D$, the FLOPs for the self-attention mechanism is:&lt;/p&gt;
&lt;p&gt;$$
\text{FLOPs} = 4ND^2 + 2N^2D
$$&lt;/p&gt;
&lt;h3 id=&#34;7-edge-detection&#34;&gt;7. Edge Detection&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Edge detection&lt;/strong&gt; identifies boundaries of objects in images by detecting areas of abrupt intensity changes. This can be achieved using image gradients, for instance with the &lt;strong&gt;Sobel&lt;/strong&gt; or &lt;strong&gt;Canny&lt;/strong&gt; edge detectors.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kr.mathworks.com/help/examples/android/win64/edge_detection_sobel.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;For an image $I$, the gradient magnitude $G$ is computed as:&lt;/p&gt;
&lt;p&gt;$$
G = \sqrt{\left(\frac{\partial I}{\partial x}\right)^2 + \left(\frac{\partial I}{\partial y}\right)^2}
$$&lt;/p&gt;
&lt;p&gt;Edge detection helps SAM refine the boundaries of the generated segmentation masks.&lt;/p&gt;
&lt;h3 id=&#34;8-ablation-studies&#34;&gt;8. Ablation Studies&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Ablation studies&lt;/strong&gt; test the contribution of different components of a model by removing or altering them and measuring the effect on performance. For SAM, ablations can focus on testing different prompt types (points, bounding boxes) or removing attention layers.&lt;/p&gt;
&lt;p&gt;The performance difference due to an ablation is quantified as:&lt;/p&gt;
&lt;p&gt;$$
\Delta L = L_{\text{full}} - L_{\text{ablated}}
$$&lt;/p&gt;
&lt;p&gt;Where $L_{\text{full}}$ is the loss of the full model and $L_{\text{ablated}}$ is the loss of the ablated model.&lt;/p&gt;
&lt;h3 id=&#34;9-compositionality&#34;&gt;9. Compositionality&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Compositionality&lt;/strong&gt; refers to the model&amp;rsquo;s ability to understand and segment complex objects composed of multiple parts. SAM&amp;rsquo;s compositionality enables it to combine segmentations of different parts into a coherent whole.&lt;/p&gt;
&lt;p&gt;Mathematically, if $M_1$ and $M_2$ are the masks for two parts of an object, the combined mask can be expressed as:&lt;/p&gt;
&lt;p&gt;$$
M_{\text{combined}} = M_1 \cup M_2
$$&lt;/p&gt;
&lt;p&gt;This allows SAM to handle multi-object segmentation effectively.&lt;/p&gt;
&lt;h3 id=&#34;10-rai-responsible-ai-analysis&#34;&gt;10. RAI (Responsible AI) Analysis&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://h2o.ai/resources/data-sheet/responsible-ai-overview/_jcr_content/root/container/section_1441453307/par/advancedcolumncontro/columns1/image.coreimg.png/1674580925697/responsible-ai-ven-diagram-wide.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Responsible AI (RAI)&lt;/strong&gt; involves ensuring that AI models are developed and used ethically and fairly. In the context of SAM, RAI considerations include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Bias&lt;/strong&gt;: Ensuring SAM performs equally well across different demographic groups.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Explainability&lt;/strong&gt;: Understanding why SAM produces specific segmentation outputs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transparency&lt;/strong&gt;: Making the model&amp;rsquo;s decisions interpretable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bias can be quantified by evaluating the difference in model performance across groups. Let $L_g$ be the loss for group $g$, the bias can be measured as:&lt;/p&gt;
&lt;p&gt;$$
\text{Bias} = \max_g L_g - \min_g L_g
$$&lt;/p&gt;
&lt;h3 id=&#34;11-compositionality&#34;&gt;11. Compositionality&lt;/h3&gt;
&lt;p&gt;Compositionality is the ability to understand and represent complex structures made of simpler parts. For example, if $M_1$ represents the mask for object part 1 and $M_2$ the mask for part 2, the compositional segmentation of the object is:&lt;/p&gt;
&lt;p&gt;$$
M_{\text{composite}} = M_1 \cup M_2
$$&lt;/p&gt;
&lt;p&gt;This helps SAM handle images with multiple overlapping objects.&lt;/p&gt;
&lt;h2 id=&#34;limitations-of-sam&#34;&gt;Limitations of SAM&lt;/h2&gt;
&lt;p&gt;While the Segment Anything Model (SAM) represents a significant advancement in image segmentation, it is important to acknowledge several limitations outlined by the authors:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generalization Challenges&lt;/strong&gt;:
Although SAM aims to generalize across diverse segmentation tasks, it may still struggle with specific domains or types of images that differ significantly from the training data. For instance, tasks requiring fine-grained segmentation in specialized fields (e.g., medical imaging) might not achieve optimal performance without additional fine-tuning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sensitivity to Prompts&lt;/strong&gt;:
The performance of SAM can be highly dependent on the quality and type of prompts provided. For instance, using less informative prompts may lead to suboptimal segmentation results. The model&amp;rsquo;s effectiveness is influenced by how well the prompts convey the intended segmentation task.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Computational Cost&lt;/strong&gt;:
SAM&amp;rsquo;s architecture, particularly its use of Vision Transformers, can lead to high computational costs during both training and inference. This may limit its applicability in real-time scenarios or on devices with constrained computational resources.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dataset Bias&lt;/strong&gt;:
The performance of SAM is contingent on the data it was trained on. While it is built on a large and diverse dataset (SA-1B), any inherent biases or limitations in this dataset can propagate into the model&amp;rsquo;s outputs, potentially affecting fairness and accuracy in various applications.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Lack of Fine-grained Control&lt;/strong&gt;:
While SAM is designed to handle a variety of segmentation tasks with minimal prompting, there may be scenarios where users require precise control over the segmentation process (e.g., specifying object boundaries). SAM&amp;rsquo;s generalized approach might not always provide the level of control necessary for such tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Zero-shot Performance Variability&lt;/strong&gt;:
Although SAM exhibits impressive zero-shot capabilities, its performance can vary significantly depending on the nature of the new tasks or image distributions it encounters. In some cases, performance may not match that of fully supervised models, particularly for complex or nuanced segmentation tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Original Paper&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., Dollár, P., &amp;amp; Girshick, R. (2023). &lt;a href=&#34;https://arxiv.org/abs/2304.02699&#34;&gt;Segment Anything&lt;/a&gt;. Meta AI.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Images&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SAM Architecture Diagram: Source from Hugging Face.&lt;/li&gt;
&lt;li&gt;Segmentation Tasks: Source from Andlukyane.&lt;/li&gt;
&lt;li&gt;Semantic vs. Instance Segmentation: Source from Encord.&lt;/li&gt;
&lt;li&gt;Human in the Loop: Source from Levity AI&lt;/li&gt;
&lt;li&gt;Dice Loss: Source from Determined AI&lt;/li&gt;
&lt;li&gt;Responsible AI: Source from H20.AI&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>[Paper Exploration] Train Once, Test Anywhere: Zero-Shot Learning for Text Classification</title>
      <link>https://ayushsubedi.github.io/posts/zero-shot-classification/</link>
      <pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/zero-shot-classification/</guid>
      <description>&lt;h1 id=&#34;paper-exploration-train-once-test-anywhere-zero-shot-learning-for-text-classification&#34;&gt;[Paper Exploration] Train Once, Test Anywhere: Zero-Shot Learning for Text Classification&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Author: Pushpankar Kumar Pushp, Muktabh Mayank Srivastava&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Published on 2017&lt;/p&gt;
&lt;/blockquote&gt;
&lt;iframe width=&#34;100%&#34; height =&#34;1024&#34; src=&#34;https://arxiv.org/pdf/1712.05972#toolbar=0&#34;&gt;&lt;/iframe&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Zero-shot Learners are models capable of predicting unseen classes. In this work, we propose a Zero-shot Learning approach for text categorization. Our method involves training model on a large corpus of sentences to learn the relationship between a sentence and embedding of sentence&amp;rsquo;s tags. Learning such relationship makes the model generalize to unseen sentences, tags, and even new datasets provided they can be put into same embedding space. The model learns to predict whether a given sentence is related to a tag or not; unlike other classifiers that learn to classify the sentence as one of the possible classes. We propose three different neural networks for the task and report their accuracy on the test set of the dataset used for training them as well as two other standard datasets for which no retraining was done. We show that our models generalize well across new unseen classes in both cases. Although the models do not achieve the accuracy level of the state of the art supervised models, yet it evidently is a step forward towards general intelligence in natural language processing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;In this work, we introduce techniques and models that can be used for zero-shot classification in text. We show that our models can get better than random classification accuracies on datasets without seeing even one example. We can say that this technique learns the concept of relatedness between a sentence and a word that can be extended beyond datasets. That said, the levels of accuracy leave a lot of scope for future work.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;glossary&#34;&gt;Glossary&lt;/h1&gt;
&lt;h2 id=&#34;text-classification&#34;&gt;Text Classification&lt;/h2&gt;
&lt;p&gt;The process of assigning predefined categories or labels to text documents based on their content. Common applications include sentiment analysis, spam detection, and topic categorization.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/v2/resize:fit:920/1*CS-OYdiRLCBMBiOpEURy0g.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;zero-shot-learning&#34;&gt;Zero-Shot Learning&lt;/h2&gt;
&lt;p&gt;A machine learning paradigm where a model is trained on certain tasks and then applied to new, unseen tasks without additional training. It leverages generalizable knowledge to perform well on tasks it has not explicitly encountered during training. (an instance og transfer learning)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; transformers &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pipeline
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pipeline(model&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;facebook/bart-large-mnli&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;I have a problem with my iphone that needs to be resolved asap!&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    candidate_labels&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;urgent&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;not urgent&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;phone&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tablet&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;computer&amp;#34;&lt;/span&gt;],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# output&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sequence&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;I have a problem with my iphone that needs to be resolved asap!!&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;labels&amp;#39;&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;urgent&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;phone&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;computer&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;not urgent&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tablet&amp;#39;&lt;/span&gt;], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;scores&amp;#39;&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;0.504&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.479&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.013&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.003&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.002&lt;/span&gt;]}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;pre-trained-language-model&#34;&gt;Pre-trained Language Model&lt;/h2&gt;
&lt;p&gt;A language model that has been previously trained on a large corpus of text data. Examples include BERT, GPT, and RoBERTa. These models capture rich linguistic information and can be fine-tuned for specific tasks.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.baeldung.com/wp-content/uploads/sites/4/2024/01/llm_tree-scaled.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;embedding&#34;&gt;Embedding&lt;/h2&gt;
&lt;p&gt;A dense vector representation of text (words, sentences, documents) where semantically similar texts are represented by similar vectors. Embeddings are obtained using models like Word2Vec, GloVe, or BERT.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://causewriter.ai/wp-content/uploads/2023/08/image-2-1024x483.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;contextual-embeddings&#34;&gt;Contextual Embeddings&lt;/h2&gt;
&lt;p&gt;Embeddings that capture the meaning of a word based on its context within a sentence. Unlike static embeddings (e.g., Word2Vec), contextual embeddings vary depending on the surrounding text.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ai.stanford.edu/blog/assets/img/posts/2020-03-24-contextual/contextual_mouse_transparent_1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;similarity-function&#34;&gt;Similarity Function&lt;/h2&gt;
&lt;p&gt;A function that measures the similarity between two vectors (e.g., text embeddings). Common similarity functions include cosine similarity, Euclidean distance, and dot product.&lt;/p&gt;
&lt;h2 id=&#34;cosine-similarity&#34;&gt;Cosine Similarity&lt;/h2&gt;
&lt;p&gt;A measure of similarity between two non-zero vectors that calculates the cosine of the angle between them. It is defined as:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LfW66-WsYkFqWc4XYJbEJg.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;cross-entropy-loss&#34;&gt;Cross-Entropy Loss&lt;/h2&gt;
&lt;p&gt;A loss function used for classification tasks. It measures the performance of a classification model whose output is a probability value between 0 and 1.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;back-to-paper&#34;&gt;Back to Paper&lt;/h1&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Addresses limitations of traditional text classification models requiring large annotated datasets for each new task.&lt;/li&gt;
&lt;li&gt;Zero-shot learning aims to eliminate the need for task-specific training by leveraging generalizable knowledge.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;proposed-approach&#34;&gt;Proposed Approach&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Introduces a zero-shot text classification method using pre-trained language models to classify text without task-specific fine-tuning.&lt;/li&gt;
&lt;li&gt;Model incorporates task descriptions (in natural language) along with input text to predict the correct class.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;methodology&#34;&gt;Methodology&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Problem Definition&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Given a set of classes  $C$ and a text input $x$, assign $x$ to one of the classes in $C$ without having seen examples from $C$ during training.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Model Architecture&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use a pre-trained language model  $\phi$ (e.g., BERT) to obtain contextual embeddings.&lt;/li&gt;
&lt;li&gt;For a text input $x$, the embedding is denoted as $\mathbf{h}_x = \phi(x)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Task Descriptions&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each class $ c \in C $ is associated with a natural language description $d_c$.&lt;/li&gt;
&lt;li&gt;The description $d_c$ is also embedded using the same pre-trained model: $\mathbf{h}_{d_c} = \phi(d_c)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Classification is based on the similarity between the embeddings of the input text $x$ and each class description $d_c$.&lt;/li&gt;
&lt;li&gt;A similarity function measures this similarity. Common choices include cosine similarity&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Prediction&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The predicted class $\hat{c}$ for an input $x$ is the one with the highest similarity score:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The model learns to produce embeddings that capture the semantic content of both inputs and class descriptions.&lt;/li&gt;
&lt;li&gt;Training objective can be a standard classification loss, such as cross-entropy, adapted to use the similarity scores&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experiments-and-results&#34;&gt;Experiments and Results&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Evaluated on several text classification benchmarks, comparing zero-shot performance with traditional supervised learning models.&lt;/li&gt;
&lt;li&gt;Metrics: accuracy, precision, recall, F1-score.&lt;/li&gt;
&lt;li&gt;Results show competitive performance, sometimes outperforming supervised models, indicating generalization capability across tasks and domains.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;advantages&#34;&gt;Advantages&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Reduces the need for large annotated datasets for each specific task.&lt;/li&gt;
&lt;li&gt;Enables rapid deployment and adaptation to new tasks without additional training.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;limitations-and-future-work&#34;&gt;Limitations and Future Work&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Dependency on the quality of task descriptions.&lt;/li&gt;
&lt;li&gt;Potential reduced performance in highly specialized tasks.&lt;/li&gt;
&lt;li&gt;Future research includes improving task descriptions and exploring zero-shot learning for other NLP tasks.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>