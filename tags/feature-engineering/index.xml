<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>feature-engineering on Ayush Subedi</title>
    <link>https://ayushsubedi.github.io/tags/feature-engineering/</link>
    <description>Recent content in feature-engineering on Ayush Subedi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 15 Jan 2023 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ayushsubedi.github.io/tags/feature-engineering/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>AWS Certified ML - Specialty exam (MLS-C01) - 2. Exploratory Data Analysis</title>
      <link>https://ayushsubedi.github.io/posts/aws_ml_speciality_eda/</link>
      <pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/aws_ml_speciality_eda/</guid>
      <description>&lt;h1 id=&#34;exploratory-data-analysis&#34;&gt;Exploratory Data Analysis&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#python-in-data-science-and-machine-learning&#34;&gt;Python in data science and machine learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-data&#34;&gt;Types of Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-distributions&#34;&gt;Data Distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trends-and-seasonality&#34;&gt;Trends and seasonality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#athena&#34;&gt;Athena&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quicksight&#34;&gt;Quicksight&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-visualization&#34;&gt;Types of visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#emr&#34;&gt;EMR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hadoop&#34;&gt;Hadoop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#apache-spark&#34;&gt;Apache Spark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#emr-notebooks,-security-and-instance-types&#34;&gt;EMR Notebooks, Security and Instance Types&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feature-engineering&#34;&gt;Feature Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#imputing-missing-data&#34;&gt;Imputing Missing Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#unbalanced-data&#34;&gt;Unbalanced Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#handling-outliers&#34;&gt;Handling Outliers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#binning,-transoforming,-encoding,-scaling,-and-shuffling&#34;&gt;Binning, Transoforming, Encoding, Scaling, and Shuffling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-sagemaker-ground-truth-and-label-generation&#34;&gt;Amazon Sagemaker Ground Truth and Label Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#misc&#34;&gt;Misc&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This section requires understanding of sanitizing and preparing data for modeling, performing feature engineering, and analyzing and visualizing data for machine learning.&lt;/p&gt;
&lt;p&gt;Exploratory Data Analysis (EDA) is a process of analyzing and summarizing a dataset in order to understand its structure and relationships. In the context of Amazon Web Services (AWS), EDA is often performed on large datasets that are stored in AWS storage services such as Amazon S3 or Amazon EBS.&lt;/p&gt;
&lt;p&gt;To perform EDA on AWS, users can use various tools and services provided by AWS. For example, users can use Amazon Elastic MapReduce (EMR) to process and analyze large datasets using tools such as Apache Spark or Hive. Users can also use Amazon Athena to query datasets stored in Amazon S3 using SQL.&lt;/p&gt;
&lt;p&gt;In addition to these tools, users can also use various AWS services and libraries to visualize and explore the data. For example, users can use Amazon QuickSight to create interactive charts and dashboards, or use libraries such as pandas and matplotlib to create custom visualizations.&lt;/p&gt;
&lt;p&gt;Overall, EDA on AWS involves using a combination of tools and services to understand the structure and relationships within a dataset, and to gain insights that can inform further analysis and decision making.&lt;/p&gt;
&lt;h2 id=&#34;python-in-data-science-and-machine-learning&#34;&gt;Python in data science and machine learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Python code will not be tested in the exam.&lt;/li&gt;
&lt;li&gt;Python is a popular language for data exploration, analysis, and machine learning. It has a number of useful libraries for loading, manipulating, and visualizing data, as well as for building and training machine learning models.&lt;/li&gt;
&lt;li&gt;For data exploration and visualization, some popular libraries include pandas, numpy, and matplotlib. Pandas is a library for working with tabular data, numpy is a library for working with numerical data, and matplotlib is a library for creating charts and plots.&lt;/li&gt;
&lt;li&gt;For machine learning, some popular libraries include scikit-learn, tensorflow, and pytorch. These libraries include a wide range of tools for tasks such as classification, regression, clustering, and deep learning.&lt;/li&gt;
&lt;li&gt;Overall, Python is a powerful and flexible language for data analysis and machine learning, and is widely used in the field.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;types-of-data&#34;&gt;Types of Data&lt;/h2&gt;
&lt;p&gt;There are many different types of data, and the type of data can often influence the analysis and techniques used to understand it. Some common types of data include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Numeric data: This includes data that is represented as numbers, such as integers or floating point values.&lt;/li&gt;
&lt;li&gt;Categorical data: This includes data that consists of categories or groups, such as gender or eye color.&lt;/li&gt;
&lt;li&gt;Ordinal data: This is similar to categorical data, but the categories have a natural ordering, such as low, medium, and high.&lt;/li&gt;
&lt;li&gt;Binary data: This is data that has only two categories, such as true/false or 0/1.&lt;/li&gt;
&lt;li&gt;Time series data: This is data that is collected over time, such as daily stock prices or monthly sales figures.&lt;/li&gt;
&lt;li&gt;Text data: This is data that is represented as text, such as emails or social media posts.&lt;/li&gt;
&lt;li&gt;Image data: This is data that is represented as images, such as photographs or videos.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-distributions&#34;&gt;Data Distributions&lt;/h2&gt;
&lt;h3 id=&#34;normal-distribution&#34;&gt;Normal Distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The normal distribution, also known as the Gaussian distribution, is a continuous probability distribution that is defined by a symmetrical bell-shaped curve.&lt;/li&gt;
&lt;li&gt;It is one of the most widely used and well-known probability distributions in statistics, and is commonly used to model real-valued random variables.&lt;/li&gt;
&lt;li&gt;The normal distribution is completely defined by its mean and standard deviation.&lt;/li&gt;
&lt;li&gt;The mean is the center of the distribution and determines the location of the peak of the curve.&lt;/li&gt;
&lt;li&gt;The standard deviation is a measure of the spread of the distribution and determines the width of the curve.&lt;/li&gt;
&lt;li&gt;A larger standard deviation means that the data is more spread out, while a smaller standard deviation means that the data is more concentrated around the mean.&lt;/li&gt;
&lt;li&gt;The normal distribution has a number of useful properties. For example, the empirical rule states that for a normal distribution, approximately 68% of the data lies within one standard deviation of the mean, 95% of the data lies within two standard deviations of the mean, and 99.7% of the data lies within three standard deviations of the mean.&lt;/li&gt;
&lt;li&gt;This means that if a dataset follows a normal distribution, a large percentage of the data will be concentrated around the mean. Overall, the normal distribution is a widely used and important distribution in statistics, and is often used to model real-valued data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;probability-mass-function-discrete-data-type&#34;&gt;Probability Mass function (discrete data type)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A probability mass function (PMF) is a function that gives the probability of a discrete random variable taking on a particular value. For a random variable X, the PMF is denoted as f(x), and it is defined as the probability that X takes on the value x.&lt;/li&gt;
&lt;li&gt;The PMF is a useful tool for describing the probability distribution of a discrete random variable. It specifies the probability of each possible outcome, and can be used to calculate various statistical quantities such as the mean, variance, and skewness of the distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bernoulli-discrete-data-type&#34;&gt;Bernoulli (discrete data type)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Bernoulli distribution is a discrete probability distribution that models the probability of a binary outcome, such as the result of a coin flip or a yes/no question. It is defined by a single parameter p, which represents the probability of success (the probability of the outcome being &amp;ldquo;yes&amp;rdquo;).&lt;/li&gt;
&lt;li&gt;The Bernoulli distribution is a special case of the binomial distribution, where the number of trials is fixed at n=1. In other words, it models a single binary event, such as the flip of a coin.&lt;/li&gt;
&lt;li&gt;The probability mass function (PMF) of a Bernoulli-distributed random variable X is given by the following formula:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(x) = p^x * (1-p)^(1-x)&lt;/p&gt;
&lt;p&gt;where x is the outcome (0 for &amp;ldquo;no&amp;rdquo; and 1 for &amp;ldquo;yes&amp;rdquo;), and p is the probability of success.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Bernoulli distribution has a mean of p and a variance of p(1-p). It is a simple but widely used distribution, and is often used as a building block for more complex models.&lt;/li&gt;
&lt;li&gt;Overall, the Bernoulli distribution is a useful tool for modeling the probability of a binary outcome, and is widely used in a variety of applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;binomial-discrete-data-type&#34;&gt;Binomial (discrete data type)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The binomial distribution is a discrete probability distribution that is used to model the probability of a certain number of successes in a fixed number of independent trials. It is defined by two parameters: the number of trials (n) and the probability of success in each trial (p).&lt;/li&gt;
&lt;li&gt;The binomial distribution can be used to model a wide variety of situations, such as the probability of flipping a coin and getting a certain number of heads in a row, or the probability of a certain number of defects occurring in a batch of products.&lt;/li&gt;
&lt;li&gt;The probability mass function (PMF) of a binomial-distributed random variable X is given by the following formula:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(x) = (n choose x) * p^x * (1-p)^(n-x)&lt;/p&gt;
&lt;p&gt;where x is the number of successes, n is the number of trials, p is the probability of success in each trial, and &amp;ldquo;choose&amp;rdquo; represents the binomial coefficient.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The binomial distribution has a number of useful properties, such as the fact that the mean and variance can be easily calculated from the parameters n and p. It is also a limiting distribution, meaning that it can be used to approximate other distributions under certain conditions.&lt;/li&gt;
&lt;li&gt;Overall, the binomial distribution is a useful tool for modeling the probability of a certain number of successes in a fixed number of independent trials, and is widely used in a variety of applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;geometric-distribution&#34;&gt;Geometric Distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The geometric distribution is a discrete probability distribution that models the number of Bernoulli trials (i.e., a series of independent &amp;ldquo;success-failure&amp;rdquo; experiments) needed to get a success. It is defined by a single parameter p, which is the probability of success on each trial.&lt;/li&gt;
&lt;li&gt;The probability mass function (PMF) of the geometric distribution is given by:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(k) = (1 - p)^(k-1) * p&lt;/p&gt;
&lt;p&gt;where k is the number of trials needed to get a success and p is the probability of success on each trial.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The geometric distribution has several characteristics:
&lt;ul&gt;
&lt;li&gt;It is a one-sided distribution, which means that it is defined only for k &amp;gt; 0.&lt;/li&gt;
&lt;li&gt;It is a discrete distribution, which means that it is defined for a specific set of values rather than for a continuous range of values.&lt;/li&gt;
&lt;li&gt;It has a mean of 1/p, which is the expected number of trials needed to get a success.&lt;/li&gt;
&lt;li&gt;The geometric distribution is often used in modeling the number of trials needed to get a success, such as the number of ads that need to be shown before a customer clicks on one, or the number of patients that need to be treated before a certain medical condition is cured. It is also used in reliability engineering to model the number of failures before a unit fails.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;poission-discrete-data-type&#34;&gt;Poission (discrete data type)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Poisson distribution is a discrete probability distribution that is used to model the number of times an event occurs within a certain period of time or space. It is commonly used to model events that occur randomly and independently, such as the number of customers arriving at a store or the number of defects in a manufactured product.&lt;/li&gt;
&lt;li&gt;The Poisson distribution is defined by a single parameter, called the rate parameter or the mean rate of occurrence. This parameter is denoted as lambda (λ) and represents the average number of times the event occurs per unit of time or space.&lt;/li&gt;
&lt;li&gt;The probability mass function (PMF) of a Poisson-distributed random variable X is given by the following formula:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(x) = (λ^x * e^(-λ)) / x!&lt;/p&gt;
&lt;p&gt;where x is the number of times the event occurs, λ is the rate parameter, and e is the base of the natural logarithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Poisson distribution has a number of useful properties, such as the fact that the mean and variance are equal to the rate parameter λ. It is also a limiting distribution, meaning that it can be used to approximate other distributions under certain conditions.&lt;/li&gt;
&lt;li&gt;Overall, the Poisson distribution is a useful tool for modeling the number of times an event occurs within a certain period of time or space, and is widely used in a variety of applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;exponential-distribution&#34;&gt;Exponential Distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The exponential distribution is a continuous probability distribution that describes the time between events in a Poisson process, which is a process in which events occur continuously and independently at a constant average rate. It is defined by a single parameter λ (lambda), which is the rate at which the events occur.&lt;/li&gt;
&lt;li&gt;The probability density function (PDF) of the exponential distribution is given by:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(x) = λ * e^(-λx)&lt;/p&gt;
&lt;p&gt;where x is the time between events and e is the base of the natural logarithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The exponential distribution has several characteristics:
&lt;ul&gt;
&lt;li&gt;It is a memoryless distribution, which means that the probability of an event occurring at time t+x, given that it has not occurred by time t, is the same as the probability of the event occurring at time x.&lt;/li&gt;
&lt;li&gt;It has a constant hazard rate, which means that the probability of an event occurring at any given time is constant.&lt;/li&gt;
&lt;li&gt;It is a one-sided distribution, which means that it is defined only for x &amp;gt; 0.&lt;/li&gt;
&lt;li&gt;The exponential distribution is often used in modeling the time between failures of equipment, the time between arrivals at a service facility, and the time between phone calls at a call center. It is also used in survival analysis to model the time until an event occurs, such as death or failure.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;weibull-distribution&#34;&gt;Weibull Distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The Weibull distribution is a continuous probability distribution that is often used to model time-to-failure data in reliability engineering. It is defined by two parameters: shape and scale.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The probability density function (PDF) of the Weibull distribution is given by:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(x) = (shape/scale) * (x/scale)^(shape-1) * e^(-(x/scale)^shape)&lt;/p&gt;
&lt;p&gt;where x is the time to failure, shape is the shape parameter, and scale is the scale parameter.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Weibull distribution has several characteristics:
&lt;ul&gt;
&lt;li&gt;It is a one-sided distribution, which means that it is defined only for x &amp;gt; 0.&lt;/li&gt;
&lt;li&gt;It has a shape parameter that controls the shape of the curve. If the shape parameter is less than 1, the curve is &amp;ldquo;skewed&amp;rdquo; to the right, meaning that it has a longer tail on the right side. - If the shape parameter is greater than 1, the curve is &amp;ldquo;skewed&amp;rdquo; to the left, meaning that it has a longer tail on the left side. If the shape parameter is equal to 1, the curve is symmetrical.&lt;/li&gt;
&lt;li&gt;It has a scale parameter that controls the spread of the curve. If the scale parameter is large, the curve is spread out and has a longer tail. If the scale parameter is small, the curve is more concentrated and has a shorter tail.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The Weibull distribution is often used in reliability engineering to model the time until failure of a component or system. It is also used in other fields, such as meteorology, to model wind speed and in economics to model stock returns.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;trends-and-seasonality&#34;&gt;Trends and seasonality&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Trends and seasonality are two common patterns that can occur in time series data.&lt;/li&gt;
&lt;li&gt;A trend is a long-term increase or decrease in the data. It can be either linear, meaning that the data increases or decreases at a constant rate, or nonlinear, meaning that the rate of change varies over time. Trends can be caused by various factors such as changes in consumer demand, economic conditions, or technological innovations.&lt;/li&gt;
&lt;li&gt;Seasonality is a pattern that repeats over a specific time period, such as annually or monthly. It can be caused by factors such as weather patterns, holidays, or consumer behavior.&lt;/li&gt;
&lt;li&gt;Both trends and seasonality can have important implications for forecasting and decision making. For example, if a company sees a trend of increasing sales, it may decide to ramp up production or hire more staff. If a company sees seasonal fluctuations in demand, it may need to adjust its inventory or staffing levels accordingly.&lt;/li&gt;
&lt;li&gt;To analyze trends and seasonality in time series data, various techniques can be used such as smoothing methods, decomposition methods, and autoregressive models. It is important to correctly identify and account for these patterns in order to make accurate forecasts and informed decisions.&lt;/li&gt;
&lt;li&gt;Additive time series data is characterized by a constant trend and constant seasonality over time. This means that the trend and seasonality do not change, and the data can be modeled using a linear function. The relationship between the data and the trend and seasonality components can be represented as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Data = Trend + Seasonality + Noise&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiplicative time series data, on the other hand, is characterized by a varying trend and varying seasonality. This means that the trend and seasonality change over time, and the data cannot be modeled using a linear function. The relationship between the data and the trend and seasonality components can be represented as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Data = Trend * Seasonality * Noise&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It is important to correctly identify whether a time series is additive or multiplicative, as this can influence the choice of modeling techniques and the interpretation of the results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Also this is possible:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Additive trend and additive seasonality&lt;/li&gt;
&lt;li&gt;Additive trend and multiplicative seasonality&lt;/li&gt;
&lt;li&gt;Multiplicative trend and additive seasonality&lt;/li&gt;
&lt;li&gt;Multiplicative trend and multiplicative seasonality&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;athena&#34;&gt;Athena&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Athena is a serverless, interactive query service that allows users to analyze data stored in Amazon S3 using SQL.&lt;/li&gt;
&lt;li&gt;It is designed to be fast and easy to use, and can be used to analyze data from a wide variety of sources such as logs, streaming data, and data lakes.&lt;/li&gt;
&lt;li&gt;To use Athena, users first define a data schema by creating tables that point to the data stored in Amazon S3.&lt;/li&gt;
&lt;li&gt;They can then use SQL to query the data and analyze it using various functions and aggregations. Athena supports a wide range of SQL functions and data types, and users can also use custom user-defined functions (UDFs) to extend its capabilities.&lt;/li&gt;
&lt;li&gt;It is also highly scalable, and can handle queries on large datasets with minimal performance degradation.&lt;/li&gt;
&lt;li&gt;Presto under the hood&lt;/li&gt;
&lt;li&gt;Supports multiple formats&lt;/li&gt;
&lt;li&gt;Unstructured, semi structured or structured&lt;/li&gt;
&lt;li&gt;Ad hoc queries, querying data before loading to Redshift, analyze Cloudtrail, integration with Jupyter, Zepplin, Integration with quicksight, integration with ODBC, JDBC&lt;/li&gt;
&lt;li&gt;AWS Glue datalog can extract the schema for Athena to use&lt;/li&gt;
&lt;li&gt;Pay as you go, inexpensive, converting to columner saves a lot of money, Glue and S3 have their own charges&lt;/li&gt;
&lt;li&gt;IAM policies, encryption is possible, TLS is possible&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;quicksight&#34;&gt;Quicksight&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon QuickSight is a business intelligence and data visualization service provided by Amazon Web Services (AWS).&lt;/li&gt;
&lt;li&gt;It allows users to create interactive dashboards and charts to visualize and analyze data from a wide variety of sources.&lt;/li&gt;
&lt;li&gt;To use QuickSight, users first need to connect it to their data sources, which can include data stored in Amazon S3, Amazon Redshift, Amazon RDS, and other AWS data stores, as well as external data sources such as spreadsheets and databases.&lt;/li&gt;
&lt;li&gt;Once the data is connected, users can use QuickSight&amp;rsquo;s visual interface to create charts, graphs, and other visualizations to explore and analyze the data.&lt;/li&gt;
&lt;li&gt;QuickSight offers a range of features and tools to help users analyze and understand their data.&lt;/li&gt;
&lt;li&gt;These include built-in analytics functions, support for custom SQL queries, and the ability to share and collaborate on dashboards with other users.&lt;/li&gt;
&lt;li&gt;Overall, Amazon QuickSight is a powerful and easy-to-use tool for creating interactive dashboards and visualizations, and is widely used in a variety of applications such as business intelligence, data exploration, and data reporting.&lt;/li&gt;
&lt;li&gt;Ad-hoc analysis&lt;/li&gt;
&lt;li&gt;Can do calculated columns etc.&lt;/li&gt;
&lt;li&gt;SPICE: Super Fast Parallel, In memory Calculation engine 10 gb&lt;/li&gt;
&lt;li&gt;Quicksight is quick because of SPICE&lt;/li&gt;
&lt;li&gt;Quicksights machine learning insights: Anomaly detection using Random cut forest, Forecasting and auto narratives (not too mature).&lt;/li&gt;
&lt;li&gt;Multifactor authentication&lt;/li&gt;
&lt;li&gt;Works with vpc, and provides row-level security&lt;/li&gt;
&lt;li&gt;Users defined via IAM or email signup&lt;/li&gt;
&lt;li&gt;AugoGraph feature selects the best graph for the respective data type&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;types-of-visualization&#34;&gt;Types of visualization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bar charts: These are used to compare categories or groups of data. They can be vertical or horizontal, and can be used to show both quantitative and categorical data.&lt;/li&gt;
&lt;li&gt;Line charts: These are used to show trends over time or other continuous variables. They can be used to show multiple data series on the same chart.&lt;/li&gt;
&lt;li&gt;Scatter plots: These are used to show the relationship between two numeric variables. They can be used to show correlations, patterns, and trends in the data.&lt;/li&gt;
&lt;li&gt;Pie charts: These are used to show proportions or percentages. They are most commonly used to show how a whole is divided into parts.&lt;/li&gt;
&lt;li&gt;Histograms: These are used to show the distribution of a continuous variable. They show the frequency or density of data points within different ranges or bins.&lt;/li&gt;
&lt;li&gt;Box plots: These are used to show the distribution and spread of a continuous variable. They show the minimum, first quartile, median, third quartile, and maximum values of the data.&lt;/li&gt;
&lt;li&gt;Heatmaps: These are used to show patterns and trends in data organized in a grid. They use color to represent the data, with warmer colors indicating higher values and cooler colors indicating lower values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;emr&#34;&gt;EMR&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Elastic MapReduce (EMR) is a fully managed, cloud-native big data processing service that makes it easy to process large amounts of data using Apache Hadoop and Apache Spark.&lt;/li&gt;
&lt;li&gt;It is designed to be scalable, highly available, and easy to use, and it integrates with a variety of data sources and destinations.&lt;/li&gt;
&lt;li&gt;With AWS EMR, users can create clusters of Amazon Elastic Compute Cloud (EC2) instances and use them to process and analyze large datasets stored in Amazon S3 or other data stores.&lt;/li&gt;
&lt;li&gt;EMR supports a variety of big data processing frameworks, including Hadoop, Spark, and Flink, and it provides tools to help users manage and monitor their clusters.&lt;/li&gt;
&lt;li&gt;AWS EMR is a useful tool for organizations that need to process and analyze large amounts of data, such as log files, scientific simulations, and machine learning models.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;li&gt;Provides notebooks&lt;/li&gt;
&lt;li&gt;Master nodes (manages the cluster), Core node (holds HDFS data and run tasks), Task node (only runs tasks)&lt;/li&gt;
&lt;li&gt;HDFS is epimerical&lt;/li&gt;
&lt;li&gt;Transient cluster vs Long running cluster&lt;/li&gt;
&lt;li&gt;IAM configure permissions&lt;/li&gt;
&lt;li&gt;CloudTrail: audit requests&lt;/li&gt;
&lt;li&gt;Data Pipeline: schedule and start clusters&lt;/li&gt;
&lt;li&gt;EMRFS: access s3 as if it were HDFS, uses DynamoDB to track consistency&lt;/li&gt;
&lt;li&gt;EBS for HDFS is also possible&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;hadoop&#34;&gt;Hadoop&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Apache Hadoop is an open-source software framework for storing and processing large amounts of data in a distributed computing environment.&lt;/li&gt;
&lt;li&gt;It is designed to handle data that is too large or complex for traditional database systems, and can process and analyze data in parallel across a large number of servers.&lt;/li&gt;
&lt;li&gt;Hadoop consists of two main components: the Hadoop Distributed File System (HDFS) and the MapReduce programming model.&lt;/li&gt;
&lt;li&gt;HDFS is a distributed file system that stores data across a large number of servers, and MapReduce is a programming model that allows developers to write programs that can process and analyze large amounts of data in parallel.&lt;/li&gt;
&lt;li&gt;Hadoop is commonly used for tasks such as data analysis, machine learning, and log processing. It is also often used in conjunction with other tools and technologies such as Apache Spark, Apache Hive, and Apache Pig to build more complex data processing pipelines.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;apache-spark&#34;&gt;Apache Spark&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Apache Spark is an open-source, distributed computing system that is designed for fast and flexible data processing.&lt;/li&gt;
&lt;li&gt;It is a popular choice for tasks such as data analytics, machine learning, and real-time stream processing.&lt;/li&gt;
&lt;li&gt;Spark is built on top of the Hadoop distributed file system (HDFS) and is designed to be highly scalable and efficient.&lt;/li&gt;
&lt;li&gt;It can process and analyze data in parallel across a large number of servers, and supports a wide range of programming languages including Python, Java, R, and Scala.&lt;/li&gt;
&lt;li&gt;One of the main benefits of Spark is its ability to process data in memory, which allows it to be much faster than other distributed computing systems that rely on disk-based storage.&lt;/li&gt;
&lt;li&gt;It also includes a number of useful libraries and tools for tasks such as machine learning, graph processing, and stream processing.&lt;/li&gt;
&lt;li&gt;in memory caching, DAGs&lt;/li&gt;
&lt;li&gt;Batch processing and real time analytics, graph processing, machine learning&lt;/li&gt;
&lt;li&gt;Spark context, cluster manager via spark or yarn, executors&lt;/li&gt;
&lt;li&gt;Spark core&lt;/li&gt;
&lt;li&gt;Spark RDD, DataFrames and Datasets are built on top of RDD, and they are most commonly used at the moment&lt;/li&gt;
&lt;li&gt;Spark Streaming is possible (works in mini batches). Unbounded database table&lt;/li&gt;
&lt;li&gt;MlLib (distributed machine learning)&lt;/li&gt;
&lt;li&gt;Graphx (distributed graph processing)&lt;/li&gt;
&lt;li&gt;Zepplin can run spark code interactively, and can also use charts/plots&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;spark-mllib&#34;&gt;Spark MLlib&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MLlib is a machine learning library for Apache Spark. It is designed to provide scalable and efficient machine learning algorithms that can be used on big data.&lt;/li&gt;
&lt;li&gt;MLlib includes a wide range of machine learning algorithms and utility functions, including algorithms for classification, regression, clustering, collaborative filtering, and dimensionality reduction.&lt;/li&gt;
&lt;li&gt;It also includes tools for feature engineering, such as feature extraction, transformation, and selection.&lt;/li&gt;
&lt;li&gt;MLlib is designed to be easy to use, and includes APIs for several programming languages including Python, Java, R, and Scala.&lt;/li&gt;
&lt;li&gt;It is also designed to be highly scalable, and can be used to build machine learning models on large datasets distributed across multiple servers.&lt;/li&gt;
&lt;li&gt;Classification: logistic regression and naive bayes&lt;/li&gt;
&lt;li&gt;Regression&lt;/li&gt;
&lt;li&gt;Decision trees&lt;/li&gt;
&lt;li&gt;Recommendation engine (ALS)&lt;/li&gt;
&lt;li&gt;Clustering (K-means)&lt;/li&gt;
&lt;li&gt;LDA (topic modeling)&lt;/li&gt;
&lt;li&gt;ML workflow utilities (pipelines, feature transformation, persistence)&lt;/li&gt;
&lt;li&gt;SVD, PCA and statistics&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;emr-notebooks-security-and-instance-types&#34;&gt;EMR Notebooks, Security and Instance Types&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon EMR Notebooks is a service that allows users to create and manage Jupyter notebooks on Amazon Elastic MapReduce (EMR) clusters. EMR is a cloud-based big data processing service, and Jupyter notebooks are interactive, web-based documents that can contain code, text, and visualizations.&lt;/li&gt;
&lt;li&gt;EMR Notebooks provides a simple and flexible way to analyze and visualize data stored in Amazon S3 or other data stores using a variety of tools and libraries such as Apache Spark, Python, and R. Users can create and edit notebooks using a web-based editor, and can also use notebooks to run and debug code, create visualizations, and collaborate with other users.&lt;/li&gt;
&lt;li&gt;EMR Notebooks is fully integrated with EMR, which means that users can easily spin up and down EMR clusters to process and analyze large datasets, and can also access other EMR features such as security and data access controls.&lt;/li&gt;
&lt;li&gt;Similar concept to Zeppelin, with more AWS integration&lt;/li&gt;
&lt;li&gt;Notebooks backed up to s3&lt;/li&gt;
&lt;li&gt;Provision clusters from the notebooks&lt;/li&gt;
&lt;li&gt;Hosted inside a vpc&lt;/li&gt;
&lt;li&gt;Accessed only via aws console&lt;/li&gt;
&lt;li&gt;IAM policies, Kerberos (a computer-network authentication protocol that works on the basis of tickets to allow nodes communicating over a non-secure network to prove their identity to one another in a secure manner), SSH, IAM roles&lt;/li&gt;
&lt;li&gt;Spot instances are good choice for task nodes, only use on core or master if you are testing or very cost sensitive, however, you are risking partial data loss&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;feature-engineering&#34;&gt;Feature Engineering&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Feature engineering is the process of creating new features or transforming existing features in a dataset in order to improve the performance of a machine learning model. It is a crucial step in the machine learning process, and can have a significant impact on the model&amp;rsquo;s accuracy and effectiveness.&lt;/li&gt;
&lt;li&gt;There are many different techniques that can be used in feature engineering, including:
&lt;ul&gt;
&lt;li&gt;Feature selection: This involves selecting a subset of the most relevant features from a dataset to use in a model. This can help to reduce overfitting, improve model interpretability, and reduce training time.&lt;/li&gt;
&lt;li&gt;Feature extraction: This involves creating new features from existing data by combining or transforming the original features. For example, a new feature could be created by taking the square root of an existing feature.&lt;/li&gt;
&lt;li&gt;Feature transformation: This involves transforming the scale or distribution of a feature in order to improve model performance. For example, data may need to be normalized or standardized in order to be used in some models.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;imputing-missing-data&#34;&gt;Imputing Missing Data&lt;/h2&gt;
&lt;p&gt;There are several ways to impute (or fill in) missing data, and the best method will depend on the specific dataset and the nature of the missing data. Some common methods for imputing missing data include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mean imputation: This involves replacing missing values with the mean (or average) of the non-missing values. This is a simple method that can be useful for numerical data, but can be biased if the data has a skewed distribution.&lt;/li&gt;
&lt;li&gt;Median imputation: This is similar to mean imputation, but uses the median (or middle value) instead of the mean. It can be less affected by outliers than mean imputation and may be a better choice for skewed data.&lt;/li&gt;
&lt;li&gt;Mode imputation: This involves replacing missing values with the most frequent (or mode) value in the dataset. It is often used for categorical data.&lt;/li&gt;
&lt;li&gt;Regression imputation: This involves using a regression model to predict the missing values based on the other available features. It can be a more powerful method, but requires a good understanding of the relationships between the features and the target variable.&lt;/li&gt;
&lt;li&gt;Nearest Neighbors (k-NN) imputation is a method for imputing missing data that uses the k-NN algorithm to fill in missing values based on the values of the nearest neighbors. It is a simple and intuitive method that is often used in machine learning and data analysis. To use k-NN imputation, the first step is to identify the nearest neighbors of the data point with missing values. This is typically done using a distance measure such as Euclidean distance, and the number of neighbors (k) is a user-defined parameter. Once the nearest neighbors have been identified, the missing values can be imputed by averaging the values of the neighbors. k-NN imputation can be a useful method for filling in missing data, especially when the data is highly correlated and the relationships between the features are well understood. However, it can be sensitive to the choice of k, and may not always produce the best results.&lt;/li&gt;
&lt;li&gt;Dropping NA&amp;rsquo;s: This method can be useful in some cases, such as when the number of missing values is small and removing them does not significantly affect the overall size of the dataset. However, it can also lead to a loss of information and may not be appropriate if the missing values are prevalent or if they are likely to be informative.&lt;/li&gt;
&lt;li&gt;MICE (multiple imputation by chained equations): Multiple imputation by chained equations (MICE) is a method for imputing missing data that involves creating multiple imputed datasets and combining them to produce a final result. It is a more advanced method that can be more robust and accurate than other imputation methods, especially when the data has a complex structure and the relationships between the features are not well understood. The final result is produced by combining the imputed datasets using appropriate statistical methods, such as averaging or weighted averaging. MICE can be a powerful method for imputing missing data, but it can also be time-consuming and requires a good understanding of statistical modeling.&lt;/li&gt;
&lt;li&gt;Categorical is usually not trivial&lt;/li&gt;
&lt;li&gt;Just get more data (if possible)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;unbalanced-data&#34;&gt;Unbalanced Data&lt;/h2&gt;
&lt;p&gt;There are several approaches for handling imbalanced data in machine learning, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Balancing the data: This can be done by oversampling the minority class or undersampling the majority class.&lt;/li&gt;
&lt;li&gt;Using a different evaluation metric: Instead of using accuracy, try using precision, recall, or F1 score, which are more sensitive to imbalanced data.&lt;/li&gt;
&lt;li&gt;Adjusting the class weight: Some algorithms allow you to adjust the weight of each class, which can be used to give more importance to the minority class.&lt;/li&gt;
&lt;li&gt;Using a different algorithm: Some algorithms are more robust to imbalanced data than others. For example, tree-based algorithms like random forests and gradient boosting tend to perform better on imbalanced data than algorithms like logistic regression.&lt;/li&gt;
&lt;li&gt;Using data augmentation: If you are working with image data, you can use data augmentation techniques to generate additional minority class examples.&lt;/li&gt;
&lt;li&gt;Anomaly detection: If the minority class represents anomalies or rare events, you can treat the problem as an anomaly detection problem rather than a classification problem.&lt;/li&gt;
&lt;li&gt;Synthetic minority oversampling technique (SMOTE): This is a popular method for oversampling the minority class by synthesizing new examples rather than simply replicating existing ones.&lt;/li&gt;
&lt;li&gt;Cost-sensitive learning: In this approach, the cost of misclassifying examples from the minority class is higher than the cost of misclassifying examples from the majority class.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;handling-outliers&#34;&gt;Handling Outliers&lt;/h2&gt;
&lt;p&gt;There are several ways to identify outliers in a dataset:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Visualization: One of the easiest ways to identify outliers is to plot the data using a box plot or histogram. Outliers will typically be located outside the &amp;ldquo;whiskers&amp;rdquo; of the box plot or outside the range of the histogram.&lt;/li&gt;
&lt;li&gt;Statistical tests: You can use statistical tests to identify outliers in a dataset. For example, you can use the Z-score method to identify outliers by calculating the distance of each data point from the mean in terms of standard deviations. Data points with a Z-score greater than a certain threshold (such as 3 or 4) can be considered outliers.&lt;/li&gt;
&lt;li&gt;Data transformation: Transforming the data, such as taking the log of the data, can make outliers more obvious.&lt;/li&gt;
&lt;li&gt;Anomaly detection algorithms: There are also machine learning algorithms specifically designed for detecting anomalies or outliers in a dataset. These algorithms include density-based methods, distance-based methods, and model-based methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are several approaches for handling outliers in a dataset:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ignore the outlier: This is an option if the outlier does not have a significant impact on the model or if the outlier is the result of a data entry error.&lt;/li&gt;
&lt;li&gt;Drop the outlier: This is an option if the outlier is not representative of the population being studied and if the outlier has a significant impact on the model.&lt;/li&gt;
&lt;li&gt;Transform the data: Some algorithms are more robust to outliers, such as decision trees and random forests. Transforming the data, such as using the log transformation, can also make the model more robust to outliers.&lt;/li&gt;
&lt;li&gt;Use robust models: Some models, such as linear regression with the Huber loss function, are less sensitive to outliers than other models.&lt;/li&gt;
&lt;li&gt;Anomaly detection: If the outlier represents an anomaly or rare event, you can treat the problem as an anomaly detection problem rather than a classification or regression problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;binning-transforming-encoding-scaling-and-shuffling&#34;&gt;Binning, Transforming, Encoding, Scaling, and Shuffling&lt;/h2&gt;
&lt;h3 id=&#34;binning&#34;&gt;Binning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Binning is a process of transforming numerical data into categorical data by dividing the data into a set of bins or intervals. This can be useful for reducing the number of unique values in a dataset, which can make it easier to visualize and analyze the data.&lt;/li&gt;
&lt;li&gt;For example, if you have a dataset with a large range of numerical values, you could use binning to group the values into a smaller set of intervals. This would allow you to plot the data on a histogram or bar chart, which would be more informative than a scatter plot.&lt;/li&gt;
&lt;li&gt;There are several ways to determine the size and number of bins to use for binning data, including:
&lt;ul&gt;
&lt;li&gt;Fixed width bins: In this approach, you specify the size of the bins and the data is divided into intervals of that size.&lt;/li&gt;
&lt;li&gt;Fixed number of bins: In this approach, you specify the number of bins and the data is divided into that number of intervals.&lt;/li&gt;
&lt;li&gt;Optimal bin width: In this approach, the optimal bin width is determined using a statistical method, such as the Scott&amp;rsquo;s normal reference rule or the Freedman-Diaconis rule.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;transforming&#34;&gt;Transforming&lt;/h3&gt;
&lt;p&gt;Data transformation is a process of converting data from one format or representation to another. This can be useful for several reasons, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data cleaning: Data transformation can be used to fix or remove errors or inconsistencies in the data.&lt;/li&gt;
&lt;li&gt;Data preparation: Data transformation can be used to prepare the data for analysis by formatting the data in a specific way or creating new variables.&lt;/li&gt;
&lt;li&gt;Data reduction: Data transformation can be used to reduce the size or complexity of the data, such as by aggregating the data or removing unnecessary variables.&lt;/li&gt;
&lt;li&gt;Data normalization: Data transformation can be used to scale the data to a common range, such as by normalizing the data to have a mean of 0 and a standard deviation of 1.&lt;/li&gt;
&lt;li&gt;Data transformation can also be used to make the data more amenable to a specific algorithm or technique, such as by binning numerical data or encoding categorical data.&lt;/li&gt;
&lt;li&gt;There are many different types of data transformation techniques, including scaling, centering, normalization, aggregation, imputation, and encoding. The appropriate transformation technique will depend on the specific characteristics of the data and the goals of the analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;encoding&#34;&gt;Encoding&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Encoding is the process of converting data from one format into another, often for the purpose of efficient storage or transmission. In the context of machine learning, encoding is often used to convert categorical data, which can&amp;rsquo;t be represented as numerical values, into numerical form.&lt;/li&gt;
&lt;li&gt;There are several types of encoding techniques that can be used, including:
&lt;ul&gt;
&lt;li&gt;One-hot encoding: This technique converts each categorical value into a new binary column, with a value of 1 indicating the presence of the categorical value and a value of 0 indicating its absence.&lt;/li&gt;
&lt;li&gt;Label encoding: This technique converts each categorical value into a numerical value, such as an integer. However, this can lead to problems if the numerical values are interpreted as having a meaningful order.&lt;/li&gt;
&lt;li&gt;Count encoding: This technique encodes the categorical values by the count of each value in the dataset.&lt;/li&gt;
&lt;li&gt;Binary encoding: This technique encodes the categorical values as binary code.&lt;/li&gt;
&lt;li&gt;Target encoding: This technique encodes the categorical values using the mean of the target variable for each value.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;scaling-and-normalizing&#34;&gt;Scaling and normalizing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Scaling and normalization are techniques used to transform variables so that they have a comparable scale. This can be useful for a variety of reasons, such as:&lt;/li&gt;
&lt;li&gt;Some machine learning algorithms are sensitive to the scale of the input variables, and can perform poorly if the variables are on a different scale. Scaling the variables to the same scale can improve the performance of these algorithms.&lt;/li&gt;
&lt;li&gt;Scaling the variables can also make it easier to compare the magnitude of the variables.&lt;/li&gt;
&lt;li&gt;There are several ways to scale and normalize data:
&lt;ul&gt;
&lt;li&gt;Min-max scaling: This scales the variables to a specific range, such as 0-1 or -1 to 1.&lt;/li&gt;
&lt;li&gt;Standardization: This scales the variables so that they have a mean of 0 and a standard deviation of 1.&lt;/li&gt;
&lt;li&gt;Normalization: This scales the variables so that they have a unit norm (a length of 1).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;shuffling&#34;&gt;Shuffling&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Shuffling is the process of randomly rearranging the rows of a dataset. This can be useful for several reasons:&lt;/li&gt;
&lt;li&gt;Machine learning algorithms often expect the data to be in a random order. Shuffling the data before training a model can help ensure that the model is not biased by the order of the data.&lt;/li&gt;
&lt;li&gt;Shuffling the data can also help ensure that the training and test sets are representative of the overall dataset. If the data is not shuffled and the rows are ordered in a certain way, the training and test sets may not be representative of the overall dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tokenization&#34;&gt;Tokenization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements, known as tokens. Tokenization is an important preprocessing step for many natural language processing (NLP) tasks, such as text classification and information retrieval.&lt;/li&gt;
&lt;li&gt;There are several approaches to tokenization, including:
&lt;ul&gt;
&lt;li&gt;Word tokenization: This involves dividing the text into words.&lt;/li&gt;
&lt;li&gt;Sentence tokenization: This involves dividing the text into sentences.&lt;/li&gt;
&lt;li&gt;Word-level tokenization: This involves dividing the text into words and punctuation, such as &amp;ldquo;don&amp;rsquo;t&amp;rdquo; being split into &amp;ldquo;do&amp;rdquo; and &amp;ldquo;n&amp;rsquo;t.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;N-gram tokenization: This involves dividing the text into contiguous sequences of n items, such as bigrams (pairs of words) or trigrams (triplets of words).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;stemming-and-lemmatization&#34;&gt;Stemming and lemmatization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stemming and lemmatization are techniques used to normalize words to their base form, known as a stem or lemma. These techniques are often used as a preprocessing step for natural language processing (NLP) tasks, such as text classification and information retrieval.&lt;/li&gt;
&lt;li&gt;Stemming involves removing the suffixes from a word to obtain the root form of the word. For example, the stem of the word &amp;ldquo;jumping&amp;rdquo; might be &amp;ldquo;jump,&amp;rdquo; and the stem of the word &amp;ldquo;stemmer,&amp;rdquo; might be &amp;ldquo;stem.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Lemmatization, on the other hand, involves determining the base form of a word based on its part of speech and meaning. For example, the lemma of the word &amp;ldquo;was&amp;rdquo; might be &amp;ldquo;be,&amp;rdquo; and the lemma of the word &amp;ldquo;better&amp;rdquo; might be &amp;ldquo;good.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Stemming and lemmatization can be useful for reducing the dimensionality of the data and improving the performance of NLP models, but they can also remove some of the context and meaning of the words.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-sagemaker-ground-truth-and-label-generation&#34;&gt;Amazon Sagemaker Ground Truth and Label Generation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon SageMaker Ground Truth is a fully managed data labeling service that allows users to build highly accurate training datasets for machine learning.&lt;/li&gt;
&lt;li&gt;It offers a variety of labeling methods, including human labeling, active learning, and automatic labeling.&lt;/li&gt;
&lt;li&gt;SageMaker Ground Truth also includes workflows for common data labeling tasks, such as image classification, object detection, and semantic segmentation.&lt;/li&gt;
&lt;li&gt;It also provides tools for managing and tracking the data labeling process, including the ability to set up labeling jobs, track their progress, and review the results.&lt;/li&gt;
&lt;li&gt;This helps users ensure that their data is labeled accurately and efficiently.&lt;/li&gt;
&lt;li&gt;Ambiguous data is sent to humans&lt;/li&gt;
&lt;li&gt;Mechanical Turk: Amazon Mechanical Turk (MTurk) is a cloud platform that enables organizations to use a network of human workers to perform tasks that are typically difficult or time-consuming for computers to perform. These tasks, known as Human Intelligence Tasks (HITs), can include data labeling, transcription, image annotation, and many other types of work.&lt;/li&gt;
&lt;li&gt;Amazon SageMaker Ground Truth Plus helps you to create high-quality training datasets without having to build labeling applications or manage a labeling workforce.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;misc&#34;&gt;Misc&lt;/h2&gt;
&lt;h3 id=&#34;synthetic-features&#34;&gt;Synthetic Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Synthetic features are artificially created features that are derived from existing features in a dataset. They are often used to improve the performance of machine learning models by providing additional information that may not be present in the original features. Synthetic features can be created using various techniques, such as combining or transforming existing features, or by applying statistical or mathematical operations to the data.&lt;/li&gt;
&lt;li&gt;One example of a synthetic feature is a polynomial feature, which is created by taking the product of a feature with itself or with other features. Polynomial features can capture nonlinear relationships between features and the target variable, and can improve the performance of linear models on nonlinear problems. Other examples of synthetic features include interactions between features, binned features, and dummy variables.&lt;/li&gt;
&lt;li&gt;Synthetic features can be useful for improving the performance of machine learning models, especially when the original features are not sufficient for making accurate predictions. However, care should be taken when creating synthetic features, as adding too many of them can lead to overfitting and degrade model performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;stop-words&#34;&gt;Stop words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stop words are common words that are typically filtered out before natural language processing (NLP) tasks, such as text classification or text mining, because they do not provide meaningful information. Examples of stop words include words like &amp;ldquo;a,&amp;rdquo; &amp;ldquo;an,&amp;rdquo; &amp;ldquo;the,&amp;rdquo; &amp;ldquo;and,&amp;rdquo; and &amp;ldquo;but.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;There are several ways to identify and handle stop words:
&lt;ul&gt;
&lt;li&gt;Use a list of stop words: Many NLP libraries and frameworks, such as NLTK and scikit-learn, include a pre-defined list of stop words that can be used to filter out common words.&lt;/li&gt;
&lt;li&gt;Identify stop words using term frequency-inverse document frequency (TF-IDF): This method involves calculating the importance of each word in a document or corpus and filtering out the least important words, which are often stop words.&lt;/li&gt;
&lt;li&gt;Customize the stop word list: You can customize the list of stop words based on the specific needs of your task. For example, if you are working with domain-specific language, you may need to add domain-specific words to the stop word list.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;To handle stop words, you can simply filter them out of the dataset before performing the NLP task. This can be done by comparing the words in the dataset to the stop word list and removing any words that are on the list.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s important to carefully consider whether or not to filter out stop words, as they can sometimes provide important context or meaning. For example, in the phrase &amp;ldquo;not good,&amp;rdquo; the word &amp;ldquo;not&amp;rdquo; is a stop word that changes the meaning of the phrase.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tf-idf&#34;&gt;TF-IDF&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Term Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure that is used to evaluate the importance of a word in a document or corpus. The importance of a word is determined by its frequency in the document and in the corpus as a whole.&lt;/li&gt;
&lt;li&gt;TF-IDF is calculated as the product of the term frequency (TF) and the inverse document frequency (IDF). The term frequency is the number of times a word appears in the document, and the inverse document frequency is the logarithm of the number of documents in the corpus divided by the number of documents that contain the word.&lt;/li&gt;
&lt;li&gt;TF-IDF is often used as a weighting factor in information retrieval and text mining, and can be useful for tasks such as document classification, clustering, and keyword extraction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;correlation&#34;&gt;Correlation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Correlation is a statistical measure that indicates the strength and direction of a linear relationship between two variables. A positive correlation indicates that as one variable increases, the other variable also increases. A negative correlation indicates that as one variable increases, the other variable decreases.&lt;/li&gt;
&lt;li&gt;The correlation coefficient, denoted by &amp;ldquo;r,&amp;rdquo; is a measure of the strength of the relationship between the variables. It ranges from -1 to 1, where -1 indicates a strong negative correlation, 0 indicates no correlation, and 1 indicates a strong positive correlation.&lt;/li&gt;
&lt;li&gt;Correlation can be useful for understanding the relationship between two variables and predicting one variable based on the other. However, it&amp;rsquo;s important to remember that correlation does not necessarily imply causation, meaning that a correlation between two variables does not necessarily mean that one variable causes the other.&lt;/li&gt;
&lt;li&gt;There are several methods for calculating the correlation between variables, including Pearson&amp;rsquo;s correlation coefficient, Spearman&amp;rsquo;s rank correlation coefficient, and Kendall&amp;rsquo;s tau. The appropriate method will depend on the characteristics of the data and the goals of the analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;p-value&#34;&gt;p-value&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The p-value is a statistical measure that is used to assess the significance of a hypothesis test. It is the probability of obtaining a test statistic at least as extreme as the one observed, assuming that the null hypothesis is true.&lt;/li&gt;
&lt;li&gt;The null hypothesis is a statement that there is no statistical relationship between the variables being tested. The alternative hypothesis is the opposite of the null hypothesis and states that there is a statistical relationship between the variables.&lt;/li&gt;
&lt;li&gt;To interpret the p-value, you compare it to a significance level, which is a predetermined cutoff value. If the p-value is less than the significance level, you can reject the null hypothesis and conclude that there is a statistical relationship between the variables. If the p-value is greater than the significance level, you fail to reject the null hypothesis and cannot conclude that there is a statistical relationship between the variables.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s important to carefully consider the appropriate significance level and the limitations of the p-value, as it can be affected by factors such as sample size and the distribution of the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;elbow-plot&#34;&gt;Elbow plot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An elbow plot is a graphical method used to determine the appropriate number of clusters to use in a cluster analysis. It plots the within-cluster sum of squared distances (WCSS) for each possible number of clusters, and the number of clusters is chosen at the &amp;ldquo;elbow&amp;rdquo; point, where the change in WCSS begins to level off.&lt;/li&gt;
&lt;li&gt;To create an elbow plot, you first perform a cluster analysis for a range of possible number of clusters, and then plot the WCSS for each number of clusters. The WCSS can be calculated as the sum of the squared distance between each point and its cluster centroid.&lt;/li&gt;
&lt;li&gt;The &amp;ldquo;elbow&amp;rdquo; point is generally considered to be the point where the WCSS starts to decrease more slowly, indicating that adding more clusters is not significantly improving the fit of the model.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s important to carefully consider the limitations of the elbow method, as it can be affected by the shape of the data and may not always clearly identify the appropriate number of clusters. Other methods, such as the silhouette method, can also be used to determine the number of clusters in a dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;summary-statistics&#34;&gt;Summary Statistics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Summary statistics are quantitative measures that describe and summarize a dataset. They provide a quick and easy way to get a sense of the characteristics and patterns in the data.&lt;/li&gt;
&lt;li&gt;Some common summary statistics include:&lt;/li&gt;
&lt;li&gt;Mean: The mean is the arithmetic average of the data. It is calculated by summing all the values and dividing by the number of values.&lt;/li&gt;
&lt;li&gt;Median: The median is the middle value of the data when it is sorted in ascending order. It is a measure of central tendency that is resistant to outliers.&lt;/li&gt;
&lt;li&gt;Mode: The mode is the most frequent value in the data.&lt;/li&gt;
&lt;li&gt;Range: The range is the difference between the maximum and minimum values in the data.&lt;/li&gt;
&lt;li&gt;Variance: The variance is a measure of the spread or dispersion of the data. It is calculated as the sum of the squared differences between each value and the mean, divided by the number of values.&lt;/li&gt;
&lt;li&gt;Standard deviation: The standard deviation is the square root of the variance. It is a measure of the spread of the data that is in the same units as the original data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;distance-norms&#34;&gt;Distance Norms&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Distance norms, also known as metrics, are functions that define a distance between two points in a space. These functions take two points as inputs and return a non-negative number that represents the distance between them. Different distance norms can be used depending on the characteristics of the data and the requirements of the application.&lt;/li&gt;
&lt;li&gt;Some common distance norms include:
&lt;ul&gt;
&lt;li&gt;Euclidean distance: This is the most common distance norm and is based on the Pythagorean theorem. It is defined as the square root of the sum of the squares of the differences between the coordinates of the two points.&lt;/li&gt;
&lt;li&gt;Manhattan distance: This distance norm is based on the sum of the absolute differences of the coordinates of the two points. It is also known as the &amp;ldquo;taxi cab&amp;rdquo; distance because it represents the distance a taxi cab would need to travel to get from one point to the other.&lt;/li&gt;
&lt;li&gt;Minkowski distance: This is a generalization of the Euclidean and Manhattan distances. It is defined as the sum of the absolute differences of the coordinates of the two points, raised to a power. The value of the power determines whether the distance is more similar to the Euclidean or Manhattan distance.&lt;/li&gt;
&lt;li&gt;Cosine distance: This distance norm is based on the cosine similarity between two vectors. It is often used in text analysis to measure the similarity between documents.&lt;/li&gt;
&lt;li&gt;Jaccard distance: This distance norm is based on the Jaccard similarity coefficient and is often used to compare the similarity of sets. It is defined as the size of the intersection of the sets divided by the size of the union of the sets.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;qq-plots&#34;&gt;QQ plots&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A Q-Q plot (short for &amp;ldquo;quantile-quantile plot&amp;rdquo;) is a graphical way to compare two probability distributions by plotting their quantiles against each other. It is a plot of the sorted data against an idealized distribution with a uniform distribution. The purpose of a Q-Q plot is to check whether two datasets come from the same distribution.&lt;/li&gt;
&lt;li&gt;To create a Q-Q plot, you first need to specify the distribution that you want to use as the reference distribution. Then, you sort both datasets and plot the quantiles of one dataset against the quantiles of the other dataset. If the two datasets come from the same distribution, the points in the Q-Q plot will lie approximately on a straight line. If the points do not lie on a straight line, it suggests that the two datasets come from different distributions.&lt;/li&gt;
&lt;li&gt;Q-Q plots are often used to check whether a dataset follows a particular distribution, such as a normal distribution. They are also useful for comparing datasets to see whether they come from the same distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;parametric-and-non-parametric-test&#34;&gt;Parametric and Non-Parametric test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A parametric test is a statistical test that assumes that the data comes from a population with a known probability distribution, such as a normal distribution. The test uses parameters of the distribution (such as the mean and standard deviation) to make statistical inferences about the population.&lt;/li&gt;
&lt;li&gt;Parametric tests are based on the assumption that the data follows a particular probability distribution, and they are generally more powerful (i.e., able to detect differences with smaller sample sizes) than nonparametric tests, which do not make any assumptions about the distribution of the data. However, if the assumption of a known distribution is not met, the results of a parametric test may be less reliable.&lt;/li&gt;
&lt;li&gt;Examples of parametric tests include the t-test, the ANOVA test, and the linear regression analysis. These tests are commonly used to compare means, variances, and relationships between variables.&lt;/li&gt;
&lt;li&gt;Nonparametric tests, on the other hand, do not assume that the data comes from a particular distribution and are more robust to departures from normality. Examples of nonparametric tests include the Wilcoxon rank-sum test and the Kruskal-Wallis test.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;markov-chain-model&#34;&gt;Markov Chain model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A Markov chain is a mathematical system that undergoes transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the system arrived at its current state, the possible future states are fixed.&lt;/li&gt;
&lt;li&gt;A Markov chain is often represented by a state transition diagram, which shows all the possible states that the system can be in, and the transitions between these states. The transitions are governed by transition probabilities, which specify the probability of moving from one state to another.&lt;/li&gt;
&lt;li&gt;Markov chains have many applications in various fields, including economics, computer science, and physics. They are used to model systems that change over time and have a finite number of states. Some examples of systems that can be modeled using Markov chains include:
&lt;ul&gt;
&lt;li&gt;The behavior of a customer moving through a website, where the states represent the different pages on the website and the transitions represent the clicks that the customer makes to move from one page to another.&lt;/li&gt;
&lt;li&gt;The weather, where the states represent different weather conditions (such as sunny, cloudy, or rainy) and the transitions represent the probability of the weather changing from one condition to another.&lt;/li&gt;
&lt;li&gt;The movement of a particle through a lattice, where the states represent the different positions that the particle can occupy and the transitions represent the probability of the particle moving from one position to another.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;optimization&#34;&gt;Optimization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Optimization is the process of finding the best solution to a problem, given certain constraints. In mathematics, optimization problems involve finding the maximum or minimum value of a function, subject to certain constraints. These constraints can be equality constraints, which specify that a certain relationship must hold among the variables, or inequality constraints, which specify that a certain relationship must not hold among the variables.&lt;/li&gt;
&lt;li&gt;There are many different methods for solving optimization problems, including gradient descent, the simplex method, and the interior point method. The choice of method depends on the specific problem and the desired properties of the solution.&lt;/li&gt;
&lt;li&gt;Optimization is used in many fields, including engineering, economics, and machine learning. Some examples of optimization problems include:
&lt;ul&gt;
&lt;li&gt;Finding the optimal allocation of resources, such as the allocation of capital to different investments, or the allocation of production capacity to different products.&lt;/li&gt;
&lt;li&gt;Finding the optimal design of a system, such as the design of an aircraft or the design of a supply chain.&lt;/li&gt;
&lt;li&gt;Finding the optimal parameters of a machine learning model, such as the weights of a neural network or the regularization parameters of a linear model.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;objective-function&#34;&gt;Objective Function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An objective function is a mathematical function that is used to represent the goal of an optimization problem. The goal of the optimization problem is to find the values of the variables that either maximize or minimize the objective function, subject to certain constraints.&lt;/li&gt;
&lt;li&gt;The objective function is also known as the cost function, the loss function, or the criterion function. It is a measure of how well a given solution meets the requirements of the problem. In general, the objective function is a scalar-valued function, meaning that it maps a vector of variables to a single scalar value.&lt;/li&gt;
&lt;li&gt;The objective function is an important component of an optimization problem, as it defines the goal of the optimization and determines the solution of the problem. The objective function is often defined in terms of the decision variables (the variables that are being optimized) and the parameters of the problem (the constants that define the problem).&lt;/li&gt;
&lt;li&gt;For example, in a linear programming problem, the objective function is a linear function that represents the cost or profit associated with a particular allocation of resources. In a nonlinear programming problem, the objective function is a nonlinear function that represents the cost or performance of a system.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;exponential-smoothing-for-outlier-detection&#34;&gt;Exponential smoothing for outlier detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Exponential smoothing is a time series forecasting method that is used to predict future values based on historical data. It is based on the idea of giving more weight to more recent observations and less weight to observations that are further in the past.&lt;/li&gt;
&lt;li&gt;Exponential smoothing can be used for outlier detection by analyzing the residuals (i.e., the differences between the predicted values and the actual values) of the time series. If the residuals contain outliers (i.e., values that are significantly different from the majority of the residuals), it may indicate that there is something unusual or unexpected happening in the time series.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;box-cox-transformation&#34;&gt;Box-Cox Transformation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Box-Cox transformation is a way to transform non-normal dependent variables into a normal shape. It is named after George Box and David Cox, who introduced the transformation in 1964. The transformation is defined as:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Y = (X^L - 1) / L&lt;/p&gt;
&lt;p&gt;where X is the variable to be transformed, Y is the transformed variable, and L is a parameter that needs to be estimated. When L = 0, the Box-Cox transformation becomes the log transformation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Box-Cox transformation can be useful in regression analysis, ANOVA, and other statistical tests when the assumptions of normality are not met. It can also be useful in improving the interpretability of the model by making the relationship between the dependent and independent variables more linear.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pca-for-visualization-and-eda&#34;&gt;PCA for visualization and EDA&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Principal component analysis (PCA) is a statistical technique that is used to reduce the dimensionality of a data set.&lt;/li&gt;
&lt;li&gt;It does this by identifying the directions in which the data vary the most, and then projecting the data onto a lower-dimensional space.&lt;/li&gt;
&lt;li&gt;This can be useful for visualization, as it can allow you to plot high-dimensional data in a 2D or 3D space.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>