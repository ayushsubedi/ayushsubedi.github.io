<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ebs on Ayush Subedi</title>
    <link>https://ayushsubedi.github.io/tags/ebs/</link>
    <description>Recent content in ebs on Ayush Subedi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 15 Jan 2023 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ayushsubedi.github.io/tags/ebs/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>AWS Certified ML - Specialty exam (MLS-C01) - 1. Data Engineering</title>
      <link>https://ayushsubedi.github.io/posts/aws_ml_speciality_data_engineering/</link>
      <pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/aws_ml_speciality_data_engineering/</guid>
      <description>&lt;h1 id=&#34;data-engineering&#34;&gt;Data Engineering&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#s3&#34;&gt;S3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kinesis&#34;&gt;Kinesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#glue&#34;&gt;Glue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#redshift&#34;&gt;Redshift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rds&#34;&gt;RDS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dynamodb&#34;&gt;DynamoDB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#opensearch&#34;&gt;OpenSearch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aws-data-pipeline&#34;&gt;AWS Data Pipeline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aws-batch&#34;&gt;AWS Batch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aws-batch&#34;&gt;AWS DMS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-functions&#34;&gt;Step Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#efs&#34;&gt;EFS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ebs&#34;&gt;EBS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#emr&#34;&gt;EMR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#misc&#34;&gt;Misc&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This domain requires understanding of creating data repositories for machine learning, identification and implementation of data ingestion solution, and
identification and implementation of a data transformation solution.&lt;/p&gt;
&lt;p&gt;Data engineering is the process of building and maintaining the infrastructure and systems that are used to store, process, and analyze data. In the context of Amazon Web Services (AWS), data engineering involves the use of various AWS services and tools to build and operate data pipelines, data lakes, and other data processing systems.&lt;/p&gt;
&lt;p&gt;Some common AWS services and tools that are used in data engineering on AWS include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amazon S3: A fully managed object storage service that is used to store and retrieve data.&lt;/li&gt;
&lt;li&gt;Amazon EMR: A fully managed big data processing service that is used to process and analyze large datasets using Apache Hadoop and Apache Spark.&lt;/li&gt;
&lt;li&gt;AWS Glue: A fully managed extract, transform, and load (ETL) service that is used to move and transform data between data stores.&lt;/li&gt;
&lt;li&gt;Amazon Redshift: A fully managed data warehouse service that is used to store and analyze large amounts of data using SQL and business intelligence tools.&lt;/li&gt;
&lt;li&gt;Amazon RDS: A fully managed database service that is used to set up, operate, and scale relational databases in the cloud.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By using these and other AWS services, data engineers can build and maintain robust, scalable, and cost-effective data processing systems on the AWS Cloud.&lt;/p&gt;
&lt;h2 id=&#34;s3&#34;&gt;S3&lt;/h2&gt;
&lt;p&gt;Amazon S3 (Simple Storage Service) is a cloud storage service that allows you to store and retrieve data at any time, from anywhere on the web. It is designed to make web-scale computing easier for developers by providing a simple, highly scalable, and cost-effective way to store and retrieve any amount of data. With S3, you can store and retrieve any amount of data, at any time, from anywhere on the web. S3 is designed to provide 99.999999999% durability and scale past trillions of objects worldwide. It is used to store and retrieve any amount of data, at any time, from anywhere on the web. It is an object storage service that offers industry-leading scalability, data availability, security, and performance.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;foundational for machine learning projects since it is a cost effective solution for datasets storage&lt;/li&gt;
&lt;li&gt;object based storage, bucket name need to be globally unique, however the storage itself is unique to regions&lt;/li&gt;
&lt;li&gt;key is the full path of the file and even though it looks like there is a folder based heirarchy, that is not how it works&lt;/li&gt;
&lt;li&gt;you can have a very long file name, in the sense that the path (key) can be very long&lt;/li&gt;
&lt;li&gt;individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 TB. The largest object that can be uploaded in a single PUT is 5 GB.&lt;/li&gt;
&lt;li&gt;object tags can be added, helpful with classification and security lifecycle (these are key value pairs)&lt;/li&gt;
&lt;li&gt;decoupling of compute and storage side&lt;/li&gt;
&lt;li&gt;perfect use case of data lake, since it can store various formats of data (object storage)&lt;/li&gt;
&lt;li&gt;it is possible to partition the storage, which is helpful (speedy) when querying via athena. Kinesis partitions the data automatically.&lt;/li&gt;
&lt;li&gt;11 9&amp;rsquo;s durability (for all storage classes)&lt;/li&gt;
&lt;li&gt;availability differs between availability classes&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;storage-classes&#34;&gt;Storage classes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;S3, Standard / General Purpose: for frequently accessed data&lt;/li&gt;
&lt;li&gt;S3, Infrequent Access: lower cost than standard, for data accessed monthly, and requires milliseconds retrival, but there is a cost associated with retrival&lt;/li&gt;
&lt;li&gt;S3, Infrequent Access, One Zone, good for secondary copies of backup, or data that can be recreated, infrequent access for cost saving&lt;/li&gt;
&lt;li&gt;S3, Glacier Instant Retrival, price per storage + price per retrival, can access within milliseconds, for low cost storage for long-lived data&lt;/li&gt;
&lt;li&gt;S3, Glacier Flexible Retrival, expedited: 1-5 mins, standard: 3-5 hrs, bulk: 5-12 hrs (free), for long term low cost storage for backups and archives  with different retrival options&lt;/li&gt;
&lt;li&gt;S3, Glacier Deep Archive: lowest cost, 180 days of minimum storage, for rarely accessed archive data&lt;/li&gt;
&lt;li&gt;S3, Intelligent Tiering: move objects between tiers with monthly monitoring and auto-tiering fee&lt;/li&gt;
&lt;li&gt;It is possible to move objects between these storage classes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://d1.awsstatic.com/reInvent/re21-pdp-tier1/s3/Amazon-S3-Storage-Classes.pdf&#34;&gt;more info&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;lifecycle-rules&#34;&gt;Lifecycle rules&lt;/h3&gt;
&lt;p&gt;Amazon S3 Lifecycle rules allow you to define policies for how Amazon S3 stores objects. You can use Lifecycle rules to specify when objects transition to different storage classes, or when they expire and are deleted. This can help you reduce your storage costs by moving objects to lower-cost storage classes or deleting them when they are no longer needed. You can set up Lifecycle rules at the bucket level or at the object level (for individual objects or for groups of objects). You can also specify different rules for different prefixes or object tags.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transitioning objects between classes is possible&lt;/li&gt;
&lt;li&gt;Transition Actions can be used to configure objects to transition to another storage class&lt;/li&gt;
&lt;li&gt;Transition Actions can also be used for expiration, incomplete multi part uploads etc.&lt;/li&gt;
&lt;li&gt;Rules can be applied to buckets, specific paths of the project or also to tags&lt;/li&gt;
&lt;li&gt;Amazon S3 analytics works exclusively on S3 standard, and S3 IA, and provides analytics on usage&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;performance-chart&#34;&gt;Performance Chart&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/s3_storage_classes.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;encryption&#34;&gt;Encryption&lt;/h3&gt;
&lt;p&gt;Amazon S3 supports several encryption options to help users secure their data at rest. These options include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SSE-S3: This option uses server-side encryption with Amazon S3-managed keys. With this option, Amazon S3 encrypts the data as it is written to disks in its data centers and decrypts it when it is accessed.&lt;/li&gt;
&lt;li&gt;SSE-KMS: This option uses server-side encryption with AWS KMS-managed keys. With this option, users can create, rotate, and manage the keys used to encrypt their data.&lt;/li&gt;
&lt;li&gt;SSE-C: This option allows users to use their own encryption keys to encrypt their data. Users are responsible for securely managing their keys and rotating them as needed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Users can enable encryption when creating a new bucket or when uploading an object to an existing bucket. They can also enable encryption for all objects in an existing bucket by enabling bucket-level encryption.&lt;/p&gt;
&lt;h3 id=&#34;security-policy&#34;&gt;Security Policy&lt;/h3&gt;
&lt;p&gt;Amazon S3 bucket policies allow users to add additional security controls to their S3 buckets and objects. A bucket policy is a JSON document that defines the permissions for an S3 bucket. It can be used to grant permissions to other AWS accounts, or to grant public access to a bucket and its objects.&lt;/p&gt;
&lt;p&gt;With a bucket policy, users can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Grant read and write permissions to a specific AWS account for all objects in a bucket.&lt;/li&gt;
&lt;li&gt;Grant read-only permissions to the anonymous user for all objects in a bucket.&lt;/li&gt;
&lt;li&gt;Grant read and write permissions to a specific AWS account for all objects with a specific prefix (such as &amp;ldquo;private/&amp;rdquo;).&lt;/li&gt;
&lt;li&gt;Deny all access to a specific AWS account for all objects in a bucket.&lt;/li&gt;
&lt;li&gt;It is important for users to carefully consider the permissions they grant in their bucket policy, as it can have wide-ranging effects on the security of the bucket and its contents.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;misc&#34;&gt;Misc&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Amazon S3 VPC Endpoints allow users to access Amazon S3 from within their virtual private cloud (VPC) without the need for an Internet gateway, NAT device, or VPN connection. With VPC Endpoints, users can access S3 from their VPC over an optimized network path, reducing Internet traffic and improving performance.&lt;/li&gt;
&lt;li&gt;Users can create a VPC Endpoint for Amazon S3 in their VPC, and then configure their VPC security groups and IAM policies to allow access to the endpoint. They can then use the endpoint to access Amazon S3 using the Amazon S3 APIs or the AWS Management Console, just as they would over the Internet.&lt;/li&gt;
&lt;li&gt;VPC Endpoints for Amazon S3 are supported in all regions and are available in two types: Gateway Endpoints and Interface Endpoints. Gateway Endpoints are powered by a highly available network gateway, while Interface Endpoints are powered by a highly available Network Load Balancer. Users can choose the endpoint type that best meets their needs.&lt;/li&gt;
&lt;li&gt;Amazon S3 CloudTrail is a service that enables users to record API calls made to Amazon S3 and log the events to an Amazon S3 bucket. This allows users to track changes to their objects, buckets, and Amazon S3 configurations, and to identify and troubleshoot issues.&lt;/li&gt;
&lt;li&gt;With CloudTrail, users can:
&lt;ul&gt;
&lt;li&gt;Track changes to their Amazon S3 objects and bucket metadata.&lt;/li&gt;
&lt;li&gt;Determine who made a change and when it was made.&lt;/li&gt;
&lt;li&gt;Audit changes to their Amazon S3 bucket and object permissions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CloudTrail logs are stored in an Amazon S3 bucket that the user specifies, and they can be delivered to an Amazon CloudWatch Logs log group or an Amazon SNS topic. Users can use the CloudTrail logs to monitor their S3 resources and to ensure compliance with their policies.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kinesis&#34;&gt;Kinesis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Kinesis is a fully managed (alternative to Kafka), cloud-based service that enables users to process and analyze streaming data in real-time. With Kinesis, users can build custom applications that process and analyze data as it arrives, and they can scale these applications to process any volume of data, at any time.&lt;/li&gt;
&lt;li&gt;Kinesis consists of three main components:
&lt;ul&gt;
&lt;li&gt;Producers: Producers are sources of data that send data records to Kinesis streams.&lt;/li&gt;
&lt;li&gt;Kinesis streams: A Kinesis stream is a sequence of data records that are persisted for a set period of time. Users can create and delete streams, and they can specify the number of shards in a stream.&lt;/li&gt;
&lt;li&gt;Consumers: Consumers are applications that read and process data records from Kinesis streams.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Kinesis is designed to be highly available and durable, and it can automatically scale to handle increases in traffic.&lt;/li&gt;
&lt;li&gt;Users can use Kinesis to build custom applications that can process and analyze real-time data streams, and they can use the service to support a wide range of use cases, such as real-time analytics, fraud detection, and Internet of Things (IoT) applications.&lt;/li&gt;
&lt;li&gt;Data is replicated to at least 3 AZ&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kinesis-streams&#34;&gt;Kinesis Streams&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Kinesis Streams is a fully managed, cloud-based service that allows real-time processing of streaming data at high scale.&lt;/li&gt;
&lt;li&gt;It can continuously capture and store terabytes of data per hour from hundreds of thousands of sources, such as website clickstreams, financial transactions, social media feeds, IT logs, and location-tracking events.&lt;/li&gt;
&lt;li&gt;With Kinesis Streams, users can build custom applications that process or analyze the data as it arrives, or they can use the provided Kinesis Data Streams API to load the data into other AWS services, such as Amazon S3, Amazon Redshift, or Amazon Elasticsearch Service, for long-term storage and analysis.&lt;/li&gt;
&lt;li&gt;Streams are divided into shards and partitions&lt;/li&gt;
&lt;li&gt;The maximum throughput of a single shard 1 mb/seconds or 1000 messages/seconds&lt;/li&gt;
&lt;li&gt;Data retention: 24 hours by default. It can go up to 365 days. This is useful for reprocessing/replaying data&lt;/li&gt;
&lt;li&gt;Immutable, 1 mb in size&lt;/li&gt;
&lt;li&gt;Provisioned mode: choose number of shards and scale manually or using an API&lt;/li&gt;
&lt;li&gt;Each shard gets 1mb/s in, 2mb/s out&lt;/li&gt;
&lt;li&gt;On demand mode: each capacity provisioned is 4mb/s&lt;/li&gt;
&lt;li&gt;If you can plan capacity, use provisioned. however, use on demand if capacity is unknown&lt;/li&gt;
&lt;li&gt;Custom code for producer or consumer is possible&lt;/li&gt;
&lt;li&gt;Real time (200 ms latency, possible all the way up to 70ms)&lt;/li&gt;
&lt;li&gt;Automatic scaling with on-demand mode&lt;/li&gt;
&lt;li&gt;Multi consumers is possible from one source&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kinesis-analytics&#34;&gt;Kinesis Analytics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Kinesis Analytics is a fully managed, cloud-based service that allows users to process and analyze streaming data in real-time with SQL.&lt;/li&gt;
&lt;li&gt;With Kinesis Analytics, users can run ad-hoc queries on the data, or they can set up a SQL-based stream processing application to perform transformations on the data as it arrives. SQL or Apache Flink can be used here.&lt;/li&gt;
&lt;li&gt;The output of these queries and transformations can be fed back into Kinesis Streams for further processing, or it can be stored in other AWS services, such as Amazon S3 or Amazon Redshift, for long-term analysis.&lt;/li&gt;
&lt;li&gt;Select columns, continious metric generation, responsive analytics, etc.&lt;/li&gt;
&lt;li&gt;Serverless, scales automatically, pay for resouces consumed but expensive&lt;/li&gt;
&lt;li&gt;Schema discovery&lt;/li&gt;
&lt;li&gt;Lambda can be used for preprocessing&lt;/li&gt;
&lt;li&gt;Two machine learning algorithms:
&lt;ul&gt;
&lt;li&gt;Random cut forest for anomaly detection on numeric columns in a stream, uses recent data to compute the model. A random cut forest (RCF) is a machine learning algorithm that is used for anomaly detection in streaming data. It works by constructing a number of decision trees on randomly selected subsets of the data, and then comparing the score for each new data point to the scores of similar points in the trees. If the score for a new data point is significantly lower than the scores of similar points in the trees, it is considered to be an anomaly. The number of trees in the forest and the size of the subsets of data used to train each tree can be adjusted to control the sensitivity of the model. RCFs are particularly well-suited for detecting anomalies in large, high-dimensional datasets, and they are often used in conjunction with streaming data platforms, such as Amazon Kinesis Streams.&lt;/li&gt;
&lt;li&gt;Hotspots: A hotspots algorithm is a type of machine learning algorithm that is used to identify spatial clusters of events or observations in a dataset. These clusters, which are also known as hotspots, are areas in which the concentration of events or observations is significantly higher than the surrounding areas. Hotspots algorithms are often used in a variety of applications, such as crime mapping, disease surveillance, and marketing analysis. There are several different approaches to identifying hotspots, including spatial clustering methods, spatial scan statistics, and kernel density estimation. These methods can be applied to a variety of types of data, including point data, such as crime incidents or disease cases, and areal data, such as census tracts or zip codes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kinesis-firehose&#34;&gt;Kinesis Firehose&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Kinesis Firehose is a fully managed service&lt;/li&gt;
&lt;li&gt;makes it easy to load streaming data into data stores and analytics tools&lt;/li&gt;
&lt;li&gt;It can capture, transform, and load data streams into Amazon S3, Amazon Redshift, and Amazon Elasticsearch Service, Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards&lt;/li&gt;
&lt;li&gt;Kinesis Firehose is a simple and reliable way to load streaming data into data stores and analytics tools.&lt;/li&gt;
&lt;li&gt;most common is firehose reading from kinesis streams&lt;/li&gt;
&lt;li&gt;near realtime service because it batch writes&lt;/li&gt;
&lt;li&gt;data desitination can be s3, redshift, elastisearch, splunk, new relic, or http endpoint&lt;/li&gt;
&lt;li&gt;60 seconds latency minimum for non full batches&lt;/li&gt;
&lt;li&gt;data ingestion into redshift, s3, elasticsearch, splunk&lt;/li&gt;
&lt;li&gt;automatic scaling&lt;/li&gt;
&lt;li&gt;conversions from csv/json to parquet and orc and requires the use of glue&lt;/li&gt;
&lt;li&gt;and transformation through lambda csv to json is possible&lt;/li&gt;
&lt;li&gt;compression is possible&lt;/li&gt;
&lt;li&gt;automates scaling&lt;/li&gt;
&lt;li&gt;no data storage&lt;/li&gt;
&lt;li&gt;no replay capability&lt;/li&gt;
&lt;li&gt;it is a serverless transformation tool&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kinesis-video-streams&#34;&gt;Kinesis Video Streams&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Web Services (AWS) Kinesis Video Streams is a fully managed service that allows users to stream live video from connected devices to the cloud.&lt;/li&gt;
&lt;li&gt;This service is designed to make it easy to build applications that process and analyze live video streams, as well as store and transmit videos securely at scale.&lt;/li&gt;
&lt;li&gt;With Kinesis Video Streams, users can stream live video from millions of devices and easily build applications for real-time video analytics and machine learning.&lt;/li&gt;
&lt;li&gt;In addition, the service allows users to stream video directly to other AWS services, such as Amazon S3, Amazon Kinesis Data Streams, and Amazon Rekognition, for further processing and analysis.&lt;/li&gt;
&lt;li&gt;Producers: security camera, body-worn cam, aws deeplens, radar data, camera&lt;/li&gt;
&lt;li&gt;One producer per video stream&lt;/li&gt;
&lt;li&gt;Video playback capability&lt;/li&gt;
&lt;li&gt;Sagemaker, rekognition video, 1 hour to 10 years of storage&lt;/li&gt;
&lt;li&gt;Checkpointing via dynamodb, frames to Sagemaker for ML inference, publish to stream, lambda can be used for notification.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;glue&#34;&gt;Glue&lt;/h2&gt;
&lt;h3 id=&#34;glue-data-catalog-and-glue-data-crawlers&#34;&gt;Glue Data Catalog and Glue Data Crawlers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;AWS Glue Data Catalog is a fully managed, cloud-native metadata store that provides a central place to store, annotate, and share metadata across AWS services, applications, and tools.&lt;/li&gt;
&lt;li&gt;It makes it easy to discover and understand data, and facilitates the development of data-driven applications.&lt;/li&gt;
&lt;li&gt;With the Glue Data Catalog, users can create, maintain, and access metadata such as database and table definitions, column names and data types, and data lineage.&lt;/li&gt;
&lt;li&gt;The Glue Data Catalog is integrated with other AWS services such as Amazon Redshift, Amazon Athena, and Amazon EMR, and is accessible through the AWS Management Console, the AWS Glue API, and the AWS Glue ETL (extract, transform, and load) library.&lt;/li&gt;
&lt;li&gt;Schemas are versioned&lt;/li&gt;
&lt;li&gt;Glue crawlers help build the Catalog&lt;/li&gt;
&lt;li&gt;Glue will also extract the partitions, this is helpful for query optimization&lt;/li&gt;
&lt;li&gt;Glue Data Crawlers are a tool within the Amazon Glue service that allows users to extract metadata from their data stores and create table definitions in the Glue Data Catalog.&lt;/li&gt;
&lt;li&gt;This enables the creation of ETL jobs and development endpoints in Glue, which can be used to move and transform data.&lt;/li&gt;
&lt;li&gt;Glue Data Crawlers can connect to various data stores, including Amazon S3 and RDS, as well as any JDBC-compliant data store.&lt;/li&gt;
&lt;li&gt;Custom connectors for other data stores can also be created using the Glue ETL library. To use Glue Data Crawlers, a Glue ETL job or development endpoint must first be created, after which the Glue ETL library can be utilized for data movement and transformation tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;glue-etl&#34;&gt;Glue ETL&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Transform data, clean data, enrich data before doing analysis&lt;/li&gt;
&lt;li&gt;Generate ETL code in python or scala, you can modify the code&lt;/li&gt;
&lt;li&gt;Possible to provide your own spark or pyspark scripts&lt;/li&gt;
&lt;li&gt;Target can be S3, JDBC or in glue data catalog&lt;/li&gt;
&lt;li&gt;Fully managed, cost effective, pay only for the resources consumed&lt;/li&gt;
&lt;li&gt;Jobs are run on a serverless Spark platform&lt;/li&gt;
&lt;li&gt;Glue scheduler to schedule the jobs&lt;/li&gt;
&lt;li&gt;Glue triggers to automate job runs based on events&lt;/li&gt;
&lt;li&gt;Transformations can be bundled (drop, filter, join, map)&lt;/li&gt;
&lt;li&gt;Machine learning transformation (find matches, duplicates even when data do not match exactly, dedup)&lt;/li&gt;
&lt;li&gt;Any apache spark transformation is possible, and changing in format is possible.&lt;/li&gt;
&lt;li&gt;Multiple ways to create glue jobs including visual editors, python notebooks, python script, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;redshift&#34;&gt;Redshift&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Redshift is a fully managed data warehouse service offered by Amazon Web Services (AWS).&lt;/li&gt;
&lt;li&gt;It is designed to handle petabyte-scale data warehouses and make it easy to analyze data using SQL and business intelligence tools.&lt;/li&gt;
&lt;li&gt;Amazon Redshift is based on PostgreSQL, and it supports many of the same data types and functions as PostgreSQL.&lt;/li&gt;
&lt;li&gt;To use Amazon Redshift, users first need to set up a cluster of compute nodes. They can then load data into the cluster and perform SQL queries on the data. Amazon Redshift integrates with various data sources and destinations, including Amazon S3, Amazon EMR, and Amazon RDS.&lt;/li&gt;
&lt;li&gt;It also integrates with a variety of business intelligence tools, such as Quicksight, Tableau, Qlik, and MicroStrategy.&lt;/li&gt;
&lt;li&gt;Amazon Redshift offers a number of features to help users manage their data warehouses, including automatic data compression, data replication, and data security. It also provides a number of performance enhancements, such as columnar storage, data caching, and parallel query execution.&lt;/li&gt;
&lt;li&gt;Overall, Amazon Redshift is a powerful and scalable data warehouse solution for analyzing large datasets in the cloud.&lt;/li&gt;
&lt;li&gt;OLAP&lt;/li&gt;
&lt;li&gt;Uses SQL to analyze structured and semi-structured data across data warehouses, operational databases, and data lakes&lt;/li&gt;
&lt;li&gt;Redshift Spectrum can directly query from S3&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;rds&#34;&gt;RDS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Relational Database Service (RDS) is a fully managed database service offered by Amazon Web Services (AWS).&lt;/li&gt;
&lt;li&gt;It makes it easy to set up, operate, and scale a relational database in the cloud.&lt;/li&gt;
&lt;li&gt;Amazon RDS supports a variety of database engines, including MySQL, MariaDB, PostgreSQL, Oracle, and Microsoft SQL Server.&lt;/li&gt;
&lt;li&gt;With Amazon RDS, users can create and manage a database without the need to install and maintain database software.&lt;/li&gt;
&lt;li&gt;Amazon RDS handles tasks such as hardware provisioning, database setup, patching, and backups.&lt;/li&gt;
&lt;li&gt;It also provides features such as automated failover and read replicas to help users improve availability and scalability.&lt;/li&gt;
&lt;li&gt;Amazon RDS is a popular choice for applications that require a relational database, such as e-commerce, content management, and customer relationship management systems.&lt;/li&gt;
&lt;li&gt;It is particularly well-suited for use cases that require high availability and low latency, such as online transaction processing (OLTP).&lt;/li&gt;
&lt;li&gt;Must provision servers in advance&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;dynamodb&#34;&gt;DynamoDB&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon DynamoDB is a fully managed NoSQL database service offered by Amazon Web Services (AWS).&lt;/li&gt;
&lt;li&gt;It is designed to be scalable, fast, and flexible, making it a good choice for applications that need high performance and low latency.&lt;/li&gt;
&lt;li&gt;DynamoDB stores data in tables, and each table has a primary key that uniquely identifies each item. The primary key can be either a simple primary key (a single attribute) or a composite primary key (a combination of two or more attributes).&lt;/li&gt;
&lt;li&gt;DynamoDB supports both key-value and document data models, and it offers a number of powerful features, such as global secondary indexes, auto scaling, and stream-based data replication.&lt;/li&gt;
&lt;li&gt;DynamoDB is a popular choice for applications that need to store large amounts of data that is frequently read or written, such as online gaming, real-time analytics, and IoT applications.&lt;/li&gt;
&lt;li&gt;It is also well-suited for applications that need to scale rapidly, as it can automatically adjust capacity to meet changing demand.&lt;/li&gt;
&lt;li&gt;Useful to store ML model (or checkpoints)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;opensearch&#34;&gt;OpenSearch&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Previously ElasticSearch&lt;/li&gt;
&lt;li&gt;Amazon OpenSearch is a search service that makes it easy to build and run search applications.&lt;/li&gt;
&lt;li&gt;It is based on the open source Apache Lucene search engine, and it provides a number of features to help users build sophisticated search experiences, such as full-text search, faceted search, and hit highlighting.&lt;/li&gt;
&lt;li&gt;With Amazon OpenSearch, users can index and search large datasets, such as websites, documents, and logs.&lt;/li&gt;
&lt;li&gt;They can also customize the search experience by adding search criteria, filters, and facets, and by displaying search results in various formats.&lt;/li&gt;
&lt;li&gt;Amazon OpenSearch also provides analytics and monitoring capabilities to help users understand how their search applications are being used.&lt;/li&gt;
&lt;li&gt;Amazon OpenSearch is a flexible and scalable search solution that is well-suited for a wide range of applications, such as e-commerce, content management, and data analysis.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;aws-data-pipeline&#34;&gt;AWS Data Pipeline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Data Pipeline is a fully managed data processing service that helps users move and transform data between data stores.&lt;/li&gt;
&lt;li&gt;It is designed to be easy to use and highly reliable, and it can handle data processing tasks of any size.&lt;/li&gt;
&lt;li&gt;With Amazon Data Pipeline, users can create pipelines that move data between data stores, such as Amazon S3, Amazon RDS, and Amazon Redshift.&lt;/li&gt;
&lt;li&gt;They can also use Data Pipeline to transform data, such as by aggregating, filtering, or joining data from different sources. Data Pipeline supports a variety of data formats and sources, and it can be used to schedule and automate data processing tasks.&lt;/li&gt;
&lt;li&gt;Amazon Data Pipeline is a useful tool for a wide range of data processing tasks, such as data warehousing, ETL, and analytics. It is particularly well-suited for use cases that involve moving and transforming large amounts of data, as it can scale to handle data processing needs of any size.&lt;/li&gt;
&lt;li&gt;Data sources can be on premise&lt;/li&gt;
&lt;li&gt;Runs on EC2 but fully managed&lt;/li&gt;
&lt;li&gt;Orchestration service&lt;/li&gt;
&lt;li&gt;Glue is managed, serverless, spark focused, ETL focused, has catalog&lt;/li&gt;
&lt;li&gt;Data Pipeline is orchestation tool, and can do more&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;aws-batch&#34;&gt;AWS Batch&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;For any non-ETL batch is usually better than glue&lt;/li&gt;
&lt;li&gt;Amazon Web Services Batch is a fully managed batch processing service that makes it easy to run batch computing workloads on the AWS Cloud.&lt;/li&gt;
&lt;li&gt;It is designed to be scalable, fault-tolerant, and flexible, and it supports a wide range of workloads, such as machine learning, data processing, and scientific simulations.&lt;/li&gt;
&lt;li&gt;With AWS Batch, users can define batch computing workloads as &amp;ldquo;jobs&amp;rdquo; and &amp;ldquo;tasks,&amp;rdquo; and the service automatically provisions the required compute resources and executes the tasks.&lt;/li&gt;
&lt;li&gt;Users can specify the desired level of concurrency and resource allocation for their jobs, and AWS Batch will automatically scale up or down as needed.&lt;/li&gt;
&lt;li&gt;AWS Batch also integrates with other AWS services, such as Amazon S3 and Amazon ECS, to provide a complete batch processing solution.&lt;/li&gt;
&lt;li&gt;AWS Batch is a useful tool for organizations that need to run large-scale batch computing workloads, such as financial analysis, scientific simulations, and media processing.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;li&gt;Batch can be scheduled using cloudwatch, step functions&lt;/li&gt;
&lt;li&gt;Not just for ETL but absolutely anything at all&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;aws-dms&#34;&gt;AWS DMS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Web Services Database Migration Service (AWS DMS) is a fully managed service that makes it easy to migrate databases to the AWS Cloud.&lt;/li&gt;
&lt;li&gt;It is designed to be reliable, efficient, and flexible, and it supports a wide range of database platforms, including Oracle, MySQL, and Microsoft SQL Server.&lt;/li&gt;
&lt;li&gt;With AWS DMS, users can migrate their databases to the AWS Cloud with minimal downtime.&lt;/li&gt;
&lt;li&gt;The service handles tasks such as data extraction, transformation, and load, and it supports both one-time and ongoing migrations.&lt;/li&gt;
&lt;li&gt;AWS DMS also provides a number of features to help users manage their database migrations, such as change data capture, data transformation, and task scheduling.&lt;/li&gt;
&lt;li&gt;AWS DMS is a useful tool for organizations that want to migrate their databases to the cloud, or that need to replicate their databases across multiple regions for disaster recovery or other purposes.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;li&gt;Supports homogeneous migrations and heterogeneous migrations&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-functions&#34;&gt;Step Functions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Web Services Step Functions is a fully managed service that makes it easy to coordinate the various components of complex, distributed applications.&lt;/li&gt;
&lt;li&gt;It is based on the concepts of tasks and state machines, and it provides a visual workflow editor to help users design and manage their applications.&lt;/li&gt;
&lt;li&gt;With AWS Step Functions, users can define and execute workflows that coordinate multiple AWS services, such as AWS Lambda, Amazon ECS, and AWS Batch.&lt;/li&gt;
&lt;li&gt;The service automatically scales to meet the needs of the workflow, and it provides features such as error handling and retry logic to help users build resilient applications.&lt;/li&gt;
&lt;li&gt;AWS Step Functions is a useful tool for organizations that need to coordinate the various components of complex, distributed applications, such as data pipelines, machine learning workflows, and microservices architectures.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;li&gt;Audit of history of workflow&lt;/li&gt;
&lt;li&gt;Allows waiting&lt;/li&gt;
&lt;li&gt;Maximum execution time of 1 year&lt;/li&gt;
&lt;li&gt;Can be used to train/tune a ML model&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;efs&#34;&gt;EFS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Elastic File System (EFS) is a fully managed, cloud-native file storage service that makes it easy to store and access files from multiple Amazon Elastic Compute Cloud (EC2) instances.&lt;/li&gt;
&lt;li&gt;It is designed to be scalable, highly available, and easy to use, and it supports the Network File System (NFS) protocol.&lt;/li&gt;
&lt;li&gt;With AWS EFS, users can create file systems and store files in them, and they can access the files from multiple EC2 instances at the same time.&lt;/li&gt;
&lt;li&gt;EFS automatically scales up or down as needed to meet the storage and performance needs of the applications, and it provides features such as file system access control and data durability to help users manage their file storage.&lt;/li&gt;
&lt;li&gt;AWS EFS is a useful tool for organizations that need to store and access files from multiple EC2 instances, such as web servers, application servers, and development environments.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ebs&#34;&gt;EBS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Elastic Block Store (EBS) is a fully managed, cloud-native block storage service that makes it easy to store and access data from Amazon Elastic Compute Cloud (EC2) instances.&lt;/li&gt;
&lt;li&gt;It is designed to be scalable, highly available, and easy to use, and it supports a variety of storage types and performance levels.&lt;/li&gt;
&lt;li&gt;With AWS EBS, users can create storage volumes and attach them to EC2 instances, and they can use the volumes to store and access data.&lt;/li&gt;
&lt;li&gt;EBS provides a number of features to help users manage their storage, such as snapshotting, data replication, and encryption.&lt;/li&gt;
&lt;li&gt;It also supports a variety of storage types, including SSD-backed volumes for high performance and HDD-backed volumes for lower cost.&lt;/li&gt;
&lt;li&gt;AWS EBS is a useful tool for organizations that need to store and access data from EC2 instances, such as databases, file systems, and applications.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;li&gt;EBS volumes are attached to specific EC2 instances, and they scale with the needs of the applications running on those instances.&lt;/li&gt;
&lt;li&gt;EFS file systems, on the other hand, can be accessed concurrently by multiple EC2 instances, and they scale automatically to meet the needs of the workload.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;emr&#34;&gt;EMR&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Elastic MapReduce (EMR) is a fully managed, cloud-native big data processing service that makes it easy to process large amounts of data using Apache Hadoop and Apache Spark.&lt;/li&gt;
&lt;li&gt;It is designed to be scalable, highly available, and easy to use, and it integrates with a variety of data sources and destinations.&lt;/li&gt;
&lt;li&gt;With AWS EMR, users can create clusters of Amazon Elastic Compute Cloud (EC2) instances and use them to process and analyze large datasets stored in Amazon S3 or other data stores.&lt;/li&gt;
&lt;li&gt;EMR supports a variety of big data processing frameworks, including Hadoop, Spark, and Flink, and it provides tools to help users manage and monitor their clusters.&lt;/li&gt;
&lt;li&gt;AWS EMR is a useful tool for organizations that need to process and analyze large amounts of data, such as log files, scientific simulations, and machine learning models.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;misc-1&#34;&gt;Misc&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;AWS DataSync: for data migrations from on-premises to AWS storage services&lt;/li&gt;
&lt;li&gt;MQTT: IOT protocol, Standard messaging protocol&lt;/li&gt;
&lt;li&gt;Apache Spark, Apache Hive, Apache Hadoop, and Apache Pig are all open-source technologies that are used for data processing and analysis. However, they are designed for different purposes and have different strengths and weaknesses.
&lt;ul&gt;
&lt;li&gt;Apache Spark is a fast, in-memory data processing engine that is used for real-time data processing and analytics. It is particularly well-suited for use cases that require fast processing times, such as streaming data and interactive data exploration.&lt;/li&gt;
&lt;li&gt;Apache Hive is a data warehousing and SQL-like query language that is used to process and analyze large datasets stored in the Hadoop Distributed File System (HDFS). It is particularly well-suited for use cases that involve complex data transformations and aggregations.&lt;/li&gt;
&lt;li&gt;Apache Hadoop is a distributed computing platform that is used to store and process large amounts of data. It is composed of several modules, including HDFS for storing data, YARN for resource management, and MapReduce for parallel data processing. Hadoop is a popular choice for batch processing and offline data analysis.&lt;/li&gt;
&lt;li&gt;Apache Pig is a high-level data processing language that is used to write and execute MapReduce jobs on Apache Hadoop. It is particularly well-suited for use cases that involve complex data transformations and complex data structures.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>