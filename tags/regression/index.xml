<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>regression on Ayush Subedi</title>
    <link>https://ayushsubedi.github.io/tags/regression/</link>
    <description>Recent content in regression on Ayush Subedi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 22 Aug 2023 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ayushsubedi.github.io/tags/regression/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Topics on High-Dimensional Data Analytics (Machine Learning 2)</title>
      <link>https://ayushsubedi.github.io/posts/topics_on_high_dimensional_data_analytics/</link>
      <pubDate>Tue, 22 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/topics_on_high_dimensional_data_analytics/</guid>
      <description>&lt;h1 id=&#34;topics-on-high-dimensional-data-analytics&#34;&gt;Topics on High-Dimensional Data Analytics&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#functional-data-analysis&#34;&gt;Functional Data Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#image-analysis&#34;&gt;Image Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tensor-data-analysis&#34;&gt;Tensor Data Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimization-application&#34;&gt;Optimization Application&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regularization&#34;&gt;Regularization&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;functional-data-analysis&#34;&gt;Functional Data Analysis&lt;/h1&gt;
&lt;p&gt;A fluctuating quantity or impulse whose variations represent information and is often represented as a function of time or space.&lt;/p&gt;
&lt;p&gt;From Wikipedia&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Functional data analysis (FDA)&lt;/strong&gt; is a branch of statistics that analyses data providing information about curves, surfaces or anything else varying over a continuum. In its most general form, under an FDA framework, each sample element of functional data is considered to be a random function. The physical continuum over which these functions are defined is often time, but may also be spatial location, wavelength, probability, etc. Intrinsically, functional data are infinite dimensional. The high intrinsic dimensionality of these data brings challenges for theory as well as computation, where these challenges vary with how the functional data were sampled. However, the high or infinite dimensional structure of the data is a rich source of information and there are many interesting challenges for research and data analysis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://lands.let.ru.nl/FDA/images/FDA_pic4website.bmp&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;regression---least-square-estimates&#34;&gt;Regression - Least square Estimates&lt;/h2&gt;
&lt;p&gt;A linear regression model assumes that the regression function $E(Y|X)$ is linear in the inputs $X_1, &amp;hellip;, X_p$. They were developed in the pre-computer age of statistics, but even in today&amp;rsquo;s computer era there are still good reasons to study and use them. They are simple and often provide an adequate and interpretable description of how the inputs affect the output.&lt;/p&gt;
&lt;p&gt;The linear regression model has the form:&lt;/p&gt;
&lt;p&gt;$f(X) = \beta_0 + \sum_{j=1}^p X_j\beta_j$&lt;/p&gt;
&lt;p&gt;Typically we have a set of training data $(x_1, y_1)&amp;hellip;(x_N, y_N)$ from which to estimate the parameters $\beta$. Each $x_i = (x_{i1}, x_{i2} &amp;hellip; x_{ip})^T$ is a vector of feature measurements for the $i$th case. The most popular estimation method is the least squares, in which we pick the coefficients $\beta = (\beta_0, \beta_1,&amp;hellip;.\beta_p)^T$ to minimize the residual sum of squares.&lt;/p&gt;
&lt;p&gt;$\sum_{i=1}^N (y_i - f(x))^2 = \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p X_j\beta_j)^2$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/lr.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Denote $X$ by the $N \times (p+1)$ matrix with each row an input vector (with a 1 in the first position, to represent the intercept), and similarity let $y$ be the $N$ vector of outputs in the training set. Then we can write the residual sum-of-squares as :&lt;/p&gt;
&lt;p&gt;$RSS(\beta) = (y-X\beta)^T(y-X\beta)$&lt;/p&gt;
&lt;p&gt;Differentiating with respect to $\beta$, &amp;hellip;.&lt;/p&gt;
&lt;p&gt;$\hat{\beta} = (X^TX)^{-1}X^Ty$&lt;/p&gt;
&lt;h2 id=&#34;geometric-interpretation&#34;&gt;Geometric Interpretation&lt;/h2&gt;
&lt;p&gt;$\hat{y} = X\hat{\beta} = X(X^TX)^{-1}X^Ty = Hy $&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Projection Matrix&lt;/strong&gt; (or Hat matrix): The outcome vector $y$ is orthogonally projected onto the hyperplane spanned by the input vectors $x_1$ and $x_2$. The Projection $\hat{y}$ represents the vector of predictions obtained by the least square method.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/ols_projection.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;properties-of-ols&#34;&gt;Properties of OLS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;They are unbiased estimators. That is the expected value of estimators and actual parameters are the same $E(\hat{\beta}) = \beta$&lt;/li&gt;
&lt;li&gt;The covariance can be obtained by $cov(\hat{\beta}) = \sigma^2 (X^TX)^{-1}$, where $\sigma^2 = SSE/(n-p)$&lt;/li&gt;
&lt;li&gt;According to the &lt;strong&gt;Gauss-Markov Theorem&lt;/strong&gt;, among all unbiased linear estimates, the least square estimate (LSE) has the minimum variance and it is unique.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Regression can be used for Feature Extraction&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;splines&#34;&gt;Splines&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Polynomial Regression&lt;/strong&gt; is a type of regression analysis where the relationship between the independent variable (input) and the dependent variable (output) is modeled as an nth-degree polynomial. In other words, instead of fitting a straight line (linear regression), a polynomial regression can fit curves of various degrees, allowing for more flexibility in capturing complex relationships. For example, a quadratic polynomial regression (degree 2) can model a parabolic relationship, and a cubic polynomial regression (degree 3) can model more intricate curves.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Polynomial regression is still considered a type of linear regression&lt;/strong&gt; because the relationship between the input and output variables is linear with respect to the coefficients, even though the input variables may be raised to different powers. The model equation for polynomial regression of degree n is:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$y = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + &amp;hellip; + \beta_mx^m + \epsilon$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nonlinear Regression&lt;/strong&gt;, on the other hand, refers to a broader class of regression models where the relationship between the independent and dependent variables is not a linear function. Nonlinear regression can encompass a wide range of functional forms, including exponential, logarithmic, sigmoidal, and other complex shapes. The main characteristic of nonlinear regression is that the model parameters are estimated in a way that best fits the chosen nonlinear function to the data.&lt;/p&gt;
&lt;p&gt;Unlike polynomial regression, nonlinear regression models can&amp;rsquo;t be expressed in terms of a simple equation with polynomial terms. The specific form of the nonlinear function needs to be determined based on the problem&amp;rsquo;s nature and domain knowledge.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages of Polynomial Regression&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Remote part of the function is very sensitive to outliers&lt;/li&gt;
&lt;li&gt;Less flexibility due to global function structure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/dis_pr.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The global function structure causes underfitting or overfitting.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The solution is to move from global to local structure -&amp;gt; Splines.&lt;/p&gt;
&lt;h3 id=&#34;splines-1&#34;&gt;Splines&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Linear combination of Piecewise Polynomial Function &lt;strong&gt;under continuity assumption&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Partition the domain of x into continuous intervals and fit polynomials in each interval separately&lt;/li&gt;
&lt;li&gt;Provides flexibility and local fitting&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose $x \in [a,b]$. Partition the x domain using the following points (a.k.a knots):&lt;/p&gt;
&lt;p&gt;$a&amp;lt;\xi_1&amp;lt;\xi_2&amp;hellip;&amp;lt;\xi_k&amp;lt;b, &amp;lt;\xi_0=a, &amp;lt;\xi_{k+1}=b$&lt;/p&gt;
&lt;p&gt;Fit a polynomial in each interval under the continuity conditions and integrate them by&lt;/p&gt;
&lt;p&gt;$f(X) = \sum_{m=1}^K \beta_mh_m(X)$&lt;/p&gt;
&lt;h3 id=&#34;simple-example&#34;&gt;Simple Example&lt;/h3&gt;
&lt;h3 id=&#34;piecewise-constant&#34;&gt;Piecewise Constant&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/pwc.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here we are using a zero order polynomial. A zero order polynomial can be defined by an indicator function. If we use OLS, the beta would be the average of point in each local region.&lt;/p&gt;
&lt;p&gt;$f(X) = \sum_{m=1}^3 \beta_mh_m(X)$&lt;/p&gt;
&lt;h3 id=&#34;piecewise-linear&#34;&gt;Piecewise Linear&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/pwl.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here we are using a first order polynomial. A first order polynomial includes slopes and intercept (and therefore $K=6$ here.)&lt;/p&gt;
&lt;p&gt;$f(X) = \sum_{m=1}^6 \beta_mh_m(X)$&lt;/p&gt;
&lt;p&gt;There are two issues here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discontinuity&lt;/li&gt;
&lt;li&gt;Underfitting&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;solving-for-discontinuity&#34;&gt;Solving for Discontinuity&lt;/h2&gt;
&lt;p&gt;We can impose continuity constraint for each knot:&lt;/p&gt;
&lt;p&gt;$f{\xi^-_1}=f(\xi^+_1)$&lt;/p&gt;
&lt;p&gt;This can be translated to $\beta_1+\xi_1\beta_4 = \beta_2 + \xi_1\beta_5$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Not sure how&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By adding constraints we are losing some degrees of freedom.
The total number of free parameters (degree of freedom) = 6 (total number of parameters -2 (total number of constraints) = 4&lt;/p&gt;
&lt;p&gt;Alternatively, once could incorporate the constraints into the basis functions:&lt;/p&gt;
&lt;p&gt;$h_1(X) = 1$,&lt;/p&gt;
&lt;p&gt;$h_2(X) = X$,&lt;/p&gt;
&lt;p&gt;$h_3(X) = (X-\xi_1)_+$,&lt;/p&gt;
&lt;p&gt;$h_4(X) = (X-\xi_2)_+$&lt;/p&gt;
&lt;p&gt;This basis is known as truncated power basis&lt;/p&gt;
&lt;p&gt;$(X-\xi_k)_+ = (X-\xi_k)$ if $x \ge xi_k$ $0$ if $x&amp;lt;xi_k$&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/790G152GYz4?si=PIkkurtDgaioUCMu&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;h2 id=&#34;solving-for-underfitting&#34;&gt;Solving for Underfitting&lt;/h2&gt;
&lt;p&gt;Splines with Higher Order of Continuity can be used to tackle underfitting.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/cp.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Continuity constraints for smoothness&lt;/p&gt;
&lt;p&gt;$f{\xi^-_1}=f(\xi^+_1)$&lt;/p&gt;
&lt;p&gt;$f^&amp;rsquo;{\xi^-_1}=f^&amp;rsquo;(\xi^+_1)$&lt;/p&gt;
&lt;p&gt;$f^{&amp;rsquo;&amp;rsquo;}{\xi^-_1}=f^{&amp;rsquo;&amp;rsquo;}(\xi^+_1)$&lt;/p&gt;
&lt;p&gt;$h_1(X) = 1$,&lt;/p&gt;
&lt;p&gt;$h_2(X) = X$,&lt;/p&gt;
&lt;p&gt;$h_3(X) = X^2$,&lt;/p&gt;
&lt;p&gt;$h_4(X) = X^3$,&lt;/p&gt;
&lt;p&gt;$h_5(X) = (X-\xi_1)^3_+$,&lt;/p&gt;
&lt;p&gt;$h_6(X) = (X-\xi_2)^3_+$&lt;/p&gt;
&lt;p&gt;The degree of freedom is calculated by:
Number of regions * Number of parameters in each region) - (Number of knots)*(Number of constraints per knot)&lt;/p&gt;
&lt;h2 id=&#34;order-m-splines&#34;&gt;Order-M Splines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;M=1 piecewise-constant splines&lt;/li&gt;
&lt;li&gt;M=2 linear splines&lt;/li&gt;
&lt;li&gt;M=3 quadratic splines&lt;/li&gt;
&lt;li&gt;M=4 cubic splines&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For Truncated power basis functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Total degree of freedom is K+M&lt;/li&gt;
&lt;li&gt;Cubic spline is the lowest order spline for which the knot discontinuity is not visible to human eyes&lt;/li&gt;
&lt;li&gt;Knots selection: a simple method is to use x quantiles. However, the choice of knots is a variable/model selection problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;estimation&#34;&gt;Estimation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;After creating the basis function, we can use OLS to estimate parameters $\beta$&lt;/li&gt;
&lt;li&gt;First of all, create a basis matrix by concatinating basis vectors. For example if we have cubic splines with two knots, we will have six basis vectors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$H = [h_1(x) \quad h_2(x) \quad h_3(x) \quad h_4(x) \quad h_5(x) \quad h_6(x)]$&lt;/p&gt;
&lt;p&gt;gives $\hat\beta = (H^TH)^-1H^Ty$&lt;/p&gt;
&lt;p&gt;Linear Smoother: $\hat y= H\hat\beta = H(H^TH)^{-1}H^Ty = Sy$&lt;/p&gt;
&lt;p&gt;Degrees of Freedom $df=trace S$&lt;/p&gt;
&lt;p&gt;Although truncated power basis functions are simple and algebraically appealing, it is not efficient for computation and ill-posed and numerically unstable. The matrix is close to singular (because of correlations among themselves, and determinant being very close to zero), and inverting it becomes challenging.&lt;/p&gt;
&lt;p&gt;The solution is to user Bsplines.&lt;/p&gt;
&lt;h2 id=&#34;bsplines&#34;&gt;Bsplines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Alternative basis vectors for piecewise polynomials that are computationally more efficient&lt;/li&gt;
&lt;li&gt;Each basis function has a local support, that is, it is nonzero over at most M (spline order) consecutive intervals&lt;/li&gt;
&lt;li&gt;The basis matrix is banded&lt;/li&gt;
&lt;li&gt;The low bandwidth of the matrix reduces the linear dependency of the columns, and therefore, removes the numeric column stability.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/bspline.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;bspline-basis&#34;&gt;Bspline Basis&lt;/h2&gt;
&lt;p&gt;Let $B_{j,m}(x)$ be the $j^{th}$ B-spline basis function of order $m(m \le M)$ for the knot sequence $\tau$&lt;/p&gt;
&lt;p&gt;$a &amp;lt; \xi_1 &amp;lt; \xi_2 &amp;lt; &amp;hellip; &amp;lt; \xi_k &amp;lt; b$&lt;/p&gt;
&lt;p&gt;Define the augmented knots sequence $\tau$&lt;/p&gt;
&lt;p&gt;$\tau_1 \le \tau_2 &amp;hellip;\le \tau_M \le \xi_0$ (before the lower bound)&lt;/p&gt;
&lt;p&gt;$\tau_{M+j} = \xi_j, j = 1, &amp;hellip; , K$&lt;/p&gt;
&lt;p&gt;$\xi_{K+1} \le \tau_{M+K+1} \le \tau_{M+K+2} \le &amp;hellip; \le \tau_{2M+K}$ (after the lower bound)&lt;/p&gt;
&lt;h3 id=&#34;smoother-matrix&#34;&gt;Smoother Matrix&lt;/h3&gt;
&lt;p&gt;Consider a regression Spline basis B&lt;/p&gt;
&lt;p&gt;$\hat f = B(B^TB)^{-1}B^Ty = Hy$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;H is the smoother matrix (projection matrix)&lt;/li&gt;
&lt;li&gt;H is idempotent ($H \times H = H$)&lt;/li&gt;
&lt;li&gt;H is symmetric&lt;/li&gt;
&lt;li&gt;Degrees of freedom trace (H)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;smoothing-splines&#34;&gt;Smoothing Splines&lt;/h2&gt;
&lt;h3 id=&#34;bspline-basis-boundary-issue&#34;&gt;Bspline basis boundary issue&lt;/h3&gt;
&lt;p&gt;Consider the following setting with the fixed training data&lt;/p&gt;
&lt;p&gt;$y_i = f(x_i) + \epsilon_i$&lt;/p&gt;
&lt;p&gt;$\epsilon_i \approx iid(0, \sigma^2)$&lt;/p&gt;
&lt;p&gt;$Var(\hat f(x)) = h(x)^T(H^TH)^{-1}h(x)\sigma^2$ (variance of estimated function using spline)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Behavior of splines tends to be sporadic near the boundaries, and extrapolation can be problematic. The main reason is that the complexity of Cubic Spline is more than the complexity of Global Cubic Polynomial, due to the large number of parameters (less bias, more variance). The solution is to use linear splines instead of cubic splines (Natural Cubic Splines).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;natural-cubic-splines&#34;&gt;Natural Cubic Splines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Additional constraints are added to make the function linear beyond the boundary knots&lt;/li&gt;
&lt;li&gt;Assuming the function is linear near the boundaries (where there is less information) is often reasonable&lt;/li&gt;
&lt;li&gt;Cubic spline; linear on $[-\inf, \xi_1]$ and $[\xi_k , \inf]$&lt;/li&gt;
&lt;li&gt;Prediction variance decreases&lt;/li&gt;
&lt;li&gt;The price is the bias near the boundaries&lt;/li&gt;
&lt;li&gt;Degrees of freedom is K, the number of knots&lt;/li&gt;
&lt;li&gt;Each of these basis functions has zero second and third derivative in the linear region.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;penalized-residual-sum-of-squares&#34;&gt;Penalized residual sum of squares&lt;/h2&gt;
&lt;p&gt;$\min_f \frac{1}{n}\sum_{i-1}^n[y_i - f(x_i)]^2+\lambda \int^a_b[f^{&amp;quot;}(x)^2dx]$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first term measures the closeness of the model to the data (related to bias)&lt;/li&gt;
&lt;li&gt;The second term penalizes curvature of the function (related to variance)&lt;/li&gt;
&lt;li&gt;$\lambda$ is the smoothing parameter controlling the trade between bias and variance&lt;/li&gt;
&lt;li&gt;$\lambda = 0$ interpolate the data (overfitting)&lt;/li&gt;
&lt;li&gt;$\lambda = \inf$ linear least-square regression (underfitting)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It can be shown that the minimizer is a natural cubic spline.&lt;/p&gt;
&lt;p&gt;Solution: $\hat \theta = (N^TN + \lambda\Omega)^{-1}N^Ty$
, $\Omega$ represents the second derivative&lt;/p&gt;
&lt;p&gt;$ f = (N^TN + \lambda\Omega)^{-1}N^Ty = S_\lambda y$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Smoothing spline estimator is a linear smoother&lt;/li&gt;
&lt;li&gt;$S_\lambda$ is the smoother matrix&lt;/li&gt;
&lt;li&gt;$S_\lambda$ is NOT idempotent&lt;/li&gt;
&lt;li&gt;$S_\lambda$ is symmetric&lt;/li&gt;
&lt;li&gt;$S_\lambda$ is positive definite&lt;/li&gt;
&lt;li&gt;Degrees of freedom: trace($S_\lambda$)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;choice-of-tuning-parameters&#34;&gt;Choice of Tuning Parameters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Train Test Validation
&lt;img src=&#34;https://ayushsubedi.github.io/img/pt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cross Validation
If an independent validation dataset is not affordable, the K-fold cross validation or leave-one-out CV can be useful&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Akaike Information Criteria (AIC)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bayesian Information Criteria (BIC)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generalized Cross-validation&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kernel-smoothers&#34;&gt;Kernel Smoothers&lt;/h2&gt;
&lt;h3 id=&#34;k-nearest-neighbor-knn&#34;&gt;K-Nearest Neighbor (KNN)&lt;/h3&gt;
&lt;p&gt;KNN Average $\hat f(x_0) = \sum_{i=1}^nw(x_0, x_i)y_i$&lt;/p&gt;
&lt;p&gt;where $\sum_{i=1}^nw(x_0, x_i)$ = $\frac{1}{K}$ if $x_i \in N_k(x_0)$ else $0$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple average of the k nearest observations to $x_0$ (local averaging)&lt;/li&gt;
&lt;li&gt;Equal weights are assigned to all neighbors&lt;/li&gt;
&lt;li&gt;However, the fitted function is in the form of a step function (non-smooth function)&lt;/li&gt;
&lt;li&gt;Also, the bias is quite high&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kernel-function&#34;&gt;Kernel Function&lt;/h3&gt;
&lt;p&gt;Any non-negative real-valued integrable function that satisfies the following conditions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\int_{-\inf}^{\inf}K(u)du=1$&lt;/li&gt;
&lt;li&gt;K is an even function; $K(-u) = K(u)$&lt;/li&gt;
&lt;li&gt;It has a finite second moment; $u^2\int_{-\inf}^{\inf}K(u)du &amp;lt; \inf$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kernel-smoother-regression&#34;&gt;Kernel Smoother Regression&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Kernel Regression&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is weighted local averaging that fits a simple model separately at each query point $x_0$&lt;/li&gt;
&lt;li&gt;More weights are assigned to closer observation&lt;/li&gt;
&lt;li&gt;Localization is defined by the weighting function&lt;/li&gt;
&lt;li&gt;Kernel regression requires little training, all calculations get done at the evaluation time.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/kregression.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;choice-of-lambda&#34;&gt;Choice of $\lambda$&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$\lambda$ defines the width of the neighbourhood&lt;/li&gt;
&lt;li&gt;Only points withing $[x_0-\lambda, x_0+\lambda]$ receive positive weights&lt;/li&gt;
&lt;li&gt;Smaller $\lambda$: rough estimate, larger bias, smaller variance&lt;/li&gt;
&lt;li&gt;Larger $\lambda$: smoother estimate, smaller bias, larger variance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cross-validation can be used for determining of $\lambda$:&lt;/p&gt;
&lt;h3 id=&#34;drawbacks-of-local-averaging&#34;&gt;Drawbacks of Local Averaging&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The local averaging can be biased on the boundaries of the domain due to the asymmetry of the kernel in that region.&lt;/li&gt;
&lt;li&gt;This can be solved by local linear regression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/nw_kernel.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Local linear regression corrects the bias on the boundaries&lt;/li&gt;
&lt;li&gt;Local polynomial regression corrects the bias in the curvature region&lt;/li&gt;
&lt;li&gt;However, local polynomial regression is complex due to higher order of polynomials, therefore, it increases the prediction variance.&lt;/li&gt;
&lt;li&gt;A good solution would be to use local linear model for points in the boundaries, and local quadratic regression in the interior regions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;functional-principal-component&#34;&gt;Functional Principal Component&lt;/h2&gt;
&lt;p&gt;Similar to PCA, FPCA aims to reduce the dimension of functional data by extracting a small set of uncorrelated features, which capture the most of the variation.&lt;/p&gt;
&lt;p&gt;Functional data (observed signals) are comprised of two main components. The first component is the continuous functional mean, and the second component is the error term, that is, the realizations from a stochastic process with mean function 0 and covariance function $C(t, t^&amp;rsquo;)$. It includes both random noise and signal-to-signal variations&lt;/p&gt;
&lt;p&gt;$s_i(t) = \mu(t) + \epsilon_i(t)$&lt;/p&gt;
&lt;p&gt;The mean function is common across all signals (notice that it does not have the $i$ subscript)&lt;/p&gt;
&lt;p&gt;Since signal variance comes from the noise function, we first focus on this for dimensionality reduction using the Karhunen-Loeve Theorem.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/kl.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The variance of $\xi_{ik}$ quickly decays with k. Therefore, only a few $\xi_{ik}$ also known as FPC-scores, would be enough to accurately approximate the noise function. That is,&lt;/p&gt;
&lt;p&gt;$\epsilon_i(t) \approx \sum_{k=1}^K \xi_{ik}\phi_{k}(t)$&lt;/p&gt;
&lt;p&gt;Signals decomposition is given by&lt;/p&gt;
&lt;p&gt;$s_i(t) = \mu(t) + \epsilon_i(t) \implies \mu(t) + \sum_{k=1}^K \xi_{ik}\phi_{k}(t)$&lt;/p&gt;
&lt;h2 id=&#34;model-estimation&#34;&gt;Model Estimation&lt;/h2&gt;
&lt;p&gt;Both the mean and covariance is unknown, and should be measured using training data. In practice, we have two types of signals/data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Complete signals: Sampled regularly&lt;/li&gt;
&lt;li&gt;Incomplete signals: Sampled irregularly, sparse, fragmented&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;h3 id=&#34;big-data&#34;&gt;Big Data&lt;/h3&gt;
&lt;p&gt;Big data is a term used to describe extremely large and complex datasets that traditional data processing applications are not well-equipped to handle. The concept of &amp;ldquo;big data&amp;rdquo; is often associated with what is referred to as the &amp;ldquo;4V&amp;rdquo; framework, which describes the key characteristics of big data:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Volume:&lt;/strong&gt;  This refers to the sheer scale of data generated and collected. Big data involves datasets that are too large to be managed and processed using traditional databases and tools. This massive volume can range from terabytes to petabytes and beyond.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Velocity:&lt;/strong&gt;  This characteristic pertains to the speed at which data is generated, collected, and processed. In today&amp;rsquo;s fast-paced digital world, data is generated at an unprecedented rate, often in real-time or near-real-time. Examples include social media interactions, sensor data from IoT devices, financial transactions, and more.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variety:&lt;/strong&gt;  Big data comes in various formats and types, such as structured, semi-structured, and unstructured data. Structured data is organized into a well-defined format (e.g., tables in a relational database), whereas unstructured data lacks a specific structure (e.g., text documents, images, videos, social media posts). Semi-structured data lies somewhere in between, having a partial structure but not fitting neatly into traditional databases.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Veracity:&lt;/strong&gt;  Veracity refers to the quality and reliability of the data. With the proliferation of data sources, there&amp;rsquo;s an increased potential for data to be incomplete, inaccurate, or inconsistent. Ensuring the accuracy and trustworthiness of big data is a significant challenge, and data quality management is crucial for meaningful insights.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;high-dimensional-data&#34;&gt;High Dimensional Data&lt;/h3&gt;
&lt;p&gt;High-dimensional data refers to datasets where the number of features or variables (dimensions) is significantly larger than the number of observations or samples. In other words, the data has a high number of attributes compared to the number of data points available. This kind of data is prevalent in various fields such as genomics, image analysis, social networks, and more.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/high_dimensional_.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;difference-between-high-dimensional-data-and-big-data&#34;&gt;Difference between High Dimensional Data and Big Data&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/diff_bet_high_and_low.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;p = dimension
n = samples&lt;/p&gt;
&lt;h3 id=&#34;the-curse-of-dimensionality&#34;&gt;The Curse of Dimensionality&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;As the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/distance_dimension.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As distance between observations increases with the dimensions, the sample size required for learning a model drastically increases.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Increased Sparsity:&lt;/strong&gt;  In higher dimensions, the available data points are spread out more thinly across the space. This means that data points become farther apart from each other, making it challenging to find meaningful clusters or patterns. It&amp;rsquo;s like having a lot of points scattered in a large, high-dimensional space, and they&amp;rsquo;re so spread out that it&amp;rsquo;s difficult to identify any consistent relationships.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;More Data Needed:&lt;/strong&gt;  With higher-dimensional data, you need a disproportionately larger amount of data to capture the underlying patterns accurately. When the data is sparse, it&amp;rsquo;s harder to generalize from the observed points to make accurate predictions or draw conclusions. As the dimensionality increases, you might need exponentially more data to maintain the same level of accuracy in your models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact on Complexity:&lt;/strong&gt;  The complexity of machine learning models increases with dimensionality. More dimensions mean more parameters to estimate, which can lead to overfitting – a situation where a model fits the training data too closely and fails to generalize well to new data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Increased Computational Demands:&lt;/strong&gt;  Processing and analyzing high-dimensional data require more computational resources and time. Many algorithms become slower and more memory-intensive as the number of dimensions grows. This can make experimentation and model training more challenging and time-consuming.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Difficulties in Visualization:&lt;/strong&gt;  Our ability to visualize data effectively diminishes as the number of dimensions increases. We are accustomed to thinking in 2D and 3D space, but visualizing data in, say, 10 dimensions is practically impossible. This can make it hard to understand the structure of the data and the relationships between variables.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;low-dimensional-learning-from-high-dimensional-data&#34;&gt;Low Dimensional Learning From High Dimensional Data&lt;/h3&gt;
&lt;p&gt;High dimensional data usually have low dimensional structure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.mathworks.com/help/examples/stats/win64/ChangeTsneSettingsExample_01.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This can be achieved through Functional Data Analysis, Tensor Analysis, Rank Deficient Methods among others.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analytics for Ride Hailing Services</title>
      <link>https://ayushsubedi.github.io/posts/ride_hailing_analytics/</link>
      <pubDate>Thu, 02 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/ride_hailing_analytics/</guid>
      <description>&lt;h2 id=&#34;analytics-for-ride-hailing-services&#34;&gt;Analytics for Ride Hailing Services&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.sanity.io/images/6xsct86j/production/e8fc0f789129b17cc8ae2e05b91e93d0752bef67-3840x2160.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction-to-ride-hailing&#34;&gt;Introduction to Ride Hailing&lt;/h2&gt;
&lt;p&gt;At present, it is pretty common to hail a ride to get from one place to the other at a tap of a button. Almost all major cities in the world have some sort of ride-hailing service. Uber, Lyft, Didi, Ola, Gojek, etc. are some examples of service providers that come to mind. Additionally, the service is also proliferating to smaller cities and has become commonplace in many parts of the world. Analytics is a key component in making sure the service is provided efficiently. All of the aforementioned companies invest heavily in data science and analytics to be competitive and to provide better services.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For this post, I will focus on Ride-Hailing services (not Ride Sharing services). See the difference &lt;a href=&#34;https://www.ecolane.com/blog/ride-hailing-vs.-ride-sharing-the-key-difference-and-why-it-matters&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Predominantly, ride-hailing functions as a &lt;em&gt;Gig Economy&lt;/em&gt;. The drivers (sometimes referred to as partners, captains, etc.) are mostly independent contractors who bring their own vehicle and work at their own time and are paid based on their time commitment. This variability requires monitoring, sophisticated algorithms, good incentives, competitive pricing to passengers, etc. which is also common in other gig economy jobs. In most cases, the analytics models that will be built for one gig economy can be tweaked to fit another one as well.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s look at a few components of Ride-hailing that will be relevant for how we frame our models and the data we use.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For this post, &amp;ldquo;passengers&amp;rdquo; are referred to as service requesters/receivers and &amp;ldquo;drivers&amp;rdquo; are referred to as service providers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;components-of-the-problem&#34;&gt;Components of the problem&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;balancing-act-supply-and-demand-and-chicken-and-egg-problem&#34;&gt;Balancing act: Supply and Demand, and Chicken and Egg Problem&lt;/h3&gt;
&lt;p&gt;There is a balancing act that all of these ride-sharing platforms need to perform to be efficient. A healthy ratio between driver and passenger (to go more granular, for a segment of geographic area at a given time) is very important.  The balancing act is even crucial when a ride-hailing service decides to introduce itself to a new city (especially one that is new to ride-hailing).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If an area has more drivers than demand from passengers, the drivers might not get ride requests causing them to lose interest and find a different job or move to a different competition.&lt;/li&gt;
&lt;li&gt;If an area has more passengers than a supply of drivers, the passengers might not get their ride requests accepted causing them to move onto another (direct/indirect) competition.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From an analytics perspective, this is a difficult problem to solve. However, good analytics can also be a competitive advantage here.&lt;/p&gt;
&lt;h3 id=&#34;pricing&#34;&gt;Pricing&lt;/h3&gt;
&lt;p&gt;Pricing is a by-product of the balancing act described above. The pricing must be competitive enough to lure the supply and the demand pool. The driver should feel like the pricing justifies the time, effort, and resources supplied. The passenger should feel the amount paid for the service justifies the service received.&lt;/p&gt;
&lt;p&gt;Few ride-hailing services opt-out for transparent and fixed payment (i.e the price is only dictated by the distance to destination), while some have complex pricing strategies to stand out, lure passengers or drivers, and manage supply and demand effectively.&lt;/p&gt;
&lt;h3 id=&#34;dynamic-pricing&#34;&gt;Dynamic Pricing&lt;/h3&gt;
&lt;p&gt;Some ride-hailing services implement dynamic pricing as a way to balance the chicken and egg problem described above. This is a large-scale, complex analytics problem involving several variables. Additionally, driver bonuses, discounts, and referrals might constitute the pricing strategy as well.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://thumbor.forbes.com/thumbor/711x274/https://blogs-images.forbes.com/nicolemartin1/files/2019/03/dynamic-pricing.jpg?width=960&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Credit: Forbes&lt;/p&gt;
&lt;h3 id=&#34;competition-direct-and-indirect&#34;&gt;Competition (Direct and Indirect)&lt;/h3&gt;
&lt;h4 id=&#34;direct-competition-passenger&#34;&gt;Direct Competition (Passenger)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;other ride hailing services&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;direct-competition-driver&#34;&gt;Direct Competition (Driver)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;other ride-hailing services&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;indirect-competition-passenger&#34;&gt;Indirect Competition (Passenger)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;public transportation&lt;/li&gt;
&lt;li&gt;taxi/cab&lt;/li&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;indirect-competition-driver&#34;&gt;Indirect Competition (Driver)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;other employment opportunities&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://thumbor.forbes.com/thumbor/fit-in/1200x0/filters%3Aformat%28jpg%29/https%3A%2F%2Fblogs-images.forbes.com%2Fliyanchen%2Ffiles%2F2015%2F09%2F0908_uber-map2_2000-1940x1487.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Credit: Forbes&lt;/p&gt;
&lt;hr&gt;
&lt;br&gt;
&lt;h2 id=&#34;descriptive-analysis&#34;&gt;Descriptive analysis&lt;/h2&gt;
&lt;p&gt;Before we build complex models, it is essential to understand how the business/service is performing. These descriptive analyses will lay the foundation for us when we build complex and combined models later on.&lt;/p&gt;
&lt;h3 id=&#34;ride-completioncancellation-rate&#34;&gt;Ride Completion/Cancellation rate&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What is the ride completion rate?&lt;/li&gt;
&lt;li&gt;To be more granular, what is the ride completion rate at a geographic segment of the city at a particular time?&lt;/li&gt;
&lt;li&gt;What is the ride cancellation rate?&lt;/li&gt;
&lt;li&gt;Similar to before, what is the ride cancellation rate at a geographic segment of the city at a particular time?&lt;/li&gt;
&lt;li&gt;Why do passengers cancel rides?&lt;/li&gt;
&lt;li&gt;Is cancellation more prominent in one area compared to the other?&lt;/li&gt;
&lt;li&gt;Is this dependent on the time of the day?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DATA&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;passenger_id&lt;/li&gt;
&lt;li&gt;driver_id&lt;/li&gt;
&lt;li&gt;latitude (pickup, drop)&lt;/li&gt;
&lt;li&gt;longitude (pickup, drop)&lt;/li&gt;
&lt;li&gt;timestamps (requested, accepted, picked up, dropped, canceled)&lt;/li&gt;
&lt;li&gt;completion_status&lt;/li&gt;
&lt;li&gt;cancellation_reason&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;late-arrival-rate&#34;&gt;Late arrival rate&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What is the late arrival rate?&lt;/li&gt;
&lt;li&gt;what is the late arrival rate at a geographic segment of the city at a particular time?&lt;/li&gt;
&lt;li&gt;Is the late arrival rate prominent for some time of the day or for a particular geographical area?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DATA&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;passenger_id&lt;/li&gt;
&lt;li&gt;driver_id&lt;/li&gt;
&lt;li&gt;latitude (pickup, drop)&lt;/li&gt;
&lt;li&gt;longitude (pickup, drop)&lt;/li&gt;
&lt;li&gt;timestamps (requested, accepted, picked up, dropped, canceled)&lt;/li&gt;
&lt;li&gt;completion_status&lt;/li&gt;
&lt;li&gt;cancellation_reason&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;activation-acquisition-retention-referral-revenue&#34;&gt;Activation, Acquisition, Retention, Referral, Revenue&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What does the pirate metric funnel look like?&lt;/li&gt;
&lt;li&gt;Is there a specific area where the business should focus to improve business/efficiency?&lt;/li&gt;
&lt;li&gt;Is the funnel leaking somewhere?&lt;/li&gt;
&lt;li&gt;What is the passenger/driver churn rate?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DATA&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;passenger_id/driver_id&lt;/li&gt;
&lt;li&gt;timestamps (created_date, last_ride_date)&lt;/li&gt;
&lt;li&gt;total_amount_spent_on_platform / total_money_made&lt;/li&gt;
&lt;li&gt;total_rides&lt;/li&gt;
&lt;li&gt;num_of_referrals&lt;/li&gt;
&lt;li&gt;acquisition_channel&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://hygger.io/wp-content/uploads/2018/01/Main-EN.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Credit: hygger.io&lt;/p&gt;
&lt;h3 id=&#34;channels&#34;&gt;Channels&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What is the acquisition rate from different marketing channels for drivers or for passengers?&lt;/li&gt;
&lt;li&gt;What marketing channel is more apt/effective for different demography/user segments?&lt;/li&gt;
&lt;li&gt;Can we use the multi-arm bandits model to identify a balance between exploration and exploitation to test on different channels?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DATA&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;passenger_id/driver_id&lt;/li&gt;
&lt;li&gt;timestamps (created_date)&lt;/li&gt;
&lt;li&gt;acquisition_channel&lt;/li&gt;
&lt;li&gt;total_amount_spent_on_platform / total_money_made&lt;/li&gt;
&lt;li&gt;passenger/driver demographic information (age, gender, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;user-analysis&#34;&gt;User Analysis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What does the demography (social, cultural, economic) of the driver look like?&lt;/li&gt;
&lt;li&gt;What does the demography (social, cultural, economic) of the passenger look like?&lt;/li&gt;
&lt;li&gt;What does the demography of the city look like?&lt;/li&gt;
&lt;li&gt;What does the demography of the segment that uses the service the most look like?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DATA&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;passenger_id/driver_id&lt;/li&gt;
&lt;li&gt;timestamps (created_date)&lt;/li&gt;
&lt;li&gt;acquisition_channel&lt;/li&gt;
&lt;li&gt;total_amount_spent_on_platform / total_money_made&lt;/li&gt;
&lt;li&gt;passenger/driver/city demographic information (age, gender etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;driver-rankingdriver-performance&#34;&gt;Driver Ranking/Driver Performance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;How is a driver performing? (this could be based on multiple factors including customer rating, and other factors)&lt;/li&gt;
&lt;li&gt;Based on the index for performance, what is the rank of a driver?&lt;/li&gt;
&lt;li&gt;What is the rank of a driver among a segment of drivers? (this will be useful for priority queue for driver dispatching)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DATA&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;driver_id&lt;/li&gt;
&lt;li&gt;timestamps&lt;/li&gt;
&lt;li&gt;average_rating&lt;/li&gt;
&lt;li&gt;rides_complete_rate&lt;/li&gt;
&lt;li&gt;last_ride_date&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;br&gt;
&lt;h2 id=&#34;predictive-analysis&#34;&gt;Predictive analysis&lt;/h2&gt;
&lt;p&gt;If we are looking to make the system more efficient, it is also very important to understand what the future holds.&lt;/p&gt;
&lt;h3 id=&#34;growth-in-rides&#34;&gt;Growth in rides&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What is the number of expected daily rides next day/week/month/year?&lt;/li&gt;
&lt;li&gt;What is the expected revenue for the next day/week/month/year?&lt;/li&gt;
&lt;li&gt;Is there a daily/weekly/monthly seasonality?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DATA&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;timestamp (daily)&lt;/li&gt;
&lt;li&gt;num_of_ride (completed rides or ride requests)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;passenger-growth&#34;&gt;Passenger growth&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What is the number of expected passenger growth next day/week/month/year?&lt;/li&gt;
&lt;li&gt;Is there a daily/weekly/monthly seasonality?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DATA&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;timestamp (daily)&lt;/li&gt;
&lt;li&gt;num_of_unique_passengers (acquisition or ride request)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;driver-growth&#34;&gt;Driver growth&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What is the number of expected driver growth next day/week/month/year?&lt;/li&gt;
&lt;li&gt;Is there a daily/weekly/monthly seasonality?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DATA&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;timestamp (daily)&lt;/li&gt;
&lt;li&gt;num_of_unique_drivers (acquisition or ride request)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;churn-over-the-period-of-time&#34;&gt;Churn over the period of time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What is the expected churn in the next day/week/month/year?&lt;/li&gt;
&lt;li&gt;Is there a daily/weekly/monthly seasonality?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DATA&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;timestamp (passenger acquisition)&lt;/li&gt;
&lt;li&gt;passenger&amp;rsquo;s number of rides each month (grouped acquisition to present)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;br&gt;
&lt;h2 id=&#34;prescriptive-analysis&#34;&gt;Prescriptive analysis&lt;/h2&gt;
&lt;p&gt;Descriptive and Predictive analysis will help us move towards prescriptive analysis, especially for optimization models. These models will help the service provider in decision making, especially with regards to an increase in efficiency for drivers and passengers.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;ratio-of-drivers-to-passengers&#34;&gt;Ratio of drivers to passengers&lt;/h3&gt;
&lt;h4 id=&#34;what-is-the-ideal-ratio-of-the-passenger-to-the-driver-to-maximize-rides-completion-rate&#34;&gt;What is the ideal ratio of the passenger to the driver to maximize rides completion rate?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Given&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Voronoi clustering for geographic indexing based on geographic hotspots (other indexing methods are more efficient like h3 developed by Uber, but Voronoi can be used to build something similar as well.)&lt;/li&gt;
&lt;li&gt;rides data (requested, canceled, completed)&lt;/li&gt;
&lt;li&gt;passenger data (raw data and data after descriptive analysis performed: Pirate metrics etc.)&lt;/li&gt;
&lt;li&gt;driver data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Use&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Optimization
&lt;ul&gt;
&lt;li&gt;with constraints: num_of_rides should be greater than a threshold (comes from future rides data)&lt;/li&gt;
&lt;li&gt;with objective functions: maximize rides completion rate for each geographic segment&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;to find an optimal driver to passenger ratio&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regression (or logistic regression if we only care about a healthy/unhealthy ratio) can also be used to do something similar as well.&lt;/li&gt;
&lt;li&gt;Additionally, the result from the model can also be used to model advertisement campaigns for the future if we find the number of driver or passenger (in a particular geographic area) need to be increased for a stable ratio.&lt;/li&gt;
&lt;li&gt;This is an important indicator because it allows the service provider to focus on growth while keeping this indicator at a healthy level.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;dynamic-pricing-1&#34;&gt;Dynamic pricing&lt;/h3&gt;
&lt;h4 id=&#34;what-should-the-dynamicsurge-pricing-be-at-a-given-time&#34;&gt;What should the dynamic/surge pricing be at a given time?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Given&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;ratio of the driver to passenger&lt;/li&gt;
&lt;li&gt;paying capacity of passengers (based on descriptive analysis of users, useful for capping at some multiplier so that it does not go wild)&lt;/li&gt;
&lt;li&gt;number of requests in the queue in a geographic segment&lt;/li&gt;
&lt;li&gt;competition surge at the moment&lt;/li&gt;
&lt;li&gt;number of requests completed in the geographic segment (and neighboring segment) in last x minutes (arbitrary but can be defined by waiting for time analysis from descriptive analysis)&lt;/li&gt;
&lt;li&gt;geographic location information (grid-based on Voronoi for the availability of drivers in other cells)&lt;/li&gt;
&lt;li&gt;number of drivers that will be free (complete a ride soon or are predicted to come online soon) in the grid or neighboring grids&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Use&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Linear regression&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;to find ideal dynamic pricing multiplier&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The cap might/might not be necessary, and that might be another analytics problem altogether. There have been some cases where a natural disaster/terrorist attack increased surge multiplier to an exorbitant number causing massive backlash.&lt;/li&gt;
&lt;li&gt;grid above refers to one unit of Voronoi based geographic segmentation&lt;/li&gt;
&lt;li&gt;It is necessary to study the correlation of some of the predictors mentioned above.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;ride-dispatching&#34;&gt;Ride Dispatching&lt;/h3&gt;
&lt;h4 id=&#34;what-is-a-robust-ride-dispatching-mechanism-that-will-increase-passengers-and-drivers&#34;&gt;What is a robust ride dispatching mechanism that will increase passengers and drivers?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Given&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Drivers in Geographic Grid (and neighboring Grid)&lt;/li&gt;
&lt;li&gt;Driver Rating/Driver Ranking&lt;/li&gt;
&lt;li&gt;Geographic Grid&lt;/li&gt;
&lt;li&gt;Pickup/Drop location (distance and Grid)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Use&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Optimization
&lt;ul&gt;
&lt;li&gt;with constraints: the probability of each driver getting ride should be close to 1, waiting time should be less than some threshold for the request to be accepted  or not accepted (which comes from descriptive analysis), the time between request dispatching (time window a driver gets before the request is passed on to a different driver, also comes from descriptive analysis) should be equal to the acceptable waiting time divided by some constant (integer)&lt;/li&gt;
&lt;li&gt;with objective functions: maximize rides completion rate for each geographic segment&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;** Notes **&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Queuing models can also be here to identify correct values for the dispatching system (waiting time, dynamic geographic grid, etc.). However, there is a need to check the distribution of different events (booking created, booking accepted, waiting time, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;It seems analytics is extremely relevant in all aspects of ride-hailing. In this project, I merely covered a few use cases, with one or two relevant models. Even with this brief exploration, I can conclude that analytics can lead to better outcomes for both drivers and passengers.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>