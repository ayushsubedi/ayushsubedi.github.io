<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>rbfkernel on Ayush Subedi</title>
    <link>https://ayushsubedi.github.io/tags/rbfkernel/</link>
    <description>Recent content in rbfkernel on Ayush Subedi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 22 Aug 2023 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ayushsubedi.github.io/tags/rbfkernel/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Topics on High-Dimensional Data Analytics (Machine Learning 2)</title>
      <link>https://ayushsubedi.github.io/posts/topics_on_high_dimensional_data_analytics/</link>
      <pubDate>Tue, 22 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/topics_on_high_dimensional_data_analytics/</guid>
      <description>&lt;h1 id=&#34;topics-on-high-dimensional-data-analytics&#34;&gt;Topics on High-Dimensional Data Analytics&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#functional-data-analysis&#34;&gt;Functional Data Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#image-analysis&#34;&gt;Image Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tensor-data-analysis&#34;&gt;Tensor Data Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimization-application&#34;&gt;Optimization Application&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regularization&#34;&gt;Regularization&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;h3 id=&#34;big-data&#34;&gt;Big Data&lt;/h3&gt;
&lt;p&gt;Big data is a term used to describe extremely large and complex datasets that traditional data processing applications are not well-equipped to handle. The concept of &amp;ldquo;big data&amp;rdquo; is often associated with what is referred to as the &amp;ldquo;4V&amp;rdquo; framework, which describes the key characteristics of big data:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Volume:&lt;/strong&gt;  This refers to the sheer scale of data generated and collected. Big data involves datasets that are too large to be managed and processed using traditional databases and tools. This massive volume can range from terabytes to petabytes and beyond.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Velocity:&lt;/strong&gt;  This characteristic pertains to the speed at which data is generated, collected, and processed. In today&amp;rsquo;s fast-paced digital world, data is generated at an unprecedented rate, often in real-time or near-real-time. Examples include social media interactions, sensor data from IoT devices, financial transactions, and more.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variety:&lt;/strong&gt;  Big data comes in various formats and types, such as structured, semi-structured, and unstructured data. Structured data is organized into a well-defined format (e.g., tables in a relational database), whereas unstructured data lacks a specific structure (e.g., text documents, images, videos, social media posts). Semi-structured data lies somewhere in between, having a partial structure but not fitting neatly into traditional databases.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Veracity:&lt;/strong&gt;  Veracity refers to the quality and reliability of the data. With the proliferation of data sources, there&amp;rsquo;s an increased potential for data to be incomplete, inaccurate, or inconsistent. Ensuring the accuracy and trustworthiness of big data is a significant challenge, and data quality management is crucial for meaningful insights.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;high-dimensional-data&#34;&gt;High Dimensional Data&lt;/h3&gt;
&lt;p&gt;High-dimensional data refers to datasets where the number of features or variables (dimensions) is significantly larger than the number of observations or samples. In other words, the data has a high number of attributes compared to the number of data points available. This kind of data is prevalent in various fields such as genomics, image analysis, social networks, and more.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/high_dimensional_.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;difference-between-high-dimensional-data-and-big-data&#34;&gt;Difference between High Dimensional Data and Big Data&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/diff_bet_high_and_low.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;p = dimension
n = samples&lt;/p&gt;
&lt;h3 id=&#34;the-curse-of-dimensionality&#34;&gt;The Curse of Dimensionality&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;As the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/distance_dimension.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As distance between observations increases with the dimensions, the sample size required for learning a model drastically increases.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Increased Sparsity:&lt;/strong&gt;  In higher dimensions, the available data points are spread out more thinly across the space. This means that data points become farther apart from each other, making it challenging to find meaningful clusters or patterns. It&amp;rsquo;s like having a lot of points scattered in a large, high-dimensional space, and they&amp;rsquo;re so spread out that it&amp;rsquo;s difficult to identify any consistent relationships.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;More Data Needed:&lt;/strong&gt;  With higher-dimensional data, you need a disproportionately larger amount of data to capture the underlying patterns accurately. When the data is sparse, it&amp;rsquo;s harder to generalize from the observed points to make accurate predictions or draw conclusions. As the dimensionality increases, you might need exponentially more data to maintain the same level of accuracy in your models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact on Complexity:&lt;/strong&gt;  The complexity of machine learning models increases with dimensionality. More dimensions mean more parameters to estimate, which can lead to overfitting â€“ a situation where a model fits the training data too closely and fails to generalize well to new data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Increased Computational Demands:&lt;/strong&gt;  Processing and analyzing high-dimensional data require more computational resources and time. Many algorithms become slower and more memory-intensive as the number of dimensions grows. This can make experimentation and model training more challenging and time-consuming.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Difficulties in Visualization:&lt;/strong&gt;  Our ability to visualize data effectively diminishes as the number of dimensions increases. We are accustomed to thinking in 2D and 3D space, but visualizing data in, say, 10 dimensions is practically impossible. This can make it hard to understand the structure of the data and the relationships between variables.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;low-dimensional-learning-from-high-dimensional-data&#34;&gt;Low Dimensional Learning From High Dimensional Data&lt;/h3&gt;
&lt;p&gt;High dimensional data usually have low dimensional structure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.mathworks.com/help/examples/stats/win64/ChangeTsneSettingsExample_01.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This can be achieved through Functional Data Analysis, Tensor Analysis, Rank Deficient Methods among others.&lt;/p&gt;
&lt;h3 id=&#34;solutions-for-the-curse-of-dimensionality&#34;&gt;Solutions for the curse of dimensionality&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Feature extraction&lt;/li&gt;
&lt;li&gt;Dimensionality reduction&lt;/li&gt;
&lt;li&gt;Collecting much more observations&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;functional-data-analysis&#34;&gt;Functional Data Analysis&lt;/h1&gt;
&lt;p&gt;A fluctuating quantity or impulse whose variations represent information and is often represented as a function of time or space.&lt;/p&gt;
&lt;p&gt;From Wikipedia&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Functional data analysis (FDA)&lt;/strong&gt; is a branch of statistics that analyses data providing information about curves, surfaces or anything else varying over a continuum. In its most general form, under an FDA framework, each sample element of functional data is considered to be a random function. The physical continuum over which these functions are defined is often time, but may also be spatial location, wavelength, probability, etc. Intrinsically, functional data are infinite dimensional. The high intrinsic dimensionality of these data brings challenges for theory as well as computation, where these challenges vary with how the functional data were sampled. However, the high or infinite dimensional structure of the data is a rich source of information and there are many interesting challenges for research and data analysis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://lands.let.ru.nl/FDA/images/FDA_pic4website.bmp&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;regression---least-square-estimates&#34;&gt;Regression - Least square Estimates&lt;/h2&gt;
&lt;p&gt;A linear regression model assumes that the regression function $E(Y|X)$ is linear in the inputs $X_1, &amp;hellip;, X_p$. They were developed in the pre-computer age of statistics, but even in today&amp;rsquo;s computer era there are still good reasons to study and use them. They are simple and often provide an adequate and interpretable description of how the inputs affect the output.&lt;/p&gt;
&lt;p&gt;The linear regression model has the form:&lt;/p&gt;
&lt;p&gt;$f(X) = \beta_0 + \sum_{j=1}^p X_j\beta_j$&lt;/p&gt;
&lt;p&gt;Typically we have a set of training data $(x_1, y_1)&amp;hellip;(x_N, y_N)$ from which to estimate the parameters $\beta$. Each $x_i = (x_{i1}, x_{i2} &amp;hellip; x_{ip})^T$ is a vector of feature measurements for the $i$th case. The most popular estimation method is the least squares, in which we pick the coefficients $\beta = (\beta_0, \beta_1,&amp;hellip;.\beta_p)^T$ to minimize the residual sum of squares.&lt;/p&gt;
&lt;p&gt;$\sum_{i=1}^N (y_i - f(x))^2 = \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p X_j\beta_j)^2$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/lr.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Denote $X$ by the $N \times (p+1)$ matrix with each row an input vector (with a 1 in the first position, to represent the intercept), and similarity let $y$ be the $N$ vector of outputs in the training set. Then we can write the residual sum-of-squares as :&lt;/p&gt;
&lt;p&gt;$RSS(\beta) = (y-X\beta)^T(y-X\beta)$&lt;/p&gt;
&lt;p&gt;Differentiating with respect to $\beta$, &amp;hellip;.&lt;/p&gt;
&lt;p&gt;$\hat{\beta} = (X^TX)^{-1}X^Ty$&lt;/p&gt;
&lt;h2 id=&#34;geometric-interpretation&#34;&gt;Geometric Interpretation&lt;/h2&gt;
&lt;p&gt;$\hat{y} = X\hat{\beta} = X(X^TX)^{-1}X^Ty = Hy $&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Projection Matrix&lt;/strong&gt; (or Hat matrix): The outcome vector $y$ is orthogonally projected onto the hyperplane spanned by the input vectors $x_1$ and $x_2$. The Projection $\hat{y}$ represents the vector of predictions obtained by the least square method.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/ols_projection.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;properties-of-ols&#34;&gt;Properties of OLS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;They are unbiased estimators. That is the expected value of estimators and actual parameters are the same $E(\hat{\beta}) = \beta$&lt;/li&gt;
&lt;li&gt;The covariance can be obtained by $cov(\hat{\beta}) = \sigma^2 (X^TX)^{-1}$, where $\sigma^2 = SSE/(n-p)$&lt;/li&gt;
&lt;li&gt;According to the &lt;strong&gt;Gauss-Markov Theorem&lt;/strong&gt;, &lt;em&gt;among all unbiased linear estimates&lt;/em&gt;, the least square estimate (LSE) has the minimum variance and it is unique.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Regression can be used for Feature Extraction&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;splines&#34;&gt;Splines&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Polynomial Regression&lt;/strong&gt; is a type of regression analysis where the relationship between the independent variable (input) and the dependent variable (output) is modeled as an nth-degree polynomial. In other words, instead of fitting a straight line (linear regression), a polynomial regression can fit curves of various degrees, allowing for more flexibility in capturing complex relationships. For example, a quadratic polynomial regression (degree 2) can model a parabolic relationship, and a cubic polynomial regression (degree 3) can model more intricate curves.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Polynomial regression is still considered a type of linear regression&lt;/strong&gt; because the relationship between the input and output variables is linear with respect to the coefficients, even though the input variables may be raised to different powers. The model equation for polynomial regression of degree n is:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$y = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + &amp;hellip; + \beta_mx^m + \epsilon$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nonlinear Regression&lt;/strong&gt;, on the other hand, refers to a broader class of regression models where the relationship between the independent and dependent variables is not a linear function. Nonlinear regression can encompass a wide range of functional forms, including exponential, logarithmic, sigmoidal, and other complex shapes. The main characteristic of nonlinear regression is that the model parameters are estimated in a way that best fits the chosen nonlinear function to the data.&lt;/p&gt;
&lt;p&gt;Unlike polynomial regression, nonlinear regression models can&amp;rsquo;t be expressed in terms of a simple equation with polynomial terms. The specific form of the nonlinear function needs to be determined based on the problem&amp;rsquo;s nature and domain knowledge.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages of Polynomial Regression&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Remote part of the function is very sensitive to outliers&lt;/li&gt;
&lt;li&gt;Less flexibility due to global function structure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/dis_pr.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The global function structure causes underfitting or overfitting.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The solution is to move from global to local structure -&amp;gt; Splines.&lt;/p&gt;
&lt;h3 id=&#34;splines-1&#34;&gt;Splines&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Linear combination of Piecewise Polynomial Function &lt;strong&gt;under continuity assumption&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Partition the domain of x into continuous intervals and fit polynomials in each interval separately&lt;/li&gt;
&lt;li&gt;Provides flexibility and local fitting&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose $x \in [a,b]$. Partition the x domain using the following points (a.k.a knots):&lt;/p&gt;
&lt;p&gt;$a&amp;lt;\xi_1&amp;lt;\xi_2&amp;hellip;&amp;lt;\xi_k&amp;lt;b, &amp;lt;\xi_0=a, &amp;lt;\xi_{k+1}=b$&lt;/p&gt;
&lt;p&gt;Fit a polynomial in each interval under the continuity conditions and integrate them by&lt;/p&gt;
&lt;p&gt;$f(X) = \sum_{m=1}^K \beta_mh_m(X)$&lt;/p&gt;
&lt;h3 id=&#34;simple-example&#34;&gt;Simple Example&lt;/h3&gt;
&lt;h3 id=&#34;piecewise-constant&#34;&gt;Piecewise Constant&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/pwc.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here we are using a zero order polynomial. A zero order polynomial can be defined by an indicator function. If we use OLS, the beta would be the average of point in each local region.&lt;/p&gt;
&lt;p&gt;$f(X) = \sum_{m=1}^3 \beta_mh_m(X)$&lt;/p&gt;
&lt;h3 id=&#34;piecewise-linear&#34;&gt;Piecewise Linear&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/pwl.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here we are using a first order polynomial. A first order polynomial includes slopes and intercept (and therefore $K=6$ here.)&lt;/p&gt;
&lt;p&gt;$f(X) = \sum_{m=1}^6 \beta_mh_m(X)$&lt;/p&gt;
&lt;p&gt;There are two issues here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discontinuity&lt;/li&gt;
&lt;li&gt;Underfitting&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;solving-for-discontinuity&#34;&gt;Solving for Discontinuity&lt;/h2&gt;
&lt;p&gt;We can impose continuity constraint for each knot:&lt;/p&gt;
&lt;p&gt;$f{\xi^-_1}=f(\xi^+_1)$&lt;/p&gt;
&lt;p&gt;This can be translated to $\beta_1+\xi_1\beta_4 = \beta_2 + \xi_1\beta_5$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Not sure how&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By adding constraints we are losing some degrees of freedom.
The total number of free parameters (degree of freedom) = 6 (total number of parameters -2 (total number of constraints) = 4&lt;/p&gt;
&lt;p&gt;Alternatively, once could incorporate the constraints into the basis functions:&lt;/p&gt;
&lt;p&gt;$h_1(X) = 1$,&lt;/p&gt;
&lt;p&gt;$h_2(X) = X$,&lt;/p&gt;
&lt;p&gt;$h_3(X) = (X-\xi_1)_+$,&lt;/p&gt;
&lt;p&gt;$h_4(X) = (X-\xi_2)_+$&lt;/p&gt;
&lt;p&gt;This basis is known as truncated power basis&lt;/p&gt;
&lt;p&gt;$(X-\xi_k)_+ = (X-\xi_k)$ if $x \ge xi_k$ $0$ if $x&amp;lt;xi_k$&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/790G152GYz4?si=PIkkurtDgaioUCMu&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;h2 id=&#34;solving-for-underfitting&#34;&gt;Solving for Underfitting&lt;/h2&gt;
&lt;p&gt;Splines with Higher Order of Continuity can be used to tackle underfitting.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/cp.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Continuity constraints for smoothness&lt;/p&gt;
&lt;p&gt;$f{\xi^-_1}=f(\xi^+_1)$&lt;/p&gt;
&lt;p&gt;$f^&amp;rsquo;{\xi^-_1}=f^&amp;rsquo;(\xi^+_1)$&lt;/p&gt;
&lt;p&gt;$f^{&amp;rsquo;&amp;rsquo;}{\xi^-_1}=f^{&amp;rsquo;&amp;rsquo;}(\xi^+_1)$&lt;/p&gt;
&lt;p&gt;$h_1(X) = 1$,&lt;/p&gt;
&lt;p&gt;$h_2(X) = X$,&lt;/p&gt;
&lt;p&gt;$h_3(X) = X^2$,&lt;/p&gt;
&lt;p&gt;$h_4(X) = X^3$,&lt;/p&gt;
&lt;p&gt;$h_5(X) = (X-\xi_1)^3_+$,&lt;/p&gt;
&lt;p&gt;$h_6(X) = (X-\xi_2)^3_+$&lt;/p&gt;
&lt;p&gt;The degree of freedom is calculated by:
Number of regions * Number of parameters in each region) - (Number of knots)*(Number of constraints per knot)&lt;/p&gt;
&lt;h2 id=&#34;order-m-splines&#34;&gt;Order-M Splines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;M=1 piecewise-constant splines&lt;/li&gt;
&lt;li&gt;M=2 linear splines&lt;/li&gt;
&lt;li&gt;M=3 quadratic splines&lt;/li&gt;
&lt;li&gt;M=4 cubic splines&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For Truncated power basis functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Total degree of freedom is K+M&lt;/li&gt;
&lt;li&gt;Cubic spline is the lowest order spline for which the knot discontinuity is not visible to human eyes&lt;/li&gt;
&lt;li&gt;Knots selection: a simple method is to use x quantiles. However, the choice of knots is a variable/model selection problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;estimation&#34;&gt;Estimation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;After creating the basis function, we can use OLS to estimate parameters $\beta$&lt;/li&gt;
&lt;li&gt;First of all, create a basis matrix by concatinating basis vectors. For example if we have cubic splines with two knots, we will have six basis vectors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$H = [h_1(x) \quad h_2(x) \quad h_3(x) \quad h_4(x) \quad h_5(x) \quad h_6(x)]$&lt;/p&gt;
&lt;p&gt;gives $\hat\beta = (H^TH)^-1H^Ty$&lt;/p&gt;
&lt;p&gt;Linear Smoother: $\hat y= H\hat\beta = H(H^TH)^{-1}H^Ty = Sy$&lt;/p&gt;
&lt;p&gt;Degrees of Freedom $df=trace S$&lt;/p&gt;
&lt;p&gt;Although truncated power basis functions are simple and algebraically appealing, it is not efficient for computation and ill-posed and numerically unstable. The matrix is close to singular (because of correlations among themselves, and determinant being very close to zero), and inverting it becomes challenging.&lt;/p&gt;
&lt;p&gt;The solution is to user Bsplines.&lt;/p&gt;
&lt;h2 id=&#34;bsplines&#34;&gt;Bsplines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Alternative basis vectors for piecewise polynomials that are computationally more efficient&lt;/li&gt;
&lt;li&gt;Each basis function has a local support, that is, it is nonzero over at most M (spline order) consecutive intervals&lt;/li&gt;
&lt;li&gt;The basis matrix is banded&lt;/li&gt;
&lt;li&gt;The low bandwidth of the matrix reduces the linear dependency of the columns, and therefore, removes the numeric column stability.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/bspline.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;bspline-basis&#34;&gt;Bspline Basis&lt;/h2&gt;
&lt;p&gt;Let $B_{j,m}(x)$ be the $j^{th}$ B-spline basis function of order $m(m \le M)$ for the knot sequence $\tau$&lt;/p&gt;
&lt;p&gt;$a &amp;lt; \xi_1 &amp;lt; \xi_2 &amp;lt; &amp;hellip; &amp;lt; \xi_k &amp;lt; b$&lt;/p&gt;
&lt;p&gt;Define the augmented knots sequence $\tau$&lt;/p&gt;
&lt;p&gt;$\tau_1 \le \tau_2 &amp;hellip;\le \tau_M \le \xi_0$ (before the lower bound)&lt;/p&gt;
&lt;p&gt;$\tau_{M+j} = \xi_j, j = 1, &amp;hellip; , K$&lt;/p&gt;
&lt;p&gt;$\xi_{K+1} \le \tau_{M+K+1} \le \tau_{M+K+2} \le &amp;hellip; \le \tau_{2M+K}$ (after the lower bound)&lt;/p&gt;
&lt;h3 id=&#34;smoother-matrix&#34;&gt;Smoother Matrix&lt;/h3&gt;
&lt;p&gt;Consider a regression Spline basis B&lt;/p&gt;
&lt;p&gt;$\hat f = B(B^TB)^{-1}B^Ty = Hy$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;H is the smoother matrix (projection matrix)&lt;/li&gt;
&lt;li&gt;H is idempotent ($H \times H = H$)&lt;/li&gt;
&lt;li&gt;H is symmetric&lt;/li&gt;
&lt;li&gt;Degrees of freedom trace (H)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;smoothing-splines&#34;&gt;Smoothing Splines&lt;/h2&gt;
&lt;h3 id=&#34;bspline-basis-boundary-issue&#34;&gt;Bspline basis boundary issue&lt;/h3&gt;
&lt;p&gt;Consider the following setting with the fixed training data&lt;/p&gt;
&lt;p&gt;$y_i = f(x_i) + \epsilon_i$&lt;/p&gt;
&lt;p&gt;$\epsilon_i \approx iid(0, \sigma^2)$&lt;/p&gt;
&lt;p&gt;$Var(\hat f(x)) = h(x)^T(H^TH)^{-1}h(x)\sigma^2$ (variance of estimated function using spline)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Behavior of splines tends to be sporadic near the boundaries, and extrapolation can be problematic. The main reason is that the complexity of Cubic Spline is more than the complexity of Global Cubic Polynomial, due to the large number of parameters (less bias, more variance). The solution is to use linear splines instead of cubic splines (Natural Cubic Splines).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;natural-cubic-splines&#34;&gt;Natural Cubic Splines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Additional constraints are added to make the function linear beyond the boundary knots&lt;/li&gt;
&lt;li&gt;Assuming the function is linear near the boundaries (where there is less information) is often reasonable&lt;/li&gt;
&lt;li&gt;Cubic spline; linear on $[-\inf, \xi_1]$ and $[\xi_k , \inf]$&lt;/li&gt;
&lt;li&gt;Prediction variance decreases&lt;/li&gt;
&lt;li&gt;The price is the bias near the boundaries&lt;/li&gt;
&lt;li&gt;Degrees of freedom is K, the number of knots&lt;/li&gt;
&lt;li&gt;Each of these basis functions has zero second and third derivative in the linear region.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;penalized-residual-sum-of-squares&#34;&gt;Penalized residual sum of squares&lt;/h2&gt;
&lt;p&gt;$\min_f \frac{1}{n}\sum_{i-1}^n[y_i - f(x_i)]^2+\lambda \int^a_b[f^{&amp;quot;}(x)^2dx]$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first term measures the closeness of the model to the data (related to bias)&lt;/li&gt;
&lt;li&gt;The second term penalizes curvature of the function (related to variance)&lt;/li&gt;
&lt;li&gt;$\lambda$ is the smoothing parameter controlling the trade between bias and variance&lt;/li&gt;
&lt;li&gt;$\lambda = 0$ interpolate the data (overfitting)&lt;/li&gt;
&lt;li&gt;$\lambda = \inf$ linear least-square regression (underfitting)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It can be shown that the minimizer is a natural cubic spline.&lt;/p&gt;
&lt;p&gt;Solution: $\hat \theta = (N^TN + \lambda\Omega)^{-1}N^Ty$
, $\Omega$ represents the second derivative&lt;/p&gt;
&lt;p&gt;$ f = (N^TN + \lambda\Omega)^{-1}N^Ty = S_\lambda y$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Smoothing spline estimator is a linear smoother&lt;/li&gt;
&lt;li&gt;$S_\lambda$ is the smoother matrix&lt;/li&gt;
&lt;li&gt;$S_\lambda$ is NOT idempotent&lt;/li&gt;
&lt;li&gt;$S_\lambda$ is symmetric&lt;/li&gt;
&lt;li&gt;$S_\lambda$ is positive definite&lt;/li&gt;
&lt;li&gt;Degrees of freedom: trace($S_\lambda$)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;choice-of-tuning-parameters&#34;&gt;Choice of Tuning Parameters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Train Test Validation
&lt;img src=&#34;https://ayushsubedi.github.io/img/pt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cross Validation
If an independent validation dataset is not affordable, the K-fold cross validation or leave-one-out CV can be useful&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Akaike Information Criteria (AIC)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bayesian Information Criteria (BIC)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generalized Cross-validation&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kernel-smoothers&#34;&gt;Kernel Smoothers&lt;/h2&gt;
&lt;h3 id=&#34;k-nearest-neighbor-knn&#34;&gt;K-Nearest Neighbor (KNN)&lt;/h3&gt;
&lt;p&gt;KNN Average $\hat f(x_0) = \sum_{i=1}^nw(x_0, x_i)y_i$&lt;/p&gt;
&lt;p&gt;where $\sum_{i=1}^nw(x_0, x_i)$ = $\frac{1}{K}$ if $x_i \in N_k(x_0)$ else $0$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple average of the k nearest observations to $x_0$ (local averaging)&lt;/li&gt;
&lt;li&gt;Equal weights are assigned to all neighbors&lt;/li&gt;
&lt;li&gt;However, the fitted function is in the form of a step function (non-smooth function)&lt;/li&gt;
&lt;li&gt;Also, the bias is quite high&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kernel-function&#34;&gt;Kernel Function&lt;/h3&gt;
&lt;p&gt;Any non-negative real-valued integrable function that satisfies the following conditions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\int_{-\inf}^{\inf}K(u)du=1$&lt;/li&gt;
&lt;li&gt;K is an even function; $K(-u) = K(u)$&lt;/li&gt;
&lt;li&gt;It has a finite second moment; $u^2\int_{-\inf}^{\inf}K(u)du &amp;lt; \inf$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kernel-smoother-regression&#34;&gt;Kernel Smoother Regression&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Kernel Regression&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is weighted local averaging that fits a simple model separately at each query point $x_0$&lt;/li&gt;
&lt;li&gt;More weights are assigned to closer observation&lt;/li&gt;
&lt;li&gt;Localization is defined by the weighting function&lt;/li&gt;
&lt;li&gt;Kernel regression requires little training, all calculations get done at the evaluation time.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/kregression.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;choice-of-lambda&#34;&gt;Choice of $\lambda$&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$\lambda$ defines the width of the neighbourhood&lt;/li&gt;
&lt;li&gt;Only points withing $[x_0-\lambda, x_0+\lambda]$ receive positive weights&lt;/li&gt;
&lt;li&gt;Smaller $\lambda$: rough estimate, larger bias, smaller variance&lt;/li&gt;
&lt;li&gt;Larger $\lambda$: smoother estimate, smaller bias, larger variance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cross-validation can be used for determining of $\lambda$:&lt;/p&gt;
&lt;h3 id=&#34;drawbacks-of-local-averaging&#34;&gt;Drawbacks of Local Averaging&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The local averaging can be biased on the boundaries of the domain due to the asymmetry of the kernel in that region.&lt;/li&gt;
&lt;li&gt;This can be solved by local linear regression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/nw_kernel.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Local linear regression corrects the bias on the boundaries&lt;/li&gt;
&lt;li&gt;Local polynomial regression corrects the bias in the curvature region&lt;/li&gt;
&lt;li&gt;However, local polynomial regression is complex due to higher order of polynomials, therefore, it increases the prediction variance.&lt;/li&gt;
&lt;li&gt;A good solution would be to use local linear model for points in the boundaries, and local quadratic regression in the interior regions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;functional-principal-component&#34;&gt;Functional Principal Component&lt;/h2&gt;
&lt;p&gt;Similar to PCA, FPCA aims to reduce the dimension of functional data by extracting a small set of uncorrelated features, which capture the most of the variation.&lt;/p&gt;
&lt;p&gt;Functional data (observed signals) are comprised of two main components. The first component is the continuous functional mean, and the second component is the error term, that is, the realizations from a stochastic process with mean function 0 and covariance function $C(t, t^&amp;rsquo;)$. It includes both random noise and signal-to-signal variations&lt;/p&gt;
&lt;p&gt;$s_i(t) = \mu(t) + \epsilon_i(t)$&lt;/p&gt;
&lt;p&gt;The mean function is common across all signals (notice that it does not have the $i$ subscript)&lt;/p&gt;
&lt;p&gt;Since signal variance comes from the noise function, we first focus on this for dimensionality reduction using the Karhunen-Loeve Theorem.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/kl.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The variance of $\xi_{ik}$ quickly decays with k. Therefore, only a few $\xi_{ik}$ also known as FPC-scores, would be enough to accurately approximate the noise function. That is,&lt;/p&gt;
&lt;p&gt;$\epsilon_i(t) \approx \sum_{k=1}^K \xi_{ik}\phi_{k}(t)$&lt;/p&gt;
&lt;p&gt;Signals decomposition is given by&lt;/p&gt;
&lt;p&gt;$s_i(t) = \mu(t) + \epsilon_i(t) \implies \mu(t) + \sum_{k=1}^K \xi_{ik}\phi_{k}(t)$&lt;/p&gt;
&lt;h2 id=&#34;model-estimation&#34;&gt;Model Estimation&lt;/h2&gt;
&lt;p&gt;Both the mean and covariance is unknown, and should be measured using training data. In practice, we have two types of signals/data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Complete signals: Sampled regularly&lt;/li&gt;
&lt;li&gt;Incomplete signals: Sampled irregularly, sparse, fragmented&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;steps-for-fpca-when-the-signals-are-incomplete&#34;&gt;Steps for FPCA when the signals are incomplete:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Estimate the mean function using local linear regression&lt;/li&gt;
&lt;li&gt;Estimate the raw covariance function using the estimated mean function&lt;/li&gt;
&lt;li&gt;Estimate the covariance surface using local quadratic regression&lt;/li&gt;
&lt;li&gt;Compute the Eigen functions&lt;/li&gt;
&lt;li&gt;Compute the FPC scores&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;image-analysis&#34;&gt;Image Analysis&lt;/h1&gt;
&lt;h2 id=&#34;introduction-to-image-processing&#34;&gt;Introduction to Image Processing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The process of processing raw images and extracting useful information for decision making.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Level 0:&lt;/strong&gt; Image representation (acquisition, sampling, quantization, compression)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Level 1:&lt;/strong&gt; Image to Image transformations (enhancement, filtering, restoration, smoothing, segmentation)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Level 2:&lt;/strong&gt; Image to vector transformation (feature extraction and dimension reduction)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Level 3:&lt;/strong&gt; Feature to decision mapping&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/image_analysis.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-is-an-image&#34;&gt;What is an Image?&lt;/h2&gt;
&lt;p&gt;A gray (color-RGB) image is a 2-D (3-D) light intensity function, $f (x_1, x_2)$, where $f$ measures brightness at position $f(x_1, x_2)$ . A digital gray (color) image is a representation of an image by a 2-D (3-D) array of discrete samples. &lt;strong&gt;Pixel&lt;/strong&gt; is referred to an element of the array.&lt;/p&gt;
&lt;p&gt;Possible values each pixel can have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Black and white image: 2&lt;/li&gt;
&lt;li&gt;8-bit Gray image: 256&lt;/li&gt;
&lt;li&gt;RGB: 256 x 256 x 256 = 16777216&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;basic-manipulation-in-python&#34;&gt;Basic Manipulation in Python&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import cv2

# Load the image
image_path = &amp;#39;your_image.jpg&amp;#39;  # Replace with the path to your image
original_image = cv2.imread(image_path)

# Check if the image was loaded successfully
if original_image is None:
    print(&amp;#34;Error: Could not open or find the image.&amp;#34;)
else:
    # Convert the image to grayscale
    gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)

    # Convert the grayscale image to black and white using thresholding
    _, binary_image = cv2.threshold(gray_image, 128, 255, cv2.THRESH_BINARY)

    # Resize the image (e.g., to a width of 800 pixels while maintaining aspect ratio)
    new_width = 800
    aspect_ratio = original_image.shape[1] / original_image.shape[0]
    new_height = int(new_width / aspect_ratio)
    resized_image = cv2.resize(binary_image, (new_width, new_height))

    # Save the processed images to disk
    cv2.imwrite(&amp;#39;gray_image.jpg&amp;#39;, gray_image)
    cv2.imwrite(&amp;#39;black_and_white_image.jpg&amp;#39;, binary_image)
    cv2.imwrite(&amp;#39;resized_image.jpg&amp;#39;, resized_image)

    print(&amp;#34;Images processed and saved successfully.&amp;#34;)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;image-transformation&#34;&gt;Image Transformation&lt;/h2&gt;
&lt;h3 id=&#34;image-histogram&#34;&gt;Image Histogram&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Histogram represents the distribution of gray levels.&lt;/li&gt;
&lt;li&gt;It is an estimate of the probability density function (pdf) of the underlying random process.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Image can be transformed by applying a function on the image matrix.&lt;/p&gt;
&lt;p&gt;$g(x,y) = T(f(x,y))$&lt;/p&gt;
&lt;p&gt;For example if a threshold function is sued as the transformation function a gray-scale image can be converted to a BW image.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/step_function.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The brightness of an image can be changed by shifting its histogram.&lt;/li&gt;
&lt;li&gt;The contrast of an image is defined by the difference in maximum and minimum pixel intensity.&lt;/li&gt;
&lt;li&gt;Gray level resolution refers to change in the shades or levels of gray in an image.&lt;/li&gt;
&lt;li&gt;The number of different colors in an image depends on bits per pixel (bpp).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$L = 2^{bpp}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gray level transformation is often used for image enchantment.&lt;/li&gt;
&lt;li&gt;Three typical transformation functions are:
&lt;ul&gt;
&lt;li&gt;Linear (negative image)&lt;/li&gt;
&lt;li&gt;Log&lt;/li&gt;
&lt;li&gt;Power-Law&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convolution-and-image-filtering&#34;&gt;Convolution and image filtering&lt;/h3&gt;
&lt;p&gt;The convolution of functions $f$ and $g$ is defined by:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/convolutions.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Convolution is widely used in image processing for denoising, blurring, sharpening, embossing, and edge detection.&lt;/li&gt;
&lt;li&gt;Image filter is a convolution of a mask (aka kernel, and convolution matrix) with an image that can be used for blurring, sharpening, edge detection, etc.&lt;/li&gt;
&lt;li&gt;A mask is a matrix convolved with an image.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;image-convolution-with-a-mask&#34;&gt;Image Convolution with a Mask&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Flip the mask (kernel) both horizontally and vertically.&lt;/li&gt;
&lt;li&gt;Put the center element of the mask at every pixel of the image. Multiply the corresponding elements and then add them up. Replace the pixel value corresponding to the center of the mask with the resulting sum.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/convolution.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For pixels on the border of image matrix, some elements of the mask might fall out of the image matrix. In this case, we can extend the image by adding zeros. This is known as padding.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/padding.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;denoising-of-smooth-images-using-splines&#34;&gt;Denoising of Smooth Images using Splines&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Another approach for denoising smooth images is to use local regression with smooth basis (eg. splines)&lt;/li&gt;
&lt;li&gt;Using Kronecker product, a 2D-spline basis can be generated from 1D basis matrices&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;image-segmentation&#34;&gt;Image Segmentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The main goal of image segmentation is to partition an image into multiple sets of pixels (segments)&lt;/li&gt;
&lt;li&gt;Image segmentation has been widely used for object detection, face and fingerprint recognition, medical imaging, video surveillance, etc.&lt;/li&gt;
&lt;li&gt;Various methods exist for image segmentation including:
&lt;ul&gt;
&lt;li&gt;Local and global thresholding&lt;/li&gt;
&lt;li&gt;Otsu&amp;rsquo;s method&lt;/li&gt;
&lt;li&gt;K-means clustering&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Thresholding is a simple segmentation approach that converts grayscale image to binary image by applying the thresholding function on histogram.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;otsus-method&#34;&gt;Otsu&amp;rsquo;s Method&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The goal is to automatically determine the threshold $t$ given an image histogram.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Otsu%27s_Method_Visualization.gif/440px-Otsu%27s_Method_Visualization.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Get the histogram of the image&lt;/li&gt;
&lt;li&gt;Calculate group mean and variance&lt;/li&gt;
&lt;li&gt;Find the maximum value for the variance&lt;/li&gt;
&lt;li&gt;Threshold the image&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import numpy as np

def compute_otsu_criteria(im, th):
    &amp;#34;&amp;#34;&amp;#34;Otsu&amp;#39;s method to compute criteria.&amp;#34;&amp;#34;&amp;#34;
    # create the thresholded image
    thresholded_im = np.zeros(im.shape)
    thresholded_im[im &amp;gt;= th] = 1

    # compute weights
    nb_pixels = im.size
    nb_pixels1 = np.count_nonzero(thresholded_im)
    weight1 = nb_pixels1 / nb_pixels
    weight0 = 1 - weight1

    # if one of the classes is empty, eg all pixels are below or above the threshold, that threshold will not be considered
    # in the search for the best threshold
    if weight1 == 0 or weight0 == 0:
        return np.inf

    # find all pixels belonging to each class
    val_pixels1 = im[thresholded_im == 1]
    val_pixels0 = im[thresholded_im == 0]

    # compute variance of these classes
    var1 = np.var(val_pixels1) if len(val_pixels1) &amp;gt; 0 else 0
    var0 = np.var(val_pixels0) if len(val_pixels0) &amp;gt; 0 else 0

    return weight0 * var0 + weight1 * var1

im = # load your image as a numpy array.
# For testing purposes, one can use for example im = np.random.randint(0,255, size = (50,50))

# testing all thresholds from 0 to the maximum of the image
threshold_range = range(np.max(im)+1)
criterias = [compute_otsu_criteria(im, th) for th in threshold_range]

# best threshold is the one minimizing the Otsu criteria
best_threshold = threshold_range[np.argmin(criterias)]
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;k-means-clustering-method&#34;&gt;K-Means Clustering Method&lt;/h3&gt;
&lt;p&gt;K-means clustering is a method for partitioning a set of observations to K clusters, such that the within-cluster variation is minimized.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rearrange the image pixels such that the number of rows in the resulting matrix is equal to the number of pixels and the number of columns is the same as the number of color channels&lt;/li&gt;
&lt;li&gt;Randomly select K centers&lt;/li&gt;
&lt;li&gt;Assign each pixel to the closest cluster&lt;/li&gt;
&lt;li&gt;Update the cluster mean&lt;/li&gt;
&lt;li&gt;Repeat the last two process until convergence&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The objective of K-means is to minimize the within cluster variation, and maximize the inter-class variation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;edge-detection&#34;&gt;Edge Detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Edges are significant local changes of intensity in an image.&lt;/li&gt;
&lt;li&gt;Edge Detection: Detect pixel with sudden intensity change&lt;/li&gt;
&lt;li&gt;Often points that lie on an edge are detected by:
&lt;ul&gt;
&lt;li&gt;Detecting the local &lt;strong&gt;maxima&lt;/strong&gt; or &lt;strong&gt;minima&lt;/strong&gt; of the first derivative.&lt;/li&gt;
&lt;li&gt;Detecting the &lt;strong&gt;zero-crossings&lt;/strong&gt; of the second derivative.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/edge.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;sobel-operator&#34;&gt;Sobel Operator&lt;/h3&gt;
&lt;p&gt;The Sobel operator works by convolving an image with a pair of 3x3 kernels or filters, one for detecting edges in the horizontal direction (often referred to as the Sobel-X operator) and the other for detecting edges in the vertical direction (often referred to as the Sobel-Y operator). These kernels are as follows:&lt;/p&gt;
&lt;p&gt;Sobel-X Kernel:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;-1  0  1
-2  0  2
-1  0  1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Sobel-Y Kernel:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;-1 -2 -1
 0  0  0
 1  2  1
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;krisch-operator&#34;&gt;Krisch Operator&lt;/h3&gt;
&lt;p&gt;Krisch is another derivative mask that finds the maximum edge strength in eight directions of a compass.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/kirsh.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It is more time consuming compare to Sobel&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;prewitt-mask&#34;&gt;Prewitt Mask&lt;/h3&gt;
&lt;p&gt;The Prewitt operator, like the Sobel operator, employs a pair of 3x3 convolution kernels, one for detecting edges in the horizontal direction and the other for detecting edges in the vertical direction.&lt;/p&gt;
&lt;p&gt;Here are the two Prewitt kernels:&lt;/p&gt;
&lt;p&gt;Prewitt-X Kernel:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;-1  0  1
-1  0  1
-1  0  1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Prewitt-Y Kernel:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;-1 -1 -1
 0  0  0
 1  1  1
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;laplacian-and-laplacian-of-gaussian-mask&#34;&gt;Laplacian and Laplacian of Gaussian Mask&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Laplacian mask is a second order derivative mask.&lt;/li&gt;
&lt;li&gt;For noisy images, is combined with a Gaussian mask to reduce the noise&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;0  1  0
1 -4  1
0  1  0
&lt;/code&gt;&lt;/pre&gt;&lt;blockquote&gt;
&lt;p&gt;Laplacian, Sobel, and Prewitt are masks used for edge detection. Gaussian is not a mask for edge detection.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>