<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ai on Ayush Subedi</title>
    <link>https://ayushsubedi.github.io/tags/ai/</link>
    <description>Recent content in ai on Ayush Subedi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 04 Apr 2021 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ayushsubedi.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Diabetic Retinopathy and Glaucoma Detection</title>
      <link>https://ayushsubedi.github.io/posts/cheers_ai/</link>
      <pubDate>Sun, 04 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/cheers_ai/</guid>
      <description>&lt;h1 id=&#34;cheers-ai&#34;&gt;Cheers AI&lt;/h1&gt;
&lt;h1 id=&#34;diabetic-retinopathy-and-glaucoma-detection&#34;&gt;Diabetic Retinopathy and Glaucoma Detection&lt;/h1&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;h4 id=&#34;efficient-prediction-models&#34;&gt;Efficient Prediction Models&lt;/h4&gt;
&lt;p&gt;Efficient models trained on Inception-v3, with weightage on recall.&lt;/p&gt;
&lt;h4 id=&#34;patient-tracking&#34;&gt;Patient Tracking&lt;/h4&gt;
&lt;p&gt;Powerful MIS to create and track patient, and patient&amp;rsquo;s historical predictions.&lt;/p&gt;
&lt;h4 id=&#34;continuous-learning&#34;&gt;Continuous Learning&lt;/h4&gt;
&lt;p&gt;Inputs reviewed by opthalmologists and added to training.&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;h3 id=&#34;the-rationale-for-developing-countries&#34;&gt;The rationale for developing countries.&lt;/h3&gt;
&lt;h3 id=&#34;diabetic-retinopathy&#34;&gt;Diabetic Retinopathy&lt;/h3&gt;
&lt;p&gt;Diabetic Retinopathy is an eye illness caused by diabetes that may lead to vision impairment and even to blindness if it isn&amp;rsquo;t identified and treated early. Of the estimated 422 million diabetics globally, more than 148 million have DR and 48 million have Vision Threating DR (VTDR).&lt;/p&gt;
&lt;p&gt;However, because of insufficient specialists and eye care health workers globally as well as locally to screen everyone at risk, the situation seems acute especially in developing countries like Nepal. Besides, Nepal has difficult geographical terrain and people living in remorse remote areas with limited or no access to clinics and screening facilities making the condition even worse.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://camo.githubusercontent.com/a6b040d6eca19246121fb7a4e3fca782ed625c42af1889778c4edf14151198ef/68747470733a2f2f6761647364656e6579652e636f6d2f77702d636f6e74656e742f75706c6f6164732f64696162657469632d726574696e6f70617468792d766563746f722e6a7067&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;glaucoma&#34;&gt;Glaucoma&lt;/h3&gt;
&lt;p&gt;Glaucoma is a diverse group of disorders representing the second prominent cause of blindness. It has already affected 91 million individuals all over the world. It has multiple risk factors such as older age, elevated intraocular pressure (IOP), and thinner central corneal thickness etc. However, one or more of these risk factors may or may not develop glaucoma making it difficult for accurate prediction of the disease. Additionally, since glaucoma can be asymptomatic, its detection before significant vision loss is critical. Hence, automated methods for predicting glaucoma could have a significant impact.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://camo.githubusercontent.com/71e24e233d0fc826e10230944b2a1cdfba81b4387862f899f1f7c400330b02c6/68747470733a2f2f7777772e696e6d6564706861726d612e636f6d2f77702d636f6e74656e742f75706c6f6164732f323032302f30352f476c6175636f6d612d636f6d70617265642d746f2d6e6f726d616c2d766973696f6e2e706e67&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;an-intuitive-app&#34;&gt;An intuitive app&lt;/h3&gt;
&lt;p&gt;Easy to use, access managed platform, with the primary focus on providing assistance to our opthalmologists.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cheersai.ml/static/img/demo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;process&#34;&gt;Process&lt;/h1&gt;
&lt;h1 id=&#34;glaucoma-prediction&#34;&gt;Glaucoma Prediction&lt;/h1&gt;
&lt;h2 id=&#34;what-worked-90-accuracy&#34;&gt;What worked? (90% accuracy)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;densenet sequential with ben on himanchu dataset, using NLLLoss criterion, Adam optimizer&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;limitation&#34;&gt;Limitation&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;very much dependent on dataset&lt;/li&gt;
&lt;li&gt;disk extraction is good but is very subjective to the dataset&lt;/li&gt;
&lt;li&gt;trained on very small dataset&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;preliminary&#34;&gt;Preliminary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a gmail account (&lt;a href=&#34;mailto:glaucomadetection@gmail.com&#34;&gt;glaucomadetection@gmail.com&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; understand the difference between possibility of glaucoma by classification (vs measurements)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; ben transformation&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; extract disk from fundus images&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; improve extraction algorithms&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; perform EDA on disk image to find troubling images (cases where crop does not work)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; convert python function to extract disk to torch transform class (failed)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; transformation to disk during training failed. create a disk dataset before training the model.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; train on new dataset with and without ben transformation&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; handle imbalanced class with class weighting&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; convert Kaggle dataset to the format that we have templated our notebooks with&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; for kaggle dataset get disks using new algorithm&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;obseverations-in-regards-to-disk-generation&#34;&gt;Obseverations in regards to disk generation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;extraction of disk does not help (too many vague areas left unfilled)&lt;/li&gt;
&lt;li&gt;however, cropping shows very good promise&lt;/li&gt;
&lt;li&gt;but, cropping requires somewhat similar of fundus images&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;datasets&#34;&gt;Datasets&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; find datasets &lt;a href=&#34;https://deepblue.lib.umich.edu/data/concern/data_sets/3b591905z&#34;&gt;https://deepblue.lib.umich.edu/data/concern/data_sets/3b591905z&lt;/a&gt;, &lt;a href=&#34;https://www.kaggle.com/andrewmvd/ocular-disease-recognition-odir5k&#34;&gt;https://www.kaggle.com/andrewmvd/ocular-disease-recognition-odir5k&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a dataset from Magrabia&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a dataset from Messidor&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a dataset from Ocular Disease Recognition&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create EDA on non measurement dataset (Ocular Disease Recognition)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a dataset from ocular disease recognition to include normal and glaucoma images&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; (Kaggle dataset, custom generated, filtered)https://www.kaggle.com/sshikamaru/glaucoma-detection?select=glaucoma.csv&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; train on Kaggle dataset (without changing anything)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inception v3 with and without ben on ocular, kaggle, and himanchu dataset&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inception v3 with ben on ocular, kaggle, and himanchu dataset (disk extracted, normal, and cropped dataset)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; densenet linear with ben on ocular, kaggle, and himanchu dataset&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; densenet linear with ben on ocular, kaggle, and himanchu dataset (disk extracted, normal, and cropped dataset)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; densenet sequential with ben on ocular, kaggle, and himanchu dataset&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; densenet sequential with ben on ocular, kaggle, and himanchu dataset (disk extracted, normal, and cropped dataset)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add datasets from cheers for testing&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add datasets from cheers for training&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;diabetic-retinopathy-prediction&#34;&gt;Diabetic Retinopathy Prediction&lt;/h1&gt;
&lt;h2 id=&#34;what-worked-90-accuracy-1&#34;&gt;What worked? (90% accuracy)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Large dataset from EyePACS (Kaggle competition used training (30%) and testing data (70%) from Kaggle. After the competition, the labels were published). Flipped the ratios for our use case.&lt;/li&gt;
&lt;li&gt;Remove out of focus images&lt;/li&gt;
&lt;li&gt;Remove too bright, and too dark images.&lt;/li&gt;
&lt;li&gt;Link to clean dataset &lt;a href=&#34;https://www.kaggle.com/ayushsubedi/drunstratified&#34;&gt;https://www.kaggle.com/ayushsubedi/drunstratified&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;To handle class imbalanced issue, used weighted random samplers. Undersampling to match no of images in the least class (4) did not work. Pickled weights for future use.&lt;/li&gt;
&lt;li&gt;Ben Graham transformation and augmentations&lt;/li&gt;
&lt;li&gt;Inception v3 fine tuning, with aux logits trained (better results compared to other architecture)&lt;/li&gt;
&lt;li&gt;Perform EDA on inference to observe what images were causing issues&lt;/li&gt;
&lt;li&gt;Removed the images and created another dataset (Link to the new dataset &lt;a href=&#34;https://www.kaggle.com/ayushsubedi/cleannonstratifieddiabeticretinopathy&#34;&gt;https://www.kaggle.com/ayushsubedi/cleannonstratifieddiabeticretinopathy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See 5, 6, and 7&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;todos&#34;&gt;TODOS&lt;/h3&gt;
&lt;h3 id=&#34;datasets-1&#34;&gt;Datasets&lt;/h3&gt;
&lt;p&gt;Binary Stratified (cleaned): &lt;a href=&#34;https://drive.google.com/drive/folders/12-60Gm7c_TMu1rhnMhSZjrkSqqAuSsQf?usp=sharing&#34;&gt;https://drive.google.com/drive/folders/12-60Gm7c_TMu1rhnMhSZjrkSqqAuSsQf?usp=sharing&lt;/a&gt;
Categorical Stratified (cleaned): &lt;a href=&#34;https://drive.google.com/drive/folders/1-A_Mx9GdeUwCd03TUxUS3vwcutQHFFSM?usp=sharing&#34;&gt;https://drive.google.com/drive/folders/1-A_Mx9GdeUwCd03TUxUS3vwcutQHFFSM?usp=sharing&lt;/a&gt;
Non Stratified (cleaned): &lt;a href=&#34;https://www.kaggle.com/ayushsubedi/drunstratified&#34;&gt;https://www.kaggle.com/ayushsubedi/drunstratified&lt;/a&gt;
Recleaned Non Stratified: &lt;a href=&#34;https://www.kaggle.com/ayushsubedi/cleannonstratifieddiabeticretinopathy&#34;&gt;https://www.kaggle.com/ayushsubedi/cleannonstratifieddiabeticretinopathy&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;priliminary&#34;&gt;Priliminary&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a new gmail account to store datasets (&lt;a href=&#34;mailto:diabeticretinopathyglaucoma@gmail.com&#34;&gt;diabeticretinopathyglaucoma@gmail.com&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=VIrkurR446s&amp;amp;ab_channel=khanacademymedicine&#34;&gt;https://www.youtube.com/watch?v=VIrkurR446s&amp;amp;ab_channel=khanacademymedicine&lt;/a&gt; What is diabetic retinopathy?&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; collect all previous analysis notebooks&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; conduct preliminary EDA (for balanced dataset, missing images etc)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create balanced test train split for DR (stratify)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; store the dataset in drive for colab&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; identify a few research papers, create a file to store subsequently found research papers&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; identify right technology stack to use (for ML, training, PM, model versioning, stage deployment, actual deployment)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; perform basic augmentation&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a version 0 base model&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; apply a random transfer learning model&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a metric for evaluation&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; store the model in zenodo, or find something for version control&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a model that takes image as an input&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a streamlit app that reads model&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; streamlit app to upload and test prediction&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; test deployment to free tier heroku&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; identify gaps&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create priliminary test set&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create folder structures for saved model in the drive&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; figure out a way to move files from kaggle to drive (without download/upload)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; research saving model (the frugal way)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; research saving model to google drive after each epoch so that during unforseen interuptions, the training of the model can be continued&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;resource&#34;&gt;Resource&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; upgrade to 25GB RAM in Google Colab possibly w/ Tesla P100 GPU&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; upgrade to Colab Pro&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;baseline&#34;&gt;Baseline&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; medicmind grading (accuracy: 0.8)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; medicmind classification (0.47)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;transfer-learning&#34;&gt;Transfer Learning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; resnet&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; alexnet&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; vgg&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; squeezenet&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; densenet&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inception&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; efficient net&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;dataset-clean-images&#34;&gt;Dataset clean images&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a backup of primary dataset (zip so that kaggle kernels can consume them too)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; find algorithms to detect black/out of focus images&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; identify correct threshold for dark and out of focus images&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; remove black images&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; remove out of focus images&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a stratified dataset with 2015 data only (convert train and test both to train and use), remove black images and out of focus images (also create test set)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create non-stratified dataset with 2015 clean data only (train, test, valid) (upload in kaggle if google drive full)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a binary dataset (train, test, valid)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create confusion matrices (train, test, valid) after clean up (dark and blurry)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; the model is confusing labels 0 and 1 as 2, is this due to disturbance in image in 0.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; concluded that the result is due to the model not capturing class 0 enough (due to undersampling)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;inference&#34;&gt;Inference&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a csv with preds probability and real label&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; calculate recall, precision, accuracy, confusion matrix&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; identify different prediction issues&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; relationship between difference in preds and accuracy&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inference issue: labels 0 being predicted as 4&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inference issue: Check images from Grade 2, 3 being predicted as Grade 0&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inference issue: Check images from Grade 4 being predicted as Grade 0&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inference issue: Check images from Grade 0 being predicted as Grade 4&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inference issue: A significant Grade 2 is being predicted as Grade 0&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inference issue: More than 50% of Grade 1 is being predicted as Grade 0&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a new dataset&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;model-improvement&#34;&gt;Model Improvement&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; research kaggle winning augmentation for DR&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; research appropriate augmentation: optical distortion, grid distortion, piecewise affine transform, horizontal flip, vertical flip, random rotation, random shift, random scale, a shift of RGB values, random brightness and contrast, additive Gaussian noise, blur, sharpening, embossing, random gamma, and cutout&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; train on various pretrained models or research which is supposed to be ideal for this case &lt;a href=&#34;https://pytorch.org/vision/stable/models.html&#34;&gt;https://pytorch.org/vision/stable/models.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create several neural nets (test different layers)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; experiment with batch size&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Reducing lighting-condition effects&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Cropping uninformative area&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Create custom dataloader based on ben graham kaggle winning strategy&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; finetune vs feature extract&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; oversample&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; undersample&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add specificity and sensitivity to indicators&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create train loss and valid loss charts&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; test regression models (treat this as a grading problem)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; pickle weights&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;additional-models&#34;&gt;Additional Models&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; check if left/right eye classification model is required&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;additional-datasets&#34;&gt;Additional datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; make datasets more extensive (add test dataset with recoverd labels to train dataset 2015)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add APTOS dataset&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; request labelled datasets from cheers&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add datasets from cheers for testing&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add datasets from cheers for training&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;test-datasets&#34;&gt;Test datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; find datasets for testing (dataset apart from APTOS and EyePACS)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; update folder structures to match our use case&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; find dataset for testing after making sure old test datasets are not in vaid/train (4 will be empty)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;conceptsresearch-papers&#34;&gt;Concepts/Research Papers&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; read reports from kaggle competition winning authors&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Deep Learning Approach to Diabetic Retinopathy Detection &lt;a href=&#34;https://arxiv.org/pdf/2003.02261.pdf&#34;&gt;https://arxiv.org/pdf/2003.02261.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Google research &lt;a href=&#34;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45732.pdf&#34;&gt;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45732.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Nature article &lt;a href=&#34;https://www.nature.com/articles/s41746-019-0172-3&#34;&gt;https://www.nature.com/articles/s41746-019-0172-3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; read ravi&amp;rsquo;s article&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;a href=&#34;https://deim.urv.cat/~itaka/itaka2/PDF/acabats/PhD_Thesis/TESI_doctoral_Jordi_De_la_Torre.pdf&#34;&gt;https://deim.urv.cat/~itaka/itaka2/PDF/acabats/PhD_Thesis/TESI_doctoral_Jordi_De_la_Torre.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; what can go wrong &lt;a href=&#34;https://yerevann.github.io/2015/08/17/diabetic-retinopathy-detection-contest-what-we-did-wrong/&#34;&gt;https://yerevann.github.io/2015/08/17/diabetic-retinopathy-detection-contest-what-we-did-wrong/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;a href=&#34;https://arxiv.org/pdf/1902.07208.pdf&#34;&gt;https://arxiv.org/pdf/1902.07208.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; identify more papers&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>