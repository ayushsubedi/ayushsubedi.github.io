<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on Ayush Subedi</title>
    <link>https://ayushsubedi.github.io/tags/ai/</link>
    <description>Recent content in AI on Ayush Subedi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 24 Jun 2025 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ayushsubedi.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Adieu Cloudfactory</title>
      <link>https://ayushsubedi.github.io/posts/cloudfactory_reflections/</link>
      <pubDate>Tue, 24 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/cloudfactory_reflections/</guid>
      <description>&lt;h1 id=&#34;adieu-cloudfactory&#34;&gt;Adieu Cloudfactory&lt;/h1&gt;
&lt;p&gt;I spent nearly four years at Cloudfactory, and every time I try to wrap my head around what it meant to me, I’m reminded that careers,like products or cities, aren’t made of logos or titles. They’re made of people, problems, alignment, late nights, bugs, breakthroughs, and sometimes… a baby.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/cf1.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;When I joined, I was already deep into data. I had built models, written pipelines, led teams. But Cloudfactory introduced a different kind of challenge: earning the trust of the business in those models. It wasn’t just about proving something worked, it was about influencing decision-makers to see the value in data-driven thinking. It was about guiding other data professionals too, helping them push for solutions that were not only technically sound, but practically adopted. Cloudfactory gave me a rare blend of freedom and responsibility, and it taught me that shaping decisions often matters more than shipping code.&lt;/p&gt;
&lt;p&gt;At the time, the company was scaling fast. Our core platform was being rebuilt, our data infrastructure was stitched together from legacy systems, and core metrics, well, they depended on who you asked. One of my earliest tasks was reconciling two dashboards showing daily active workers. Both technically correct. Both contextually wrong. That was the first of many fires I learned to enjoy putting out.&lt;/p&gt;
&lt;p&gt;Between those fires and the deeper rebuilds, we started growing a team. First, it was just a handful of engineers across scattered projects. Then came the analysts, the scientists, the PMs. Eventually, I was leading a 25-person organization across data science, engineering, and research, including a focused team of eight working on AV Lidar, simulation, fraud detection, unsupervised learning, and workforce modeling. Shaping that team and helping it grow into its own identity was one of the most fulfilling parts of my journey.&lt;/p&gt;
&lt;p&gt;And of course, there were people. So many brilliant, humble, and absurdly kind people. Folks like Prakash, who joined me in late-night rabbit holes to investigate annotation throughput spikes that made no sense. Reema, who brought rigor and process to our first serious data QA workflows. Suranjan, who debugged a production issue by literally drawing the logic upside down on a whiteboard,because sometimes you just have to look at a problem differently. It worked.&lt;/p&gt;
&lt;p&gt;In parallel, life outside work was happening too. Somewhere along the way, I finished my Master’s in Analytics from Georgia Tech. I remember switching between stakeholder meetings and thesis work, from forecasting models to future-of-AI seminars. And then I became a father. Context-switching between machine learning infrastructure and diaper changes became the new rhythm of life.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/cf3.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;There were moments of doubt too. Was the work we were doing impactful enough? Was I doing enough? But then I’d talk to a delivery owner in Kenya or a product manager in Reading and hear how our changes helped them improve SLAs or understand quality breakdowns. That’s when it hit me, good data work doesn’t scream. It hums quietly in the background, making the rest of the machine work better.&lt;/p&gt;
&lt;p&gt;In the later chapters, we started leaning into LLMs,not for the buzz, but for real operational use. We built early systems that flagged potential fraud, extracted and classified topics from agent conversations, and surfaced actionable themes hidden in text feedback. It was buggy, early, and exciting. Alongside that, I was also working on prescriptive analytics, simulation frameworks, pricing models, and experimentation strategies to help the business make better, faster decisions. These tools helped us derisk client engagements and secure new contracts, while also shifting the mindset from dashboards to predictions, and from insights to actions.&lt;/p&gt;
&lt;p&gt;I also had the chance to pay it forward, whether by mentoring early-career scientists, co-leading events like AI for Managers and Data Career Mapping, or hiring and onboarding associates who reminded me exactly why I chose this field. In our onboarding tradition, we were asked to share a favorite quote. I chose: &amp;ldquo;Be who you needed when you were younger.&amp;rdquo; Four years later, that quote means even more. If I managed to live by that during my time here, I’m leaving fulfilled.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/cf4.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I don’t work at Cloudfactory anymore, but I carry it with me,the late nights, the post-mortems, the product councils, the joy of finally getting a Prefect DAG to not crash on Mondays. And most of all, the people. They say you don’t remember tasks,you remember how people made you feel. If that’s true, Cloudfactory felt like a place that demanded growth, nurtured curiosity, and let you bring your full self to the table, every single day.&lt;/p&gt;
&lt;p&gt;Talent is everywhere. Opportunity is not. And I’ll keep doing my part to bridge that gap, just as Cloudfactory taught me to.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/cf2.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A reflection on the journey from Data Science to AI engineering</title>
      <link>https://ayushsubedi.github.io/posts/ds_to_ai_engineering/</link>
      <pubDate>Wed, 18 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/ds_to_ai_engineering/</guid>
      <description>&lt;h1 id=&#34;a-reflection-on-the-journey-from-data-science-to-ai-engineering&#34;&gt;A reflection on the journey from Data Science to AI engineering&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; A lot has changed, but no change has been too troublesome. In fact, I sometimes miss the quiet nuances of data science.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/dstoai.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;These days, I spend most of my time building AI systems. Not just models, but agents, automations, workflows that talk to vector databases, orchestrate multiple LLMs, route decisions, and invoke tools, real tools, to take action. I am tuning base models, stacking retrieval pipelines, experimenting with synthetic data, and wrestling with the latency, memory, and quality triangle that comes with building anything remotely intelligent.&lt;/p&gt;
&lt;p&gt;But it was not always this way.&lt;/p&gt;
&lt;p&gt;For most of my career, I lived in the world of data science. It was a quieter kind of work. A Jupyter-notebook-shaped world where progress looked like cleaner metrics, better coverage, tighter confidence intervals, and slowly building trust with stakeholders. It was not about agents reasoning or hallucinating. It was about making sense of the mess, methodically and often manually. And oddly enough, I miss that sometimes.&lt;/p&gt;
&lt;p&gt;In data science, nuance was the whole game. You did not just care about whether the model worked. You cared about whether it was fair, stable, interpretable, and whether the metric matched what the business actually needed. You obsessed over leakage, feature drift, and baseline comparisons. You wrote SQL slowly, like poetry. You explained why precision mattered more than recall, or why a dashboard should lag by a day to avoid false alarms. There was something beautiful about that kind of care.&lt;/p&gt;
&lt;p&gt;Then LLMs exploded. And like many others, I moved.&lt;/p&gt;
&lt;p&gt;What started with basic embeddings and zero-shot prompts quickly turned into full-fledged systems. Retrieval augmented generation, function calling, evaluation frameworks, autonomous agents, memory stores, and even fine-tuning workflows. What fascinates me now is not just what these models can say, but what they can do. They are not just answering questions anymore, they are rewriting the workflows themselves.&lt;/p&gt;
&lt;p&gt;And yet, despite the excitement, I have realized something important. The further you go into AI engineering, the easier it becomes to lose the rigor that data science forced you to have. It is easier to chase demos over decisions. Easier to fall in love with the illusion of intelligence instead of questioning what is underneath. The best of both worlds, AI that works and makes sense, still requires the discipline I learned back in the data science trenches.&lt;/p&gt;
&lt;p&gt;What has been most fulfilling, though, is seeing how the two worlds meet. How all the thinking around metrics, inference patterns, experimentation, and stakeholder communication still shows up, just with different tools and different failure modes. I find myself explaining vector search precision the way I used to explain ROC curves. I still write post-mortems. I still obsess over edge cases. And I still believe that trust is the real deliverable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So what changed?&lt;/strong&gt;&lt;br&gt;
A lot.&lt;br&gt;
But nothing that made me feel out of place.&lt;/p&gt;
&lt;p&gt;If anything, this new chapter has deepened my appreciation for the last one. Data science gave me the instincts. AI engineering is giving me the playground. And between the two, I feel like I am finally building systems that both work and matter.&lt;/p&gt;
&lt;p&gt;Onward.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;or gradient descent?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/a/a3/Gradient_descent.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Diabetic Retinopathy and Glaucoma Detection (Cheers AI Demo)</title>
      <link>https://ayushsubedi.github.io/posts/cheers_ai_demo/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/cheers_ai_demo/</guid>
      <description>&lt;h1 id=&#34;cheers-ai-demo-for-diabetic-retinopathy-and-glaucoma-detection&#34;&gt;Cheers AI Demo for Diabetic Retinopathy and Glaucoma Detection&lt;/h1&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;420&#34; src=&#34;https://www.youtube.com/embed/TXy0J3wydnU&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;h4 id=&#34;efficient-prediction-models&#34;&gt;Efficient Prediction Models&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Efficient models trained on Inception-v3, with weightage on recall.&lt;/li&gt;
&lt;li&gt;Powerful hospital-MIS to create and track patient, and patient&amp;rsquo;s historical predictions.&lt;/li&gt;
&lt;li&gt;Inputs reviewed by opthalmologists and added to training.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;diabetic-retinopathy&#34;&gt;Diabetic Retinopathy&lt;/h3&gt;
&lt;p&gt;Diabetic Retinopathy is an eye illness caused by diabetes that may lead to vision impairment and even to blindness if it isn&amp;rsquo;t identified and treated early. Of the estimated 422 million diabetics globally, more than 148 million have DR and 48 million have Vision Threating DR (VTDR).&lt;/p&gt;
&lt;p&gt;However, because of insufficient specialists and eye care health workers globally as well as locally to screen everyone at risk, the situation seems acute especially in developing countries like Nepal. Besides, Nepal has difficult geographical terrain and people living in remorse remote areas with limited or no access to clinics and screening facilities making the condition even worse.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://camo.githubusercontent.com/a6b040d6eca19246121fb7a4e3fca782ed625c42af1889778c4edf14151198ef/68747470733a2f2f6761647364656e6579652e636f6d2f77702d636f6e74656e742f75706c6f6164732f64696162657469632d726574696e6f70617468792d766563746f722e6a7067&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;glaucoma&#34;&gt;Glaucoma&lt;/h3&gt;
&lt;p&gt;Glaucoma is a diverse group of disorders representing the second prominent cause of blindness. It has already affected 91 million individuals all over the world. It has multiple risk factors such as older age, elevated intraocular pressure (IOP), and thinner central corneal thickness etc. However, one or more of these risk factors may or may not develop glaucoma making it difficult for accurate prediction of the disease. Additionally, since glaucoma can be asymptomatic, its detection before significant vision loss is critical. Hence, automated methods for predicting glaucoma could have a significant impact.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://camo.githubusercontent.com/71e24e233d0fc826e10230944b2a1cdfba81b4387862f899f1f7c400330b02c6/68747470733a2f2f7777772e696e6d6564706861726d612e636f6d2f77702d636f6e74656e742f75706c6f6164732f323032302f30352f476c6175636f6d612d636f6d70617265642d746f2d6e6f726d616c2d766973696f6e2e706e67&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;an-intuitive-app&#34;&gt;An intuitive app&lt;/h3&gt;
&lt;p&gt;Easy to use, access managed platform, with the primary focus on providing assistance to our opthalmologists.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cheersai.ml/static/img/demo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;steps-involved-in-research-model-creation-and-deployment&#34;&gt;Steps involved in Research, Model Creation, and Deployment&lt;/h1&gt;
&lt;h1 id=&#34;glaucoma-prediction&#34;&gt;Glaucoma Prediction&lt;/h1&gt;
&lt;h2 id=&#34;what-worked-90-accuracy&#34;&gt;What worked? (90% accuracy)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;densenet sequential with ben on himanchu dataset, using NLLLoss criterion, Adam optimizer&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;limitation&#34;&gt;Limitation&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;very much dependent on dataset&lt;/li&gt;
&lt;li&gt;disk extraction is good but is very subjective to the dataset&lt;/li&gt;
&lt;li&gt;trained on very small dataset&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;preliminary&#34;&gt;Preliminary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; understand the difference between possibility of glaucoma by classification (vs measurements)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; ben transformation&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; extract disk from fundus images&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; improve extraction algorithms&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; perform EDA on disk image to find troubling images (cases where crop does not work)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; convert python function to extract disk to torch transform class (failed)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; transformation to disk during training failed. create a disk dataset before training the model.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; train on new dataset with and without ben transformation&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; handle imbalanced class with class weighting&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; convert Kaggle dataset to the format that we have templated our notebooks with&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; for kaggle dataset get disks using new algorithm&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;obseverations-in-regards-to-disk-generation&#34;&gt;Obseverations in regards to disk generation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;extraction of disk does not help (too many vague areas left unfilled)&lt;/li&gt;
&lt;li&gt;however, cropping shows very good promise&lt;/li&gt;
&lt;li&gt;but, cropping requires somewhat similar of fundus images&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;datasets&#34;&gt;Datasets&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; find datasets &lt;a href=&#34;https://deepblue.lib.umich.edu/data/concern/data_sets/3b591905z&#34;&gt;https://deepblue.lib.umich.edu/data/concern/data_sets/3b591905z&lt;/a&gt;, &lt;a href=&#34;https://www.kaggle.com/andrewmvd/ocular-disease-recognition-odir5k&#34;&gt;https://www.kaggle.com/andrewmvd/ocular-disease-recognition-odir5k&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a dataset from Magrabia&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a dataset from Messidor&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a dataset from Ocular Disease Recognition&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create EDA on non measurement dataset (Ocular Disease Recognition)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a dataset from ocular disease recognition to include normal and glaucoma images&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; (Kaggle dataset, custom generated, filtered)https://www.kaggle.com/sshikamaru/glaucoma-detection?select=glaucoma.csv&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; train on Kaggle dataset (without changing anything)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inception v3 with and without ben on ocular, kaggle, and himanchu dataset&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inception v3 with ben on ocular, kaggle, and himanchu dataset (disk extracted, normal, and cropped dataset)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; densenet linear with ben on ocular, kaggle, and himanchu dataset&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; densenet linear with ben on ocular, kaggle, and himanchu dataset (disk extracted, normal, and cropped dataset)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; densenet sequential with ben on ocular, kaggle, and himanchu dataset&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; densenet sequential with ben on ocular, kaggle, and himanchu dataset (disk extracted, normal, and cropped dataset)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add datasets from cheers for testing&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add datasets from cheers for training&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;diabetic-retinopathy-prediction&#34;&gt;Diabetic Retinopathy Prediction&lt;/h1&gt;
&lt;h2 id=&#34;what-worked-90-accuracy-1&#34;&gt;What worked? (90% accuracy)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Large dataset from EyePACS (Kaggle competition used training (30%) and testing data (70%) from Kaggle. After the competition, the labels were published). Flipped the ratios for our use case.&lt;/li&gt;
&lt;li&gt;Remove out of focus images&lt;/li&gt;
&lt;li&gt;Remove too bright, and too dark images.&lt;/li&gt;
&lt;li&gt;Link to clean dataset &lt;a href=&#34;https://www.kaggle.com/ayushsubedi/drunstratified&#34;&gt;https://www.kaggle.com/ayushsubedi/drunstratified&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;To handle class imbalanced issue, used weighted random samplers. Undersampling to match no of images in the least class (4) did not work. Pickled weights for future use.&lt;/li&gt;
&lt;li&gt;Ben Graham transformation and augmentations&lt;/li&gt;
&lt;li&gt;Inception v3 fine tuning, with aux logits trained (better results compared to other architecture)&lt;/li&gt;
&lt;li&gt;Perform EDA on inference to observe what images were causing issues&lt;/li&gt;
&lt;li&gt;Removed the images and created another dataset (Link to the new dataset &lt;a href=&#34;https://www.kaggle.com/ayushsubedi/cleannonstratifieddiabeticretinopathy&#34;&gt;https://www.kaggle.com/ayushsubedi/cleannonstratifieddiabeticretinopathy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See 5, 6, and 7&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;datasets-1&#34;&gt;Datasets&lt;/h3&gt;
&lt;p&gt;Binary Stratified (cleaned): &lt;a href=&#34;https://drive.google.com/drive/folders/12-60Gm7c_TMu1rhnMhSZjrkSqqAuSsQf?usp=sharing&#34;&gt;https://drive.google.com/drive/folders/12-60Gm7c_TMu1rhnMhSZjrkSqqAuSsQf?usp=sharing&lt;/a&gt;
Categorical Stratified (cleaned): &lt;a href=&#34;https://drive.google.com/drive/folders/1-A_Mx9GdeUwCd03TUxUS3vwcutQHFFSM?usp=sharing&#34;&gt;https://drive.google.com/drive/folders/1-A_Mx9GdeUwCd03TUxUS3vwcutQHFFSM?usp=sharing&lt;/a&gt;
Non Stratified (cleaned): &lt;a href=&#34;https://www.kaggle.com/ayushsubedi/drunstratified&#34;&gt;https://www.kaggle.com/ayushsubedi/drunstratified&lt;/a&gt;
Recleaned Non Stratified: &lt;a href=&#34;https://www.kaggle.com/ayushsubedi/cleannonstratifieddiabeticretinopathy&#34;&gt;https://www.kaggle.com/ayushsubedi/cleannonstratifieddiabeticretinopathy&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;priliminary&#34;&gt;Priliminary&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=VIrkurR446s&amp;amp;ab_channel=khanacademymedicine&#34;&gt;https://www.youtube.com/watch?v=VIrkurR446s&amp;amp;ab_channel=khanacademymedicine&lt;/a&gt; What is diabetic retinopathy?&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; collect all previous analysis notebooks&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; conduct preliminary EDA (for balanced dataset, missing images etc)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create balanced test train split for DR (stratify)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; store the dataset in drive for colab&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; identify a few research papers, create a file to store subsequently found research papers&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; identify right technology stack to use (for ML, training, PM, model versioning, stage deployment, actual deployment)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; perform basic augmentation&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a version 0 base model&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; apply a random transfer learning model&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a metric for evaluation&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; store the model in zenodo, or find something for version control&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a model that takes image as an input&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a streamlit app that reads model&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; streamlit app to upload and test prediction&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; test deployment to free tier heroku&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; identify gaps&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create priliminary test set&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create folder structures for saved model in the drive&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; figure out a way to move files from kaggle to drive (without download/upload)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; research saving model (the frugal way)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; research saving model to google drive after each epoch so that during unforseen interuptions, the training of the model can be continued&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;resource&#34;&gt;Resource&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; upgrade to 25GB RAM in Google Colab possibly w/ Tesla P100 GPU&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; upgrade to Colab Pro&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;baseline&#34;&gt;Baseline&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; medicmind grading (accuracy: 0.8)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; medicmind classification (0.47)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;transfer-learning&#34;&gt;Transfer Learning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; resnet&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; alexnet&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; vgg&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; squeezenet&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; densenet&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inception&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; efficient net&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;dataset-clean-images&#34;&gt;Dataset clean images&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a backup of primary dataset (zip so that kaggle kernels can consume them too)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; find algorithms to detect black/out of focus images&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; identify correct threshold for dark and out of focus images&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; remove black images&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; remove out of focus images&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a stratified dataset with 2015 data only (convert train and test both to train and use), remove black images and out of focus images (also create test set)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create non-stratified dataset with 2015 clean data only (train, test, valid) (upload in kaggle if google drive full)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a binary dataset (train, test, valid)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create confusion matrices (train, test, valid) after clean up (dark and blurry)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; the model is confusing labels 0 and 1 as 2, is this due to disturbance in image in 0.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; concluded that the result is due to the model not capturing class 0 enough (due to undersampling)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;inference&#34;&gt;Inference&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a csv with preds probability and real label&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; calculate recall, precision, accuracy, confusion matrix&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; identify different prediction issues&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; relationship between difference in preds and accuracy&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inference issue: labels 0 being predicted as 4&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inference issue: Check images from Grade 2, 3 being predicted as Grade 0&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inference issue: Check images from Grade 4 being predicted as Grade 0&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inference issue: Check images from Grade 0 being predicted as Grade 4&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inference issue: A significant Grade 2 is being predicted as Grade 0&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inference issue: More than 50% of Grade 1 is being predicted as Grade 0&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a new dataset&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;model-improvement&#34;&gt;Model Improvement&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; research kaggle winning augmentation for DR&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; research appropriate augmentation: optical distortion, grid distortion, piecewise affine transform, horizontal flip, vertical flip, random rotation, random shift, random scale, a shift of RGB values, random brightness and contrast, additive Gaussian noise, blur, sharpening, embossing, random gamma, and cutout&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; train on various pretrained models or research which is supposed to be ideal for this case &lt;a href=&#34;https://pytorch.org/vision/stable/models.html&#34;&gt;https://pytorch.org/vision/stable/models.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create several neural nets (test different layers)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; experiment with batch size&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Reducing lighting-condition effects&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Cropping uninformative area&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Create custom dataloader based on ben graham kaggle winning strategy&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; finetune vs feature extract&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; oversample&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; undersample&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add specificity and sensitivity to indicators&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create train loss and valid loss charts&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; test regression models (treat this as a grading problem)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; pickle weights&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;additional-models&#34;&gt;Additional Models&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; check if left/right eye classification model is required&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;additional-datasets&#34;&gt;Additional datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; make datasets more extensive (add test dataset with recoverd labels to train dataset 2015)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add APTOS dataset&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; request labelled datasets from cheers&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add datasets from cheers for testing&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add datasets from cheers for training&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;test-datasets&#34;&gt;Test datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; find datasets for testing (dataset apart from APTOS and EyePACS)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; update folder structures to match our use case&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; find dataset for testing after making sure old test datasets are not in vaid/train (4 will be empty)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;conceptsresearch-papers&#34;&gt;Concepts/Research Papers&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; read reports from kaggle competition winning authors&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Deep Learning Approach to Diabetic Retinopathy Detection &lt;a href=&#34;https://arxiv.org/pdf/2003.02261.pdf&#34;&gt;https://arxiv.org/pdf/2003.02261.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Google research &lt;a href=&#34;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45732.pdf&#34;&gt;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45732.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Nature article &lt;a href=&#34;https://www.nature.com/articles/s41746-019-0172-3&#34;&gt;https://www.nature.com/articles/s41746-019-0172-3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;a href=&#34;https://deim.urv.cat/~itaka/itaka2/PDF/acabats/PhD_Thesis/TESI_doctoral_Jordi_De_la_Torre.pdf&#34;&gt;https://deim.urv.cat/~itaka/itaka2/PDF/acabats/PhD_Thesis/TESI_doctoral_Jordi_De_la_Torre.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; what can go wrong &lt;a href=&#34;https://yerevann.github.io/2015/08/17/diabetic-retinopathy-detection-contest-what-we-did-wrong/&#34;&gt;https://yerevann.github.io/2015/08/17/diabetic-retinopathy-detection-contest-what-we-did-wrong/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;a href=&#34;https://arxiv.org/pdf/1902.07208.pdf&#34;&gt;https://arxiv.org/pdf/1902.07208.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>