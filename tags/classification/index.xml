<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>classification on Ayush Subedi</title>
    <link>https://ayushsubedi.github.io/tags/classification/</link>
    <description>Recent content in classification on Ayush Subedi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 30 Jul 2023 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ayushsubedi.github.io/tags/classification/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>(WIP) Computational Data Analytics (Machine Learning 1)</title>
      <link>https://ayushsubedi.github.io/posts/computational_data_analytics/</link>
      <pubDate>Sun, 30 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/computational_data_analytics/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clustering&#34;&gt;Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pca&#34;&gt;PCA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#density-estimation&#34;&gt;Density Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gaussin-mixture-model-and-em-algorithm&#34;&gt;Gaussian Mixture Model and EM Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimization&#34;&gt;Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#classification&#34;&gt;Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#support-vector-machines&#34;&gt;Support Vector Machines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-to-neural-nets&#34;&gt;Introduction to Neural Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#boosting&#34;&gt;Boosting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-forest&#34;&gt;Random Forest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cross-validation&#34;&gt;Cross Validation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://www.mathworks.com/discovery/reinforcement-learning/_jcr_content/mainParsys3/discoverysubsection/mainParsys/image.adapt.full.medium.png/1689243268873.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Application&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Desired Outcomes&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Inputs (Data)&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Learning Paradigms&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Image Classification&lt;/td&gt;
&lt;td&gt;Classify objects in images&lt;/td&gt;
&lt;td&gt;Labeled images&lt;/td&gt;
&lt;td&gt;Supervised Learning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Natural Language Processing (NLP)&lt;/td&gt;
&lt;td&gt;Text understanding, sentiment analysis&lt;/td&gt;
&lt;td&gt;Text data, language corpora&lt;/td&gt;
&lt;td&gt;Supervised/Unsupervised Learning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Speech Recognition&lt;/td&gt;
&lt;td&gt;Convert spoken language to text&lt;/td&gt;
&lt;td&gt;Audio data&lt;/td&gt;
&lt;td&gt;Supervised Learning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fraud Detection&lt;/td&gt;
&lt;td&gt;Identify fraudulent transactions&lt;/td&gt;
&lt;td&gt;Transaction data, labeled instances&lt;/td&gt;
&lt;td&gt;Supervised Learning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Recommender Systems&lt;/td&gt;
&lt;td&gt;Recommend products, movies, etc. to users&lt;/td&gt;
&lt;td&gt;User-item interactions, preferences&lt;/td&gt;
&lt;td&gt;Collaborative Filtering, Content-Based, Hybrid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Autonomous Vehicles&lt;/td&gt;
&lt;td&gt;Navigate and drive without human intervention&lt;/td&gt;
&lt;td&gt;Sensor data (cameras, LiDAR, etc.)&lt;/td&gt;
&lt;td&gt;Reinforcement Learning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Medical Diagnosis&lt;/td&gt;
&lt;td&gt;Diagnose diseases or conditions&lt;/td&gt;
&lt;td&gt;Patient data, medical records&lt;/td&gt;
&lt;td&gt;Supervised Learning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sentiment Analysis&lt;/td&gt;
&lt;td&gt;Determine sentiment (positive/negative)&lt;/td&gt;
&lt;td&gt;Text data (reviews, social media)&lt;/td&gt;
&lt;td&gt;Supervised Learning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Time Series Forecasting&lt;/td&gt;
&lt;td&gt;Predict future values in time series data&lt;/td&gt;
&lt;td&gt;Historical time series data&lt;/td&gt;
&lt;td&gt;Supervised/Unsupervised Learning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Virtual Assistants&lt;/td&gt;
&lt;td&gt;Provide intelligent responses and assistance&lt;/td&gt;
&lt;td&gt;Voice/text data, user interactions&lt;/td&gt;
&lt;td&gt;NLP, Supervised Learning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Finding Communities&lt;/td&gt;
&lt;td&gt;Identify groups of densely connected nodes&lt;/td&gt;
&lt;td&gt;Network/graph data&lt;/td&gt;
&lt;td&gt;Unsupervised Learning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Spam Detection&lt;/td&gt;
&lt;td&gt;Identify and filter out spam emails/messages&lt;/td&gt;
&lt;td&gt;Email/text data&lt;/td&gt;
&lt;td&gt;Supervised Learning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Novelty Detection&lt;/td&gt;
&lt;td&gt;Detect anomalies or new patterns&lt;/td&gt;
&lt;td&gt;Feature vectors, sensor data&lt;/td&gt;
&lt;td&gt;Unsupervised Learning&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;h3 id=&#34;probabilities&#34;&gt;Probabilities&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Distribution&lt;/li&gt;
&lt;li&gt;Densities&lt;/li&gt;
&lt;li&gt;Marginalization&lt;/li&gt;
&lt;li&gt;Conditioning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;statistics&#34;&gt;Statistics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Mean&lt;/li&gt;
&lt;li&gt;Variance&lt;/li&gt;
&lt;li&gt;Maximum Likelihood Estimation&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;linear-algebra&#34;&gt;Linear Algebra&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Vector&lt;/li&gt;
&lt;li&gt;Matrix&lt;/li&gt;
&lt;li&gt;Multiplication&lt;/li&gt;
&lt;li&gt;Inversion&lt;/li&gt;
&lt;li&gt;Eigen-decomposition&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;algorithms-and-programming&#34;&gt;Algorithms and Programming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Basic Data Structures&lt;/li&gt;
&lt;li&gt;Computational Complexity&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convex-optimization&#34;&gt;Convex optimization&lt;/h3&gt;
&lt;h1 id=&#34;clustering&#34;&gt;Clustering&lt;/h1&gt;
&lt;h1 id=&#34;pca&#34;&gt;PCA&lt;/h1&gt;
&lt;h1 id=&#34;nonlinear-dimensionality-reduction&#34;&gt;Nonlinear Dimensionality Reduction&lt;/h1&gt;
&lt;h1 id=&#34;density-estimation&#34;&gt;Density Estimation&lt;/h1&gt;
&lt;h1 id=&#34;gaussian-mixture-model-and-em-algorithm&#34;&gt;Gaussian Mixture Model and EM Algorithm&lt;/h1&gt;
&lt;h1 id=&#34;optimization&#34;&gt;Optimization&lt;/h1&gt;
&lt;h1 id=&#34;classification&#34;&gt;Classification&lt;/h1&gt;
&lt;h1 id=&#34;support-vector-machines&#34;&gt;Support Vector Machines&lt;/h1&gt;
&lt;h1 id=&#34;introduction-to-neural-nets&#34;&gt;Introduction to Neural Nets&lt;/h1&gt;
&lt;h1 id=&#34;boosting&#34;&gt;Boosting&lt;/h1&gt;
&lt;h1 id=&#34;random-forest&#34;&gt;Random Forest&lt;/h1&gt;
&lt;h1 id=&#34;cross-validation&#34;&gt;Cross Validation&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Birds, Plane, Superman</title>
      <link>https://ayushsubedi.github.io/posts/bird_plane_superman/</link>
      <pubDate>Fri, 02 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/bird_plane_superman/</guid>
      <description>&lt;h1 id=&#34;heading&#34;&gt;&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://4.bp.blogspot.com/-VRnyjR8-1FQ/W5EZ4WhvRpI/AAAAAAAAOPM/rNiNP_X9haIqqSt4692inhEUiucUuILewCLcBGAs/s400/001109.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;br&gt;
I have been experimenting with Deep Learning models in &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt; for a couple of weeks now. PyTorch is an open source python package that provides Tensor computation (similar to numpy) with GPU support. The dataset used for this particular blog post does no justice to the real-life usage of PyTorch for image classification. However, it serves as a general idea of how Transfer Learning can be used for more complicated image classification. Transfer learning, in a nutshell, is reusing a model developed for some other classification task, for your classification purposes. The dataset was created by scraping images from google image search.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Creating the dataset&lt;/strong&gt;&lt;br&gt;
For our dataset, we need images of birds, planes, and Superman. We will be using the &lt;a href=&#34;https://github.com/hellock/icrawler&#34;&gt;icrawler&lt;/a&gt; package to download the images from google image search.&lt;/p&gt;
&lt;p&gt;We repeat the same for birds and Superman. Once all the files have been downloaded, we will restructure the folders to contain our training, testing and validating samples. I am allocating 70% for training, 20% for validating and 10% for testing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1.bp.blogspot.com/--JBHptSr22k/W5EhKG9S0FI/AAAAAAAAOPY/b1JuDndQ6qAbXSgomBTZUZkjPRNEK-N-ACLcBGAs/s320/Capture.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Loading the data&lt;/strong&gt;&lt;br&gt;
PyTorch uses generators to read the data. Since datasets are usually large, it makes sense to not load everything in memory. Let&amp;rsquo;s import useful libraries that we will be using for classification.&lt;/p&gt;
&lt;p&gt;Now that we have imported useful libraries, we need to augment and normalize the images. Torchvision transforms is used to augment the training data with random scaling, rotations, mirroring and cropping. We do not need to rotate or flip our testing and validating sets. The data for each set will also be loaded with Torchivision&amp;rsquo;s DataLoader and ImageFolder.&lt;/p&gt;
&lt;p&gt;Let us visualize a few training images to understand the data augmentation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://3.bp.blogspot.com/-LNJ3iv_C5Zs/W5Elb_VXgmI/AAAAAAAAOPk/ThiNb-NKIUU5fFx-2oEaO4FzRcMTBvuwgCLcBGAs/s640/download%2B%25281%2529.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Loading a pre-trained model&lt;/strong&gt;
We will be using Densenet for our purposes.&lt;/p&gt;
&lt;p&gt;The pre-trained model&amp;rsquo;s classifier takes 1920 features as input. We need to be consistent with that. However, the output feature for our case is 3 (bird, plane, and Superman).&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s create our classifier and replace the model&amp;rsquo;s classifier.&lt;/p&gt;
&lt;p&gt;We are using ReLU activation function with random dropouts with a probability of 20% in the hidden layers. For the output layer, we are using LogSoftmax.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Training Criterion, Optimizer, and Decay&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;Model Training and Testing&lt;/strong&gt;&lt;br&gt;
Let us calculate the accuracy of the model without training it first.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://4.bp.blogspot.com/-heWEFxARMek/W5E49Y_TsPI/AAAAAAAAOPw/ha1h5ZBOs8ApPT0t5RcDsRWfvOXHwcnWACLcBGAs/s640/Capture.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The accuracy is pretty low at this time, which is expected. The &lt;em&gt;cuda&lt;/em&gt; parameter here is the boolean object passed for the availability of GPU hardware in the machine.&lt;/p&gt;
&lt;p&gt;Let us train the model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://4.bp.blogspot.com/-InDGqH9w1co/W5E520-6ANI/AAAAAAAAOP4/QJ_tiogtGmcdNkHB33ieKWk-UJ2AHfQqwCLcBGAs/s640/Capture.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since GPU is supported, the training took around 10 mins. The validation accuracy is almost 99%. Let us check the accuracy over training data again.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://3.bp.blogspot.com/-TwYWdmXYGyI/W5E7v95GTiI/AAAAAAAAOQE/FCIpQ7MPr4wXEFY_YqxsCl46ZdZlebg1ACLcBGAs/s640/Capture.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Image Preprocessing&lt;/strong&gt;
We declare a few functions to preprocess images and pass on the trained model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://3.bp.blogspot.com/-q3hQVfCnDmE/W5E9S5vSizI/AAAAAAAAOQQ/r1hP9xV1Tw4eoh6E5MzGbFGh7haNkn3BQCLcBGAs/s640/Untitled-1.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Predicting by passing an image&lt;/strong&gt;&lt;br&gt;
Since our model is ready and we have built functions that allows us to visualize, let us try it out on one of the sample images.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://3.bp.blogspot.com/-mv2wuvBJIcA/W5E_R-qFjZI/AAAAAAAAOQk/HDEZYXgYYM8kYGVvg5w5Y3KNHkUZs8KJQCLcBGAs/s640/Capture.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;So, that is it.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>