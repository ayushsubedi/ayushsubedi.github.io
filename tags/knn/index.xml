<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>knn on Ayush Subedi</title>
    <link>https://ayushsubedi.github.io/tags/knn/</link>
    <description>Recent content in knn on Ayush Subedi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 16 Sep 2023 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ayushsubedi.github.io/tags/knn/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Topics on High-Dimensional Data Analytics (Machine Learning 2)</title>
      <link>https://ayushsubedi.github.io/posts/topics_on_high_dimensional_data_analytics/</link>
      <pubDate>Sat, 16 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/topics_on_high_dimensional_data_analytics/</guid>
      <description>&lt;h1 id=&#34;topics-on-high-dimensional-data-analytics&#34;&gt;Topics on High-Dimensional Data Analytics&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#functional-data-analysis&#34;&gt;Functional Data Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#image-analysis&#34;&gt;Image Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tensor-data-analysis&#34;&gt;Tensor Data Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimization-and-application&#34;&gt;Optimization and Application&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regularization&#34;&gt;Regularization&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- ## Tensor Decomposition Methods - CP Decomposition
## Tensor Decomposition Methods - Tucker Decomposition
## Tensor Analysis Applications I
## Tensor Analysis Application II --&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;h3 id=&#34;big-data&#34;&gt;Big Data&lt;/h3&gt;
&lt;p&gt;Big data is a term used to describe extremely large and complex datasets that traditional data processing applications are not well-equipped to handle. The concept of &amp;ldquo;big data&amp;rdquo; is often associated with what is referred to as the &amp;ldquo;4V&amp;rdquo; framework, which describes the key characteristics of big data:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Volume:&lt;/strong&gt;  This refers to the sheer scale of data generated and collected. Big data involves datasets that are too large to be managed and processed using traditional databases and tools. This massive volume can range from terabytes to petabytes and beyond.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Velocity:&lt;/strong&gt;  This characteristic pertains to the speed at which data is generated, collected, and processed. In today&amp;rsquo;s fast-paced digital world, data is generated at an unprecedented rate, often in real-time or near-real-time. Examples include social media interactions, sensor data from IoT devices, financial transactions, and more.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variety:&lt;/strong&gt;  Big data comes in various formats and types, such as structured, semi-structured, and unstructured data. Structured data is organized into a well-defined format (e.g., tables in a relational database), whereas unstructured data lacks a specific structure (e.g., text documents, images, videos, social media posts). Semi-structured data lies somewhere in between, having a partial structure but not fitting neatly into traditional databases.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Veracity:&lt;/strong&gt;  Veracity refers to the quality and reliability of the data. With the proliferation of data sources, there&amp;rsquo;s an increased potential for data to be incomplete, inaccurate, or inconsistent. Ensuring the accuracy and trustworthiness of big data is a significant challenge, and data quality management is crucial for meaningful insights.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;high-dimensional-data&#34;&gt;High Dimensional Data&lt;/h3&gt;
&lt;p&gt;High-dimensional data refers to datasets where the number of features or variables (dimensions) is significantly larger than the number of observations or samples. In other words, the data has a high number of attributes compared to the number of data points available. This kind of data is prevalent in various fields such as genomics, image analysis, social networks, and more.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/high_dimensional_.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;difference-between-high-dimensional-data-and-big-data&#34;&gt;Difference between High Dimensional Data and Big Data&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/diff_bet_high_and_low.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;p = dimension
n = samples&lt;/p&gt;
&lt;h3 id=&#34;the-curse-of-dimensionality&#34;&gt;The Curse of Dimensionality&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;As the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/distance_dimension.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As distance between observations increases with the dimensions, the sample size required for learning a model drastically increases.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Increased Sparsity:&lt;/strong&gt;  In higher dimensions, the available data points are spread out more thinly across the space. This means that data points become farther apart from each other, making it challenging to find meaningful clusters or patterns. It&amp;rsquo;s like having a lot of points scattered in a large, high-dimensional space, and they&amp;rsquo;re so spread out that it&amp;rsquo;s difficult to identify any consistent relationships.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;More Data Needed:&lt;/strong&gt;  With higher-dimensional data, you need a disproportionately larger amount of data to capture the underlying patterns accurately. When the data is sparse, it&amp;rsquo;s harder to generalize from the observed points to make accurate predictions or draw conclusions. As the dimensionality increases, you might need exponentially more data to maintain the same level of accuracy in your models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact on Complexity:&lt;/strong&gt;  The complexity of machine learning models increases with dimensionality. More dimensions mean more parameters to estimate, which can lead to overfitting â€“ a situation where a model fits the training data too closely and fails to generalize well to new data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Increased Computational Demands:&lt;/strong&gt;  Processing and analyzing high-dimensional data require more computational resources and time. Many algorithms become slower and more memory-intensive as the number of dimensions grows. This can make experimentation and model training more challenging and time-consuming.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Difficulties in Visualization:&lt;/strong&gt;  Our ability to visualize data effectively diminishes as the number of dimensions increases. We are accustomed to thinking in 2D and 3D space, but visualizing data in, say, 10 dimensions is practically impossible. This can make it hard to understand the structure of the data and the relationships between variables.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;low-dimensional-learning-from-high-dimensional-data&#34;&gt;Low Dimensional Learning From High Dimensional Data&lt;/h3&gt;
&lt;p&gt;High dimensional data usually have low dimensional structure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.mathworks.com/help/examples/stats/win64/ChangeTsneSettingsExample_01.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This can be achieved through Functional Data Analysis, Tensor Analysis, Rank Deficient Methods among others.&lt;/p&gt;
&lt;h3 id=&#34;solutions-for-the-curse-of-dimensionality&#34;&gt;Solutions for the curse of dimensionality&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Feature extraction&lt;/li&gt;
&lt;li&gt;Dimensionality reduction&lt;/li&gt;
&lt;li&gt;Collecting much more observations&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;functional-data-analysis&#34;&gt;Functional Data Analysis&lt;/h1&gt;
&lt;p&gt;A fluctuating quantity or impulse whose variations represent information and is often represented as a function of time or space.&lt;/p&gt;
&lt;p&gt;From Wikipedia&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Functional data analysis (FDA)&lt;/strong&gt; is a branch of statistics that analyses data providing information about curves, surfaces or anything else varying over a continuum. In its most general form, under an FDA framework, each sample element of functional data is considered to be a random function. The physical continuum over which these functions are defined is often time, but may also be spatial location, wavelength, probability, etc. Intrinsically, functional data are infinite dimensional. The high intrinsic dimensionality of these data brings challenges for theory as well as computation, where these challenges vary with how the functional data were sampled. However, the high or infinite dimensional structure of the data is a rich source of information and there are many interesting challenges for research and data analysis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://lands.let.ru.nl/FDA/images/FDA_pic4website.bmp&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;regression---least-square-estimates&#34;&gt;Regression - Least square Estimates&lt;/h2&gt;
&lt;p&gt;A linear regression model assumes that the regression function $E(Y|X)$ is linear in the inputs $X_1, &amp;hellip;, X_p$. They were developed in the pre-computer age of statistics, but even in today&amp;rsquo;s computer era there are still good reasons to study and use them. They are simple and often provide an adequate and interpretable description of how the inputs affect the output.&lt;/p&gt;
&lt;p&gt;The linear regression model has the form:&lt;/p&gt;
&lt;p&gt;$f(X) = \beta_0 + \sum_{j=1}^p X_j\beta_j$&lt;/p&gt;
&lt;p&gt;Typically we have a set of training data $(x_1, y_1)&amp;hellip;(x_N, y_N)$ from which to estimate the parameters $\beta$. Each $x_i = (x_{i1}, x_{i2} &amp;hellip; x_{ip})^T$ is a vector of feature measurements for the $i$th case. The most popular estimation method is the least squares, in which we pick the coefficients $\beta = (\beta_0, \beta_1,&amp;hellip;.\beta_p)^T$ to minimize the residual sum of squares.&lt;/p&gt;
&lt;p&gt;$\sum_{i=1}^N (y_i - f(x))^2 = \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p X_j\beta_j)^2$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/lr.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Denote $X$ by the $N \times (p+1)$ matrix with each row an input vector (with a 1 in the first position, to represent the intercept), and similarity let $y$ be the $N$ vector of outputs in the training set. Then we can write the residual sum-of-squares as :&lt;/p&gt;
&lt;p&gt;$RSS(\beta) = (y-X\beta)^T(y-X\beta)$&lt;/p&gt;
&lt;p&gt;Differentiating with respect to $\beta$, &amp;hellip;.&lt;/p&gt;
&lt;p&gt;$\hat{\beta} = (X^TX)^{-1}X^Ty$&lt;/p&gt;
&lt;h2 id=&#34;geometric-interpretation&#34;&gt;Geometric Interpretation&lt;/h2&gt;
&lt;p&gt;$\hat{y} = X\hat{\beta} = X(X^TX)^{-1}X^Ty = Hy $&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Projection Matrix&lt;/strong&gt; (or Hat matrix): The outcome vector $y$ is orthogonally projected onto the hyperplane spanned by the input vectors $x_1$ and $x_2$. The Projection $\hat{y}$ represents the vector of predictions obtained by the least square method.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/ols_projection.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;properties-of-ols&#34;&gt;Properties of OLS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;They are unbiased estimators. That is the expected value of estimators and actual parameters are the same $E(\hat{\beta}) = \beta$&lt;/li&gt;
&lt;li&gt;The covariance can be obtained by $cov(\hat{\beta}) = \sigma^2 (X^TX)^{-1}$, where $\sigma^2 = SSE/(n-p)$&lt;/li&gt;
&lt;li&gt;According to the &lt;strong&gt;Gauss-Markov Theorem&lt;/strong&gt;, &lt;em&gt;among all unbiased linear estimates&lt;/em&gt;, the least square estimate (LSE) has the minimum variance and it is unique.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Regression can be used for Feature Extraction&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;splines&#34;&gt;Splines&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Polynomial Regression&lt;/strong&gt; is a type of regression analysis where the relationship between the independent variable (input) and the dependent variable (output) is modeled as an nth-degree polynomial. In other words, instead of fitting a straight line (linear regression), a polynomial regression can fit curves of various degrees, allowing for more flexibility in capturing complex relationships. For example, a quadratic polynomial regression (degree 2) can model a parabolic relationship, and a cubic polynomial regression (degree 3) can model more intricate curves.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Polynomial regression is still considered a type of linear regression&lt;/strong&gt; because the relationship between the input and output variables is linear with respect to the coefficients, even though the input variables may be raised to different powers. The model equation for polynomial regression of degree n is:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$y = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + &amp;hellip; + \beta_mx^m + \epsilon$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nonlinear Regression&lt;/strong&gt;, on the other hand, refers to a broader class of regression models where the relationship between the independent and dependent variables is not a linear function. Nonlinear regression can encompass a wide range of functional forms, including exponential, logarithmic, sigmoidal, and other complex shapes. The main characteristic of nonlinear regression is that the model parameters are estimated in a way that best fits the chosen nonlinear function to the data.&lt;/p&gt;
&lt;p&gt;Unlike polynomial regression, nonlinear regression models can&amp;rsquo;t be expressed in terms of a simple equation with polynomial terms. The specific form of the nonlinear function needs to be determined based on the problem&amp;rsquo;s nature and domain knowledge.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages of Polynomial Regression&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Remote part of the function is very sensitive to outliers&lt;/li&gt;
&lt;li&gt;Less flexibility due to global function structure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/dis_pr.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The global function structure causes underfitting or overfitting.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The solution is to move from global to local structure -&amp;gt; Splines.&lt;/p&gt;
&lt;h3 id=&#34;splines-1&#34;&gt;Splines&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Linear combination of Piecewise Polynomial Function &lt;strong&gt;under continuity assumption&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Partition the domain of x into continuous intervals and fit polynomials in each interval separately&lt;/li&gt;
&lt;li&gt;Provides flexibility and local fitting&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose $x \in [a,b]$. Partition the x domain using the following points (a.k.a knots):&lt;/p&gt;
&lt;p&gt;$a&amp;lt;\xi_1&amp;lt;\xi_2&amp;hellip;&amp;lt;\xi_k&amp;lt;b, &amp;lt;\xi_0=a, &amp;lt;\xi_{k+1}=b$&lt;/p&gt;
&lt;p&gt;Fit a polynomial in each interval under the continuity conditions and integrate them by&lt;/p&gt;
&lt;p&gt;$f(X) = \sum_{m=1}^K \beta_mh_m(X)$&lt;/p&gt;
&lt;h3 id=&#34;simple-example&#34;&gt;Simple Example&lt;/h3&gt;
&lt;h3 id=&#34;piecewise-constant&#34;&gt;Piecewise Constant&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/pwc.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here we are using a zero order polynomial. A zero order polynomial can be defined by an indicator function. If we use OLS, the beta would be the average of point in each local region.&lt;/p&gt;
&lt;p&gt;$f(X) = \sum_{m=1}^3 \beta_mh_m(X)$&lt;/p&gt;
&lt;h3 id=&#34;piecewise-linear&#34;&gt;Piecewise Linear&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/pwl.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here we are using a first order polynomial. A first order polynomial includes slopes and intercept (and therefore $K=6$ here.)&lt;/p&gt;
&lt;p&gt;$f(X) = \sum_{m=1}^6 \beta_mh_m(X)$&lt;/p&gt;
&lt;p&gt;There are two issues here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discontinuity&lt;/li&gt;
&lt;li&gt;Underfitting&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;solving-for-discontinuity&#34;&gt;Solving for Discontinuity&lt;/h2&gt;
&lt;p&gt;We can impose continuity constraint for each knot:&lt;/p&gt;
&lt;p&gt;$f{\xi^-_1}=f(\xi^+_1)$&lt;/p&gt;
&lt;p&gt;This can be translated to $\beta_1+\xi_1\beta_4 = \beta_2 + \xi_1\beta_5$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Not sure how&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By adding constraints we are losing some degrees of freedom.
The total number of free parameters (degree of freedom) = 6 (total number of parameters -2 (total number of constraints) = 4&lt;/p&gt;
&lt;p&gt;Alternatively, once could incorporate the constraints into the basis functions:&lt;/p&gt;
&lt;p&gt;$h_1(X) = 1$,&lt;/p&gt;
&lt;p&gt;$h_2(X) = X$,&lt;/p&gt;
&lt;p&gt;$h_3(X) = (X-\xi_1)_+$,&lt;/p&gt;
&lt;p&gt;$h_4(X) = (X-\xi_2)_+$&lt;/p&gt;
&lt;p&gt;This basis is known as truncated power basis&lt;/p&gt;
&lt;p&gt;$(X-\xi_k)_+ = (X-\xi_k)$ if $x \ge xi_k$ $0$ if $x&amp;lt;xi_k$&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/790G152GYz4?si=PIkkurtDgaioUCMu&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;h2 id=&#34;solving-for-underfitting&#34;&gt;Solving for Underfitting&lt;/h2&gt;
&lt;p&gt;Splines with Higher Order of Continuity can be used to tackle underfitting.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/cp.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Continuity constraints for smoothness&lt;/p&gt;
&lt;p&gt;$f{\xi^-_1}=f(\xi^+_1)$&lt;/p&gt;
&lt;p&gt;$f^&amp;rsquo;{\xi^-_1}=f^&amp;rsquo;(\xi^+_1)$&lt;/p&gt;
&lt;p&gt;$f^{&amp;rsquo;&amp;rsquo;}{\xi^-_1}=f^{&amp;rsquo;&amp;rsquo;}(\xi^+_1)$&lt;/p&gt;
&lt;p&gt;$h_1(X) = 1$,&lt;/p&gt;
&lt;p&gt;$h_2(X) = X$,&lt;/p&gt;
&lt;p&gt;$h_3(X) = X^2$,&lt;/p&gt;
&lt;p&gt;$h_4(X) = X^3$,&lt;/p&gt;
&lt;p&gt;$h_5(X) = (X-\xi_1)^3_+$,&lt;/p&gt;
&lt;p&gt;$h_6(X) = (X-\xi_2)^3_+$&lt;/p&gt;
&lt;p&gt;The degree of freedom is calculated by:
Number of regions * Number of parameters in each region) - (Number of knots)*(Number of constraints per knot)&lt;/p&gt;
&lt;h2 id=&#34;order-m-splines&#34;&gt;Order-M Splines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;M=1 piecewise-constant splines&lt;/li&gt;
&lt;li&gt;M=2 linear splines&lt;/li&gt;
&lt;li&gt;M=3 quadratic splines&lt;/li&gt;
&lt;li&gt;M=4 cubic splines&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For Truncated power basis functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Total degree of freedom is K+M&lt;/li&gt;
&lt;li&gt;Cubic spline is the lowest order spline for which the knot discontinuity is not visible to human eyes&lt;/li&gt;
&lt;li&gt;Knots selection: a simple method is to use x quantiles. However, the choice of knots is a variable/model selection problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;estimation&#34;&gt;Estimation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;After creating the basis function, we can use OLS to estimate parameters $\beta$&lt;/li&gt;
&lt;li&gt;First of all, create a basis matrix by concatinating basis vectors. For example if we have cubic splines with two knots, we will have six basis vectors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$H = [h_1(x) \quad h_2(x) \quad h_3(x) \quad h_4(x) \quad h_5(x) \quad h_6(x)]$&lt;/p&gt;
&lt;p&gt;gives $\hat\beta = (H^TH)^-1H^Ty$&lt;/p&gt;
&lt;p&gt;Linear Smoother: $\hat y= H\hat\beta = H(H^TH)^{-1}H^Ty = Sy$&lt;/p&gt;
&lt;p&gt;Degrees of Freedom $df=trace S$&lt;/p&gt;
&lt;p&gt;Although truncated power basis functions are simple and algebraically appealing, it is not efficient for computation and ill-posed and numerically unstable. The matrix is close to singular (because of correlations among themselves, and determinant being very close to zero), and inverting it becomes challenging.&lt;/p&gt;
&lt;p&gt;The solution is to user Bsplines.&lt;/p&gt;
&lt;h2 id=&#34;bsplines&#34;&gt;Bsplines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Alternative basis vectors for piecewise polynomials that are computationally more efficient&lt;/li&gt;
&lt;li&gt;Each basis function has a local support, that is, it is nonzero over at most M (spline order) consecutive intervals&lt;/li&gt;
&lt;li&gt;The basis matrix is banded&lt;/li&gt;
&lt;li&gt;The low bandwidth of the matrix reduces the linear dependency of the columns, and therefore, removes the numeric column stability.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/bspline.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;bspline-basis&#34;&gt;Bspline Basis&lt;/h2&gt;
&lt;p&gt;Let $B_{j,m}(x)$ be the $j^{th}$ B-spline basis function of order $m(m \le M)$ for the knot sequence $\tau$&lt;/p&gt;
&lt;p&gt;$a &amp;lt; \xi_1 &amp;lt; \xi_2 &amp;lt; &amp;hellip; &amp;lt; \xi_k &amp;lt; b$&lt;/p&gt;
&lt;p&gt;Define the augmented knots sequence $\tau$&lt;/p&gt;
&lt;p&gt;$\tau_1 \le \tau_2 &amp;hellip;\le \tau_M \le \xi_0$ (before the lower bound)&lt;/p&gt;
&lt;p&gt;$\tau_{M+j} = \xi_j, j = 1, &amp;hellip; , K$&lt;/p&gt;
&lt;p&gt;$\xi_{K+1} \le \tau_{M+K+1} \le \tau_{M+K+2} \le &amp;hellip; \le \tau_{2M+K}$ (after the lower bound)&lt;/p&gt;
&lt;h3 id=&#34;smoother-matrix&#34;&gt;Smoother Matrix&lt;/h3&gt;
&lt;p&gt;Consider a regression Spline basis B&lt;/p&gt;
&lt;p&gt;$\hat f = B(B^TB)^{-1}B^Ty = Hy$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;H is the smoother matrix (projection matrix)&lt;/li&gt;
&lt;li&gt;H is idempotent ($H \times H = H$)&lt;/li&gt;
&lt;li&gt;H is symmetric&lt;/li&gt;
&lt;li&gt;Degrees of freedom trace (H)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;smoothing-splines&#34;&gt;Smoothing Splines&lt;/h2&gt;
&lt;h3 id=&#34;bspline-basis-boundary-issue&#34;&gt;Bspline basis boundary issue&lt;/h3&gt;
&lt;p&gt;Consider the following setting with the fixed training data&lt;/p&gt;
&lt;p&gt;$y_i = f(x_i) + \epsilon_i$&lt;/p&gt;
&lt;p&gt;$\epsilon_i \approx iid(0, \sigma^2)$&lt;/p&gt;
&lt;p&gt;$Var(\hat f(x)) = h(x)^T(H^TH)^{-1}h(x)\sigma^2$ (variance of estimated function using spline)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Behavior of splines tends to be sporadic near the boundaries, and extrapolation can be problematic. The main reason is that the complexity of Cubic Spline is more than the complexity of Global Cubic Polynomial, due to the large number of parameters (less bias, more variance). The solution is to use linear splines instead of cubic splines (Natural Cubic Splines).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;natural-cubic-splines&#34;&gt;Natural Cubic Splines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Additional constraints are added to make the function linear beyond the boundary knots&lt;/li&gt;
&lt;li&gt;Assuming the function is linear near the boundaries (where there is less information) is often reasonable&lt;/li&gt;
&lt;li&gt;Cubic spline; linear on $[-\inf, \xi_1]$ and $[\xi_k , \inf]$&lt;/li&gt;
&lt;li&gt;Prediction variance decreases&lt;/li&gt;
&lt;li&gt;The price is the bias near the boundaries&lt;/li&gt;
&lt;li&gt;Degrees of freedom is K, the number of knots&lt;/li&gt;
&lt;li&gt;Each of these basis functions has zero second and third derivative in the linear region.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;penalized-residual-sum-of-squares&#34;&gt;Penalized residual sum of squares&lt;/h2&gt;
&lt;p&gt;$\min_f \frac{1}{n}\sum_{i-1}^n[y_i - f(x_i)]^2+\lambda \int^a_b[f^{&amp;quot;}(x)^2dx]$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first term measures the closeness of the model to the data (related to bias)&lt;/li&gt;
&lt;li&gt;The second term penalizes curvature of the function (related to variance)&lt;/li&gt;
&lt;li&gt;$\lambda$ is the smoothing parameter controlling the trade between bias and variance&lt;/li&gt;
&lt;li&gt;$\lambda = 0$ interpolate the data (overfitting)&lt;/li&gt;
&lt;li&gt;$\lambda = \inf$ linear least-square regression (underfitting)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It can be shown that the minimizer is a natural cubic spline.&lt;/p&gt;
&lt;p&gt;Solution: $\hat \theta = (N^TN + \lambda\Omega)^{-1}N^Ty$
, $\Omega$ represents the second derivative&lt;/p&gt;
&lt;p&gt;$ f = (N^TN + \lambda\Omega)^{-1}N^Ty = S_\lambda y$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Smoothing spline estimator is a linear smoother&lt;/li&gt;
&lt;li&gt;$S_\lambda$ is the smoother matrix&lt;/li&gt;
&lt;li&gt;$S_\lambda$ is NOT idempotent&lt;/li&gt;
&lt;li&gt;$S_\lambda$ is symmetric&lt;/li&gt;
&lt;li&gt;$S_\lambda$ is positive definite&lt;/li&gt;
&lt;li&gt;Degrees of freedom: trace($S_\lambda$)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;choice-of-tuning-parameters&#34;&gt;Choice of Tuning Parameters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Train Test Validation
&lt;img src=&#34;https://ayushsubedi.github.io/img/pt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cross Validation
If an independent validation dataset is not affordable, the K-fold cross validation or leave-one-out CV can be useful&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Akaike Information Criteria (AIC)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bayesian Information Criteria (BIC)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generalized Cross-validation&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kernel-smoothers&#34;&gt;Kernel Smoothers&lt;/h2&gt;
&lt;h3 id=&#34;k-nearest-neighbor-knn&#34;&gt;K-Nearest Neighbor (KNN)&lt;/h3&gt;
&lt;p&gt;KNN Average $\hat f(x_0) = \sum_{i=1}^nw(x_0, x_i)y_i$&lt;/p&gt;
&lt;p&gt;where $\sum_{i=1}^nw(x_0, x_i)$ = $\frac{1}{K}$ if $x_i \in N_k(x_0)$ else $0$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple average of the k nearest observations to $x_0$ (local averaging)&lt;/li&gt;
&lt;li&gt;Equal weights are assigned to all neighbors&lt;/li&gt;
&lt;li&gt;However, the fitted function is in the form of a step function (non-smooth function)&lt;/li&gt;
&lt;li&gt;Also, the bias is quite high&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kernel-function&#34;&gt;Kernel Function&lt;/h3&gt;
&lt;p&gt;Any non-negative real-valued integrable function that satisfies the following conditions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\int_{-\inf}^{\inf}K(u)du=1$&lt;/li&gt;
&lt;li&gt;K is an even function; $K(-u) = K(u)$&lt;/li&gt;
&lt;li&gt;It has a finite second moment; $u^2\int_{-\inf}^{\inf}K(u)du &amp;lt; \inf$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kernel-smoother-regression&#34;&gt;Kernel Smoother Regression&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Kernel Regression&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is weighted local averaging that fits a simple model separately at each query point $x_0$&lt;/li&gt;
&lt;li&gt;More weights are assigned to closer observation&lt;/li&gt;
&lt;li&gt;Localization is defined by the weighting function&lt;/li&gt;
&lt;li&gt;Kernel regression requires little training, all calculations get done at the evaluation time.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/kregression.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;choice-of-lambda&#34;&gt;Choice of $\lambda$&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$\lambda$ defines the width of the neighbourhood&lt;/li&gt;
&lt;li&gt;Only points withing $[x_0-\lambda, x_0+\lambda]$ receive positive weights&lt;/li&gt;
&lt;li&gt;Smaller $\lambda$: rough estimate, larger bias, smaller variance&lt;/li&gt;
&lt;li&gt;Larger $\lambda$: smoother estimate, smaller bias, larger variance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cross-validation can be used for determining of $\lambda$:&lt;/p&gt;
&lt;h3 id=&#34;drawbacks-of-local-averaging&#34;&gt;Drawbacks of Local Averaging&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The local averaging can be biased on the boundaries of the domain due to the asymmetry of the kernel in that region.&lt;/li&gt;
&lt;li&gt;This can be solved by local linear regression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/nw_kernel.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Local linear regression corrects the bias on the boundaries&lt;/li&gt;
&lt;li&gt;Local polynomial regression corrects the bias in the curvature region&lt;/li&gt;
&lt;li&gt;However, local polynomial regression is complex due to higher order of polynomials, therefore, it increases the prediction variance.&lt;/li&gt;
&lt;li&gt;A good solution would be to use local linear model for points in the boundaries, and local quadratic regression in the interior regions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;functional-principal-component&#34;&gt;Functional Principal Component&lt;/h2&gt;
&lt;p&gt;Similar to PCA, FPCA aims to reduce the dimension of functional data by extracting a small set of uncorrelated features, which capture the most of the variation.&lt;/p&gt;
&lt;p&gt;Functional data (observed signals) are comprised of two main components. The first component is the continuous functional mean, and the second component is the error term, that is, the realizations from a stochastic process with mean function 0 and covariance function $C(t, t^&amp;rsquo;)$. It includes both random noise and signal-to-signal variations&lt;/p&gt;
&lt;p&gt;$s_i(t) = \mu(t) + \epsilon_i(t)$&lt;/p&gt;
&lt;p&gt;The mean function is common across all signals (notice that it does not have the $i$ subscript)&lt;/p&gt;
&lt;p&gt;Since signal variance comes from the noise function, we first focus on this for dimensionality reduction using the Karhunen-Loeve Theorem.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/kl.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The variance of $\xi_{ik}$ quickly decays with k. Therefore, only a few $\xi_{ik}$ also known as FPC-scores, would be enough to accurately approximate the noise function. That is,&lt;/p&gt;
&lt;p&gt;$\epsilon_i(t) \approx \sum_{k=1}^K \xi_{ik}\phi_{k}(t)$&lt;/p&gt;
&lt;p&gt;Signals decomposition is given by&lt;/p&gt;
&lt;p&gt;$s_i(t) = \mu(t) + \epsilon_i(t) \implies \mu(t) + \sum_{k=1}^K \xi_{ik}\phi_{k}(t)$&lt;/p&gt;
&lt;h2 id=&#34;model-estimation&#34;&gt;Model Estimation&lt;/h2&gt;
&lt;p&gt;Both the mean and covariance is unknown, and should be measured using training data. In practice, we have two types of signals/data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Complete signals: Sampled regularly&lt;/li&gt;
&lt;li&gt;Incomplete signals: Sampled irregularly, sparse, fragmented&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;steps-for-fpca-when-the-signals-are-incomplete&#34;&gt;Steps for FPCA when the signals are incomplete:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Estimate the mean function using local linear regression&lt;/li&gt;
&lt;li&gt;Estimate the raw covariance function using the estimated mean function&lt;/li&gt;
&lt;li&gt;Estimate the covariance surface using local quadratic regression&lt;/li&gt;
&lt;li&gt;Compute the Eigen functions&lt;/li&gt;
&lt;li&gt;Compute the FPC scores&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;image-analysis&#34;&gt;Image Analysis&lt;/h1&gt;
&lt;h2 id=&#34;introduction-to-image-processing&#34;&gt;Introduction to Image Processing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The process of processing raw images and extracting useful information for decision making.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Level 0:&lt;/strong&gt; Image representation (acquisition, sampling, quantization, compression)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Level 1:&lt;/strong&gt; Image to Image transformations (enhancement, filtering, restoration, smoothing, segmentation)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Level 2:&lt;/strong&gt; Image to vector transformation (feature extraction and dimension reduction)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Level 3:&lt;/strong&gt; Feature to decision mapping&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/image_analysis.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-is-an-image&#34;&gt;What is an Image?&lt;/h2&gt;
&lt;p&gt;A gray (color-RGB) image is a 2-D (3-D) light intensity function, $f (x_1, x_2)$, where $f$ measures brightness at position $f(x_1, x_2)$ . A digital gray (color) image is a representation of an image by a 2-D (3-D) array of discrete samples. &lt;strong&gt;Pixel&lt;/strong&gt; is referred to an element of the array.&lt;/p&gt;
&lt;p&gt;Possible values each pixel can have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Black and white image: 2&lt;/li&gt;
&lt;li&gt;8-bit Gray image: 256&lt;/li&gt;
&lt;li&gt;RGB: 256 x 256 x 256 = 16777216&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;basic-manipulation-in-python&#34;&gt;Basic Manipulation in Python&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; cv2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Load the image&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;image_path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;your_image.jpg&amp;#39;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# Replace with the path to your image&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;original_image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imread(image_path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Check if the image was loaded successfully&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; original_image &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error: Could not open or find the image.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Convert the image to grayscale&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    gray_image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cvtColor(original_image, cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLOR_BGR2GRAY)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Convert the grayscale image to black and white using thresholding&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    _, binary_image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;threshold(gray_image, &lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;, cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;THRESH_BINARY)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Resize the image (e.g., to a width of 800 pixels while maintaining aspect ratio)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    new_width &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;800&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    aspect_ratio &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; original_image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; original_image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    new_height &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int(new_width &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; aspect_ratio)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    resized_image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;resize(binary_image, (new_width, new_height))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Save the processed images to disk&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imwrite(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;gray_image.jpg&amp;#39;&lt;/span&gt;, gray_image)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imwrite(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;black_and_white_image.jpg&amp;#39;&lt;/span&gt;, binary_image)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imwrite(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;resized_image.jpg&amp;#39;&lt;/span&gt;, resized_image)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Images processed and saved successfully.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;image-transformation&#34;&gt;Image Transformation&lt;/h2&gt;
&lt;h3 id=&#34;image-histogram&#34;&gt;Image Histogram&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Histogram represents the distribution of gray levels.&lt;/li&gt;
&lt;li&gt;It is an estimate of the probability density function (pdf) of the underlying random process.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Image can be transformed by applying a function on the image matrix.&lt;/p&gt;
&lt;p&gt;$g(x,y) = T(f(x,y))$&lt;/p&gt;
&lt;p&gt;For example if a threshold function is sued as the transformation function a gray-scale image can be converted to a BW image.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/step_function.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The brightness of an image can be changed by shifting its histogram.&lt;/li&gt;
&lt;li&gt;The contrast of an image is defined by the difference in maximum and minimum pixel intensity.&lt;/li&gt;
&lt;li&gt;Gray level resolution refers to change in the shades or levels of gray in an image.&lt;/li&gt;
&lt;li&gt;The number of different colors in an image depends on bits per pixel (bpp).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$L = 2^{bpp}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gray level transformation is often used for image enchantment.&lt;/li&gt;
&lt;li&gt;Three typical transformation functions are:
&lt;ul&gt;
&lt;li&gt;Linear (negative image)&lt;/li&gt;
&lt;li&gt;Log&lt;/li&gt;
&lt;li&gt;Power-Law&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convolution-and-image-filtering&#34;&gt;Convolution and image filtering&lt;/h3&gt;
&lt;p&gt;The convolution of functions $f$ and $g$ is defined by:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/convolutions.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Convolution is widely used in image processing for denoising, blurring, sharpening, embossing, and edge detection.&lt;/li&gt;
&lt;li&gt;Image filter is a convolution of a mask (aka kernel, and convolution matrix) with an image that can be used for blurring, sharpening, edge detection, etc.&lt;/li&gt;
&lt;li&gt;A mask is a matrix convolved with an image.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;image-convolution-with-a-mask&#34;&gt;Image Convolution with a Mask&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Flip the mask (kernel) both horizontally and vertically.&lt;/li&gt;
&lt;li&gt;Put the center element of the mask at every pixel of the image. Multiply the corresponding elements and then add them up. Replace the pixel value corresponding to the center of the mask with the resulting sum.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/convolution.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For pixels on the border of image matrix, some elements of the mask might fall out of the image matrix. In this case, we can extend the image by adding zeros. This is known as padding.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/padding.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;denoising-of-smooth-images-using-splines&#34;&gt;Denoising of Smooth Images using Splines&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Another approach for denoising smooth images is to use local regression with smooth basis (eg. splines)&lt;/li&gt;
&lt;li&gt;Using Kronecker product, a 2D-spline basis can be generated from 1D basis matrices&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;image-segmentation&#34;&gt;Image Segmentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The main goal of image segmentation is to partition an image into multiple sets of pixels (segments)&lt;/li&gt;
&lt;li&gt;Image segmentation has been widely used for object detection, face and fingerprint recognition, medical imaging, video surveillance, etc.&lt;/li&gt;
&lt;li&gt;Various methods exist for image segmentation including:
&lt;ul&gt;
&lt;li&gt;Local and global thresholding&lt;/li&gt;
&lt;li&gt;Otsu&amp;rsquo;s method&lt;/li&gt;
&lt;li&gt;K-means clustering&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Thresholding is a simple segmentation approach that converts grayscale image to binary image by applying the thresholding function on histogram.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;otsus-method&#34;&gt;Otsu&amp;rsquo;s Method&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The goal is to automatically determine the threshold $t$ given an image histogram.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Otsu%27s_Method_Visualization.gif/440px-Otsu%27s_Method_Visualization.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Get the histogram of the image&lt;/li&gt;
&lt;li&gt;Calculate group mean and variance&lt;/li&gt;
&lt;li&gt;Find the maximum value for the variance&lt;/li&gt;
&lt;li&gt;Threshold the image&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;compute_otsu_criteria&lt;/span&gt;(im, th):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Otsu&amp;#39;s method to compute criteria.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# create the thresholded image&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    thresholded_im &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros(im&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    thresholded_im[im &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; th] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# compute weights&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    nb_pixels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; im&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;size
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    nb_pixels1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count_nonzero(thresholded_im)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    weight1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nb_pixels1 &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; nb_pixels
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    weight0 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; weight1
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# if one of the classes is empty, eg all pixels are below or above the threshold, that threshold will not be considered&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# in the search for the best threshold&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; weight1 &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; weight0 &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;inf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# find all pixels belonging to each class&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    val_pixels1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; im[thresholded_im &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    val_pixels0 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; im[thresholded_im &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# compute variance of these classes&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    var1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;var(val_pixels1) &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; len(val_pixels1) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    var0 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;var(val_pixels0) &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; len(val_pixels0) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; weight0 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; var0 &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; weight1 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; var1
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;im &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# load your image as a numpy array.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# For testing purposes, one can use for example im = np.random.randint(0,255, size = (50,50))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# testing all thresholds from 0 to the maximum of the image&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;threshold_range &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max(im)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;criterias &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [compute_otsu_criteria(im, th) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; th &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; threshold_range]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# best threshold is the one minimizing the Otsu criteria&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;best_threshold &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; threshold_range[np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argmin(criterias)]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;k-means-clustering-method&#34;&gt;K-Means Clustering Method&lt;/h3&gt;
&lt;p&gt;K-means clustering is a method for partitioning a set of observations to K clusters, such that the within-cluster variation is minimized.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rearrange the image pixels such that the number of rows in the resulting matrix is equal to the number of pixels and the number of columns is the same as the number of color channels&lt;/li&gt;
&lt;li&gt;Randomly select K centers&lt;/li&gt;
&lt;li&gt;Assign each pixel to the closest cluster&lt;/li&gt;
&lt;li&gt;Update the cluster mean&lt;/li&gt;
&lt;li&gt;Repeat the last two process until convergence&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The objective of K-means is to minimize the within cluster variation, and maximize the inter-class variation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;edge-detection&#34;&gt;Edge Detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Edges are significant local changes of intensity in an image.&lt;/li&gt;
&lt;li&gt;Edge Detection: Detect pixel with sudden intensity change&lt;/li&gt;
&lt;li&gt;Often points that lie on an edge are detected by:
&lt;ul&gt;
&lt;li&gt;Detecting the local &lt;strong&gt;maxima&lt;/strong&gt; or &lt;strong&gt;minima&lt;/strong&gt; of the first derivative.&lt;/li&gt;
&lt;li&gt;Detecting the &lt;strong&gt;zero-crossings&lt;/strong&gt; of the second derivative.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/edge.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;sobel-operator&#34;&gt;Sobel Operator&lt;/h3&gt;
&lt;p&gt;The Sobel operator works by convolving an image with a pair of 3x3 kernels or filters, one for detecting edges in the horizontal direction (often referred to as the Sobel-X operator) and the other for detecting edges in the vertical direction (often referred to as the Sobel-Y operator). These kernels are as follows:&lt;/p&gt;
&lt;p&gt;Sobel-X Kernel:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Sobel-Y Kernel:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;krisch-operator&#34;&gt;Krisch Operator&lt;/h3&gt;
&lt;p&gt;Krisch is another derivative mask that finds the maximum edge strength in eight directions of a compass.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/kirsh.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It is more time consuming compare to Sobel&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;prewitt-mask&#34;&gt;Prewitt Mask&lt;/h3&gt;
&lt;p&gt;The Prewitt operator, like the Sobel operator, employs a pair of 3x3 convolution kernels, one for detecting edges in the horizontal direction and the other for detecting edges in the vertical direction.&lt;/p&gt;
&lt;p&gt;Here are the two Prewitt kernels:&lt;/p&gt;
&lt;p&gt;Prewitt-X Kernel:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Prewitt-Y Kernel:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;laplacian-and-laplacian-of-gaussian-mask&#34;&gt;Laplacian and Laplacian of Gaussian Mask&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Laplacian mask is a second order derivative mask.&lt;/li&gt;
&lt;li&gt;For noisy images, is combined with a Gaussian mask to reduce the noise&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;Laplacian, Sobel, and Prewitt are masks used for edge detection. Gaussian is not a mask for edge detection.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;tensor-data-analysis&#34;&gt;Tensor Data Analysis&lt;/h1&gt;
&lt;h2 id=&#34;tensor-introduction&#34;&gt;Tensor Introduction&lt;/h2&gt;
&lt;p&gt;A tensor is an algebraic object that describes a multi-linear relationship between sets of algebraic objects related to a vector space. Tensors may map between different objects such as vectors, scalars, and even other tensors.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hkilter.com/images/7/7a/Tensors.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;terminologies&#34;&gt;Terminologies:&lt;/h3&gt;
&lt;h4 id=&#34;order&#34;&gt;Order&lt;/h4&gt;
&lt;p&gt;The order of a tensor refers to the number of indices or subscripts needed to specify its components in a given coordinate system. Tensors can have different orders, and the order determines their mathematical properties and how they transform under coordinate transformations. Here&amp;rsquo;s a brief overview of tensor orders:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Zeroth-Order Tensor (Scalar)&lt;/strong&gt;: A zeroth-order tensor is also known as a scalar. It has no indices and represents a single numerical value. Scalars are invariant under coordinate transformations.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Temperature at a point in space.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;First-Order Tensor (Vector)&lt;/strong&gt;: A first-order tensor, also known as a vector, has one index. Vectors represent quantities with both magnitude and direction and transform linearly under coordinate transformations.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Velocity, force, displacement.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Second-Order Tensor (Matrix)&lt;/strong&gt;: A second-order tensor has two indices. It represents a linear transformation that maps one vector to another. Matrices are used to represent various physical quantities, such as stress tensors, moment of inertia tensors, and more.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Stress tensor, moment of inertia tensor.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Third-Order Tensor:&lt;/strong&gt; A third-order tensor has three indices, and it is used to represent more complex relationships between vectors and matrices. These tensors are less common but can arise in various physical and mathematical contexts.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: Piezoelectric tensor in materials science.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/tensor_order.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;fibers&#34;&gt;Fibers&lt;/h4&gt;
&lt;p&gt;A fiber, the higher order analogue of matrix row and column, is defined by fixing every index but one, e.g.,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A matrix column is a mode-1 fiber and a matrix row is a mode-2 fiber&lt;/li&gt;
&lt;li&gt;Third-order tensors have column, row, and tube fibers&lt;/li&gt;
&lt;li&gt;Extracted fibers from a tensor are assumed to be oriented as column vectors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/fibers.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;slices&#34;&gt;Slices&lt;/h4&gt;
&lt;p&gt;Two-dimensional sections of a tensor, defined by fixing all but two indices.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/slices.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;norm&#34;&gt;Norm&lt;/h4&gt;
&lt;p&gt;Norm of a tensor $X \in \R^{I_1 \times I_2 \times &amp;hellip;. \times I_N}$ is the square root of the sum of the squares of all its elements.&lt;/p&gt;
&lt;p&gt;This is analogous to the matrix Frobenius norm, which is denoted $||A||_F$ for matrix $A$&lt;/p&gt;
&lt;h4 id=&#34;outer-product&#34;&gt;Outer Product&lt;/h4&gt;
&lt;p&gt;A multi-way vector outer product is a tensor where each element is the product of corresponding elements in vectors&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/outer_product.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;inner-product&#34;&gt;Inner Product&lt;/h4&gt;
&lt;p&gt;The inner product of two tensors is a generalization of the dot product operation for vectors as calculated by dot. A dot product operation (multiply and sum) is performed on all corresponding dimensions in the tensors, so the operation returns a scalar value. For this operation, the tensors must have the same size.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/innter_product.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;using-tensorly-for-inner-and-outer-product&#34;&gt;Using Tensorly for inner and outer product&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorly &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; tl
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Define the shape of the random tensors&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;shape &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Generate random data for X and Y using NumPy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;X_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rand(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;shape)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Y_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rand(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;shape)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Convert the NumPy arrays to TensorLy tensors&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tensor(X_data)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tensor(Y_data)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the inner product of X and Y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;inner_product &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tenalg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;inner(X, Y)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the outer product of X and Y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;outer_product &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tenalg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;outer(X, Y)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print the results&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Inner Product:&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(inner_product)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Outer Product:&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(outer_product)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;using-tensorly-for-unfolding-and-flattening&#34;&gt;Using Tensorly for unfolding, and flattening&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Unfold an N-way tensor into a matrix&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorly &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; tl
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a random 2x2x2 tensor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;random_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rand(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tensor(random_data)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Mode-1 matricization (unfold along the first mode)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mode1_matricization &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unfold(tensor, mode&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Mode-2 matricization (unfold along the second mode)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mode2_matricization &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unfold(tensor, mode&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Mode-3 matricization (unfold along the third mode)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mode3_matricization &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unfold(tensor, mode&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print the results&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Mode-1 Matricization:&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(mode1_matricization)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Mode-2 Matricization:&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(mode2_matricization)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Mode-3 Matricization:&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(mode3_matricization)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/matrixization.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;tensor-multiplication&#34;&gt;Tensor Multiplication&lt;/h2&gt;
&lt;p&gt;The n-mode product is referred to as multiplying a tensor by a matrix (or a vector) in mode n.&lt;/p&gt;
&lt;p&gt;The n-mode (matrix) product of a tensor $X \in \R^{I_1 \times I_2 \times &amp;hellip;. \times I_N}$ with a matrix $U \in R^{J \times I_n}$ is denoted by $X \times_n U$ and is of size $I_1 \times I_2 \times &amp;hellip;. \times I_{n-1} \times J \times I_{n+1} \times &amp;hellip;\times I_N $&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorly &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; tl
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a random 3x2x4 tensor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;random_tensor_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rand(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tensor(random_tensor_data)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a random matrix compatible with mode-1 multiplication&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;random_matrix_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rand(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;matrix &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tensor(random_matrix_data)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Multiply the tensor and matrix in mode-1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tenalg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mode_dot(tensor, matrix, mode&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print the result&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Random Tensor:&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(tensor)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Random Matrix:&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(matrix)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Result of Mode-1 Multiplication:&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(result)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;n-mode-vector-product&#34;&gt;n-Mode Vector Product&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorly &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; tl
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a random 3x2x4 tensor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;random_tensor_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rand(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tensor(random_tensor_data)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a random vector compatible with mode-1 multiplication&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;random_vector_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rand(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vector &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tensor(random_vector_data)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Multiply the tensor and vector in mode-1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tenalg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mode_dot(tensor, vector, mode&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print the random tensor, random vector, and the result of Mode-1 multiplication&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Random Tensor:&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(tensor)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Random Vector:&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(vector)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Result of Mode-1 Multiplication:&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(result)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;kronecker-product&#34;&gt;Kronecker Product&lt;/h4&gt;
&lt;p&gt;The Kronecker product, denoted by âŠ—, is a mathematical operation that combines two matrices to create a larger matrix. It is a tensor product of two matrices and results in a block matrix where each block is a scalar multiple of one of the elements of the first matrix, multiplied by the second matrix.&lt;/p&gt;
&lt;p&gt;The Kronecker Product of matrices $A \in R^{I \times J}$ and $B \in R^{K \times L}$ is denoted by $A \bigotimes B$. The result is a matrix size ($IK) \times (JL)$ and defined by&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/kronecker.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;examples&#34;&gt;Examples&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://wikimedia.org/api/rest_v1/media/math/render/svg/74fc4867467d053ae700ebb040ddfbe42600288c&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wikimedia.org/api/rest_v1/media/math/render/svg/1d5453c59a261174eb2458c21ff9bdd30dc2c87d&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create two matrices&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;B &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Compute the Kronecker product&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kronecker_product &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;kron(A, B)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print the result&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Kronecker Product:&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(kronecker_product)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;khatri-rao-product&#34;&gt;Khatri-Rao Product&lt;/h4&gt;
&lt;p&gt;The Khatri-Rao product, also known as the column-wise Kronecker product or simply the Khatri-Rao product, is an operation on two matrices that results in a matrix. It&amp;rsquo;s used in various applications in signal processing and linear algebra, especially in multilinear models and factorization problems. The Khatri-Rao product is denoted by âŠ™.&lt;/p&gt;
&lt;p&gt;Given two matrices, A of size m x n and B of size p x n, the Khatri-Rao product of A and B results in a matrix C of size (m * p) x n, where each column of C is formed by taking the Kronecker product of the corresponding columns of A and B.&lt;/p&gt;
&lt;h4 id=&#34;example&#34;&gt;Example&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://wikimedia.org/api/rest_v1/media/math/render/svg/311fb96a2459096ea05d8f0461e67a8b49f5ee43&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;so that:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wikimedia.org/api/rest_v1/media/math/render/svg/1e951f306d0dd52a9a56a35d767f2117db8a5ee6&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create two matrices A and B&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;B &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Compute the Khatri-Rao product&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;C &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;kron(A, B)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print the result&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Khatri-Rao Product:&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(C)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;If a and b are vectors, then the Khatri-Rao and Kronecker products are identical&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;hadamard-product&#34;&gt;Hadamard Product&lt;/h4&gt;
&lt;p&gt;The Hadamard product, also known as the element-wise product or Schur product, is an operation between two matrices or vectors of the same size, resulting in another matrix or vector of the same size. In this operation, each element of the resulting matrix is the product of the corresponding elements of the input matrices. It is denoted by âŠ™.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create two matrices or vectors of the same size&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;B &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Compute the Hadamard product&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;C &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; B
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print the result&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hadamard Product:&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(C)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;tensor-decomposition&#34;&gt;Tensor Decomposition&lt;/h2&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/L8uT6hgMt00?si=l6KQPaHQk80f7Nh9&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;rank-one-tensor&#34;&gt;Rank-One Tensor&lt;/h3&gt;
&lt;p&gt;A Rank-One Tensor can be created by the outer product of multiple vectors, e.g., a 3-order rank-one tensor is obtained by&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/outer_product.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;candecompparafac-cp-decomposition&#34;&gt;Candecomp/Parafac (CP) Decomposition&lt;/h3&gt;
&lt;p&gt;Candecomp/Parafac (CP)(Parallel Factor Analysis) decomposition is a tensor decomposition method used in multilinear algebra and multivariate data analysis. It is an extension of the matrix factorization technique, like Singular Value Decomposition (SVD), to higher-order tensors, often referred to as multi-way arrays.&lt;/p&gt;
&lt;p&gt;The CP decomposition factorizes a tensor into a sum of component rank-one tensors, e.g. given a third-order tensor $X \in R^{I \times J\ times K}$, CP decomposition is given by,&lt;/p&gt;
&lt;p&gt;$X \approx \sum_{r=1}^{R} a_r \cdot b_r \cdot c_r$&lt;/p&gt;
&lt;p&gt;$R$ is a positive integer.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If R is the rank of higher-tensor then CP decomposition will be exact and unique.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;rank-of-tensor&#34;&gt;Rank of Tensor&lt;/h3&gt;
&lt;p&gt;Rank of a tensor $X$, denoted by $rank(X)$ is the smallest number of rank-one tensors whose sum can generate $X$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Determining the rank of a tensor is an NP-hard problem. Some weaker upper bounds, however, exits that helps restrict the rank space. For example,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$X^{I \times J \times K}$,&lt;/p&gt;
&lt;p&gt;$rank(X)$ $\le min(IJ,JK, IK)$&lt;/p&gt;
&lt;h3 id=&#34;cp-decomposition-in-python&#34;&gt;CP Decomposition in Python&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorly &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; tl
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Define the dimensions of the tensor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;I &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;J &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;K &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a random tensor with the specified dimensions&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rand(I, J, K)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Set the desired rank for the decomposition&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rank &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Perform CP decomposition using TensorLy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;factors &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;decomposition&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parafac(X, rank&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;rank)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Reconstruct the original tensor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;reconstructed_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;kruskal_to_tensor(factors)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Evaluate the reconstruction error&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;reconstruction_error &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;norm(X &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; reconstructed_X, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Reconstruction Error: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;reconstruction_error&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;tucker-decomposition&#34;&gt;Tucker Decomposition&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Tucker Decomposition is &lt;strong&gt;not&lt;/strong&gt; a special case of CP Decomposition&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Tucker decomposition, also known as Tucker factorization or Tucker model, is a tensor decomposition method used to represent a multi-dimensional tensor as a core tensor and a set of factor matrices that capture the relationships between the tensor&amp;rsquo;s modes or dimensions. Tucker decomposition is a higher-order extension of matrix factorization techniques like Singular Value Decomposition (SVD) to tensors.&lt;/p&gt;
&lt;p&gt;In Tucker decomposition, a given tensor is approximated as the product of a core tensor and a set of factor matrices for each mode. The core tensor contains the most important information about the original tensor&amp;rsquo;s structure, while the factor matrices capture how each mode contributes to the overall tensor. Here&amp;rsquo;s an overview of the Tucker decomposition process:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorly &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; tl
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Define the dimensions of the tensor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;shape &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)  &lt;span style=&#34;color:#75715e&#34;&gt;# Change these dimensions according to your data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a random tensor with the specified dimensions&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tensor(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rand(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;shape))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Specify the Tucker rank (adjust these values as needed)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rank &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Perform Tucker decomposition using TensorLy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;core, factors &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;decomposition&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tucker(X, rank&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;rank)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Reconstruct the original tensor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;reconstructed_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tucker_to_tensor(core, factors)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Evaluate the reconstruction error&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;reconstruction_error &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;norm(X &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; reconstructed_X, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Reconstruction Error: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;reconstruction_error&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;optimization-and-application&#34;&gt;Optimization and Application&lt;/h1&gt;
&lt;p&gt;In plain English, &lt;strong&gt;optimization&lt;/strong&gt; is the action of making the best or most effective use of a situation or resource. Optimization problems are of great practical interest. For example, in manufacturing, how should one cut plates of a material so that the waste is minimized? In business, how should a company allocate the available resources that its profit is maximized? Some of the first optimization problems have been solved in ancient Greece and are regarded among the most significant discoveries of that time. In the first century A.D., the Alexandrian mathematician Heron solved the problem of finding the shortest path between two points by way of the mirror.&lt;/p&gt;
&lt;p&gt;This result, also known as Heronâ€™s theorem of the light ray, can be viewed as the origin of the theory of geometrical optics. The problem of finding extreme values gained special importance in the seventeenth century, when it served as one of the motivations in the invention of differential calculus, which is the foundation of the modern theory of mathematical optimization.&lt;/p&gt;
&lt;h2 id=&#34;generic-form-of-optimization-problem&#34;&gt;Generic form of optimization problem:&lt;/h2&gt;
&lt;p&gt;$min$ $f(x)$ $s.t.$ $x \in X $&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The vector $x = (x_1, . . . , x_n)$ is the optimization variable (or decision variable) of the problem&lt;/li&gt;
&lt;li&gt;The function $f$ is the objective function&lt;/li&gt;
&lt;li&gt;A vector $x$ is called optimal, or a solution (not optimal solution) of the problem, if it has the smallest objective value among all vectors that satisfy the constraints&lt;/li&gt;
&lt;li&gt;$X$ is the set of inequality constraints&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mathematical-ingredients&#34;&gt;Mathematical ingredients:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Encode decisions/actions as &lt;strong&gt;decision variables&lt;/strong&gt; whose values we are seeking&lt;/li&gt;
&lt;li&gt;Identify the relevant &lt;strong&gt;problem data&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Express &lt;strong&gt;constraints&lt;/strong&gt; on the values of the decision variables as mathematical relationships (inequalities) between the variables and problem data&lt;/li&gt;
&lt;li&gt;Express the &lt;strong&gt;objective function&lt;/strong&gt; as a function of the decision variables and the problem data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Minimize or Maximize an objective function of decision variable subject to constraints on the values of the decision variables.&lt;/strong&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;min or max f(x1, x2, .... , xn)
subject to gi(x1, x2, ...., ) &amp;lt;= bi     i = 1,....,m 
        xj is continuous or discrete    j = 1,....,n
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;the-problem-setting&#34;&gt;The problem setting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Finite number of decision variables&lt;/li&gt;
&lt;li&gt;A single objective function of decision variables and problem data
&lt;ul&gt;
&lt;li&gt;Multiple objective functions are handled by either taking a weighted combination of them or by optimizing one of the objectives while ensuring the other objectives meet target requirements.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The constraints are defined by a finite number of inequalities or equalities involving functions of the decision variables and problem data&lt;/li&gt;
&lt;li&gt;There may be domain restrictions (continuous or discrete) on some of the variables&lt;/li&gt;
&lt;li&gt;The functions defining the objective and constraints are algebraic (typically with rational coefficients)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;minimization-vs-maximization&#34;&gt;Minimization vs Maximization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Without the loss of generality, it is sufficient to consider a minimization objective since maximization of objective function is minimization of the negation of the objective function&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;program-vs-optimization&#34;&gt;Program vs Optimization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A program or mathematical program is an optimization problem with a finite number of variables and constraints written out using explicit mathematical (algebraic) expressions&lt;/li&gt;
&lt;li&gt;The word program means plan/planning&lt;/li&gt;
&lt;li&gt;Early application of optimization arose in planning resource allocations and gave rise to programming to mean optimization (predates computer programming)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example-designing-a-box&#34;&gt;Example: Designing a box:&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Given a $1$ feet by $1$ feet piece of cardboard, cut out corners and fold to make a box of maximum volume:&lt;/strong&gt;&lt;br/&gt;
&lt;strong&gt;Decision:&lt;/strong&gt; $x$ = how much to cut from each of the corners?&lt;br/&gt;
&lt;strong&gt;Alternatives:&lt;/strong&gt; $0&amp;lt;=x&amp;lt;=1/2$&lt;br/&gt;
&lt;strong&gt;Best:&lt;/strong&gt; Maximize volume: $V(x) = x(1-2x)^2$ ($x$ is the height and $(1-2x)^2$ is the base, and their product is the volume)&lt;br/&gt;
&lt;strong&gt;Optimization formulation:&lt;/strong&gt; $max$ $x(1-2x)^2$ subject to $0&amp;lt;=x&amp;lt;=1/2$ (which are the constraints in this case)&lt;br/&gt;&lt;/p&gt;
&lt;iframe src=&#34;https://www.desmos.com/calculator/ily45jyfsv?embed&#34; width=&#34;100%&#34; height=&#34;500&#34; style=&#34;border: 1px solid #ccc&#34; frameborder=0&gt;&lt;/iframe&gt;
&lt;p&gt;This is an unconstrained optimization problem since the constraint is a simple bound based.&lt;/p&gt;
&lt;h3 id=&#34;example-data-fitting&#34;&gt;Example: Data Fitting:&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Given $N$ data points $(y_1, x_1)&amp;hellip;(y_N, x_N)$ where $y_i$ belongs to $\mathbb{R}$ and $x_i$ belongs to $\mathbb{R}^n$, for all $i = 1..N$, find a line $y = a^Tx+b$ that best fits the data.&lt;/strong&gt;&lt;br/&gt;
&lt;strong&gt;Decision&lt;/strong&gt;: A vector $a$ that belongs to $\mathbb{R}^n$ and a scalar $b$ that belongs to $\mathbb{R}$&lt;br/&gt;
&lt;strong&gt;Alternatives&lt;/strong&gt;: All $n$-dimensional vectors and scalars&lt;br/&gt;
&lt;strong&gt;Best&lt;/strong&gt;: Minimize the sum of squared errors&lt;br/&gt;
&lt;strong&gt;Optimization formulation&lt;/strong&gt;:
$\begin{array}{ll}\min &amp;amp; \sum_{i=1}^N\left(y_i-a^{\top} x_i-b\right)^2 \ \text { s.t. } &amp;amp; a \in \mathbb{R}^n, b \in \mathbb{R}\end{array}$&lt;/p&gt;
&lt;p&gt;This is also an unconstrained optimization problem.&lt;/p&gt;
&lt;h3 id=&#34;example-product-mix&#34;&gt;Example: Product Mix:&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A firm make $n$ different products using $m$ types of resources. Each unit of product $i$ generates $p_i$ dollars of profit, and requires $r_{ij}$ units of resource $j$. The firm has $u_j$ units of resource $j$ available. How much of each product should the firm make to maximize profits?&lt;/strong&gt;&lt;br/&gt;
&lt;strong&gt;Decision&lt;/strong&gt;: how much of each product to make&lt;br/&gt;
&lt;strong&gt;Alternatives&lt;/strong&gt;: defined by the resource limits&lt;br/&gt;
&lt;strong&gt;Best&lt;/strong&gt;: Maximize profits&lt;br/&gt;
&lt;strong&gt;Optimization formulation:&lt;/strong&gt; &lt;br/&gt;
Sum notation: $\begin{array}{lll}\max &amp;amp; \sum_{i=1}^n p_i x_i \ \text { s.t. } &amp;amp; \sum_{i=1}^n r_{i j} x_i \leq u_j &amp;amp; \forall j=1, \ldots, m \ &amp;amp; x_i \geq 0 &amp;amp; \forall i=1, \ldots, n\end{array}$ &lt;br/&gt;
Matrix notation: $\begin{array}{cl}\max &amp;amp; p^{\top} x \ \text { s.t. } &amp;amp; R x \leq u \ &amp;amp; x \geq 0\end{array}$&lt;/p&gt;
&lt;h3 id=&#34;example-project-investment&#34;&gt;Example: Project investment&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt; A firm is considering investing in $n$ different R&amp;amp;D projects. Project $j$ requires an investment of $c_j$ dollars and promises a return of $r_j$ dollars. The firm has a budget of $B$ dollars. Which projects should the firm invest in?&lt;/strong&gt;&lt;br/&gt;
&lt;strong&gt;Decision&lt;/strong&gt;: Whether or not to invest in project&lt;br/&gt;
&lt;strong&gt;Alternatives&lt;/strong&gt;: Defined by budget&lt;br/&gt;
&lt;strong&gt;Best&lt;/strong&gt;: Maximize return on investment&lt;br/&gt;
Sum notation: $\begin{aligned} \max &amp;amp; \sum_{j=1}^n r_j x_j \ \text { s.t. } &amp;amp; \sum_{j=1}^n c_j x_j \leq B \ &amp;amp; x_j \in{0,1} \forall j=1, \ldots, n\end{aligned}$ &lt;br/&gt;
Matrix notation: $\begin{aligned} \max  &amp;amp; r^{\top} x \ \text { s.t. } &amp;amp; c^{\top} x \leq B \ &amp;amp; x \in{0,1}^n\end{aligned}$&lt;/p&gt;
&lt;p&gt;This is not an unconstrained problem.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Identify basic portfolio optimization and associated issues&lt;/li&gt;
&lt;li&gt;Examine the Markowitz Portfolio Optimization approach
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Markowitz Principle&lt;/strong&gt;: Select a portfolio that attempts to maximize the expected return and minimize the variance of returns (risk)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For multi objective problem (like defined by the Markowitz Principle), two objectives can be combined:
&lt;ul&gt;
&lt;li&gt;Maximize Expected Return - $\lambda$*risk&lt;/li&gt;
&lt;li&gt;Maximize Expected Return subject to risk &amp;lt;= s_max (constraint on risk)&lt;/li&gt;
&lt;li&gt;Minimize Risk subject to return &amp;gt;= r_min (threshold on expected returns)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Optimization Problem Statement&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Given $1000, how much should we invest in each of the three stocks MSFT, V and WMT so as to :
- have a one month expected return of at least a given threshold
- minimize the risk(variance) of the portfolio return
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Decision&lt;/strong&gt;: investment in each stock&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;alternatives&lt;/strong&gt;: any investment that meets the budget and the minimum expected return requirement&lt;/li&gt;
&lt;li&gt;best: minimize variance&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key trade-off&lt;/strong&gt;: How much of the detail of the actual problem to consider while maintaining computational tractability of the mathematical model?&lt;/li&gt;
&lt;li&gt;Requires making simplifying assumptions, either because some of the problem characteristics are not well-defined mathematically, or because we wish to develop a model that can actually be solved&lt;/li&gt;
&lt;li&gt;Need to exercise great caution in these assumptions and not loose sight of the true underlying problem&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assumptions&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;No transaction cost&lt;/li&gt;
&lt;li&gt;Stocks does not need to be bought in blocks (any amount &amp;gt;=0 is fine)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimization Process&lt;/strong&gt;: Decision Problem -&amp;gt; Model -&amp;gt; Data Collection -&amp;gt; Model Solution -&amp;gt; Analysis -&amp;gt; Problem solution&lt;/li&gt;
&lt;li&gt;No clear cut recipe&lt;/li&gt;
&lt;li&gt;Lots of feedbacks and iterations&lt;/li&gt;
&lt;li&gt;Approximations and assumptions involved in each stage&lt;/li&gt;
&lt;li&gt;Success requires good understanding of the actual problem (domain knowledge is important)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;classification-of-optimization-problems&#34;&gt;Classification of optimization problems&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The tractability of a large scale optimization problem depends on the structure of the functions that make up the objective and constraints, and the domain restrictions on the variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Functions&lt;/th&gt;
&lt;th&gt;Variable domains&lt;/th&gt;
&lt;th&gt;Problem Type&lt;/th&gt;
&lt;th&gt;Difficulty&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;All linear&lt;/td&gt;
&lt;td&gt;Continuous variables&lt;/td&gt;
&lt;td&gt;Linear Program&lt;/td&gt;
&lt;td&gt;Easy&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Some nonlinear&lt;/td&gt;
&lt;td&gt;Continuous variables&lt;/td&gt;
&lt;td&gt;Nonlinear Program or Nonlinear Optimization Problem&lt;/td&gt;
&lt;td&gt;Easy/Difficult&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Linear/nonlinear&lt;/td&gt;
&lt;td&gt;Some discrete&lt;/td&gt;
&lt;td&gt;Integer Problem or Discrete Optimization Problem&lt;/td&gt;
&lt;td&gt;Difficult&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Optimization Problem&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Difficulty&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Linear Programming&lt;/td&gt;
&lt;td&gt;A linear programming problem involves maximizing or minimizing a linear objective function subject to a set of linear constraints&lt;/td&gt;
&lt;td&gt;Easy to moderate&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Nonlinear Programming&lt;/td&gt;
&lt;td&gt;A nonlinear programming problem involves optimizing a function that is not linear, subject to a set of nonlinear constraints&lt;/td&gt;
&lt;td&gt;Moderate to hard&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Quadratic Programming&lt;/td&gt;
&lt;td&gt;A quadratic programming problem involves optimizing a quadratic objective function subject to a set of linear constraints&lt;/td&gt;
&lt;td&gt;Moderate&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Convex Optimization&lt;/td&gt;
&lt;td&gt;A convex optimization problem involves optimizing a convex function subject to a set of linear or convex constraints&lt;/td&gt;
&lt;td&gt;Easy to moderate&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Integer Programming&lt;/td&gt;
&lt;td&gt;An integer programming problem involves optimizing a linear or nonlinear objective function subject to a set of linear or nonlinear constraints, where some or all of the variables are restricted to integer values&lt;/td&gt;
&lt;td&gt;Hard&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mixed-integer Programming&lt;/td&gt;
&lt;td&gt;A mixed-integer programming problem is a generalization of integer programming where some or all of the variables can be restricted to integer values or continuous values&lt;/td&gt;
&lt;td&gt;Hard&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Global Optimization&lt;/td&gt;
&lt;td&gt;A global optimization problem involves finding the global optimum of a function subject to a set of constraints, which may be nonlinear or non-convex&lt;/td&gt;
&lt;td&gt;Hard&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stochastic Optimization&lt;/td&gt;
&lt;td&gt;A stochastic optimization problem involves optimizing an objective function that depends on random variables, subject to a set of constraints&lt;/td&gt;
&lt;td&gt;Hard&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;subclasses-of-nlp-non-linear-problem&#34;&gt;Subclasses of NLP (Non Linear Problem)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Unconstrained optimization&lt;/strong&gt;: No constraints or simple bound constraints on the variables (Box design example above)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Quadratic programming&lt;/strong&gt;: Objectives and constraints involve quadratic functions (Data fitting example above), &lt;strong&gt;subset of NLP&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;subclasses-of-ip-integer-programming&#34;&gt;Subclasses of IP (Integer Programming)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mixed Integer Linear Program&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;All linear functions&lt;/li&gt;
&lt;li&gt;Some variables are continuous and some are discrete&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mixed Integer Nonlinear Program (MINLP)&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Some nonlinear functions&lt;/li&gt;
&lt;li&gt;Some variables are continuous and some are discrete&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mixed Integer Quadratic Program (MIQLP)&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Nonlinear functions are quadratic&lt;/li&gt;
&lt;li&gt;Some variables are continuous and some are discrete&lt;/li&gt;
&lt;li&gt;subset of MINLP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;why-and-how-to-classify&#34;&gt;Why and how to classify?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Important to recognize the type of an optimization problem:
&lt;ul&gt;
&lt;li&gt;to formulate problems to be amenable to certain solution methods&lt;/li&gt;
&lt;li&gt;to anticipate the difficulty of solving the problem&lt;/li&gt;
&lt;li&gt;to know which solution methods to use&lt;/li&gt;
&lt;li&gt;to design customized solution methods&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;how to classify:
&lt;ul&gt;
&lt;li&gt;check domain restriction on variables&lt;/li&gt;
&lt;li&gt;check the structure of the functions involved&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;taylor-approximation&#34;&gt;Taylor Approximation&lt;/h3&gt;
&lt;p&gt;The Taylor series of a real or complex-valued function fâ€‰(x) that is infinitely differentiable at a real or complex number a is the power series.&lt;/p&gt;
&lt;p&gt;Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a differentiable function and $\mathbf{x}^0 \in \mathbb{R}^n$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First order Taylor&amp;rsquo;s approximation of $f$ at $\mathbf{x}^0$ :
$$
f(\mathbf{x}) \approx f\left(\mathbf{x}^0\right)+\nabla f\left(\mathbf{x}^0\right)^{\top}\left(\mathbf{x}-\mathbf{x}^0\right)
$$&lt;/li&gt;
&lt;li&gt;Second order Taylor&amp;rsquo;s approximation of $f$ at $\mathbf{x}^0$ :
$$
f(\mathbf{x}) \approx f\left(\mathbf{x}^0\right)+\nabla f\left(\mathbf{x}^0\right)^{\top}\left(\mathbf{x}-\mathbf{x}^0\right)+\frac{1}{2}\left(\mathbf{x}-\mathbf{x}^0\right)^{\top} \nabla^2 f\left(\mathbf{x}^0\right)\left(\mathbf{x}-\mathbf{x}^0\right)
$$
`&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sets-in-optimization-problems&#34;&gt;Sets in Optimization Problems&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A set is &lt;strong&gt;closed&lt;/strong&gt; if it includes its boundary points.&lt;/li&gt;
&lt;li&gt;Intersection of closed sets is closed.&lt;/li&gt;
&lt;li&gt;Typically, if none of inequalities are strict, then the set is closed.&lt;/li&gt;
&lt;li&gt;A set is convex if a line segment connecting two points in the set lies entirely in the set.&lt;/li&gt;
&lt;li&gt;A set is bounded if it can be enclosed in a large enough (hyper)-sphere or a box.&lt;/li&gt;
&lt;li&gt;A set that is both bounded and closed is called compact.
&lt;ul&gt;
&lt;li&gt;$R^2$ is closed but not bounded&lt;/li&gt;
&lt;li&gt;$x^2+y^2&amp;lt;1$ is bounded but not closed&lt;/li&gt;
&lt;li&gt;$x+y&amp;gt;=1$ is closed but not bounded&lt;/li&gt;
&lt;li&gt;$x^2+y^2&amp;lt;=1$ is closed and bounded (compact)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;An optimal solution of maximizing a convex function over a compact set lies on the boundary
of the set.&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe src=&#34;https://www.desmos.com/calculator/49e59msg7u?embed&#34; width=&#34;100%&#34; height=&#34;500&#34; style=&#34;border: 1px solid #ccc&#34; frameborder=0&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;convex-function&#34;&gt;Convex Function&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/ConvexFunction.svg/1280px-ConvexFunction.svg.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is convex if
$$
f(\lambda \mathbf{x}+(1-\lambda) \mathbf{y}) \leq \lambda f(\mathbf{x})+(1-\lambda) f(\mathbf{y}) \quad \forall \mathbf{x}, \mathbf{y} \in \mathbb{R}^n \text { and } \lambda \in[0,1]
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Function value at the average is less than the average of the function values&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;This also implies that $a^Tx+b$ is convex (and concave)&lt;/li&gt;
&lt;li&gt;For a convex function the first order Taylor&amp;rsquo;s approximation is a global under estimator&lt;/li&gt;
&lt;li&gt;A convex optimization problem has a convex objective and convex set of solutions.&lt;/li&gt;
&lt;li&gt;Linear programs (LPs) can be seen as a special case of convex optimization problems. In an LP, the objective function and constraints are linear, which means that the feasible region defined by the constraints is a convex set. As a result, the optimal solution to an LP is guaranteed to be at a vertex (corner) of the feasible region, which makes it a convex optimization problem.&lt;/li&gt;
&lt;li&gt;A twice differentiable univariate function is convex if $f^{&amp;rsquo;&amp;rsquo;}(x)&amp;gt;=0$ for all $x \in R$&lt;/li&gt;
&lt;li&gt;To generalize, a twice differentiable function is convex if and only if the Hessian matrix is positive semi definite.&lt;/li&gt;
&lt;li&gt;A positive semi-definite (PSD) matrix is a matrix that is symmetric and has non-negative eigenvalues. In the context of a Hessian matrix, it represents the second-order partial derivatives of a multivariate function and reflects the curvature of the function. If the Hessian is PSD, it indicates that the function is locally convex, meaning that it has a minimum value in the vicinity of that point. On the other hand, if the Hessian is not PSD, the function may have a saddle point or be locally non-convex. The PSD property of a Hessian matrix is important in optimization, as it guarantees the existence of a minimum value for the function.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sylvester&amp;rsquo;s criterion&lt;/strong&gt; is a method for determining if a matrix is positive definite or positive semi-definite. The criterion states that a real symmetric matrix is positive definite if and only if all of its leading principal minors (i.e. determinants of the submatrices formed by taking the first few rows and columns of the matrix) are positive. If all the leading principal minors are non-negative, then the matrix is positive semi-definite.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;first-order-methods&#34;&gt;First Order Methods&lt;/h2&gt;
&lt;h3 id=&#34;gradient-descent&#34;&gt;Gradient descent&lt;/h3&gt;
&lt;p&gt;Gradient descent is an iterative optimization algorithm used to find the minimum of a function, typically used in machine learning and deep learning to update the parameters of a model during training. The basic idea behind gradient descent is to adjust the parameters in the direction of steepest descent (negative gradient) to minimize a cost or loss function. Here&amp;rsquo;s a detailed explanation of gradient descent:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Objective Function&lt;/strong&gt; : Gradient descent begins with an objective function (also called a cost or loss function) that you want to minimize. In machine learning, this function typically represents the error between the model&amp;rsquo;s predictions and the actual data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s denote the objective function as $J(Î¸)$, where $Î¸$ represents a vector of parameters that we want to optimize.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Initialization&lt;/strong&gt; : You start by initializing the parameter vector Î¸ with some arbitrary values or often with random values. This is the starting point of the optimization process.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gradient Calculation&lt;/strong&gt; : Calculate the gradient of the objective function with respect to the parameters. The gradient is a vector that points in the direction of the steepest increase in the function. Mathematically, the gradient is represented as:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$(\nabla J(\theta) = \left[\frac{\partial J(\theta)}{\partial \theta_1}, \frac{\partial J(\theta)}{\partial \theta_2}, \ldots, \frac{\partial J(\theta)}{\partial \theta_n}\right])
$&lt;/p&gt;
&lt;p&gt;Here, $âˆ‚J(Î¸)/âˆ‚Î¸_i$ represents the partial derivative of $J(Î¸)$ with respect to the i-th parameter $Î¸_i$.&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;Update Parameters&lt;/strong&gt; : Update the parameters Î¸ using the gradient. The update rule is as follows:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$Î¸_{new} = Î¸_{old} - Î± * âˆ‡J(Î¸_{old})$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Î¸_{new}$ is the updated parameter vector.&lt;/li&gt;
&lt;li&gt;$Î¸_{old}$ is the current parameter vector.&lt;/li&gt;
&lt;li&gt;$Î±$ (alpha) is the learning rate, a hyperparameter that controls the step size or how much to move in the direction of the gradient. It&amp;rsquo;s a small positive value typically chosen in advance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This step is performed iteratively until a stopping criterion is met.&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stopping Criterion&lt;/strong&gt; : The algorithm continues to update the parameters and compute the gradient until a stopping criterion is satisfied. Common stopping criteria include a maximum number of iterations, a minimum change in the objective function, or when the gradient becomes close to zero.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Convergence&lt;/strong&gt; : Gradient descent converges when it reaches a local or global minimum of the objective function, or when it satisfies the stopping criterion. The choice of learning rate (Î±) and the convergence behavior are important aspects to consider when using gradient descent.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are variations of gradient descent, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Batch Gradient Descent&lt;/strong&gt; : In each iteration, it computes the gradient using the entire dataset. This can be computationally expensive for large datasets.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stochastic Gradient Descent (SGD)&lt;/strong&gt; : In each iteration, it computes the gradient using only one random data point from the dataset. It&amp;rsquo;s faster but has more noise in its updates.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mini-batch Gradient Descent&lt;/strong&gt; : It uses a small random subset (mini-batch) of the dataset in each iteration, striking a balance between the computational efficiency of SGD and the stability of batch gradient descent.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Momentum, Adagrad, RMSprop, and Adam&lt;/strong&gt; : These are advanced optimization techniques that incorporate additional mechanisms to improve convergence speed and stability.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The choice of the specific gradient descent variant and its hyperparameters depends on the problem at hand, the dataset, and computational resources. Proper tuning of the learning rate and monitoring convergence is crucial for successful optimization.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;gradient_descent&lt;/span&gt;(obj_func, gradient_func, initial_theta, learning_rate&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.01&lt;/span&gt;, max_iterations&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;, tolerance&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1e-5&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    theta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; initial_theta
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; iteration &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(max_iterations):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        gradient &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; gradient_func(theta)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        theta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; theta &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; learning_rate &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; gradient
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the magnitude of the gradient for convergence check&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        gradient_magnitude &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linalg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;norm(gradient)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; gradient_magnitude &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; tolerance:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Converged after &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;iteration&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; iterations.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; theta
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;objective_function&lt;/span&gt;(theta):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; theta[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; theta[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;gradient_function&lt;/span&gt;(theta):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; theta[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; theta[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Initial parameters&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;initial_theta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;3.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4.0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Call gradient descent&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;final_theta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; gradient_descent(objective_function, gradient_function, initial_theta)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Optimal parameters:&amp;#34;&lt;/span&gt;, final_theta)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Minimum value of the objective function:&amp;#34;&lt;/span&gt;, objective_function(final_theta))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;accelerated-gradient-descent&#34;&gt;Accelerated Gradient Descent&lt;/h3&gt;
&lt;p&gt;Accelerated Gradient Descent, also known as Nesterov Accelerated Gradient (NAG) or simply Nesterov momentum, is an optimization algorithm that improves upon the standard gradient descent by adding momentum to the updates. This momentum helps the algorithm converge faster and provides better stability, especially in the presence of noisy gradients. Here&amp;rsquo;s how it works:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Initialization&lt;/strong&gt; : Initialize the parameter vector Î¸ with some arbitrary values, and initialize the momentum vector &lt;code&gt;v&lt;/code&gt; to zero.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Update Momentum (v)&lt;/strong&gt; : In each iteration, before computing the gradient, update the momentum vector &lt;code&gt;v&lt;/code&gt; using the previous momentum and the gradient from the previous iteration. This is the key difference between NAG and traditional momentum.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$v_t = Î³ * v_{t-1} + Î± * âˆ‡J(Î¸_{t-1} - Î³ * v_{t-1})$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Î³$ is the momentum coefficient (typically a value close to 0.9).&lt;/li&gt;
&lt;li&gt;$Î±$ is the learning rate.&lt;/li&gt;
&lt;li&gt;$âˆ‡J(Î¸_{t-1} - Î³ * v_{t-1})$ is the gradient of the loss function evaluated at the updated position using the momentum.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Update Parameters (Î¸)&lt;/strong&gt; : Update the parameters Î¸ using the momentum &lt;code&gt;v&lt;/code&gt; computed in the previous step.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$Î¸_t = Î¸_{t-1} - v_t$&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;Repeat&lt;/strong&gt; : Repeat steps 2 and 3 until a stopping criterion is met, such as a maximum number of iterations or a small gradient magnitude.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The key insight in Nesterov Accelerated Gradient Descent is that it calculates the gradient at a &amp;ldquo;lookahead&amp;rdquo; position by subtracting the previous momentum-scaled update from the current parameters before computing the gradient. This lookahead helps in achieving smoother convergence by reducing oscillations.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a Python implementation of Nesterov Accelerated Gradient Descent:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;nesterov_accelerated_gradient_descent&lt;/span&gt;(obj_func, gradient_func, initial_theta, learning_rate&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.01&lt;/span&gt;, momentum&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.9&lt;/span&gt;, max_iterations&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;, tolerance&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1e-5&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    theta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; initial_theta
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros_like(initial_theta)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; iteration &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(max_iterations):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        gradient &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; gradient_func(theta &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; momentum &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; v)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; momentum &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; learning_rate &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; gradient
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        theta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; theta &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; v
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the magnitude of the gradient for convergence check&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        gradient_magnitude &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linalg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;norm(gradient)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; gradient_magnitude &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; tolerance:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Converged after &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;iteration&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; iterations.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; theta
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Example usage:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Define your objective function and gradient function&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;objective_function&lt;/span&gt;(theta):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; theta[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; theta[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;gradient_function&lt;/span&gt;(theta):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; theta[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; theta[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Initial parameters&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;initial_theta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;3.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4.0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Call Nesterov Accelerated Gradient Descent&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;final_theta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nesterov_accelerated_gradient_descent(objective_function, gradient_function, initial_theta)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Optimal parameters:&amp;#34;&lt;/span&gt;, final_theta)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Minimum value of the objective function:&amp;#34;&lt;/span&gt;, objective_function(final_theta))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this implementation, &lt;code&gt;momentum&lt;/code&gt; is the momentum coefficient (typically close to 0.9), and &lt;code&gt;learning_rate&lt;/code&gt; is the step size. The rest of the algorithm follows the Nesterov Accelerated Gradient Descent procedure described earlier.&lt;/p&gt;
&lt;h3 id=&#34;stochastic-gradient-descent-sgd&#34;&gt;Stochastic Gradient Descent (SGD)&lt;/h3&gt;
&lt;p&gt;Stochastic Gradient Descent (SGD) is an optimization algorithm used for training machine learning models, particularly in cases where you have a large dataset. Unlike traditional gradient descent, which computes the gradient of the cost function using the entire dataset in each iteration, SGD updates the model&amp;rsquo;s parameters by considering only a single randomly chosen data point (or a small batch of data points) in each iteration. This makes it much faster and enables it to handle large datasets.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s how Stochastic Gradient Descent works:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Initialization&lt;/strong&gt; : Start with an initial guess for the model&amp;rsquo;s parameters, often set to small random values.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Shuffling&lt;/strong&gt; : Shuffle the training dataset randomly. This step ensures that the data points are processed in a random order in each epoch (a complete pass through the dataset).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Iterative Update&lt;/strong&gt; : For each iteration:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Select a random data point (or a small mini-batch) from the shuffled dataset.&lt;/li&gt;
&lt;li&gt;Compute the gradient of the loss function with respect to the model&amp;rsquo;s parameters using only the selected data point (or mini-batch).&lt;/li&gt;
&lt;li&gt;Update the model&amp;rsquo;s parameters in the opposite direction of the gradient:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$Î¸ = Î¸ - learningrate * gradient$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Î¸&lt;/code&gt; represents the model&amp;rsquo;s parameters.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;learningrate&lt;/code&gt; is a positive scalar called the learning rate, which controls the step size in the parameter update.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gradient&lt;/code&gt; is the gradient of the loss function with respect to the parameters computed using the selected data point (or mini-batch).&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;Repeat&lt;/strong&gt; : Continue this process for a fixed number of iterations (epochs) or until a stopping criterion is met.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;SGD has several advantages, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Efficiency: It&amp;rsquo;s computationally efficient because it uses only one or a few data points at a time, making it suitable for large datasets.&lt;/li&gt;
&lt;li&gt;Regularization: The inherent randomness in SGD acts as a form of regularization, which can help prevent overfitting.&lt;/li&gt;
&lt;li&gt;Escape Local Minima: The noise introduced by using individual data points can help the algorithm escape local minima in the loss landscape.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, SGD can have high variance in its parameter updates because of the noisy gradient estimates, which can lead to oscillations and slow convergence. To address this, variations of SGD have been developed, including Mini-batch Gradient Descent, which uses small random mini-batches of data, and techniques like learning rate schedules and momentum to stabilize and accelerate convergence.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a simple Python implementation of Stochastic Gradient Descent:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;stochastic_gradient_descent&lt;/span&gt;(obj_func, gradient_func, initial_theta, learning_rate&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.01&lt;/span&gt;, max_epochs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;, tolerance&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1e-5&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    theta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; initial_theta
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    num_examples &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(obj_func)  &lt;span style=&#34;color:#75715e&#34;&gt;# Number of data points&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; epoch &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(max_epochs):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Shuffle the data for each epoch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        shuffled_indices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;permutation(num_examples)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; shuffled_indices:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            gradient &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; gradient_func(theta, i)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            theta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; theta &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; learning_rate &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; gradient
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the magnitude of the gradient for convergence check&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            gradient_magnitude &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linalg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;norm(gradient)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; gradient_magnitude &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; tolerance:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Converged after &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;epoch&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; epochs.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; theta
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Did not converge.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; theta
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Initial parameters&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;initial_theta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;3.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4.0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Call Stochastic Gradient Descent&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;final_theta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; stochastic_gradient_descent(obj_func, gradient_func, initial_theta)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Optimal parameters:&amp;#34;&lt;/span&gt;, final_theta)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Minimum value of the objective function:&amp;#34;&lt;/span&gt;, obj_func(final_theta))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this implementation, &lt;code&gt;obj_func&lt;/code&gt; represents the objective function you want to minimize, and &lt;code&gt;gradient_func&lt;/code&gt; computes the gradient of the loss function with respect to the model&amp;rsquo;s parameters for a specific data point &lt;code&gt;i&lt;/code&gt;. The rest of the algorithm follows the SGD procedure described earlier.&lt;/p&gt;
&lt;h2 id=&#34;second-order-methods&#34;&gt;Second order Methods&lt;/h2&gt;
&lt;h3 id=&#34;newton-method&#34;&gt;Newton method&lt;/h3&gt;
&lt;p&gt;Newton-Raphson method, often referred to as the Newton method, is an iterative numerical technique used to find the approximate roots (or solutions) of real-valued functions. It is particularly effective for finding the roots of nonlinear equations, optimizing functions, and solving systems of nonlinear equations. The method is named after Sir Isaac Newton and Joseph Raphson, who contributed to its development.&lt;/p&gt;
&lt;p&gt;The basic idea behind the Newton method is to iteratively refine an initial guess for the root by approximating the function with a linear equation (a tangent line) near the current guess. Mathematically, the algorithm can be summarized as follows:&lt;/p&gt;
&lt;p&gt;Given a function $f(x)$ and an initial guess $x_0$â€‹, repeat the following steps until a convergence criterion is met:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Compute the value of the function $f(x_n)$ and its derivative $fâ€²(x_n)$ at the current guess $x_n$â€‹.&lt;/li&gt;
&lt;li&gt;Update the guess for the root using the formula:
$(x_{n+1} = x_{n} - \frac{f&amp;rsquo;(x_{n})}{f(x_{n})})$&lt;/li&gt;
&lt;li&gt;Repeat steps 1 and 2 until $âˆ£f(xn+1)âˆ£$ is sufficiently close to zero or until a maximum number of iterations is reached.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The method converges rapidly if the initial guess is reasonably close to the actual root and if the function is well-behaved (continuous and with a well-defined derivative).&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a Python implementation of the Newton method to find the root of a single-variable function:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;newton_method&lt;/span&gt;(f, df, x0, tol&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1e-6&lt;/span&gt;, max_iter&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x0
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; iteration &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(max_iter):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        fx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; f(x)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        dfx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df(x)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Check for small derivatives to avoid division by zero&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; abs(dfx) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1e-6&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Derivative is too small.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x_new &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; fx &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; dfx
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; abs(x_new &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; x) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; tol:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Converged after &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;iteration&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; iterations.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x_new
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x_new
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Did not converge.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Example usage:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Define your function f(x) and its derivative df(x)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;f&lt;/span&gt;(x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;df&lt;/span&gt;(x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Initial guess&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x0 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Call Newton&amp;#39;s method&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;root &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; newton_method(f, df, x0)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Approximate root:&amp;#34;&lt;/span&gt;, root)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this example, &lt;code&gt;f(x)&lt;/code&gt; represents the function you want to find the root of, and &lt;code&gt;df(x)&lt;/code&gt; is its derivative. The algorithm iteratively refines the estimate of the root (&lt;code&gt;x&lt;/code&gt;) until it converges within a specified tolerance (&lt;code&gt;tol&lt;/code&gt;) or until a maximum number of iterations (&lt;code&gt;max_iter&lt;/code&gt;) is reached.&lt;/p&gt;
&lt;h3 id=&#34;gauss-newton-method&#34;&gt;Gauss-Newton method&lt;/h3&gt;
&lt;p&gt;Gauss-Newton method iteratively updates the parameter vector ppp to minimize the objective function. In each iteration, it approximates the objective function using a linearization around the current estimate of ppp, which is why it&amp;rsquo;s often considered an extension of the Newton-Raphson method for nonlinear optimization.&lt;/p&gt;
&lt;p&gt;Here are the main steps of the Gauss-Newton method:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Initialization&lt;/strong&gt; : Start with an initial guess for the parameter vector ppp.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Iterative Update&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;For each iteration:
&lt;ul&gt;
&lt;li&gt;Calculate the Jacobian matrix $J(p)$, which contains the partial derivatives of the model predictions $f_i(p)$ with respect to each parameter.&lt;/li&gt;
&lt;li&gt;Calculate the residual vector $r(p)$, which is the difference between the observed data $y_i$ and the model predictions $f_i(p)$ for all data points.&lt;/li&gt;
&lt;li&gt;Update the parameter vector $p$ using the following formula:
$p_{\text{new}} = p_{\text{old}} + \Delta p$
where
$\Delta p = (J^T J)^{-1} J^T r$
$J^T$ is the transpose of the Jacobian matrix, and $(J^T J)^{-1}$ is the pseudoinverse of the Jacobian matrix.&lt;/li&gt;
&lt;li&gt;Repeat the iterative update until a convergence criterion is met, such as small changes in the parameter estimates or a maximum number of iterations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Convergence&lt;/strong&gt;: The algorithm converges when the parameter estimates $p$ stabilize or when a predefined stopping criterion is satisfied.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here&amp;rsquo;s a Python example demonstrating the Gauss-Newton method for nonlinear regression:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; scipy.optimize &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; least_squares
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Define the model function and its Jacobian&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;model&lt;/span&gt;(p, x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; p[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;p[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; x)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;jacobian&lt;/span&gt;(p, x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;vstack((&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;p[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; x), x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; p[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;p[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; x)))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;T
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Generate synthetic data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;seed(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linspace(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;true_params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;2.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model(true_params, x_data) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn(len(x_data))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Define the objective function (residuals)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;objective&lt;/span&gt;(p):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; y_data &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; model(p, x_data)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Initial guess for parameters&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;initial_guess &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2.0&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Use SciPy&amp;#39;s least_squares function to perform Gauss-Newton optimization&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; least_squares(objective, initial_guess, jac&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;jacobian, method&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;lm&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Extract the estimated parameters&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;estimated_params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; result&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;True parameters:&amp;#34;&lt;/span&gt;, true_params)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Estimated parameters:&amp;#34;&lt;/span&gt;, estimated_params)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this example, we define a simple exponential model and generate synthetic data with added noise. Then, we use SciPy&amp;rsquo;s &lt;code&gt;least_squares&lt;/code&gt; function to perform the Gauss-Newton optimization to estimate the parameters of the model. The estimated parameters are compared to the true parameters to assess the quality of the fit.&lt;/p&gt;
&lt;h3 id=&#34;quasi-newton-methods&#34;&gt;Quasi-Newton methods&lt;/h3&gt;
&lt;p&gt;Quasi-Newton methods are a class of optimization algorithms used for finding the minimum of a scalar function of several variables. These methods belong to the broader category of gradient-based optimization techniques and are particularly useful when it&amp;rsquo;s computationally expensive to compute the exact Hessian matrix (the matrix of second derivatives) of the objective function. Instead of calculating the Hessian directly, quasi-Newton methods approximate it using updates based on gradient information, making them more efficient for many practical optimization problems.&lt;/p&gt;
&lt;p&gt;The most well-known quasi-Newton method is the Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno (BFGS) algorithm, but there are other variants like the Limited-memory BFGS (L-BFGS) that are suitable for large-scale problems. Here&amp;rsquo;s an overview of how quasi-Newton methods work:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Initialization&lt;/strong&gt; : Start with an initial guess for the parameters (variables) you want to optimize.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Initialization of the Approximate Hessian&lt;/strong&gt; : Initialize an approximation of the Hessian matrix. In BFGS, for example, this is usually done with the identity matrix or a scaled version of it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Iterative Update&lt;/strong&gt; : Quasi-Newton methods iteratively update the parameter vector and the approximation of the Hessian matrix until convergence is achieved. The main steps in each iteration are as follows:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Calculate the Gradient&lt;/strong&gt; : Compute the gradient of the objective function with respect to the parameters at the current parameter values.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solve the Quasi-Newton Equation&lt;/strong&gt; : Quasi-Newton methods update the approximation of the Hessian matrix using information from the gradient and parameter changes. The update formula depends on the specific quasi-Newton method (e.g., BFGS or L-BFGS). The objective is to construct an approximation of the Hessian that preserves important curvature information of the objective function.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Update Parameters&lt;/strong&gt; : Update the parameter vector using the approximate Hessian matrix. This update step usually involves solving a linear system of equations that is determined by the approximation of the Hessian.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Convergence Check&lt;/strong&gt; : Check for convergence criteria, such as the magnitude of the gradient, small changes in the parameters, or a predefined number of iterations.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;Convergence&lt;/strong&gt; : The optimization process terminates when one or more convergence criteria are met.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Quasi-Newton methods are efficient and widely used because they can handle large-scale optimization problems without explicitly computing and storing the full Hessian matrix, which can be computationally expensive and memory-intensive. Instead, they maintain an approximation of the Hessian using a limited amount of memory.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an example of how to use the L-BFGS-B variant of the L-BFGS quasi-Newton method in Python using SciPy&amp;rsquo;s optimization module:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; scipy.optimize &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; minimize
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Define the objective function to minimize&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;objective_function&lt;/span&gt;(x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (x[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Initial guess for the parameters&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;initial_guess &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Use L-BFGS-B for optimization&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; minimize(objective_function, initial_guess, method&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;L-BFGS-B&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Extract the optimized parameters&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;optimized_parameters &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; result&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Optimized parameters:&amp;#34;&lt;/span&gt;, optimized_parameters)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Minimum value of the objective function:&amp;#34;&lt;/span&gt;, result&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fun)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this example, we define a simple quadratic objective function to minimize, and we use the &lt;code&gt;minimize&lt;/code&gt; function from SciPy&amp;rsquo;s optimization module with the L-BFGS-B method to find the minimum. The &lt;code&gt;result&lt;/code&gt; object contains the optimized parameters and the minimum value of the objective function.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning for Trading</title>
      <link>https://ayushsubedi.github.io/posts/machine_learning_for_trading/</link>
      <pubDate>Sun, 04 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/machine_learning_for_trading/</guid>
      <description>&lt;h1 id=&#34;machine-learning-for-trading&#34;&gt;Machine Learning for Trading&lt;/h1&gt;
&lt;p&gt;Machine learning plays a vital role in trading by enabling the analysis of vast amounts of financial data and the development of predictive models. It leverages algorithms and statistical techniques to identify patterns, make predictions, and generate insights for informed trading decisions. Machine learning algorithms can be applied to various aspects of trading, including price prediction, risk management, portfolio optimization, market analysis, and automated trading. By leveraging machine learning, traders can uncover hidden patterns in data, adapt to changing market conditions, and improve decision-making processes, ultimately aiming to achieve better trading performance and profitability.&lt;/p&gt;
&lt;h1 id=&#34;sections&#34;&gt;Sections&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#manipulating-financial-data&#34;&gt;Manipulating Financial Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#computational-investing&#34;&gt;Computational Investing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#learning-algorithms-for-trading&#34;&gt;Learning algorithms for Trading&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;manipulating-financial-data&#34;&gt;Manipulating Financial Data&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://i.ytimg.com/vi/_z6I9K6Sy6A/maxresdefault.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;pandas&#34;&gt;Pandas&lt;/h2&gt;
&lt;p&gt;Pandas is a popular Python library that provides powerful data manipulation and analysis tools. It&amp;rsquo;s widely used for working with various types of data, including stock data analysis.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Importing Pandas:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; pd
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Loading Data:&lt;/strong&gt; Load the stock data into a Pandas DataFrame. There are various ways to load data, such as reading from a CSV file or querying an API. Here&amp;rsquo;s an example of loading data from a CSV file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;stock_data.csv&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Exploring Data:&lt;/strong&gt; Use various Pandas functions to explore and understand the data. Some commonly used functions include &lt;code&gt;head()&lt;/code&gt;, &lt;code&gt;tail()&lt;/code&gt;, &lt;code&gt;info()&lt;/code&gt;, &lt;code&gt;describe()&lt;/code&gt;, and &lt;code&gt;shape&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()  &lt;span style=&#34;color:#75715e&#34;&gt;# Display the first few rows of the DataFrame&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;info()  &lt;span style=&#34;color:#75715e&#34;&gt;# Get information about the DataFrame&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;describe()  &lt;span style=&#34;color:#75715e&#34;&gt;# Statistical summary of the data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape  &lt;span style=&#34;color:#75715e&#34;&gt;# Get the number of rows and columns in the DataFrame&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Data Cleaning:&lt;/strong&gt; Perform any necessary data cleaning steps, such as handling missing values, removing duplicates, and converting data types.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dropna()  &lt;span style=&#34;color:#75715e&#34;&gt;# Drop rows with missing values&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;drop_duplicates()  &lt;span style=&#34;color:#75715e&#34;&gt;# Remove duplicate rows&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_datetime(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;])  &lt;span style=&#34;color:#75715e&#34;&gt;# Convert the &amp;#39;date&amp;#39; column to datetime&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Data Manipulation:&lt;/strong&gt; Pandas functions can be used to manipulate the data according to any analysis requirements. It can be used to filter rows, select specific columns, create new columns, apply mathematical operations, and more.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Selecting specific columns&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;close_price&amp;#39;&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Filtering rows&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df[df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;volume&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1000000&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Creating new columns&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;returns&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;close_price&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pct_change()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Applying mathematical operations&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;moving_average&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;close_price&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rolling(window&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Data Visualization:&lt;/strong&gt; Pandas can work well with other libraries like Matplotlib or Seaborn to create visualizations of the stock data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot(x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;, y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;close_price&amp;#39;&lt;/span&gt;, title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Stock Price&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;These are just a few examples of how Pandas can be used for stock data analysis. Pandas provides a wide range of functions and methods that can be used to manipulate, analyze, and visualize stock data effectively.&lt;/p&gt;
&lt;h3 id=&#34;slicing-and-indexing&#34;&gt;Slicing and indexing&lt;/h3&gt;
&lt;p&gt;Pandas provides several methods for slicing and indexing data in a DataFrame. Here are some commonly used techniques for slicing data with Pandas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Column Selection:&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;To select a single column, the square bracket notation with the column name as a string can be used:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;column_name&amp;#39;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;To select multiple columns, provide a list of column names within the square brackets:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;column_name1&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;column_name2&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Row Selection:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;To select rows based on a specific condition, use boolean indexing:&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df[condition]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;For example, to select rows where the &amp;lsquo;price&amp;rsquo; column is greater than 100:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df[df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;price&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Slicing Rows:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;To slice rows based on their position, use the &lt;code&gt;loc&lt;/code&gt; or &lt;code&gt;iloc&lt;/code&gt; accessor:&lt;/li&gt;
&lt;li&gt;&lt;code&gt;loc&lt;/code&gt; is label-based and inclusive of the endpoints.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;iloc&lt;/code&gt; is index-based and exclusive of the endpoints.&lt;/li&gt;
&lt;li&gt;For example, to slice the first five rows:&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;iloc[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]  &lt;span style=&#34;color:#75715e&#34;&gt;# Exclusive of the endpoint&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;To slice rows by labels, use:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;loc[start_label:end_label]  &lt;span style=&#34;color:#75715e&#34;&gt;# Inclusive of the endpoints&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Slicing Rows and Columns:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;To slice both rows and columns simultaneously, use the &lt;code&gt;loc&lt;/code&gt; or &lt;code&gt;iloc&lt;/code&gt; accessor with row and column selections separated by a comma:&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;loc[start_label:end_label, [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;column_name1&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;column_name2&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;iloc[start_index:end_index, [column_index1, column_index2, &lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For example, to slice the first five rows and select columns &amp;lsquo;price&amp;rsquo; and &amp;lsquo;volume&amp;rsquo;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;iloc[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;price&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;volume&amp;#39;&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;numpy&#34;&gt;Numpy&lt;/h2&gt;
&lt;p&gt;Numpy is a fundamental Python library that provides efficient numerical computing capabilities. It offers a powerful array data structure and a wide range of mathematical functions, making it useful for financial research and analysis. Here are some key points about Numpy focused on its application in financial research:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Numerical Data Handling:&lt;/strong&gt; Numpy provides the &lt;code&gt;ndarray&lt;/code&gt; (N-dimensional array) data structure, which is highly efficient for handling large volumes of numerical data. It allows for fast element-wise operations and supports various numerical data types, including integers, floating-point numbers, and complex numbers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Array Creation and Manipulation:&lt;/strong&gt; Numpy offers functions to create and manipulate arrays, such as &lt;code&gt;np.array()&lt;/code&gt;, &lt;code&gt;np.zeros()&lt;/code&gt;, &lt;code&gt;np.ones()&lt;/code&gt;, &lt;code&gt;np.arange()&lt;/code&gt;, and &lt;code&gt;np.linspace()&lt;/code&gt;. These functions are beneficial for creating arrays representing financial data, such as price series, returns, or volume data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mathematical Operations:&lt;/strong&gt; Numpy provides a comprehensive set of mathematical functions and operators that can be applied to arrays. These include basic arithmetic operations, statistical functions (&lt;code&gt;mean()&lt;/code&gt;, &lt;code&gt;std()&lt;/code&gt;, &lt;code&gt;min()&lt;/code&gt;, &lt;code&gt;max()&lt;/code&gt;, etc.), linear algebra functions (&lt;code&gt;dot()&lt;/code&gt;, &lt;code&gt;inv()&lt;/code&gt;, &lt;code&gt;eig()&lt;/code&gt;, etc.), and more advanced functions for trigonometry, exponentials, logarithms, and random number generation. These operations can be leveraged to perform calculations on financial data efficiently.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Aggregation and Summary Statistics:&lt;/strong&gt; Numpy functions are helpful for calculating summary statistics on financial data. Functions like &lt;code&gt;np.sum()&lt;/code&gt;, &lt;code&gt;np.mean()&lt;/code&gt;, &lt;code&gt;np.std()&lt;/code&gt;, &lt;code&gt;np.median()&lt;/code&gt;, and &lt;code&gt;np.percentile()&lt;/code&gt; allow you to calculate aggregate measures, central tendency, dispersion, and percentiles on arrays or subsets of data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Time Series Analysis:&lt;/strong&gt; Numpy provides tools for working with time series data, including date and time handling. The &lt;code&gt;np.datetime64&lt;/code&gt; data type enables storing and manipulating date and time values, allowing for easy handling of temporal aspects in financial research.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Broadcasting and Vectorization:&lt;/strong&gt; Numpy&amp;rsquo;s broadcasting feature allows for performing element-wise operations between arrays of different shapes and sizes, making it efficient for vectorized calculations. This feature is particularly useful when working with arrays representing financial data, as it enables applying operations across entire arrays without explicit looping.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integration with Other Libraries:&lt;/strong&gt; Numpy plays a vital role in the scientific Python ecosystem and integrates well with other libraries commonly used in financial research. For example, Numpy arrays can be seamlessly used with Pandas DataFrames, providing efficient data processing and analysis capabilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By leveraging Numpy&amp;rsquo;s capabilities, financial researchers can efficiently handle and analyze large datasets, perform mathematical computations, calculate summary statistics, and conduct time series analysis. Its fast execution and integration with other libraries make it a valuable tool for financial research and analysis.&lt;/p&gt;
&lt;h2 id=&#34;global-statistics&#34;&gt;Global Statistics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To calculate global statistics of stock prices in Python, you can use the Pandas library to load and manipulate stock price data. Here&amp;rsquo;s an example of how you can calculate common statistics such as mean, standard deviation, minimum, maximum, and percentiles for stock prices:
-Import the necessary libraries:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; pd
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Load the stock price data into a Pandas DataFrame. Assuming you have a CSV file named &amp;lsquo;stock_prices.csv&amp;rsquo; with a &amp;lsquo;price&amp;rsquo; column containing the stock prices, you can use the following code:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;stock_prices.csv&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Calculate the desired statistics using Numpy functions on the &amp;lsquo;price&amp;rsquo; column:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mean_price &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;price&amp;#39;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;std_price &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;std(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;price&amp;#39;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;min_price &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;min(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;price&amp;#39;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;max_price &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;price&amp;#39;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;percentiles &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;percentile(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;price&amp;#39;&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;25&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;75&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Print or use the calculated statistics as needed:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Mean price:&amp;#34;&lt;/span&gt;, mean_price)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Standard deviation:&amp;#34;&lt;/span&gt;, std_price)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Minimum price:&amp;#34;&lt;/span&gt;, min_price)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Maximum price:&amp;#34;&lt;/span&gt;, max_price)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;25th, 50th, and 75th percentiles:&amp;#34;&lt;/span&gt;, percentiles)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;rolling-statistics&#34;&gt;Rolling Statistics&lt;/h2&gt;
&lt;p&gt;To calculate rolling statistics for stock prices in Python, you can use the rolling window functionality provided by Pandas. Here&amp;rsquo;s an example of how you can calculate rolling mean and standard deviation for stock prices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Import the necessary libraries:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; pd
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Load the stock price data into a Pandas DataFrame. Assuming you have a CSV file named &amp;lsquo;stock_prices.csv&amp;rsquo; with a &amp;lsquo;price&amp;rsquo; column containing the stock prices, you can use the following code:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;stock_prices.csv&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Convert the date column to a datetime type if it is not already in that format:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_datetime(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Sort the DataFrame by the date column in ascending order:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sort_values(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Calculate the rolling mean and standard deviation using the &lt;code&gt;rolling()&lt;/code&gt; function on the &amp;lsquo;price&amp;rsquo; column:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;window_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# Define the rolling window size&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rolling_mean&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;price&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rolling(window&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;window_size)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rolling_std&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;price&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rolling(window&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;window_size)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;std()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the code above, &lt;code&gt;window_size&lt;/code&gt; represents the number of observations to include in each rolling window. You can adjust it based on your specific requirements.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Print or use the rolling statistics as needed:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;price&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rolling_mean&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rolling_std&amp;#39;&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This code will display the &amp;lsquo;date&amp;rsquo;, &amp;lsquo;price&amp;rsquo;, &amp;lsquo;rolling_mean&amp;rsquo;, and &amp;lsquo;rolling_std&amp;rsquo; columns of the DataFrame, showing the calculated rolling statistics.&lt;/p&gt;
&lt;p&gt;By applying these steps, you can calculate rolling statistics, such as the rolling mean and standard deviation, for stock prices using Python and Pandas. Feel free to modify the code to incorporate additional rolling statistics or customize the output to suit your needs.&lt;/p&gt;
&lt;h2 id=&#34;bollinger-bands&#34;&gt;Bollinger bands&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://www.investopedia.com/thmb/XOTAkeqxe65MifNefOz1vGXiQq0=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/dotdash_Final_Using_Bollinger_Bands_to_Gauge_Trends_Oct_2020-01-73f4b5749a6e445585bc2751d6e39d34.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Bollinger Bands is a popular technical analysis tool used to identify potential price trends and volatility in financial markets. It consists of three lines plotted on a price chart: the middle band (usually a simple moving average), an upper band (typically two standard deviations above the middle band), and a lower band (usually two standard deviations below the middle band). Here&amp;rsquo;s an example of how you can calculate and plot Bollinger Bands using Python and Pandas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Import the necessary libraries:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; pd
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Load the stock price data into a Pandas DataFrame. Assuming you have a CSV file named &amp;lsquo;stock_prices.csv&amp;rsquo; with a &amp;lsquo;price&amp;rsquo; column containing the stock prices, you can use the following code:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;stock_prices.csv&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Calculate the middle band, upper band, and lower band using rolling mean and standard deviation:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;window_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# Define the rolling window size&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;middle_band&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;price&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rolling(window&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;window_size)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;std&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;price&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rolling(window&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;window_size)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;std()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;upper_band&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;middle_band&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;std&amp;#39;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;lower_band&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;middle_band&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;std&amp;#39;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the code above, the &amp;lsquo;middle_band&amp;rsquo; is calculated as the rolling mean of the &amp;lsquo;price&amp;rsquo; column, while the &amp;lsquo;std&amp;rsquo; represents the rolling standard deviation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plot the Bollinger Bands:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;price&amp;#39;&lt;/span&gt;], label&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Price&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;middle_band&amp;#39;&lt;/span&gt;], label&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Middle Band&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;upper_band&amp;#39;&lt;/span&gt;], label&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Upper Band&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;lower_band&amp;#39;&lt;/span&gt;], label&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Lower Band&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Bollinger Bands&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Date&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ylabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Price&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;legend()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;The code above will create a line plot with the stock price (&amp;lsquo;price&amp;rsquo;) and the Bollinger Bands: the middle band (&amp;lsquo;middle_band&amp;rsquo;), upper band (&amp;lsquo;upper_band&amp;rsquo;), and lower band (&amp;rsquo;lower_band&amp;rsquo;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;daily-returns&#34;&gt;Daily returns&lt;/h2&gt;
&lt;p&gt;Daily returns refer to the percentage change in the value of an asset from one trading day to the next. It is a commonly used metric to measure the performance and volatility of an asset over time. Daily returns can be calculated using the following mathematical equation:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Daily Return = (Price_today - Price_yesterday) / Price_yesterday
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where Price_today is the closing price of the asset on the current day, and Price_yesterday is the closing price of the asset on the previous day.&lt;/p&gt;
&lt;p&gt;To calculate daily returns in Python, you can use the Pandas library. Here&amp;rsquo;s an example of Python code that calculates daily returns from a DataFrame containing historical price data:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; pd
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Assuming you have a DataFrame named &amp;#39;df&amp;#39; with a &amp;#39;closing_price&amp;#39; column&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;daily_return&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;closing_price&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pct_change()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print the DataFrame with daily returns&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;closing_price&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;daily_return&amp;#39;&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the code above, the &lt;code&gt;pct_change()&lt;/code&gt; function is used to calculate the percentage change between consecutive values in the &amp;lsquo;closing_price&amp;rsquo; column. The result is stored in a new column named &amp;lsquo;daily_return&amp;rsquo; in the DataFrame.&lt;/p&gt;
&lt;p&gt;The printed DataFrame will display the &amp;lsquo;date&amp;rsquo;, &amp;lsquo;closing_price&amp;rsquo;, and &amp;lsquo;daily_return&amp;rsquo; columns, showing the historical prices and corresponding daily returns.&lt;/p&gt;
&lt;h2 id=&#34;cumulative-returns&#34;&gt;Cumulative returns&lt;/h2&gt;
&lt;p&gt;Cumulative returns, in finance and trading, represent the total percentage change in the value of an asset over a given period. It provides an understanding of the overall performance and growth of an investment over time. Cumulative returns can be calculated by multiplying the daily returns together and then subtracting 1. The mathematical equation for calculating cumulative returns is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-scss&#34; data-lang=&#34;scss&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;Cumulative&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;Return&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;Daily&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;Return_1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;Daily&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;Return_2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;...&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;Daily&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;Return_n&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;where Daily Return_1, Daily Return_2, &amp;hellip;, Daily Return_n are the daily returns for each respective trading day.&lt;/p&gt;
&lt;p&gt;To calculate cumulative returns in Python, you can use the Pandas library. Here&amp;rsquo;s an example of Python code that calculates cumulative returns from a DataFrame containing daily return data:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; pd
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Assuming you have a DataFrame named &amp;#39;df&amp;#39; with a &amp;#39;daily_return&amp;#39; column&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;cumulative_return&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;daily_return&amp;#39;&lt;/span&gt;])&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cumprod() &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print the DataFrame with cumulative returns&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;daily_return&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;cumulative_return&amp;#39;&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the code above, the &lt;code&gt;cumprod()&lt;/code&gt; function is used to calculate the cumulative product of the (1 + daily_return) values. The result is then subtracted by 1 to obtain the cumulative return. The cumulative returns are stored in a new column named &amp;lsquo;cumulative_return&amp;rsquo; in the DataFrame.&lt;/p&gt;
&lt;p&gt;The printed DataFrame will display the &amp;lsquo;date&amp;rsquo;, &amp;lsquo;daily_return&amp;rsquo;, and &amp;lsquo;cumulative_return&amp;rsquo; columns, showing the historical daily returns and corresponding cumulative returns.&lt;/p&gt;
&lt;h2 id=&#34;histograms-and-scatter-plots&#34;&gt;Histograms and Scatter Plots&lt;/h2&gt;
&lt;p&gt;Histograms provide a graphical representation of the distribution of a dataset. In the context of market analysis, histograms are often used to visualize the frequency distribution of stock prices, trading volumes, or other relevant financial variables. They display the number of occurrences or the probability of data falling within different intervals, allowing analysts to identify patterns, outliers, and the shape of the distribution. Histograms help in understanding the central tendency, dispersion, and skewness of the data, providing valuable insights into market dynamics.&lt;/p&gt;
&lt;p&gt;Scatter plots, on the other hand, visualize the relationship between two variables. In market analysis, scatter plots are commonly used to explore the correlation or association between two financial variables, such as the relationship between stock prices and trading volumes. Each data point represents a pair of values for the two variables, and their positions on the plot indicate the values of the variables. Scatter plots provide a visual indication of the strength, direction, and pattern of the relationship between the variables. They can help identify trends, patterns, outliers, or potential trading opportunities based on the observed relationships between variables.&lt;/p&gt;
&lt;p&gt;Both histograms and scatter plots facilitate the exploration and analysis of financial data, enabling market analysts to uncover patterns, relationships, and potential insights that can inform trading strategies and decision-making processes.&lt;/p&gt;
&lt;h2 id=&#34;kurtosis&#34;&gt;Kurtosis&lt;/h2&gt;
&lt;p&gt;Kurtosis is a statistical measure that quantifies the shape of a probability distribution. In market analysis, kurtosis helps evaluate the distribution of returns or other financial variables. It measures the tail-heaviness or tail-thinness of the distribution compared to a normal distribution. High kurtosis indicates heavy tails, implying a higher likelihood of extreme values, while low kurtosis suggests lighter tails and a more peaked distribution. Kurtosis analysis aids in understanding the level of risk and potential outliers in the data, which are crucial considerations for assessing investment strategies and managing portfolio risk.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://av-eks-blogoptimized.s3.amazonaws.com/57983kurt1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;beta-vs-correlation&#34;&gt;Beta vs correlation&lt;/h2&gt;
&lt;p&gt;Beta and correlation are both metrics used in finance to measure the relationship between two variables, but they serve different purposes and provide distinct insights.&lt;/p&gt;
&lt;p&gt;Correlation measures the strength and direction of the linear relationship between two variables. It ranges between -1 and +1, where -1 represents a perfect negative correlation, +1 represents a perfect positive correlation, and 0 indicates no correlation. Correlation helps in understanding the degree to which changes in one variable are associated with changes in another variable. In finance, correlation is commonly used to assess the relationship between the returns of different assets or the relationship between an asset&amp;rsquo;s returns and a benchmark index. It helps to identify diversification opportunities and understand how assets move in relation to each other.&lt;/p&gt;
&lt;p&gt;Beta, on the other hand, is a measure of systematic risk or volatility of an asset relative to a benchmark, usually the overall market represented by an index such as the S&amp;amp;P 500. It quantifies the sensitivity of an asset&amp;rsquo;s returns to the movements of the market. A beta of 1 indicates that the asset tends to move in sync with the market, while a beta greater than 1 indicates higher volatility than the market, and a beta less than 1 indicates lower volatility. Beta is used to evaluate the risk-reward tradeoff of an asset and to assess its potential impact on a portfolio&amp;rsquo;s overall risk. Investors often consider beta when constructing portfolios to balance risk exposure and diversify holdings.&lt;/p&gt;
&lt;p&gt;In summary, correlation measures the degree of linear relationship between two variables, while beta measures the relative volatility or risk of an asset compared to a benchmark. Correlation helps identify associations between variables, while beta aids in assessing the systematic risk of an asset and its impact on portfolio performance. Both metrics provide valuable insights in different aspects of financial analysis and decision-making.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://camo.githubusercontent.com/8d963291b8f1c6c5f8f52136f23f73489b5726845b623149fb716d223b0b3555/68747470733a2f2f6173736574732e6f6d7363732e696f2f6e6f7465732f323032302d30312d31352d32322d31352d32342e706e67&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;daily-portfolio-values&#34;&gt;Daily Portfolio values&lt;/h2&gt;
&lt;p&gt;The daily portfolio value can be calculated by normalizing it with the values of the first day, allocating the portfolio based on the desired weights, and then calculating the position values by multiplying the allocated weights with the starting values of each asset. Finally, the portfolio value is obtained by summing the position values.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Normalize the daily portfolio value by dividing it by the value of the portfolio on the first day. This normalization allows for comparison and analysis of the portfolio&amp;rsquo;s performance over time.&lt;/li&gt;
&lt;li&gt;Calculate the allocation of the portfolio by determining the desired weights for each asset. The allocation specifies the proportion of the portfolio&amp;rsquo;s total value that will be invested in each asset. These weights can be based on factors like risk tolerance, investment strategy, or market conditions.&lt;/li&gt;
&lt;li&gt;Compute the position values by multiplying the allocated weights with the starting values of each asset. This step determines the initial value of each asset position in the portfolio.&lt;/li&gt;
&lt;li&gt;Calculate the portfolio value by summing the position values. The portfolio value represents the total worth of the portfolio on a given trading day, taking into account the values of all the assets held in the portfolio.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;portfolio-statistics&#34;&gt;Portfolio statistics&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Daily Returns:&lt;/strong&gt;
Daily Return = (Portfolio Value_today - Portfolio Value_yesterday) / Portfolio Value_yesterday&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cumulative Returns:&lt;/strong&gt;
Cumulative Return = (Portfolio Value_today - Portfolio Value_start) / Portfolio Value_start&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Average Daily Returns:&lt;/strong&gt;
Average Daily Return = mean(Daily Returns)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Standard Deviation of Daily Returns:&lt;/strong&gt;
Standard Deviation = std(Daily Returns)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sharpe Ratio:&lt;/strong&gt;
Sharpe Ratio = (Average Daily Return - Risk-Free Rate) / Standard Deviation of Daily Returns&lt;/p&gt;
&lt;h2 id=&#34;sharpe-ratio&#34;&gt;Sharpe ratio&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Risk adjusted return&lt;/li&gt;
&lt;li&gt;All else being equal
&lt;ul&gt;
&lt;li&gt;lower risk is better&lt;/li&gt;
&lt;li&gt;higher return is better&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SR also considers risk free rate of return (which is 0% for practical purposes)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;parameterized-model&#34;&gt;Parameterized model&lt;/h2&gt;
&lt;p&gt;A parameterized model, in the context of finance and trading, refers to a mathematical or statistical model that includes parameters as variables that can be adjusted or optimized based on specific criteria or data. These models provide a flexible framework for analyzing financial data, making predictions, and generating insights.&lt;/p&gt;
&lt;p&gt;In a parameterized model, the parameters represent various characteristics or assumptions that govern the behavior of the model. These parameters can be estimated, calibrated, or optimized using historical data, statistical techniques, or other methods. By adjusting the values of the parameters, analysts can test different scenarios, evaluate the model&amp;rsquo;s performance, and make informed decisions based on the desired objectives.&lt;/p&gt;
&lt;p&gt;The advantage of parameterized models lies in their ability to adapt to different market conditions, asset classes, or investment strategies. By incorporating parameters, the models can capture specific features or dynamics of the financial markets and provide more accurate predictions or analysis.&lt;/p&gt;
&lt;p&gt;Examples of parameterized models in finance include regression models, time series models like ARIMA or GARCH, option pricing models such as Black-Scholes, and machine learning models like neural networks or random forests. Each of these models contains parameters that can be adjusted or optimized to enhance their performance and align them with the characteristics of the data or the specific requirements of the analysis.&lt;/p&gt;
&lt;p&gt;By utilizing parameterized models, market analysts and researchers can gain deeper insights into financial data, forecast future market trends, manage risk, and optimize investment strategies. The flexibility and adaptability of these models make them valuable tools for decision-making and analysis in the dynamic and complex world of finance.&lt;/p&gt;
&lt;h2 id=&#34;optimizer&#34;&gt;Optimizer&lt;/h2&gt;
&lt;p&gt;An optimizer, in the context of finance and mathematical modeling, refers to a computational algorithm or method used to find the optimal solution for a given problem. It is designed to search through a space of possible solutions and identify the values or configurations that optimize a specific objective or satisfy certain constraints.&lt;/p&gt;
&lt;p&gt;An optimizer typically works by iteratively adjusting the input variables or parameters of a model, evaluating the corresponding output or objective function, and updating the variables based on a defined optimization criterion. The process continues until a satisfactory solution is found, often the one that minimizes or maximizes the objective function within the given constraints.&lt;/p&gt;
&lt;p&gt;In finance, optimizers are extensively used in areas such as portfolio optimization, asset allocation, risk management, and trading strategy development. They enable investors and analysts to find the optimal allocation of assets, determine the optimal weights or positions for a portfolio, or identify the optimal parameters for a trading strategy.&lt;/p&gt;
&lt;p&gt;Various optimization algorithms exist, ranging from simple techniques like grid search and random search to more advanced methods such as gradient-based optimization (e.g., gradient descent), evolutionary algorithms, or convex optimization algorithms. The choice of optimizer depends on the nature of the problem, the complexity of the model, and the desired solution accuracy.&lt;/p&gt;
&lt;h1 id=&#34;computational-investing&#34;&gt;Computational Investing&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Liquidity&lt;/strong&gt; is a measurement of how easy it is to buy or sell shares in a fund. ETFs, or exchange-traded funds are the most liquid of funds. They can be bought and sold easily and near-instantly during the trading day just like individual stocks; ETFs, though, represent some distribution of stocks. The volume of an ETF is just as important to its liquidity: because there are often millions of people trading it, itâ€™s easy to get your buy / sell order filled.&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;large-cap stock&lt;/strong&gt; like Apple refers to a stock with a large market capitalization. Market capitalization is a metric of a stockâ€™s total shares times its price. Itâ€™s worth noting that the &lt;em&gt;price of a stock has no relation to the value of a company&lt;/em&gt;; it only describes the cost of owning a single share in that company. If you can afford the market capitalization of a company, you can afford to buy the company in its entirety and take over its ownership.&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;bull market&lt;/strong&gt; or a &lt;strong&gt;bullish position&lt;/strong&gt; on a stock is an optimistic viewpoint that implies that things will continue to grow. On the other hand, a &lt;strong&gt;bear market&lt;/strong&gt; or a &lt;strong&gt;bearish position&lt;/strong&gt; is pessimistic (or cautionary, or realistic, depending on how you see the glass) about the future of an asset.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;types-of-managed-funds&#34;&gt;Types of Managed Funds&lt;/h2&gt;
&lt;h3 id=&#34;etfs-exchange-traded-funds&#34;&gt;ETFs (Exchange Traded Funds)&lt;/h3&gt;
&lt;p&gt;ETFs, or exchange-traded funds, are investment funds that are traded on stock exchanges, similar to individual stocks. They are designed to track the performance of a specific index, sector, commodity, or asset class. ETFs offer investors a way to gain exposure to a diversified portfolio of assets without directly owning the underlying securities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Structure:&lt;/strong&gt; ETFs are structured as open-end investment companies or unit investment trusts. They issue shares to investors, and these shares represent an ownership interest in the ETF&amp;rsquo;s underlying assets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Underlying Assets:&lt;/strong&gt; ETFs can track a wide range of underlying assets, including stock indexes (such as the S&amp;amp;P 500), bond indexes, commodity prices, currencies, or a combination of assets. The ETF&amp;rsquo;s performance is designed to closely mirror that of its underlying index or asset class.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Creation and Redemption:&lt;/strong&gt; Authorized Participants (APs) play a crucial role in the creation and redemption of ETF shares. They are typically large institutional investors, such as market makers or authorized broker-dealers. APs create new shares of an ETF by delivering a basket of the underlying assets to the ETF issuer, and in return, they receive ETF shares. Conversely, they can redeem ETF shares by returning them to the issuer in exchange for the underlying assets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Listing and Trading:&lt;/strong&gt; ETFs are listed on stock exchanges, making them easily tradable throughout the trading day. Investors can buy and sell ETF shares through brokerage accounts, just like they would trade individual stocks. The price of an ETF share is determined by market demand and supply and can sometimes deviate slightly from the net asset value (NAV) of the underlying assets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Benefits of ETFs:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Diversification:&lt;/strong&gt; ETFs offer investors exposure to a broad range of securities within a single investment. This diversification can help reduce risk compared to investing in individual stocks or bonds.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Liquidity:&lt;/strong&gt; ETFs are traded on stock exchanges, providing investors with liquidity. They can be bought or sold throughout the trading day at market prices.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transparency:&lt;/strong&gt; ETFs disclose their holdings on a daily basis, allowing investors to see exactly which securities they own. This transparency helps investors make informed decisions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lower Costs:&lt;/strong&gt; ETFs generally have lower expense ratios compared to mutual funds. They often passively track an index rather than actively managed funds, resulting in lower management fees.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; ETFs can be used for various investment strategies, including long-term investing, short-term trading, or tactical asset allocation.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It&amp;rsquo;s important to note that while ETFs offer many benefits, they also carry risks. The value of an ETF can fluctuate based on the performance of its underlying assets, and there are potential risks associated with market volatility, liquidity, and tracking error.&lt;/p&gt;
&lt;h3 id=&#34;mutual-funds&#34;&gt;Mutual Funds&lt;/h3&gt;
&lt;p&gt;Mutual funds are investment vehicles that pool money from multiple investors to invest in a diversified portfolio of securities, such as stocks, bonds, or a combination of both. They are managed by professional investment firms or asset management companies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Structure:&lt;/strong&gt; Mutual funds are set up as open-end investment companies. This means that the fund continuously issues and redeems shares based on investor demand. Investors purchase shares of the mutual fund at the net asset value (NAV), which is calculated by dividing the total value of the fund&amp;rsquo;s assets by the number of shares outstanding.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Professional Management:&lt;/strong&gt; Mutual funds are managed by professional fund managers or investment teams who make investment decisions on behalf of the fund. The fund manager conducts research, performs security analysis, and selects investments based on the fund&amp;rsquo;s investment objective and strategy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Investment Objectives and Strategies:&lt;/strong&gt; Mutual funds can have various investment objectives and strategies. For example, a mutual fund may aim to achieve long-term capital appreciation, income generation, or a blend of both. The investment strategy could be actively managed, where the fund manager actively selects and manages the fund&amp;rsquo;s portfolio, or passively managed, where the fund aims to replicate the performance of a specific index.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Diversification:&lt;/strong&gt; Mutual funds provide diversification by investing in a wide range of securities. By pooling money from multiple investors, the fund can hold a diversified portfolio of stocks, bonds, or other assets. This diversification helps spread the investment risk and reduces the impact of any single security&amp;rsquo;s performance on the overall portfolio.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Net Asset Value (NAV):&lt;/strong&gt; The NAV of a mutual fund represents the per-share value of the fund&amp;rsquo;s assets. It is calculated by subtracting the fund&amp;rsquo;s liabilities from its total assets and dividing the result by the number of shares outstanding. The NAV is typically calculated at the end of each trading day.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fees and Expenses:&lt;/strong&gt; Mutual funds charge fees and expenses to cover the costs of managing the fund. These fees may include an expense ratio, which covers management fees, administrative expenses, and other operational costs. Additionally, some funds may charge sales loads, which are fees paid when purchasing or selling shares of the fund.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Liquidity:&lt;/strong&gt; Mutual funds are priced and traded at the NAV at the end of each trading day. Investors can buy or sell shares directly with the fund company or through brokerage accounts. Mutual funds are generally considered to be liquid investments, as they provide investors with the ability to buy or sell shares on any business day.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Benefits of Mutual Funds:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Professional Management:&lt;/strong&gt; Mutual funds are managed by experienced professionals who make investment decisions based on their expertise and research.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Diversification:&lt;/strong&gt; Mutual funds offer instant diversification by investing in a broad range of securities, reducing the risk associated with investing in individual stocks or bonds.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accessibility:&lt;/strong&gt; Mutual funds are accessible to a wide range of investors, as they have relatively low minimum investment requirements.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Liquidity:&lt;/strong&gt; Investors can typically buy or sell mutual fund shares on any business day at the NAV, providing liquidity.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; Mutual funds offer various investment strategies and asset classes to cater to different investor preferences and goals.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Risks of Mutual Funds:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Market Risk:&lt;/strong&gt; The value of mutual fund shares can fluctuate based on the performance of the underlying securities, and investors may experience losses if the market declines.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fees and Expenses:&lt;/strong&gt; Mutual funds charge fees and expenses, which can affect the overall returns earned by investors.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Management Risk:&lt;/strong&gt; The performance of a mutual fund depends on the investment decisions made by the fund manager. Poor investment choices or ineffective management can negatively impact returns.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No Guarantees:&lt;/strong&gt; Mutual funds do not provide guaranteed returns, and investors may not receive back the full amount of their initial investment.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;hedge-funds&#34;&gt;Hedge Funds&lt;/h2&gt;
&lt;p&gt;Hedge funds are alternative investment vehicles that are designed for wealthy individuals or institutional investors. Unlike mutual funds, hedge funds are typically only available to accredited investors due to their complex nature and higher risk profile. Hedge funds employ a range of investment strategies and techniques to seek higher returns, often through active management and the use of leverage.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Structure:&lt;/strong&gt; Hedge funds are structured as private investment partnerships or limited liability companies. They are managed by professional investment managers or investment firms who act as general partners or managers of the fund.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Investment Strategies:&lt;/strong&gt; Hedge funds employ various investment strategies with the goal of generating higher returns than traditional investments. These strategies can include long and short positions in stocks, bonds, commodities, currencies, derivatives, and other financial instruments. Hedge funds can also utilize leverage (borrowed money) to amplify potential returns.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Limited Regulation:&lt;/strong&gt; Hedge funds often operate with fewer regulatory restrictions compared to mutual funds. This allows them to have more flexibility in their investment strategies, including the ability to engage in short selling, derivative trading, and alternative investments.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Performance Fees:&lt;/strong&gt; Hedge funds typically charge performance fees in addition to management fees. The performance fee is a percentage of the fund&amp;rsquo;s profits, usually around 20%. This fee structure aligns the interests of the fund managers with those of the investors, as the managers earn higher fees when they generate positive returns.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Risk Management:&lt;/strong&gt; Hedge funds often employ risk management techniques to mitigate potential losses. This can involve diversifying investments, hedging against market downturns, and implementing risk controls. However, it&amp;rsquo;s important to note that hedge funds can still be subject to substantial risk, and their strategies may not always be successful.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Access and Investor Requirements:&lt;/strong&gt; Hedge funds generally have higher minimum investment requirements compared to mutual funds, often ranging from hundreds of thousands to millions of dollars. They are typically open only to accredited investors, who have higher income or net worth thresholds set by regulatory authorities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Liquidity and Lock-up Periods:&lt;/strong&gt; Hedge funds often have restrictions on liquidity. Investors may face limited redemption options and longer lock-up periods, where their investment is tied up for a specific period, typically one year or more. This illiquidity is intended to provide fund managers with more flexibility in managing investments and executing strategies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Benefits of Hedge Funds:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Potential Higher Returns:&lt;/strong&gt; Hedge funds aim to generate higher returns by using sophisticated investment strategies, including short selling, leverage, and alternative investments.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Diversification:&lt;/strong&gt; Hedge funds often employ a wide range of investment strategies and can invest across multiple asset classes, offering potential diversification benefits to investors.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Active Management:&lt;/strong&gt; Hedge fund managers actively monitor and adjust their investment portfolios, seeking opportunities to capitalize on market inefficiencies and generate alpha (excess returns).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Risks of Hedge Funds:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Higher Risk&lt;/strong&gt;: Hedge funds typically carry higher risk compared to traditional investments. The use of leverage, complex strategies, and alternative investments can amplify potential losses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limited Transparency&lt;/strong&gt;: Hedge funds are less regulated than mutual funds, and they often have limited disclosure requirements. Investors may have less visibility into the fund&amp;rsquo;s holdings and investment decisions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limited Liquidity&lt;/strong&gt;: Hedge funds may have restrictions on withdrawals and longer lock-up periods, limiting investors&amp;rsquo; access to their capital.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Potential for High Fees&lt;/strong&gt;: Hedge funds generally charge higher management and performance fees compared to traditional investment options, which can erode overall returns.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;compensation&#34;&gt;Compensation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Assets under management (AUM):&lt;/strong&gt; The amount of other people&amp;rsquo;s money the fund manager is responsible for.&lt;/li&gt;
&lt;li&gt;Managers of ETFs are paid in expense ratio (0.01% to 1.00% of AUM)&lt;/li&gt;
&lt;li&gt;Mutual Funds are pain in expense ratio (0.5% to 3.00% of AUM)&lt;/li&gt;
&lt;li&gt;Hedge Funds Two and Twenty structure. 2% of AUM and 20% of profits&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;who-are-the-investors-in-hedge-funds&#34;&gt;Who are the investors in Hedge Funds?&lt;/h2&gt;
&lt;p&gt;Hedge fund investors can be a diverse group of individuals, institutions, and organizations. Here are some common types of hedge fund investors:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;High-Net-Worth Individuals (HNWIs):&lt;/strong&gt; These are wealthy individuals who have a substantial amount of investable assets. HNWIs often invest in hedge funds to diversify their portfolios and seek higher returns.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Family Offices:&lt;/strong&gt; Family offices manage the financial affairs and investments of wealthy families. They may allocate a portion of their assets to hedge funds to achieve specific investment goals.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pension Funds:&lt;/strong&gt; Pension funds manage retirement assets on behalf of employees. Some pension funds, especially those with larger assets, invest in hedge funds to diversify their portfolios and potentially enhance returns.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Endowments and Foundations:&lt;/strong&gt; Educational institutions, charitable foundations, and other similar organizations may invest in hedge funds to generate income for their operations or to support their philanthropic activities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Insurance Companies:&lt;/strong&gt; Some insurance companies allocate a portion of their investment portfolios to hedge funds in order to enhance overall returns and manage risk.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sovereign Wealth Funds:&lt;/strong&gt; These funds are created by governments to manage and invest surplus funds, often derived from commodity exports or foreign exchange reserves. Sovereign wealth funds may invest in hedge funds as part of their overall investment strategy.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Funds of Funds:&lt;/strong&gt; These are investment vehicles that pool capital from multiple investors to invest in a portfolio of hedge funds. Funds of funds provide diversification and professional management for investors who may not have direct access to hedge funds.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Institutional Investors:&lt;/strong&gt; This category includes various institutions such as banks, asset management firms, and corporations. Institutional investors often have dedicated teams or departments that manage their investments, which may include hedge fund allocations.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;goals-of-hedge-funds&#34;&gt;Goals of hedge funds&lt;/h2&gt;
&lt;p&gt;The goals of hedge funds can vary depending on their investment strategies and the preferences of their managers. However, there are several common goals that hedge funds typically aim to achieve:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Capital Appreciation:&lt;/strong&gt; Hedge funds often seek to generate positive returns on their investments, aiming for capital appreciation and growth of the fund&amp;rsquo;s assets over time. The primary goal is to outperform traditional investment vehicles, such as stock market indices or mutual funds.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Risk Management and Preservation of Capital:&lt;/strong&gt; While hedge funds are known for their potential to generate high returns, they also prioritize risk management. Hedge fund managers employ various strategies to mitigate downside risks and preserve capital, aiming to protect investors&amp;rsquo; assets during market downturns.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Absolute Returns:&lt;/strong&gt; Hedge funds typically pursue absolute returns, aiming to generate positive performance regardless of market conditions. Unlike traditional investment funds that often benchmark their performance against a specific market index, hedge funds aim to generate returns that are not reliant on overall market performance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Diversification:&lt;/strong&gt; Hedge funds often use diverse investment strategies across different asset classes, including stocks, bonds, commodities, currencies, and derivatives. By diversifying their investments, hedge funds aim to reduce risk and potentially enhance returns through exposure to various market opportunities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Active Management and Flexibility:&lt;/strong&gt; Hedge funds have the advantage of flexibility and the ability to implement active investment strategies. They can take both long and short positions, engage in leverage, use derivatives, and employ other sophisticated techniques to exploit market inefficiencies and generate returns.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Capital Preservation in Down Markets:&lt;/strong&gt; Some hedge funds aim to provide downside protection during market downturns. They may use strategies such as hedging, short-selling, or employing market-neutral approaches to reduce correlation with broader market movements and potentially deliver positive returns even in challenging market conditions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Alpha Generation:&lt;/strong&gt; Hedge funds often strive to generate alpha, which represents the excess return earned beyond what would be expected based on the risk exposure of their investments. By identifying and exploiting market inefficiencies or mispriced assets, hedge funds aim to generate alpha and deliver superior risk-adjusted returns.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;hedge-funds-metrics&#34;&gt;Hedge funds metrics&lt;/h2&gt;
&lt;p&gt;Hedge funds employ a wide range of metrics and indicators to evaluate investment opportunities, monitor portfolio performance, and make informed decisions. The specific metrics they chase can vary depending on the fund&amp;rsquo;s investment strategy and objectives. Here are some commonly used metrics in the hedge fund industry:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Return on Investment (ROI):&lt;/strong&gt; ROI is a fundamental metric that measures the profitability of an investment. Hedge funds closely track the returns generated by their investments to assess the success of their strategies and compare them against their targets or benchmarks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Alpha:&lt;/strong&gt; Alpha represents the excess return generated by a hedge fund compared to its expected return based on its risk exposure. Hedge funds aim to achieve positive alpha, as it indicates that they have outperformed the market or their benchmark, taking into account the level of risk undertaken.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sharpe Ratio:&lt;/strong&gt; The Sharpe ratio measures the risk-adjusted return of an investment by considering the excess return earned relative to its volatility or risk. Hedge funds often strive for higher Sharpe ratios, indicating that they are generating superior returns for the level of risk taken.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Volatility:&lt;/strong&gt; Volatility measures the degree of price fluctuations in an investment or a portfolio. Hedge funds may target specific levels of volatility based on their risk appetite and investment strategies. Some funds may seek to reduce volatility by employing hedging or risk management techniques.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Maximum Drawdown:&lt;/strong&gt; Maximum drawdown refers to the largest peak-to-trough decline in the value of a hedge fund or investment portfolio over a specific period. Hedge funds aim to minimize drawdowns as they can significantly impact investor capital. Lower maximum drawdowns indicate better risk management.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Information Ratio:&lt;/strong&gt; The information ratio measures the excess return generated by a hedge fund relative to a benchmark, considering the level of active risk taken. It assesses the fund manager&amp;rsquo;s ability to generate returns through active management decisions and market insights.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Risk Metrics:&lt;/strong&gt; Hedge funds closely monitor various risk metrics such as Value-at-Risk (VaR), which estimates the potential loss under adverse market conditions, and tracking error, which measures the deviation of a fund&amp;rsquo;s returns from its benchmark. These metrics help hedge funds assess and manage the risks associated with their investment strategies.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Liquidity Metrics:&lt;/strong&gt; Hedge funds may track liquidity metrics to assess the ease of buying or selling assets in their portfolios. Measures such as bid-ask spreads, trading volumes, and market depth can help hedge funds gauge the liquidity of their investments and ensure they can exit positions when necessary.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;computing-in-a-hedge-fund&#34;&gt;Computing in a Hedge Fund&lt;/h2&gt;
&lt;p&gt;Computing plays a crucial role in the operations of hedge funds, enabling efficient data analysis, trading strategies, risk management, and overall portfolio management. Here are some key aspects of computing within a hedge fund:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Data Management:&lt;/strong&gt; Hedge funds handle vast amounts of data from various sources, including market data, economic indicators, company financials, news feeds, and more. Computing systems are used to collect, store, and organize this data for analysis and decision-making. This may involve the use of databases, data warehouses, and data lakes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Quantitative Analysis:&lt;/strong&gt; Hedge funds often employ quantitative analysts (quants) who develop mathematical models and algorithms to analyze data, identify patterns, and generate trading signals. These models can range from statistical models and machine learning algorithms to more complex quantitative finance models. High-performance computing systems are often used to perform computationally intensive tasks and backtest strategies.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Algorithmic Trading:&lt;/strong&gt; Hedge funds commonly utilize algorithmic trading, where computer algorithms execute trades based on predefined rules and strategies. These algorithms take into account various factors such as market conditions, pricing data, and order book information. Low-latency computing systems are often employed to execute trades quickly and efficiently.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Risk Management:&lt;/strong&gt; Hedge funds have sophisticated risk management systems to monitor and assess potential risks associated with their portfolios. These systems use computing power to calculate risk metrics, such as Value-at-Risk (VaR), stress tests, and scenario analyses. Risk models are often run on computing clusters to analyze the potential impact of different market conditions on the fund&amp;rsquo;s holdings.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Portfolio Management and Optimization:&lt;/strong&gt; Computing systems are used for portfolio management tasks, including portfolio construction, rebalancing, and optimization. Advanced optimization algorithms help hedge funds determine optimal asset allocations based on desired risk-return trade-offs, constraints, and market conditions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Market Data Analysis:&lt;/strong&gt; Hedge funds analyze market data in real-time to identify trading opportunities, monitor market trends, and make informed investment decisions. This involves processing and analyzing vast amounts of streaming market data using computing systems, often with the help of complex event processing (CEP) techniques.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Infrastructure and Connectivity:&lt;/strong&gt; Hedge funds require robust computing infrastructure to support their operations. This includes servers, data storage systems, network infrastructure, and connectivity to exchanges, brokers, and other trading platforms. Redundancy and high availability are critical to ensure uninterrupted operations and minimize downtime.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Security:&lt;/strong&gt; Hedge funds handle sensitive financial data and must maintain strict data security measures. This includes encryption, access controls, secure networks, and data backup systems to protect against unauthorized access, data breaches, and system failures.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;the-order-book&#34;&gt;The Order Book&lt;/h2&gt;
&lt;p&gt;An order book is a key component of financial markets, particularly in the context of exchanges or trading platforms. It is a record of buy and sell orders for a particular security, such as stocks, bonds, or cryptocurrencies, organized by price and time. The order book provides market participants with transparency regarding the supply and demand dynamics of the security.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s how an order book typically works:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Buy and Sell Orders:&lt;/strong&gt; Market participants can submit buy or sell orders for a specific security. Buy orders represent the demand for the security at a certain price, while sell orders represent the supply of the security at a given price.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Price Levels:&lt;/strong&gt; The order book organizes these buy and sell orders into different price levels. Each price level represents a specific price at which orders are placed. The highest bid price (buy orders) and the lowest ask price (sell orders) are often displayed prominently.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Quantity:&lt;/strong&gt; Along with the price, the order book also shows the quantity or volume of shares or contracts being bid or offered at each price level. This provides information about the liquidity available at different price points.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best Bid and Ask:&lt;/strong&gt; The order book highlights the best bid price and the best ask price, which represent the highest bid and lowest ask prices available in the market at a given moment. The difference between the best bid and ask prices is known as the bid-ask spread.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Market Depth:&lt;/strong&gt; Market depth refers to the cumulative quantity of buy and sell orders available at different price levels. It shows the potential buying and selling pressure in the market and helps market participants assess the level of liquidity.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Market Order Execution:&lt;/strong&gt; When a market participant submits a market order to buy or sell a security, it is typically executed against the best available prices in the order book. The market order consumes the available liquidity in the order book until the entire order is filled.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limit Order Execution:&lt;/strong&gt; Limit orders specify the desired price at which a participant wants to buy or sell a security. These orders are placed in the order book and remain there until they are matched with a counterparty. If a buy limit order matches a sell limit order at the specified price, a trade occurs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Order Book Updates:&lt;/strong&gt; The order book is continuously updated as new orders are submitted or existing orders are modified or canceled. The order book reflects real-time changes in supply and demand dynamics, allowing participants to observe shifts in market sentiment.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The order book is an essential tool for traders, providing them with visibility into market liquidity, price levels, and potential trading opportunities. By analyzing the order book, traders can make informed decisions about when to place orders, at what price, and how much liquidity is available to support their trades.&lt;/p&gt;
&lt;h2 id=&#34;how-orders-get-to-the-exchange&#34;&gt;How orders get to the exchange?&lt;/h2&gt;
&lt;p&gt;Orders can reach exchanges through various channels, including direct connections, brokers, and alternative trading venues. Here&amp;rsquo;s a general overview of how orders reach exchanges and the role of dark pools:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Direct Market Access (DMA)&lt;/strong&gt;: Institutional investors and some high-frequency trading firms have direct market access to exchanges. They establish direct connections to the exchange&amp;rsquo;s trading system, enabling them to send orders directly without intermediaries. DMA allows for faster order execution and greater control over the order routing process.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Brokers and Trading Platforms:&lt;/strong&gt; Most individual investors and some institutional investors route their orders through brokers or trading platforms. These intermediaries receive orders from clients and act as an interface between the client and the exchange. Brokers typically offer access to multiple exchanges, allowing clients to choose the desired trading venue.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smart Order Routing (SOR):&lt;/strong&gt; When an order is received by a broker or a trading platform, they may use smart order routing technology. SOR algorithms analyze various factors such as price, liquidity, execution speed, and regulatory requirements to determine the optimal destination for the order. SOR aims to maximize the chances of obtaining the best execution possible by routing the order to the most suitable market or venue.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Primary Exchanges:&lt;/strong&gt; The primary exchanges, such as the New York Stock Exchange (NYSE) or NASDAQ, are the most widely known trading venues. Orders sent directly to these exchanges or routed through brokers are executed on their centralized order books. These exchanges provide transparent markets where orders are visible to all participants, allowing for price discovery and liquidity.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dark Pools:&lt;/strong&gt; Dark pools are alternative trading venues that offer a level of anonymity and reduced market impact for large institutional orders. Dark pools operate differently from primary exchanges as they do not display order details in the public order book. Instead, they match buy and sell orders internally, away from public view. Dark pools are designed to facilitate large block trades with reduced information leakage and minimize market impact.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Crossing Networks:&lt;/strong&gt; Some brokers operate crossing networks, which are internal matching engines that facilitate the execution of orders from their own clients. These orders are not routed to external exchanges. Crossing networks aim to match buy and sell orders within the broker&amp;rsquo;s client base, providing potential price improvement and confidentiality.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Electronic Communication Networks (ECNs):&lt;/strong&gt; ECNs are electronic platforms that connect buyers and sellers directly. They provide a venue for trading securities and can be accessed by market participants, including institutional investors and retail traders. ECNs often offer fast order matching, access to multiple markets, and display order information for transparency.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;geographic-arbitrage&#34;&gt;Geographic arbitrage&lt;/h2&gt;
&lt;p&gt;Geographic arbitrage refers to the practice of taking advantage of price or valuation discrepancies between different geographic regions or markets. It involves exploiting the differences in prices, costs, or economic conditions across countries or regions to generate profits.&lt;/p&gt;
&lt;h2 id=&#34;stop-loss&#34;&gt;Stop Loss&lt;/h2&gt;
&lt;p&gt;Stop Loss is an order placed by an investor to automatically sell a security if it reaches a specified price, limiting potential losses.&lt;/p&gt;
&lt;h2 id=&#34;stop-gain&#34;&gt;Stop Gain&lt;/h2&gt;
&lt;p&gt;Stop Gain is an order placed by an investor to automatically sell a security if it reaches a specified price, securing profits and preventing potential losses.&lt;/p&gt;
&lt;h2 id=&#34;trailing-stop&#34;&gt;Trailing Stop&lt;/h2&gt;
&lt;p&gt;A trailing stop is a type of stop loss order that adjusts dynamically with the market price, moving in lockstep to protect profits by automatically selling a security if its price drops a certain percentage or amount from its highest point.&lt;/p&gt;
&lt;h2 id=&#34;short-selling&#34;&gt;Short selling&lt;/h2&gt;
&lt;p&gt;Short selling is a trading strategy where an investor borrows a security from a broker and sells it in the market, anticipating that the price of the security will decline. The investor aims to buy back the security at a lower price in the future to return it to the broker, thereby profiting from the price difference. Short selling allows investors to potentially profit from falling prices and is commonly used for speculative purposes, hedging, or market-making activities. However, it carries inherent risks, as there is unlimited potential for loss if the price of the security being shorted rises significantly.&lt;/p&gt;
&lt;h2 id=&#34;evaluating-the-true-value-of-a-company&#34;&gt;Evaluating the &amp;ldquo;true&amp;rdquo; value of a company&lt;/h2&gt;
&lt;h3 id=&#34;intrinsic-value-of-a-company&#34;&gt;Intrinsic value of a company&lt;/h3&gt;
&lt;p&gt;The intrinsic value of a company refers to the estimated underlying worth or fair value of the company&amp;rsquo;s business, assets, and cash flows. It is an assessment of what the company is truly worth based on its fundamental characteristics, financial performance, growth prospects, and other relevant factors.&lt;/p&gt;
&lt;p&gt;Calculating the intrinsic value involves analyzing various aspects of the company, such as its earnings, revenue, cash flow, assets, liabilities, industry trends, competitive position, management quality, and overall economic conditions. Different valuation methods, such as discounted cash flow (DCF) analysis, comparable company analysis, or asset-based valuation, can be used to estimate the intrinsic value.&lt;/p&gt;
&lt;p&gt;The intrinsic value is often compared to the market price of the company&amp;rsquo;s stock to determine if the stock is overvalued or undervalued. If the intrinsic value is higher than the market price, the stock may be considered undervalued and potentially a good investment opportunity. Conversely, if the intrinsic value is lower than the market price, the stock may be considered overvalued, signaling a potential selling opportunity.&lt;/p&gt;
&lt;h3 id=&#34;book-value-of-the-company&#34;&gt;Book value of the company&lt;/h3&gt;
&lt;p&gt;The book value of a company, also known as the net book value or shareholder&amp;rsquo;s equity, represents the value of a company&amp;rsquo;s assets minus its liabilities as reported on the balance sheet. It provides an accounting-based measure of the company&amp;rsquo;s net worth or equity position.&lt;/p&gt;
&lt;h3 id=&#34;market-cap&#34;&gt;Market cap&lt;/h3&gt;
&lt;p&gt;The market capitalization (market cap) of a company is a measure of its total market value, representing the worth of the company as perceived by the market. It is calculated by multiplying the company&amp;rsquo;s current stock price by the total number of outstanding shares.&lt;/p&gt;
&lt;p&gt;The formula for market cap is as follows:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Market Cap = Stock Price x Number of Outstanding Shares
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;rule-of-72&#34;&gt;Rule of 72&lt;/h2&gt;
&lt;p&gt;The Rule of 72 is a simplified mathematical rule used to estimate the time it takes for an investment or a sum of money to double, given a fixed interest rate. It provides a quick approximation of the doubling time based on the concept of compound interest.&lt;/p&gt;
&lt;p&gt;The Rule of 72 is applied as follows:&lt;/p&gt;
&lt;p&gt;Doubling Time â‰ˆ 72 / Interest Rate&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;Interest Rate â‰ˆ 72 / Doubling Time&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Doubling Time represents the estimated time it takes for an investment or sum of money to double.&lt;/li&gt;
&lt;li&gt;Interest Rate represents the fixed annual interest rate or rate of return.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, if you have an investment with an annual interest rate of 6%, you can estimate that it will take approximately 12 years (72 / 6) for your investment to double.&lt;/p&gt;
&lt;p&gt;The Rule of 72 is a simple approximation and assumes a constant interest rate and compound interest. It is most accurate for interest rates in the range of 6% to 10%. However, for higher or lower interest rates, the approximation becomes less precise. Additionally, it does not take into account factors such as inflation, taxes, or other variables that may affect investment returns.&lt;/p&gt;
&lt;h2 id=&#34;the-future-value-of-money&#34;&gt;The future value of money&lt;/h2&gt;
&lt;p&gt;The present value (PV) and future value (FV) of money are related through a mathematical formula that takes into account the time period and the interest rate. The formula to calculate the present value (PV) based on a future value (FV) is as follows:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;PV = FV / (1 + r)^n

Where:
PV = Present Value
FV = Future Value
r = Interest rate (expressed as a decimal)
n = Number of periods or time period
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;the-capital-asset-pricing-model&#34;&gt;The Capital Asset Pricing Model&lt;/h2&gt;
&lt;p&gt;The Capital Asset Pricing Model (CAPM) is a financial model used to estimate the expected return on an investment by considering the relationship between its systematic risk and expected return. It provides a framework for pricing risky securities and determining an appropriate required rate of return.&lt;/p&gt;
&lt;p&gt;The CAPM is based on the following formula:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Expected Return = Risk-Free Rate + Beta x (Market Return - Risk-Free Rate)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Expected Return is the anticipated return on the investment.&lt;/li&gt;
&lt;li&gt;Risk-Free Rate is the return on a risk-free investment, typically represented by the yield on government bonds.&lt;/li&gt;
&lt;li&gt;Beta is a measure of the investment&amp;rsquo;s systematic risk or sensitivity to market movements.&lt;/li&gt;
&lt;li&gt;Market Return is the expected return on the overall market.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The CAPM assumes that investors are risk-averse and require compensation for bearing systematic risk beyond the risk-free rate. It suggests that an investment&amp;rsquo;s expected return should increase in proportion to its systematic risk (as measured by beta). The formula calculates the expected return by adding a risk premium (Beta x (Market Return - Risk-Free Rate)) to the risk-free rate.&lt;/p&gt;
&lt;p&gt;Key assumptions of the CAPM include efficient markets, where all relevant information is reflected in asset prices, and a single-period investment horizon. The model also assumes that investors have homogeneous expectations and hold well-diversified portfolios.&lt;/p&gt;
&lt;p&gt;The CAPM is widely used in finance for determining the appropriate discount rate for investment valuation, evaluating the performance of investment portfolios, and estimating the cost of equity capital for companies. However, it has its limitations and critics, as it relies on simplifying assumptions and may not fully capture the complexities of real-world market dynamics.&lt;/p&gt;
&lt;h2 id=&#34;passive-vs-active-investing&#34;&gt;Passive vs Active Investing&lt;/h2&gt;
&lt;p&gt;Passive investing and active investing are two contrasting investment approaches that differ in terms of strategy, management style, and investment philosophy. Here&amp;rsquo;s an overview of each:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Passive Investing:
Passive investing, also known as index investing or passive management, involves constructing a portfolio that aims to replicate the performance of a specific market index, such as the S&amp;amp;P 500. The primary goal is to match the returns of the chosen index rather than trying to outperform it. Passive investors believe that markets are efficient and that it is challenging to consistently beat the market over the long term.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Key characteristics of passive investing include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Index-based approach: Passive investors invest in index funds or exchange-traded funds (ETFs) that hold a diversified portfolio of securities to mimic the performance of a specific index.&lt;/li&gt;
&lt;li&gt;Lower costs: Passive investing generally incurs lower fees and expenses compared to active investing, as it requires minimal research and portfolio management.&lt;/li&gt;
&lt;li&gt;Buy and hold strategy: Passive investors typically maintain a long-term investment approach, avoiding frequent trading or market timing.&lt;/li&gt;
&lt;li&gt;Broad market exposure: Passive strategies offer exposure to an entire market or a specific segment, providing diversification and representing the overall market performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Active Investing:
Active investing involves actively managing a portfolio with the goal of outperforming the market or a specific benchmark. Active investors believe that it is possible to identify undervalued securities or exploit market inefficiencies through research, analysis, and active decision-making.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Key characteristics of active investing include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Individual security selection: Active investors analyze and select specific stocks, bonds, or other securities based on their research and evaluation of company fundamentals, market trends, and other factors.&lt;/li&gt;
&lt;li&gt;Higher costs: Active investing typically involves higher costs compared to passive investing, as it requires more research, analysis, and trading activity.&lt;/li&gt;
&lt;li&gt;Portfolio turnover: Active managers frequently buy and sell securities in an attempt to take advantage of market opportunities or manage risk.&lt;/li&gt;
&lt;li&gt;Flexibility and customization: Active investing allows for a more tailored approach, with the ability to deviate from market indices and adjust the portfolio based on the manager&amp;rsquo;s outlook and investment strategy.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;efficient-market-hypothesis&#34;&gt;Efficient market hypothesis:&lt;/h2&gt;
&lt;p&gt;The Efficient Market Hypothesis (EMH) is a theory in finance that suggests financial markets are efficient in reflecting all available information into security prices. According to the EMH, it is not possible to consistently achieve above-average returns through stock picking or market timing, as stock prices already incorporate all relevant information.&lt;/p&gt;
&lt;p&gt;Key principles of the Efficient Market Hypothesis include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Information Efficiency: The EMH assumes that financial markets efficiently incorporate all publicly available information, including historical data, financial statements, news, and other market-relevant information. In an efficient market, prices adjust quickly and accurately to new information, making it difficult for investors to gain an advantage by acting upon it.&lt;/li&gt;
&lt;li&gt;Three Forms of Market Efficiency: The EMH categorizes market efficiency into three forms:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Weak Form Efficiency: Prices reflect past trading information, such as historical prices and trading volume. Technical analysis techniques based on past price patterns would not consistently generate abnormal returns.&lt;/li&gt;
&lt;li&gt;Semi-Strong Form Efficiency: Prices reflect all publicly available information, including not only past trading data but also fundamental and non-public information, such as earnings reports, news announcements, and analyst recommendations. Neither technical nor fundamental analysis would consistently yield superior returns.&lt;/li&gt;
&lt;li&gt;Strong Form Efficiency: Prices reflect all information, including public and non-public information. This implies that even insider information would not provide an advantage, as it is already factored into prices.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Implications for Investors: The EMH suggests that investors cannot systematically beat the market or consistently identify mispriced securities, as any available information is already incorporated into prices. Therefore, passive investing through strategies like index funds or exchange-traded funds (ETFs) that track broad market indices is considered a rational approach.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;While the Efficient Market Hypothesis provides a framework for understanding market efficiency, it has been subject to criticism. Critics argue that markets may not always be fully efficient due to behavioral biases, information asymmetry, or temporary market inefficiencies that can be exploited by skilled investors. As a result, various investment strategies, such as active management or value investing, continue to be pursued by those who believe in the potential to outperform the market.&lt;/p&gt;
&lt;h2 id=&#34;arbitrage-pricing-theory&#34;&gt;Arbitrage Pricing Theory&lt;/h2&gt;
&lt;p&gt;The Arbitrage Pricing Theory (APT) is a financial theory that attempts to explain the relationship between the expected returns of an asset and its risk factors. It is an alternative to the Capital Asset Pricing Model (CAPM) and provides a multi-factor model for asset pricing.&lt;/p&gt;
&lt;p&gt;Key features of the Arbitrage Pricing Theory include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Multi-Factor Model: APT posits that the expected return of an asset is influenced by multiple risk factors, which are systematic influences that affect the asset&amp;rsquo;s returns. These risk factors can be economic variables such as interest rates, inflation, market indices, or industry-specific factors.&lt;/li&gt;
&lt;li&gt;No Arbitrage: APT assumes the absence of arbitrage opportunities, meaning that it is not possible to make riskless profits by exploiting mispriced securities. The theory suggests that market prices adjust quickly to eliminate any potential arbitrage opportunities.&lt;/li&gt;
&lt;li&gt;Linear Relationship: APT assumes a linear relationship between the risk factors and the expected returns of an asset. It suggests that the sensitivity of an asset&amp;rsquo;s returns to each risk factor can be quantified through factor loadings or coefficients.&lt;/li&gt;
&lt;li&gt;Risk Premiums: APT predicts that investors require a risk premium for exposure to each risk factor. The size of the risk premium depends on the perceived riskiness of the factor and its impact on the asset&amp;rsquo;s returns.&lt;/li&gt;
&lt;li&gt;Arbitrage Pricing: APT allows for the identification of mispriced assets by comparing their expected returns, as estimated using the multi-factor model, with their actual market prices. If an asset&amp;rsquo;s expected return does not match the return implied by the APT model, an arbitrage opportunity may exist.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;APT is a more flexible model compared to the CAPM, as it considers multiple risk factors and does not rely on the assumptions of market efficiency or a single market portfolio. However, APT requires identifying and estimating the relevant risk factors specific to a particular asset or market, which can be challenging.&lt;/p&gt;
&lt;p&gt;While APT provides a framework for understanding asset pricing, it is not as widely used as the CAPM in practical applications. Nevertheless, it has contributed to the development of factor-based investing and the understanding of the relationship between risk factors and asset returns.&lt;/p&gt;
&lt;h1 id=&#34;technical-analysis&#34;&gt;Technical Analysis&lt;/h1&gt;
&lt;p&gt;Technical analysis is a methodology used in financial markets to evaluate and forecast future price movements of securities, such as stocks, currencies, commodities, and indices. It relies on the analysis of historical price and volume data, along with various technical indicators and chart patterns, to make investment decisions.&lt;/p&gt;
&lt;p&gt;Key aspects of technical analysis include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Price Patterns:&lt;/strong&gt; Technical analysts study various patterns formed by historical price data, such as trends (uptrends, downtrends, or sideways movements), support and resistance levels, chart patterns (e.g., head and shoulders, double tops/bottoms), and trend lines. These patterns are believed to provide insights into future price movements.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Technical Indicators:&lt;/strong&gt; Technical analysts use a wide range of indicators that mathematically analyze price and volume data to generate trading signals. Examples of popular indicators include moving averages, oscillators (e.g., Relative Strength Index - RSI, Stochastic Oscillator), and momentum indicators (e.g., Moving Average Convergence Divergence - MACD). These indicators help identify overbought or oversold conditions, trend strength, and potential reversals.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Volume Analysis:&lt;/strong&gt; Volume, the number of shares or contracts traded, is considered a significant factor in technical analysis. Changes in trading volume can indicate the strength or weakness of price movements, confirmation or divergence of trends, or the presence of buying or selling pressure.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Market Sentiment:&lt;/strong&gt; Technical analysis takes into account market sentiment, which reflects the collective psychological and emotional outlook of market participants. It is believed that market sentiment can influence price movements and can be inferred from indicators like the put/call ratio, investor surveys, or sentiment indicators.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Timeframes:&lt;/strong&gt; Technical analysis can be applied to various timeframes, ranging from intraday charts to long-term charts. Different timeframes may reveal different patterns and trends, catering to traders with different investment horizons.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Technical analysis assumes that historical price patterns, along with associated indicators and patterns, can provide insights into future price movements. Critics argue that technical analysis is based on subjective interpretations and lacks a solid foundation in fundamental analysis or economic factors.&lt;/p&gt;
&lt;p&gt;Traders and investors who use technical analysis aim to identify trading opportunities, determine entry and exit points, manage risk, and assess the probability of price movements. It is often used alongside other forms of analysis, such as fundamental analysis, to make more informed investment decisions.&lt;/p&gt;
&lt;h2 id=&#34;technical-indicator-momentum&#34;&gt;Technical Indicator: Momentum&lt;/h2&gt;
&lt;p&gt;Momentum, in the context of financial markets, refers to the tendency of an asset&amp;rsquo;s price to continue moving in the same direction over a certain period of time. It is a key concept in technical analysis and is based on the belief that assets that have performed well or poorly in the recent past will continue to do so in the near future.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Price Trend:&lt;/strong&gt; Momentum focuses on identifying and capitalizing on existing price trends. It assumes that assets that have been rising in price will continue to rise, while those that have been falling will continue to decline.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relative Strength:&lt;/strong&gt; Momentum analysis often involves comparing the performance of one asset relative to others in the same market or sector. Assets that have demonstrated relatively stronger performance compared to their peers are considered to have positive momentum.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Time Frame:&lt;/strong&gt; Momentum analysis can be applied to various timeframes, ranging from short-term intraday movements to longer-term trends. Different traders and investors may use different timeframes to capture momentum opportunities based on their trading strategies and investment goals.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Momentum Indicators:&lt;/strong&gt; Technical analysts use various momentum indicators to identify and quantify the strength of price trends. Examples of momentum indicators include the Relative Strength Index (RSI), Moving Average Convergence Divergence (MACD), and Stochastic Oscillator. These indicators help assess whether an asset is overbought or oversold and whether the momentum is likely to continue or reverse.&lt;/p&gt;
&lt;p&gt;Momentum trading strategies typically involve buying assets that have exhibited positive momentum and selling or short-selling assets that have shown negative momentum. Traders aim to profit from the continuation of trends by entering positions in the direction of the established momentum. Risk management techniques, such as stop-loss orders, are often employed to limit potential losses if the momentum reverses.&lt;/p&gt;
&lt;h2 id=&#34;dealing-with-data&#34;&gt;Dealing with Data&lt;/h2&gt;
&lt;h3 id=&#34;tick&#34;&gt;Tick&lt;/h3&gt;
&lt;p&gt;A &amp;ldquo;tick&amp;rdquo; refers to the smallest possible price movement for a financial instrument, such as a stock, futures contract, or currency pair. The tick size is the minimum price increment that the price can move up or down. It represents the precision with which prices are quoted in the market.&lt;/p&gt;
&lt;p&gt;The tick size varies depending on the specific financial instrument and the exchange where it is traded. For example, in the stock market, the tick size is typically a penny (or a fraction of a penny), while in the futures market, it may be a different amount.&lt;/p&gt;
&lt;h3 id=&#34;stock-split&#34;&gt;Stock Split&lt;/h3&gt;
&lt;p&gt;A stock split is a corporate action taken by a publicly traded company to increase the number of its outstanding shares while simultaneously reducing the share price in order to make the shares more affordable to investors. The overall value of the company remains the same after a stock split.&lt;/p&gt;
&lt;p&gt;Stock splits are usually expressed as a ratio, such as 2-for-1, 3-for-1, or any other combination. Here&amp;rsquo;s how it works:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;2-for-1 Stock Split: In a 2-for-1 stock split, for every one share an investor owns before the split, they receive two shares after the split. For example, if an investor holds 100 shares of a company&amp;rsquo;s stock trading at 100 dollars per share, after the 2-for-1 split, they will have 200 shares priced at 50 dollars per share (100 shares x 2).&lt;/li&gt;
&lt;li&gt;3-for-1 Stock Split: In a 3-for-1 stock split, for every one share an investor owns before the split, they receive three shares after the split. If they held 50 shares priced at 150 dollars per share before the split, they would have 150 shares priced at 50 dollars per share after the 3-for-1 split.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The primary purpose of a stock split is to make the company&amp;rsquo;s stock more accessible to a broader range of investors, especially those with smaller amounts of capital. When the share price is lower, investors with limited funds can participate in the market more easily. Stock splits do not change the total market capitalization of the company or the proportional ownership of shareholders.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s important to note that a stock split is different from a stock dividend. In a stock dividend, the company issues additional shares to its existing shareholders as a way of distributing its profits or retained earnings.&lt;/p&gt;
&lt;p&gt;Stock splits are typically a sign of a company&amp;rsquo;s confidence in its future growth prospects. They are not uncommon for companies that experience significant share price appreciation and want to maintain a reasonable share price for retail investors.&lt;/p&gt;
&lt;h3 id=&#34;dividends&#34;&gt;Dividends&lt;/h3&gt;
&lt;p&gt;Dividends are payments made by a corporation to its shareholders as a distribution of the company&amp;rsquo;s profits or retained earnings. When a company earns a profit, it has several options for using that money, such as reinvesting it back into the business for expansion or paying off debts. Another common option is to return some of the profits to the shareholders in the form of dividends.&lt;/p&gt;
&lt;p&gt;Dividends are typically paid out in cash, but they can also be paid in the form of additional shares of stock or other property. The amount of dividends paid to each shareholder is usually proportional to the number of shares they own. For example, if a company declares a dividend of 0.50 dollars per share and a shareholder owns 100 shares, they would receive 50 dollars in dividends.&lt;/p&gt;
&lt;p&gt;Dividends can be paid on a regular basis, such as quarterly or annually, or the company may decide to pay special or one-time dividends based on its financial performance or specific events. The decision to pay dividends is made by the company&amp;rsquo;s board of directors, and the amount and frequency of dividends can vary depending on the company&amp;rsquo;s profitability, financial health, and growth opportunities.&lt;/p&gt;
&lt;p&gt;Investors often see dividends as a way to generate income from their investments, especially in stable and mature companies with a history of consistent dividend payments. Dividend-paying stocks are popular among income-seeking investors, retirees, and those looking for a steady income stream.&lt;/p&gt;
&lt;h3 id=&#34;efficient-market-hypothesis-1&#34;&gt;Efficient Market Hypothesis&lt;/h3&gt;
&lt;p&gt;The Efficient Market Hypothesis (EMH) is a theory in financial economics that suggests that financial markets are efficient and that asset prices always fully reflect all available information. In other words, according to the EMH, it is impossible to consistently &amp;ldquo;beat the market&amp;rdquo; by identifying undervalued or overvalued assets because all relevant information is already incorporated into the prices.&lt;/p&gt;
&lt;p&gt;The concept of the Efficient Market Hypothesis was developed by economist Eugene Fama in the 1960s and has been a fundamental principle in modern finance theory ever since. The hypothesis is based on three key assumptions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Perfect Competition: The hypothesis assumes that financial markets are characterized by perfect competition, meaning there are many buyers and sellers, and no individual participant can significantly influence prices.&lt;/li&gt;
&lt;li&gt;Rational Investors: It assumes that all market participants are rational and always act in a way to maximize their expected utility, based on all available information.&lt;/li&gt;
&lt;li&gt;Immediate Information Processing: The EMH assumes that all relevant information is available to investors at the same time and that they immediately and accurately process that information to adjust prices accordingly.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The Efficient Market Hypothesis is usually divided into three forms:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Weak Form EMH: This form of the hypothesis asserts that stock prices already reflect all past trading information, including price and volume data. In other words, technical analysis, which relies on historical price patterns, should not be able to consistently predict future price movements.&lt;/li&gt;
&lt;li&gt;Semi-Strong Form EMH: This version of the hypothesis states that stock prices already reflect all publicly available information, including financial statements, news, and other non-confidential information. Thus, fundamental analysis, which involves examining a company&amp;rsquo;s financials and prospects, should not provide an advantage in predicting future prices.&lt;/li&gt;
&lt;li&gt;Strong Form EMH: The strong form asserts that stock prices already reflect all information, whether it is public or private. This includes insider information that is not available to the general public. If the strong form holds, then no individual or entity, not even insiders, can consistently earn above-average returns based on private information.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;the-fundamental-law-of-active-portfolio-management&#34;&gt;The Fundamental Law of Active Portfolio Management&lt;/h2&gt;
&lt;p&gt;The Fundamental Law of Active Portfolio Management, also known as the Fundamental Law of Active Management or simply the Fundamental Law, is a key concept in the field of portfolio management. It was developed by Richard Grinold, a finance professor, and Ronald Kahn, a quantitative analyst, and was first published in their 1999 book &amp;ldquo;Active Portfolio Management.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The Fundamental Law relates a portfolio&amp;rsquo;s expected excess return to two fundamental components: skill and breadth. It provides a quantitative framework for evaluating the performance of active portfolio managers, helping to distinguish between luck and skill in their investment decisions.&lt;/p&gt;
&lt;p&gt;The formula for the Fundamental Law of Active Portfolio Management is as follows:&lt;/p&gt;
&lt;p&gt;Information Ratio (IR) = IC (Information Coefficient) * âˆš(BR) (Breadth)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Information Ratio (IR): The Information Ratio measures the portfolio manager&amp;rsquo;s ability to generate excess returns relative to a benchmark, adjusted for the level of risk taken. It is calculated as the ratio of the expected excess return (active return) to the tracking error of the portfolio. The higher the Information Ratio, the better the manager&amp;rsquo;s skill in generating consistent excess returns.&lt;/li&gt;
&lt;li&gt;Information Coefficient (IC): The Information Coefficient represents the manager&amp;rsquo;s ability to generate forecasts that are accurate and valuable. It quantifies the correlation between the manager&amp;rsquo;s forecasted returns and the realized returns. A perfect forecast would have an IC of 1, while an IC of 0 indicates that the manager&amp;rsquo;s forecasts are no better than random guesses.&lt;/li&gt;
&lt;li&gt;Breadth (BR): The Breadth component captures the number of independent investment opportunities that the portfolio manager can exploit. It reflects the diversification of the active positions within the portfolio. A larger breadth implies more opportunities to generate excess returns.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The Fundamental Law states that to achieve a higher Information Ratio, a portfolio manager can do one of the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Increase the Information Coefficient (IC): Improve the accuracy of their forecasts and the ability to identify mispriced assets or alpha-generating opportunities.&lt;/li&gt;
&lt;li&gt;Increase the Breadth (BR): Diversify the portfolio to include more independent alpha sources, which reduces the impact of idiosyncratic risk and improves the overall risk-adjusted performance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The Fundamental Law of Active Portfolio Management is a valuable tool for understanding the relationship between skill, diversification, and the ability to generate alpha in active portfolio management. It helps investors and portfolio managers assess the effectiveness of their investment strategies and identify potential areas for improvement.&lt;/p&gt;
&lt;h2 id=&#34;portfolio-optimization-and-efficient-frontier&#34;&gt;Portfolio Optimization and efficient frontier&lt;/h2&gt;
&lt;p&gt;Mean-Variance Optimization (MVO) is a widely used quantitative approach in finance and portfolio management to construct an optimal portfolio that maximizes expected returns for a given level of risk or minimizes risk for a given level of expected returns. It was first introduced by Harry Markowitz in his seminal paper &amp;ldquo;Portfolio Selection&amp;rdquo; in 1952, which laid the foundation for modern portfolio theory.&lt;/p&gt;
&lt;p&gt;The key idea behind Mean-Variance Optimization is to find the allocation of assets in a portfolio that strikes a balance between the desire for higher returns and the aversion to risk. The process involves the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Expected Returns: Investors first estimate the expected returns of each asset in the portfolio based on historical data, forecasts, or other relevant information. These expected returns represent the mean or average return that investors expect to earn from each asset.&lt;/li&gt;
&lt;li&gt;Risk (Variance or Standard Deviation): The risk of an asset is typically measured by its variance or standard deviation. Variance quantifies the dispersion of an asset&amp;rsquo;s returns from its expected return. Standard deviation is simply the square root of variance. The higher the variance (or standard deviation), the higher the asset&amp;rsquo;s risk.&lt;/li&gt;
&lt;li&gt;Covariance and Correlation: Investors also need to calculate the covariance or correlation between each pair of assets in the portfolio. Covariance measures how two assets move together, while correlation standardizes the covariance to a value between -1 and +1, where -1 indicates a perfect negative relationship, +1 indicates a perfect positive relationship, and 0 indicates no relationship.&lt;/li&gt;
&lt;li&gt;Efficient Frontier: Mean-Variance Optimization seeks to find the combination of assets that generates the highest expected return for a given level of risk or the lowest risk for a given level of expected return. This set of optimal portfolios is referred to as the &amp;ldquo;efficient frontier.&amp;rdquo; It represents the set of portfolios that provides the best risk-reward trade-offs.&lt;/li&gt;
&lt;li&gt;Risk Tolerance: Finally, investors must define their risk tolerance level, which indicates how much risk they are willing to bear in pursuit of higher returns. The choice of portfolio from the efficient frontier will depend on an investor&amp;rsquo;s risk preferences.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Mean-Variance Optimization has been a cornerstone of modern portfolio theory and has greatly influenced the practice of portfolio management. However, critics argue that it makes some simplifying assumptions, such as assuming that returns follow a normal distribution and that investors are solely focused on risk and return, neglecting other aspects like liquidity preferences or behavioral biases. As a result, alternative approaches, like Black-Litterman model and Conditional Value-at-Risk (CVaR) optimization, have been proposed to address some of these limitations.&lt;/p&gt;
&lt;h1 id=&#34;learning-algorithms-for-trading&#34;&gt;Learning Algorithms for Trading&lt;/h1&gt;
&lt;h2 id=&#34;parametric-vs-non-parametric&#34;&gt;Parametric vs non parametric&lt;/h2&gt;
&lt;p&gt;A parametric learner, in the context of machine learning, refers to a model that makes strong assumptions about the underlying data distribution. It assumes a specific functional form or structure for the relationship between the input variables and the output variable. In other words, the model is characterized by a fixed number of parameters that need to be estimated from the training data. Examples of parametric learners include linear regression, logistic regression, and neural networks. Once the parameters are estimated, the model can make predictions or classifications based on new input data. Parametric learners tend to be computationally efficient and require less training data, but their performance heavily depends on the accuracy of the assumed parametric form.&lt;/p&gt;
&lt;p&gt;On the other hand, a non-parametric learner does not make explicit assumptions about the underlying data distribution or functional form. Instead, it seeks to directly learn the relationship between the input variables and the output variable from the training data. Non-parametric learners, such as k-nearest neighbors, decision trees, and support vector machines, can adapt to more complex and flexible relationships in the data. They typically have more parameters and their complexity grows with the size of the training set. Non-parametric learners may require more data for training and can be computationally more expensive, but they offer greater flexibility in capturing intricate patterns in the data.&lt;/p&gt;
&lt;h2 id=&#34;knn&#34;&gt;KNN&lt;/h2&gt;
&lt;p&gt;K-Nearest Neighbors (KNN) is a popular algorithm used in machine learning for both classification and regression tasks. In the context of classification, KNN predicts the class of a new data point based on the classes of its K nearest neighbors in the feature space. The algorithm assumes that similar instances tend to have similar labels.&lt;/p&gt;
&lt;p&gt;Overfitting occurs when a model learns too much from the training data, including noise and irrelevant patterns, which leads to poor generalization on unseen data. KNN can be prone to overfitting when the value of K is too small. With a small K, the model can become overly sensitive to the local characteristics of the training data, potentially causing the model to memorize the training examples and perform poorly on new instances.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;KNNClassifier&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, k):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; k
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fit&lt;/span&gt;(self, X, y):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;X_train &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; X
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;y_train &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;predict&lt;/span&gt;(self, X):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y_pred &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; sample &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; X:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            distances &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum((self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;X_train &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; sample)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            nearest_indices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argsort(distances)[:self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;k]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            nearest_labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;y_train[nearest_indices]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            unique, counts &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unique(nearest_labels, return_counts&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            y_pred&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(unique[np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argmax(counts)])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; y_pred
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;kernel-regression&#34;&gt;Kernel regression&lt;/h2&gt;
&lt;p&gt;In kernel regression, the main idea is to assign weights to nearby data points based on their distance from the point being estimated. These weights, known as kernel weights, determine the influence of each data point on the estimation. The closer a data point is to the target point, the higher its weight and vice versa.&lt;/p&gt;
&lt;h2 id=&#34;rmse&#34;&gt;RMSE&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Root Mean Square Error (RMSE) is a commonly used metric to evaluate the performance of regression models. It measures the average deviation between the predicted and actual values of the target variable. RMSE provides a quantitative measure of the model&amp;rsquo;s accuracy by calculating the square root of the mean of squared differences between the predicted and actual values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pros-of-rmse&#34;&gt;Pros of RMSE:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;RMSE takes into account both the magnitude and direction of errors, giving a comprehensive assessment of the model&amp;rsquo;s performance.&lt;/li&gt;
&lt;li&gt;It is widely used and easily interpretable, allowing for meaningful comparisons between different models or techniques.&lt;/li&gt;
&lt;li&gt;RMSE penalizes larger errors more heavily than mean absolute error, making it more sensitive to outliers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cons-of-rmse&#34;&gt;Cons of RMSE:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Since RMSE is based on squared differences, it amplifies the impact of large errors, which can be problematic if outliers or extreme values are present in the data.&lt;/li&gt;
&lt;li&gt;RMSE does not have the same unit of measurement as the target variable, making it less interpretable in terms of the original scale.&lt;/li&gt;
&lt;li&gt;It assumes that errors follow a Gaussian distribution and that there is no heteroscedasticity (unequal variance) in the residuals.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here&amp;rsquo;s an example of Python code for calculating RMSE from scratch:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rmse&lt;/span&gt;(y_true, y_pred):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    squared_errors &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (y_true &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; y_pred) &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    mean_squared_error &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(squared_errors)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    rmse &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt(mean_squared_error)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; rmse
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the code above, the &lt;code&gt;rmse&lt;/code&gt; function takes the true values (&lt;code&gt;y_true&lt;/code&gt;) and predicted values (&lt;code&gt;y_pred&lt;/code&gt;) as input. It calculates the squared differences between the true and predicted values, computes the mean squared error, and returns the square root of the mean squared error as the RMSE.&lt;/p&gt;
&lt;p&gt;When using this implementation, it&amp;rsquo;s important to ensure that the true and predicted values are in the same format and shape. Additionally, data preprocessing, feature engineering, and model selection should be performed prior to calculating RMSE to ensure accurate evaluation of the model&amp;rsquo;s performance.&lt;/p&gt;
&lt;h2 id=&#34;mae&#34;&gt;MAE&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Mean Absolute Error (MAE) is a widely used metric for evaluating the performance of regression models. It measures the average absolute difference between the predicted and actual values of the target variable. MAE provides a straightforward measure of the model&amp;rsquo;s accuracy without considering the direction of errors.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pros-of-mae&#34;&gt;Pros of MAE:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;MAE is robust to outliers since it does not involve squaring the differences between predicted and actual values. It treats all errors equally regardless of their magnitude.&lt;/li&gt;
&lt;li&gt;It is easily interpretable as it has the same unit of measurement as the target variable, allowing for direct comparison and understanding of the model&amp;rsquo;s performance.&lt;/li&gt;
&lt;li&gt;MAE does not make any assumptions about the underlying distribution of errors and is less sensitive to heteroscedasticity.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cons-of-mae&#34;&gt;Cons of MAE:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Since MAE does not square the errors, it may be less sensitive to large errors compared to metrics like RMSE, which can be a disadvantage when outliers need to be given more weight in the evaluation.&lt;/li&gt;
&lt;li&gt;MAE does not provide information on the variance or distribution of errors, making it less informative for certain types of analysis or decision-making.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here&amp;rsquo;s an example of Python code for calculating MAE from scratch:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mae&lt;/span&gt;(y_true, y_pred):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    absolute_errors &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs(y_true &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; y_pred)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    mean_absolute_error &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(absolute_errors)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; mean_absolute_error
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the code above, the &lt;code&gt;mae&lt;/code&gt; function takes the true values (&lt;code&gt;y_true&lt;/code&gt;) and predicted values (&lt;code&gt;y_pred&lt;/code&gt;) as input. It calculates the absolute differences between the true and predicted values, computes the mean of these absolute differences, and returns it as the MAE.&lt;/p&gt;
&lt;p&gt;When using this implementation, ensure that the true and predicted values are in the same format and shape. Additionally, perform any necessary data preprocessing, feature engineering, and model selection before calculating MAE to ensure accurate evaluation of the model&amp;rsquo;s performance.&lt;/p&gt;
&lt;h2 id=&#34;cross-validation&#34;&gt;Cross validation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Cross-validation is a resampling technique used in machine learning to assess the performance and generalization ability of a model. It involves partitioning the available data into multiple subsets or folds, where each fold is used as both a training set and a validation set in a series of iterations. Cross-validation provides a more reliable estimate of the model&amp;rsquo;s performance by evaluating its consistency across different data subsets.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pros-of-cross-validation&#34;&gt;Pros of Cross-Validation:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cross-validation provides a more robust evaluation of the model&amp;rsquo;s performance compared to a single train-test split, as it utilizes multiple subsets of the data for training and testing.&lt;/li&gt;
&lt;li&gt;It helps to estimate how well the model generalizes to unseen data and provides insights into the model&amp;rsquo;s stability and consistency.&lt;/li&gt;
&lt;li&gt;Cross-validation allows for tuning hyperparameters and selecting the best model configuration by comparing the performance across different folds.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cons-of-cross-validation&#34;&gt;Cons of Cross-Validation:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Implementing cross-validation can be computationally expensive, especially for large datasets or complex models, as it requires fitting and evaluating the model multiple times.&lt;/li&gt;
&lt;li&gt;In some cases, the performance of a model can vary significantly across different folds, leading to a less reliable estimate of its generalization ability.&lt;/li&gt;
&lt;li&gt;Cross-validation may not account for certain types of data dependencies, such as time-series data, where the order of observations is important.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here&amp;rsquo;s an example of Python code for implementing k-fold cross-validation from scratch:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cross_validation&lt;/span&gt;(X, y, model, k):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(X)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    fold_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; k
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    scores &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(k):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        start &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; fold_size
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        end &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; start &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; fold_size
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        X_train &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concatenate((X[:start], X[end:]), axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y_train &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concatenate((y[:start], y[end:]), axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        X_val &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; X[start:end]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y_val &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y[start:end]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train, y_train)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        score &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;evaluate(X_val, y_val)  &lt;span style=&#34;color:#75715e&#34;&gt;# Evaluation metric specific to the model&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        scores&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(score)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; scores
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the code above, the &lt;code&gt;cross_validation&lt;/code&gt; function takes the input features (&lt;code&gt;X&lt;/code&gt;), target variable (&lt;code&gt;y&lt;/code&gt;), the model to evaluate, and the number of folds (&lt;code&gt;k&lt;/code&gt;) as input. It iteratively partitions the data into training and validation sets, fits the model on the training data, and evaluates its performance using a specific evaluation metric. The function returns a list of scores obtained from each fold.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s important to note that the code provided is a basic implementation and may need to be modified or extended depending on the specific requirements of the model and evaluation metric. Additionally, the &lt;code&gt;model.fit&lt;/code&gt; and &lt;code&gt;model.evaluate&lt;/code&gt; methods represent placeholder functions and should be replaced with the appropriate methods for the chosen model.&lt;/p&gt;
&lt;h2 id=&#34;ensemble-learners&#34;&gt;Ensemble learners&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ensemble learning is a machine learning technique that combines multiple individual models, called base models or weak learners, to improve predictive performance and generalization ability. The idea behind ensemble learning is to leverage the diversity of the base models and aggregate their predictions to make a final prediction that is often more accurate and robust than that of any individual model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ensemble learners can be categorized into two main types: bagging and boosting.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Bagging&lt;/strong&gt;: Bagging stands for bootstrap aggregating. It involves training multiple base models independently on different subsets of the training data, created through bootstrap sampling (sampling with replacement). The predictions from these models are then combined, typically through majority voting (for classification) or averaging (for regression), to obtain the final prediction. The goal is to reduce variance and improve generalization by reducing the impact of individual noisy or overfitting models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Boosting&lt;/strong&gt;: Boosting aims to sequentially train a series of base models, where each subsequent model focuses on correcting the mistakes made by the previous models. In boosting, the training data is reweighted, giving higher importance to the instances that were misclassified by previous models. The predictions of the base models are combined by weighted voting or weighted averaging to obtain the final prediction. Boosting methods, such as AdaBoost, Gradient Boosting, and XGBoost, often achieve high accuracy by iteratively building strong models from weak ones.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here&amp;rsquo;s an example of Python code for implementing ensemble learning using the Random Forest algorithm, which is a popular ensemble method based on bagging:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.ensemble &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; RandomForestClassifier
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create an ensemble of 100 decision tree classifiers&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ensemble &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RandomForestClassifier(n_estimators&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Train the ensemble on the training data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ensemble&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train, y_train)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Make predictions using the ensemble&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;predictions &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ensemble&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the code above, the &lt;code&gt;RandomForestClassifier&lt;/code&gt; class from the scikit-learn library is used to create an ensemble of 100 decision tree classifiers. The &lt;code&gt;n_estimators&lt;/code&gt; parameter specifies the number of base models in the ensemble. The ensemble is then trained on the training data (&lt;code&gt;X_train&lt;/code&gt; and &lt;code&gt;y_train&lt;/code&gt;), and predictions are made on the test data (&lt;code&gt;X_test&lt;/code&gt;) using the &lt;code&gt;predict&lt;/code&gt; method.&lt;/p&gt;
&lt;h2 id=&#34;reinforcement-learning&#34;&gt;Reinforcement Learning&lt;/h2&gt;
&lt;p&gt;Reinforcement Learning (RL) is a type of machine learning paradigm where an agent learns to make decisions and take actions in an environment to achieve a specific goal. Unlike supervised learning, where the model is trained on labeled data, or unsupervised learning, where the model finds patterns and structures in unlabeled data, RL focuses on learning through interaction with an environment and receiving feedback in the form of rewards or penalties.&lt;/p&gt;
&lt;p&gt;The basic components of a reinforcement learning system are as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Agent: The agent is the learner or decision-maker that interacts with the environment. It makes observations, takes actions, and learns from the rewards or penalties it receives.&lt;/li&gt;
&lt;li&gt;Environment: The environment is the context or setting in which the agent operates. It can be anything from a virtual environment in a computer simulation to a real-world scenario.&lt;/li&gt;
&lt;li&gt;Actions: At each time step, the agent chooses an action from a set of possible actions based on its current state and the information it has learned from previous interactions.&lt;/li&gt;
&lt;li&gt;State: The state represents the current situation or context of the agent within the environment. It captures the relevant information necessary for the agent to make decisions.&lt;/li&gt;
&lt;li&gt;Rewards: After taking an action, the agent receives feedback in the form of rewards or penalties from the environment. Positive rewards encourage the agent to take actions that lead to the desired goal, while negative rewards discourage undesired actions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The objective of the agent in reinforcement learning is to learn a policy, which is a mapping from states to actions, that maximizes the cumulative reward over time. The agent employs exploration and exploitation strategies to balance between trying out new actions (exploration) and exploiting the knowledge it has gained so far to make optimal decisions (exploitation).&lt;/p&gt;
&lt;p&gt;Reinforcement learning has been successfully applied in various fields, including robotics, game playing (e.g., AlphaGo), autonomous vehicles, recommendation systems, finance, and more. Deep Reinforcement Learning (DRL), which combines reinforcement learning with deep neural networks, has shown remarkable achievements in complex tasks by utilizing deep learning&amp;rsquo;s ability to handle high-dimensional input data.&lt;/p&gt;
&lt;p&gt;One of the key challenges in reinforcement learning is the trade-off between exploration and exploitation, and the potential for the agent to get stuck in suboptimal solutions (local optima). Researchers continue to develop new algorithms and techniques to address these challenges and further advance the capabilities of reinforcement learning in practical applications.&lt;/p&gt;
&lt;h2 id=&#34;q-learning&#34;&gt;Q Learning&lt;/h2&gt;
&lt;p&gt;Q-learning is a popular model-free reinforcement learning algorithm used to find an optimal policy for an agent to make decisions in an environment. It was developed by Christopher Watkins in his PhD thesis in 1989. Q-learning is a type of Temporal Difference (TD) learning, which means it learns from the difference between its predictions and the observed rewards obtained from the environment.&lt;/p&gt;
&lt;p&gt;The central idea behind Q-learning is to estimate the value of taking a particular action in a given state, called the action-value function or Q-function. The Q-value represents the expected cumulative reward the agent can achieve by starting in a particular state, taking a specific action, and following an optimal policy thereafter.&lt;/p&gt;
&lt;p&gt;The Q-learning algorithm works as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialization: Initialize the Q-function arbitrarily for all state-action pairs. Typically, the Q-values are initialized to zero, or a small random value.&lt;/li&gt;
&lt;li&gt;Exploration vs. Exploitation: The agent interacts with the environment by taking actions based on its current policy. Initially, it often explores the environment by selecting random actions (exploration) to discover new strategies. As the learning progresses, the agent starts exploiting the Q-values it has learned to choose the actions with the highest Q-values.&lt;/li&gt;
&lt;li&gt;Update Q-values: After each action, the agent receives a reward from the environment and observes the new state. The Q-value for the (state, action) pair is updated using the Bellman equation:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Q(s, a) = Q(s, a) + Î± * [r + Î³ * max Q(s&amp;rsquo;, a&amp;rsquo;) - Q(s, a)]&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Q(s, a): The Q-value for state s and action a.&lt;/li&gt;
&lt;li&gt;Î±: The learning rate, which determines how much the agent updates its Q-values based on new information.&lt;/li&gt;
&lt;li&gt;r: The reward received by taking action a in state s.&lt;/li&gt;
&lt;li&gt;Î³: The discount factor, which balances immediate rewards versus future rewards.&lt;/li&gt;
&lt;li&gt;max Q(s&amp;rsquo;, a&amp;rsquo;): The maximum Q-value for the next state s&amp;rsquo; and all possible actions a&amp;rsquo;.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Continue Exploration and Exploitation: The agent continues to interact with the environment, updating Q-values after each action, and refining its policy to improve performance over time.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Q-learning is known to converge to the optimal Q-values and an optimal policy in the limit as the agent explores the environment indefinitely. It is especially effective in situations where the agent has no prior knowledge of the environment, and the transition model and reward function are unknown.&lt;/p&gt;
&lt;p&gt;Q-learning has been widely used in various applications, such as game playing, robotic control, and optimization problems, and has paved the way for more advanced deep reinforcement learning algorithms like Deep Q-Networks (DQNs) that leverage deep neural networks to approximate the Q-function in high-dimensional state spaces.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>