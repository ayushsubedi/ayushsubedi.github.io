<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cart on Ayush Subedi</title>
    <link>https://ayushsubedi.github.io/tags/cart/</link>
    <description>Recent content in Cart on Ayush Subedi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 22 Feb 2024 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ayushsubedi.github.io/tags/cart/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>[Paper Exploration] Statistical Modeling: The Two Cultures</title>
      <link>https://ayushsubedi.github.io/posts/statistical_modelling_two_cultures/</link>
      <pubDate>Thu, 22 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/statistical_modelling_two_cultures/</guid>
      <description>&lt;h1 id=&#34;paper-exploration-statistical-modeling-the-two-cultures&#34;&gt;[Paper Exploration] Statistical Modeling: The Two Cultures&lt;/h1&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;There are &lt;strong&gt;two cultures in the use of statistical modeling&lt;/strong&gt; to reach conclusions from data. One assumes that the data are generated by a given &lt;strong&gt;stochastic data model&lt;/strong&gt;. The other uses &lt;strong&gt;algorithmic models&lt;/strong&gt; and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. &lt;strong&gt;If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Author: Leo Breiman&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Published on 2001&lt;/p&gt;
&lt;/blockquote&gt;
&lt;iframe width=&#34;100%&#34; height =&#34;1024&#34; src=&#34;https://www2.math.uu.se/~thulin/mm/breiman.pdf#toolbar=0&#34;&gt;&lt;/iframe&gt;
&lt;hr&gt;
&lt;h1 id=&#34;leo-breiman&#34;&gt;Leo Breiman&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/leo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Leo Breiman was an influential American statistician and professor, best known for his significant contributions to the field of statistics and machine learning.&lt;/li&gt;
&lt;li&gt;Breiman made significant contributions to various areas of statistics, including classification and regression trees, ensemble learning methods, and random forests.&lt;/li&gt;
&lt;li&gt;One of Breiman&amp;rsquo;s most notable contributions is the development of the Random Forest algorithm, introduced in his seminal paper &amp;ldquo;Random Forests&amp;rdquo; published in 2001.&lt;/li&gt;
&lt;li&gt;His ideas continue to be studied, extended, and applied in various domains, contributing to the advancement of data science and predictive modeling.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;the-two-cultures&#34;&gt;The Two Cultures&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Statistics start with data&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Nature functions to associate the predictor variables with the response variables&lt;/li&gt;
&lt;li&gt;There are two goals in analyzing the data:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Prediction&lt;/strong&gt;: To be able to predict what the responses are going to be to future input variables&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Information&lt;/strong&gt;: To extract some information about how nature is associating the response variables to the input variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/data_cultures.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-data-modeling-culture&#34;&gt;The Data Modeling Culture&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The analysis in this culture starts with assuming a stochastic data model for the inside of the black box&lt;/li&gt;
&lt;li&gt;Response variables = f(predictor variables, random noise, parameters)&lt;/li&gt;
&lt;li&gt;The values of the parameters are estimated from the data and the model then used for information and/or prediction.&lt;/li&gt;
&lt;li&gt;Model validation. Yes–no using goodness-of-fit tests and residual examination.&lt;/li&gt;
&lt;li&gt;Estimated culture population. 98% of all statisticians.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-algorithmic-modeling-culture&#34;&gt;The Algorithmic Modeling Culture&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The analysis in this culture considers the inside of the box complex and unknown.&lt;/li&gt;
&lt;li&gt;Their approach is to find a function fx—an algorithm that operates on x to predict the responses y.&lt;/li&gt;
&lt;li&gt;Model validation. Measured by predictive accuracy.&lt;/li&gt;
&lt;li&gt;Estimated culture population. 2% of statisticians, many in other fields.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;motivation-why-review-this-paper&#34;&gt;Motivation (Why review this paper?)&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/stats.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I graduated with a Bachelors in Mathematics when the algorithmic modeling culture was not commonplace.&lt;/li&gt;
&lt;li&gt;I graduated with a Masters in Machine Learning at a time where the algorithmic modeling culture everywhere.&lt;/li&gt;
&lt;li&gt;I work in a company that delivers Deep Learning solutions, but requires data modeling culture for its own solution-ing (experimentation, distribution, sampling, etc.).&lt;/li&gt;
&lt;li&gt;I work with students in Nepal who want to implement Large Language Models, sophisticated Deep Learning Models, but do not want to learn about foundational statistics/linear algebra/calculus/optimization.&lt;/li&gt;
&lt;li&gt;In many ways, I am currently trying to ask students in Nepal to do the exact opposite of what Breiman had to do with this paper. The 98%-2% might have flipped on its head. I do not like it.
&lt;blockquote&gt;
&lt;p&gt;For the last point, I am not implying that prediction accuracy is not important, or should not be pursued. I am implying that it is not the only thing to be pursued. That persuasion has several shortcuts with the advancement of ML packages. This not only makes the data model a black box, but also make the machine learning implementation a black box.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;breimans-call-to-statisticians-to-join-the-2&#34;&gt;Breiman&amp;rsquo;s call to statisticians to join the 2%&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Breiman argues that the focus in the statistical community on data models has:
&lt;ul&gt;
&lt;li&gt;Led to irrelevant theory and questionable scientific conclusions&lt;/li&gt;
&lt;li&gt;Kept statisticians from using more suitable algorithmic models&lt;/li&gt;
&lt;li&gt;Prevented statisticians from working on exciting new problems&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;breiman-as-a-consultant&#34;&gt;Breiman as a consultant&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Breiman&amp;rsquo;s experiences as a consultant formed his views about algorithmic modeling&lt;/li&gt;
&lt;li&gt;Breiman&amp;rsquo;s perceptions on Statistical Analysis:
&lt;ul&gt;
&lt;li&gt;Focus on finding a good solution—that’s what consultants get paid for.&lt;/li&gt;
&lt;li&gt;Live with the data before you plunge into modeling.&lt;/li&gt;
&lt;li&gt;Search for a model that gives a good solution, either algorithmic or data.&lt;/li&gt;
&lt;li&gt;Predictive accuracy on test sets is the criterion for how good the model is.&lt;/li&gt;
&lt;li&gt;Computers are an indispensable partner&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;breiman-after-returning-to-university&#34;&gt;Breiman after returning to University&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;I had one tip about what research in the university was like. A friend of mine, a prominent statistician from the Berkeley Statistics Department, visited me in Los Angeles in the late 1970s. After I described the decision tree method to him, his first question was, “What’s the model for the data?”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Upon my return, I started reading the Annals of Statistics, the flagship journal of theoretical statistics, and was bemused. Every article started with: Assume that the data are generated by the following model: &amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;rashomon-effect-and-the-data-modeling-culture&#34;&gt;Rashomon Effect and the Data Modeling Culture&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nyCVLZBd0QV1Y8Ce.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;occams-razor-and-the-data-modeling-culture&#34;&gt;Occam&amp;rsquo;s razor and the Data Modeling Culture&lt;/h1&gt;
&lt;p&gt;Occam&amp;rsquo;s razor is a principle often attributed to 14th–century friar William of Ockham that says that if you have two competing ideas to explain the same phenomenon, you should prefer the simpler one.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kjtradingsystems.com/uploads/3/4/0/2/34026855/newimage23_orig.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/v2/resize:fit:1400/1*hf-SRyQ9md812DJYe0gt7Q.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Side note. Check this &lt;a href=&#34;https://fs.blog/mental-models/&#34;&gt;amazing article on mental models&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;the-curse-of-dimensionality-the-data-modeling-culture&#34;&gt;The curse of dimensionality the Data Modeling Culture&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;As the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/distance_dimension.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As distance between observations increases with the dimensions, the sample size required for learning a model drastically increases.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Increased Sparsity:&lt;/strong&gt;  In higher dimensions, the available data points are spread out more thinly across the space. This means that data points become farther apart from each other, making it challenging to find meaningful clusters or patterns. It&amp;rsquo;s like having a lot of points scattered in a large, high-dimensional space, and they&amp;rsquo;re so spread out that it&amp;rsquo;s difficult to identify any consistent relationships.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;More Data Needed:&lt;/strong&gt;  With higher-dimensional data, you need a disproportionately larger amount of data to capture the underlying patterns accurately. When the data is sparse, it&amp;rsquo;s harder to generalize from the observed points to make accurate predictions or draw conclusions. As the dimensionality increases, you might need exponentially more data to maintain the same level of accuracy in your models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact on Complexity:&lt;/strong&gt;  The complexity of machine learning models increases with dimensionality. More dimensions mean more parameters to estimate, which can lead to overfitting – a situation where a model fits the training data too closely and fails to generalize well to new data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Increased Computational Demands:&lt;/strong&gt;  Processing and analyzing high-dimensional data require more computational resources and time. Many algorithms become slower and more memory-intensive as the number of dimensions grows. This can make experimentation and model training more challenging and time-consuming.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Difficulties in Visualization:&lt;/strong&gt;  Our ability to visualize data effectively diminishes as the number of dimensions increases. We are accustomed to thinking in 2D and 3D space, but visualizing data in, say, 10 dimensions is practically impossible. This can make it hard to understand the structure of the data and the relationships between variables.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.ensemble &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; RandomForestClassifier
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.model_selection &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; train_test_split
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; accuracy_score
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;X_train, X_test, y_train, y_test &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_test_split(X, y, test_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;, random_state&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;42&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;random_forest &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RandomForestClassifier(n_estimators&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, random_state&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;42&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;random_forest&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train, y_train)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y_pred &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random_forest&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;accuracy &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; accuracy_score(y_test, y_pred)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Accuracy:&amp;#34;&lt;/span&gt;, accuracy)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>