<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>explainability on Ayush Subedi</title>
    <link>https://ayushsubedi.github.io/tags/explainability/</link>
    <description>Recent content in explainability on Ayush Subedi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 06 Sep 2023 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ayushsubedi.github.io/tags/explainability/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Paper Exploration: A Unified Approach to Interpreting Model Predictions</title>
      <link>https://ayushsubedi.github.io/posts/shap_exploration/</link>
      <pubDate>Wed, 06 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/shap_exploration/</guid>
      <description>&lt;h1 id=&#34;paper-exploration-a-unified-approach-to-interpreting-model-predictions&#34;&gt;Paper Exploration: A Unified Approach to Interpreting Model Predictions&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Paper Authors&lt;/strong&gt;: Scott M. Lundberg, Su-In Lee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/shap/shap&#34;&gt;https://github.com/shap/shap&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Original Paper&lt;/strong&gt;:&lt;/p&gt;
&lt;iframe width=&#34;100%&#34; height =&#34;512&#34; src=&#34;https://arxiv.org/pdf/1705.07874v2.pdf#toolbar=0&#34;&gt;&lt;/iframe&gt;
&lt;h2 id=&#34;exploration&#34;&gt;Exploration&lt;/h2&gt;
&lt;h3 id=&#34;problem&#34;&gt;Problem&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Understanding model predictions is crucial for many applications. However,  complex models, like ensemble or deep learning models, (while they usually achieve high accuracy) are generally difficult to interpret.&lt;/li&gt;
&lt;li&gt;Existing interpretation methods lack clarity about their relationships and preferences.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;proposed-solution-by-authors&#34;&gt;Proposed Solution by Authors&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Introduce a unified framework called &lt;strong&gt;SHAP (SHapley Additive exPlanations)&lt;/strong&gt; for interpreting predictions.&lt;/li&gt;
&lt;li&gt;Assign importance values to each feature for a specific prediction.&lt;/li&gt;
&lt;li&gt;Develop a new class of additive feature importance measures.&lt;/li&gt;
&lt;li&gt;Prove the uniqueness of a solution within this class with desirable properties.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;history&#34;&gt;History&lt;/h2&gt;
&lt;h3 id=&#34;lloyd-shapley&#34;&gt;Lloyd Shapley&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/c/cb/Shapley%2C_Lloyd_%281980%29.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Lloyd Shapley was an American mathematician and economist who made significant contributions to game theory and cooperative game theory. He was awarded the Nobel Prize in Economic Sciences in 2012, along with Alvin E. Roth, for his work on the theory of stable allocations and the Shapley value, which has applications in various fields, including economics, politics, and computer science. Shapley&amp;rsquo;s pioneering research has had a lasting impact on understanding and solving problems involving cooperation and allocation of resources in competitive environments.&lt;/p&gt;
&lt;h3 id=&#34;shapley-value&#34;&gt;Shapley Value&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://www.wallstreetmojo.com/wp-content/uploads/2022/10/Shapley-Value.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If we have a coalition C that collaborates to produce a value V, how much did each individual member contribute to the final value?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/coalition.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Unfortunately, there will be interaction effect, certain permutation cause members to contribute to more than the sum of their parts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The Shapley value is a concept from cooperative game theory introduced by Lloyd Shapley. It is used to fairly distribute the value or payoff created by cooperation among a group of participants. The Shapley value provides a unique way to allocate the total value generated by a coalition of players to each individual player based on their marginal contributions to different possible coalitions.&lt;/p&gt;
&lt;p&gt;Key characteristics of the Shapley value:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Fairness&lt;/strong&gt;: It ensures that each player receives a share of the total value that is proportional to their contribution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Symmetry&lt;/strong&gt;: The Shapley value treats all players equally and satisfies a symmetry property, meaning that if two players have the same contributions to all coalitions, they should receive the same allocation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;: It guarantees that the sum of allocations to all players adds up to the total value generated by all possible coalitions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Shapley value for member 1:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sample a coalition that contains member 1, and looking at coalition formed by removing that member.&lt;/li&gt;
&lt;li&gt;Look at the respective values of the coalition, and compare the difference between the two. The difference is the marginal contribution of Member 1 to the group.&lt;/li&gt;
&lt;li&gt;The mean marginal contribution is the Sharpley value.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/c.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://ayushsubedi.github.io/img/v.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;mathematically&#34;&gt;Mathematically&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://wikimedia.org/api/rest_v1/media/math/render/svg/47cb4304f990859ccc7589f5d15ca00575678ac3&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;machine-learning&#34;&gt;Machine Learning&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/shap_models.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;shapley-additive-explanations&#34;&gt;Shapley Additive Explanations&lt;/h3&gt;
&lt;p&gt;The novel unified approach to interpreting model predictions proposed by the authors:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The perspective of viewing any explanation of a model&amp;rsquo;s prediction as a model itself (referred to as the &lt;em&gt;explanation model&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Show that the game theory results guarantees a unique solution to the entire class of additive feature attribution methods, and propose &lt;em&gt;SHAP&lt;/em&gt; values as a unified measure of feature importance that various methods approximate.&lt;/li&gt;
&lt;li&gt;Propose new SHAP value estimation methods and demonstrate that they are better aligned with human intuition as measured by user studies.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;additive-feature-attribution-methods&#34;&gt;Additive Feature Attribution Methods&lt;/h3&gt;
&lt;p&gt;$g(z^&amp;rsquo;)$ = $\phi_0 + \sum_{i=1}^M\phi_iz^&amp;rsquo;_i,$&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>