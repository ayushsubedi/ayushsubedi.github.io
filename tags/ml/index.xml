<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ml on Ayush Subedi</title>
    <link>https://ayushsubedi.github.io/tags/ml/</link>
    <description>Recent content in ml on Ayush Subedi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 04 Apr 2021 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ayushsubedi.github.io/tags/ml/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Diabetic Retinopathy and Glaucoma Detection</title>
      <link>https://ayushsubedi.github.io/posts/cheers_ai/</link>
      <pubDate>Sun, 04 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/cheers_ai/</guid>
      <description>&lt;h1 id=&#34;cheers-ai&#34;&gt;Cheers AI&lt;/h1&gt;
&lt;h1 id=&#34;diabetic-retinopathy-and-glaucoma-detection&#34;&gt;Diabetic Retinopathy and Glaucoma Detection&lt;/h1&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;h4 id=&#34;efficient-prediction-models&#34;&gt;Efficient Prediction Models&lt;/h4&gt;
&lt;p&gt;Efficient models trained on Inception-v3, with weightage on recall.&lt;/p&gt;
&lt;h4 id=&#34;patient-tracking&#34;&gt;Patient Tracking&lt;/h4&gt;
&lt;p&gt;Powerful MIS to create and track patient, and patient&amp;rsquo;s historical predictions.&lt;/p&gt;
&lt;h4 id=&#34;continuous-learning&#34;&gt;Continuous Learning&lt;/h4&gt;
&lt;p&gt;Inputs reviewed by opthalmologists and added to training.&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;h3 id=&#34;the-rationale-for-developing-countries&#34;&gt;The rationale for developing countries.&lt;/h3&gt;
&lt;h3 id=&#34;diabetic-retinopathy&#34;&gt;Diabetic Retinopathy&lt;/h3&gt;
&lt;p&gt;Diabetic Retinopathy is an eye illness caused by diabetes that may lead to vision impairment and even to blindness if it isn&amp;rsquo;t identified and treated early. Of the estimated 422 million diabetics globally, more than 148 million have DR and 48 million have Vision Threating DR (VTDR).&lt;/p&gt;
&lt;p&gt;However, because of insufficient specialists and eye care health workers globally as well as locally to screen everyone at risk, the situation seems acute especially in developing countries like Nepal. Besides, Nepal has difficult geographical terrain and people living in remorse remote areas with limited or no access to clinics and screening facilities making the condition even worse.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://camo.githubusercontent.com/a6b040d6eca19246121fb7a4e3fca782ed625c42af1889778c4edf14151198ef/68747470733a2f2f6761647364656e6579652e636f6d2f77702d636f6e74656e742f75706c6f6164732f64696162657469632d726574696e6f70617468792d766563746f722e6a7067&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;glaucoma&#34;&gt;Glaucoma&lt;/h3&gt;
&lt;p&gt;Glaucoma is a diverse group of disorders representing the second prominent cause of blindness. It has already affected 91 million individuals all over the world. It has multiple risk factors such as older age, elevated intraocular pressure (IOP), and thinner central corneal thickness etc. However, one or more of these risk factors may or may not develop glaucoma making it difficult for accurate prediction of the disease. Additionally, since glaucoma can be asymptomatic, its detection before significant vision loss is critical. Hence, automated methods for predicting glaucoma could have a significant impact.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://camo.githubusercontent.com/71e24e233d0fc826e10230944b2a1cdfba81b4387862f899f1f7c400330b02c6/68747470733a2f2f7777772e696e6d6564706861726d612e636f6d2f77702d636f6e74656e742f75706c6f6164732f323032302f30352f476c6175636f6d612d636f6d70617265642d746f2d6e6f726d616c2d766973696f6e2e706e67&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;an-intuitive-app&#34;&gt;An intuitive app&lt;/h3&gt;
&lt;p&gt;Easy to use, access managed platform, with the primary focus on providing assistance to our opthalmologists.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cheersai.ml/static/img/demo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;process&#34;&gt;Process&lt;/h1&gt;
&lt;h1 id=&#34;glaucoma-prediction&#34;&gt;Glaucoma Prediction&lt;/h1&gt;
&lt;h2 id=&#34;what-worked-90-accuracy&#34;&gt;What worked? (90% accuracy)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;densenet sequential with ben on himanchu dataset, using NLLLoss criterion, Adam optimizer&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;limitation&#34;&gt;Limitation&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;very much dependent on dataset&lt;/li&gt;
&lt;li&gt;disk extraction is good but is very subjective to the dataset&lt;/li&gt;
&lt;li&gt;trained on very small dataset&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;preliminary&#34;&gt;Preliminary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a gmail account (&lt;a href=&#34;mailto:glaucomadetection@gmail.com&#34;&gt;glaucomadetection@gmail.com&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; understand the difference between possibility of glaucoma by classification (vs measurements)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; ben transformation&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; extract disk from fundus images&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; improve extraction algorithms&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; perform EDA on disk image to find troubling images (cases where crop does not work)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; convert python function to extract disk to torch transform class (failed)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; transformation to disk during training failed. create a disk dataset before training the model.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; train on new dataset with and without ben transformation&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; handle imbalanced class with class weighting&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; convert Kaggle dataset to the format that we have templated our notebooks with&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; for kaggle dataset get disks using new algorithm&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;obseverations-in-regards-to-disk-generation&#34;&gt;Obseverations in regards to disk generation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;extraction of disk does not help (too many vague areas left unfilled)&lt;/li&gt;
&lt;li&gt;however, cropping shows very good promise&lt;/li&gt;
&lt;li&gt;but, cropping requires somewhat similar of fundus images&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;datasets&#34;&gt;Datasets&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; find datasets &lt;a href=&#34;https://deepblue.lib.umich.edu/data/concern/data_sets/3b591905z&#34;&gt;https://deepblue.lib.umich.edu/data/concern/data_sets/3b591905z&lt;/a&gt;, &lt;a href=&#34;https://www.kaggle.com/andrewmvd/ocular-disease-recognition-odir5k&#34;&gt;https://www.kaggle.com/andrewmvd/ocular-disease-recognition-odir5k&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a dataset from Magrabia&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a dataset from Messidor&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a dataset from Ocular Disease Recognition&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create EDA on non measurement dataset (Ocular Disease Recognition)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a dataset from ocular disease recognition to include normal and glaucoma images&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; (Kaggle dataset, custom generated, filtered)https://www.kaggle.com/sshikamaru/glaucoma-detection?select=glaucoma.csv&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; train on Kaggle dataset (without changing anything)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inception v3 with and without ben on ocular, kaggle, and himanchu dataset&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inception v3 with ben on ocular, kaggle, and himanchu dataset (disk extracted, normal, and cropped dataset)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; densenet linear with ben on ocular, kaggle, and himanchu dataset&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; densenet linear with ben on ocular, kaggle, and himanchu dataset (disk extracted, normal, and cropped dataset)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; densenet sequential with ben on ocular, kaggle, and himanchu dataset&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; densenet sequential with ben on ocular, kaggle, and himanchu dataset (disk extracted, normal, and cropped dataset)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add datasets from cheers for testing&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add datasets from cheers for training&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;diabetic-retinopathy-prediction&#34;&gt;Diabetic Retinopathy Prediction&lt;/h1&gt;
&lt;h2 id=&#34;what-worked-90-accuracy-1&#34;&gt;What worked? (90% accuracy)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Large dataset from EyePACS (Kaggle competition used training (30%) and testing data (70%) from Kaggle. After the competition, the labels were published). Flipped the ratios for our use case.&lt;/li&gt;
&lt;li&gt;Remove out of focus images&lt;/li&gt;
&lt;li&gt;Remove too bright, and too dark images.&lt;/li&gt;
&lt;li&gt;Link to clean dataset &lt;a href=&#34;https://www.kaggle.com/ayushsubedi/drunstratified&#34;&gt;https://www.kaggle.com/ayushsubedi/drunstratified&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;To handle class imbalanced issue, used weighted random samplers. Undersampling to match no of images in the least class (4) did not work. Pickled weights for future use.&lt;/li&gt;
&lt;li&gt;Ben Graham transformation and augmentations&lt;/li&gt;
&lt;li&gt;Inception v3 fine tuning, with aux logits trained (better results compared to other architecture)&lt;/li&gt;
&lt;li&gt;Perform EDA on inference to observe what images were causing issues&lt;/li&gt;
&lt;li&gt;Removed the images and created another dataset (Link to the new dataset &lt;a href=&#34;https://www.kaggle.com/ayushsubedi/cleannonstratifieddiabeticretinopathy&#34;&gt;https://www.kaggle.com/ayushsubedi/cleannonstratifieddiabeticretinopathy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See 5, 6, and 7&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;todos&#34;&gt;TODOS&lt;/h3&gt;
&lt;h3 id=&#34;datasets-1&#34;&gt;Datasets&lt;/h3&gt;
&lt;p&gt;Binary Stratified (cleaned): &lt;a href=&#34;https://drive.google.com/drive/folders/12-60Gm7c_TMu1rhnMhSZjrkSqqAuSsQf?usp=sharing&#34;&gt;https://drive.google.com/drive/folders/12-60Gm7c_TMu1rhnMhSZjrkSqqAuSsQf?usp=sharing&lt;/a&gt;
Categorical Stratified (cleaned): &lt;a href=&#34;https://drive.google.com/drive/folders/1-A_Mx9GdeUwCd03TUxUS3vwcutQHFFSM?usp=sharing&#34;&gt;https://drive.google.com/drive/folders/1-A_Mx9GdeUwCd03TUxUS3vwcutQHFFSM?usp=sharing&lt;/a&gt;
Non Stratified (cleaned): &lt;a href=&#34;https://www.kaggle.com/ayushsubedi/drunstratified&#34;&gt;https://www.kaggle.com/ayushsubedi/drunstratified&lt;/a&gt;
Recleaned Non Stratified: &lt;a href=&#34;https://www.kaggle.com/ayushsubedi/cleannonstratifieddiabeticretinopathy&#34;&gt;https://www.kaggle.com/ayushsubedi/cleannonstratifieddiabeticretinopathy&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;priliminary&#34;&gt;Priliminary&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a new gmail account to store datasets (&lt;a href=&#34;mailto:diabeticretinopathyglaucoma@gmail.com&#34;&gt;diabeticretinopathyglaucoma@gmail.com&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=VIrkurR446s&amp;amp;ab_channel=khanacademymedicine&#34;&gt;https://www.youtube.com/watch?v=VIrkurR446s&amp;amp;ab_channel=khanacademymedicine&lt;/a&gt; What is diabetic retinopathy?&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; collect all previous analysis notebooks&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; conduct preliminary EDA (for balanced dataset, missing images etc)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create balanced test train split for DR (stratify)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; store the dataset in drive for colab&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; identify a few research papers, create a file to store subsequently found research papers&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; identify right technology stack to use (for ML, training, PM, model versioning, stage deployment, actual deployment)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; perform basic augmentation&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a version 0 base model&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; apply a random transfer learning model&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a metric for evaluation&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; store the model in zenodo, or find something for version control&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a model that takes image as an input&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a streamlit app that reads model&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; streamlit app to upload and test prediction&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; test deployment to free tier heroku&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; identify gaps&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create priliminary test set&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create folder structures for saved model in the drive&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; figure out a way to move files from kaggle to drive (without download/upload)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; research saving model (the frugal way)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; research saving model to google drive after each epoch so that during unforseen interuptions, the training of the model can be continued&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;resource&#34;&gt;Resource&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; upgrade to 25GB RAM in Google Colab possibly w/ Tesla P100 GPU&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; upgrade to Colab Pro&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;baseline&#34;&gt;Baseline&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; medicmind grading (accuracy: 0.8)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; medicmind classification (0.47)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;transfer-learning&#34;&gt;Transfer Learning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; resnet&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; alexnet&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; vgg&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; squeezenet&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; densenet&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inception&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; efficient net&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;dataset-clean-images&#34;&gt;Dataset clean images&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a backup of primary dataset (zip so that kaggle kernels can consume them too)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; find algorithms to detect black/out of focus images&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; identify correct threshold for dark and out of focus images&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; remove black images&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; remove out of focus images&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a stratified dataset with 2015 data only (convert train and test both to train and use), remove black images and out of focus images (also create test set)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create non-stratified dataset with 2015 clean data only (train, test, valid) (upload in kaggle if google drive full)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a binary dataset (train, test, valid)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create confusion matrices (train, test, valid) after clean up (dark and blurry)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; the model is confusing labels 0 and 1 as 2, is this due to disturbance in image in 0.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; concluded that the result is due to the model not capturing class 0 enough (due to undersampling)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;inference&#34;&gt;Inference&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a csv with preds probability and real label&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; calculate recall, precision, accuracy, confusion matrix&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; identify different prediction issues&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; relationship between difference in preds and accuracy&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inference issue: labels 0 being predicted as 4&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inference issue: Check images from Grade 2, 3 being predicted as Grade 0&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inference issue: Check images from Grade 4 being predicted as Grade 0&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inference issue: Check images from Grade 0 being predicted as Grade 4&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inference issue: A significant Grade 2 is being predicted as Grade 0&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; inference issue: More than 50% of Grade 1 is being predicted as Grade 0&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a new dataset&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;model-improvement&#34;&gt;Model Improvement&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; research kaggle winning augmentation for DR&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; research appropriate augmentation: optical distortion, grid distortion, piecewise affine transform, horizontal flip, vertical flip, random rotation, random shift, random scale, a shift of RGB values, random brightness and contrast, additive Gaussian noise, blur, sharpening, embossing, random gamma, and cutout&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; train on various pretrained models or research which is supposed to be ideal for this case &lt;a href=&#34;https://pytorch.org/vision/stable/models.html&#34;&gt;https://pytorch.org/vision/stable/models.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create several neural nets (test different layers)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; experiment with batch size&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Reducing lighting-condition effects&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Cropping uninformative area&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Create custom dataloader based on ben graham kaggle winning strategy&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; finetune vs feature extract&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; oversample&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; undersample&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add specificity and sensitivity to indicators&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create train loss and valid loss charts&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; test regression models (treat this as a grading problem)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; pickle weights&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;additional-models&#34;&gt;Additional Models&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; check if left/right eye classification model is required&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;additional-datasets&#34;&gt;Additional datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; make datasets more extensive (add test dataset with recoverd labels to train dataset 2015)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add APTOS dataset&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; request labelled datasets from cheers&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add datasets from cheers for testing&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add datasets from cheers for training&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;test-datasets&#34;&gt;Test datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; find datasets for testing (dataset apart from APTOS and EyePACS)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; update folder structures to match our use case&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; find dataset for testing after making sure old test datasets are not in vaid/train (4 will be empty)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;conceptsresearch-papers&#34;&gt;Concepts/Research Papers&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; read reports from kaggle competition winning authors&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Deep Learning Approach to Diabetic Retinopathy Detection &lt;a href=&#34;https://arxiv.org/pdf/2003.02261.pdf&#34;&gt;https://arxiv.org/pdf/2003.02261.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Google research &lt;a href=&#34;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45732.pdf&#34;&gt;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45732.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Nature article &lt;a href=&#34;https://www.nature.com/articles/s41746-019-0172-3&#34;&gt;https://www.nature.com/articles/s41746-019-0172-3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; read ravi&amp;rsquo;s article&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;a href=&#34;https://deim.urv.cat/~itaka/itaka2/PDF/acabats/PhD_Thesis/TESI_doctoral_Jordi_De_la_Torre.pdf&#34;&gt;https://deim.urv.cat/~itaka/itaka2/PDF/acabats/PhD_Thesis/TESI_doctoral_Jordi_De_la_Torre.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; what can go wrong &lt;a href=&#34;https://yerevann.github.io/2015/08/17/diabetic-retinopathy-detection-contest-what-we-did-wrong/&#34;&gt;https://yerevann.github.io/2015/08/17/diabetic-retinopathy-detection-contest-what-we-did-wrong/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;a href=&#34;https://arxiv.org/pdf/1902.07208.pdf&#34;&gt;https://arxiv.org/pdf/1902.07208.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; identify more papers&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Cheers Hospital Analysis</title>
      <link>https://ayushsubedi.github.io/posts/cheers_analysis/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/cheers_analysis/</guid>
      <description>&lt;h1 id=&#34;data-analysis-of-cheers-hospital&#34;&gt;Data analysis of Cheers hospital&lt;/h1&gt;
&lt;p&gt;Since it’s inception, Hospital for Children Eye ENT and Rehabilitation Service (CHEERS) has amassed data from various sources such as all the software used in daily operations, research department and the finance department. With priority in budgeting which is always a significant factor, CHEERS wanted Moonlit Solutions to analyze the data from different sectors within the hospital and create forecasting models to help with their operational efficiency.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/cheers.jpg&#34; alt=&#34;Cheers&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;scope-of-work&#34;&gt;Scope of work&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Data curation from different sectors of the hospital (but only limited to their Hospital Management Information System)&lt;/li&gt;
&lt;li&gt;Data wrangling and principal component analysis&lt;/li&gt;
&lt;li&gt;Exploratory data analysis&lt;/li&gt;
&lt;li&gt;Creation and validation of different forecasting models&lt;/li&gt;
&lt;li&gt;Assist in the creation of the forecasting section in the hospital master plan that is currently being developed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13403428/38305927-fc5e91f6-380e-11e8-9e20-7bd866a034a5.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;methodology&#34;&gt;Methodology&lt;/h2&gt;
&lt;p&gt;After the acquisition stage, all analysis presented in notebooks followed the following methodology:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gaining domain expertise&lt;/li&gt;
&lt;li&gt;Data wrangling and cleaning (a lot of imputing using the domain knowledge and meeting stakeholders)&lt;/li&gt;
&lt;li&gt;Exploratory Data Analysis&lt;/li&gt;
&lt;li&gt;Forecasting with multiple models (AR, MA, ARIMA, SARIMA, Bayesian, Deep Learning)and different KPIs (Daily sales, Daily patient visits)&lt;/li&gt;
&lt;li&gt;Optimization (Parameter tuning)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;outputs&#34;&gt;Outputs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Robust forecasting models predicting future sales&lt;/li&gt;
&lt;li&gt;A report document&lt;/li&gt;
&lt;li&gt;Dashboard showcasing interactive charts and tables (and all analysis notebooks for research reproducibility)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/754/1*io2zk4HVTX8764K_fJAerw.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The exploratory data analysis and forecasts generated went through various levels of quality assurance to validate the correctness. Unfortunately, the COVID-19 epidemic and subsequent lockdowns was going to affect the business. The model created wasBAU (Business As Usual). It did not forecast out of the world scenarios like COVID-19 (a Black Swan event). However, it will create a benchmark for measuring the effects of COVID-19 in the future. The forecasting can still be used to understand the impact of COVID-19 on the business. The exploratory data on the other hand, provides significant insights for the hospital&amp;rsquo;s strategy development.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fraud Detection</title>
      <link>https://ayushsubedi.github.io/posts/fraud_detection/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/fraud_detection/</guid>
      <description>&lt;h1 id=&#34;fraud-detection&#34;&gt;Fraud Detection&lt;/h1&gt;
&lt;h4 id=&#34;research-items&#34;&gt;Research Items&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Data Sets&lt;/li&gt;
&lt;li&gt;Relevant Papers&lt;/li&gt;
&lt;li&gt;Available Solutions&lt;/li&gt;
&lt;li&gt;Machine learning
&lt;ul&gt;
&lt;li&gt;Pre-processing&lt;/li&gt;
&lt;li&gt;Features analysis&lt;/li&gt;
&lt;li&gt;Modelling&lt;/li&gt;
&lt;li&gt;Evaluation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Suggested Solution&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1-data-sets&#34;&gt;1. Data Sets&lt;/h2&gt;
&lt;p&gt;The first step is to find fraud data sets for modeling purposes. Unfortunately, fraud data sets are really difficult to find publicly because of the confidential information that they contain. Listed below are some of the data sets found and notes on them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Real world data set from Kaggle (ULB)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/mlg-ulb/creditcardfraud&#34;&gt;https://www.kaggle.com/mlg-ulb/creditcardfraud&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The data set contains labelled credit card transactions labeled as fraudulent or genuine. Unfortunately, the column labels do not make sense because PCA has been applied for dimensional reduction. Therefore, it is very difficult to understand what each of the columns represent. Nonetheless, it is real world data.  The data sets contains transactions made by credit cards in September 2013 by European cardholders. This data set presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. &lt;strong&gt;The data set is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This data set has been analysed and models have been created below in the document, with F1 score of 94%.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Synthetic data set from Kaggle (NTNU)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/ntnu-testimon/paysim1&#34;&gt;https://www.kaggle.com/ntnu-testimon/paysim1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The data set contains synthetic (created) transaction data. The advantage of using this data set is that PCA has not been pre-performed, thus allowing extraction of all useful information. However, the data set is scaled down to 1/4th of the original data set.&lt;/p&gt;
&lt;h2 id=&#34;2-papers&#34;&gt;2. Papers:&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Link&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Title&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.aaai.org/Papers/KDD/1998/KDD98-026.pdf&#34;&gt;https://www.aaai.org/Papers/KDD/1998/KDD98-026.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Toward Scalable Learning with Non-uniform Class and Cost Distributions: A Case Study in Credit Card Fraud Detection&lt;/td&gt;
&lt;td&gt;Handling skewed datasets, compares credit card fraud detection models, and evaluate how the different sets of features have an impact on the results with the help of a real credit card fraud dataset provided by a large European card processing company (the dataset above). The results show an average increase in savings of 13% by including the proposed periodic features into the methods. &lt;br&gt;&lt;br&gt;Using 50-50 split in fraud, non-fraud leads to better models.&lt;br&gt;&lt;br&gt;von Mises distribution: &lt;a href=&#34;https://en.wikipedia.org/wiki/Von_Mises_distribution&#34;&gt;https://en.wikipedia.org/wiki/Von_Mises_distribution&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.aaai.org/Papers/Workshops/1997/WS-97-07/WS97-07-015.pdf&#34;&gt;https://www.aaai.org/Papers/Workshops/1997/WS-97-07/WS97-07-015.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Credit Card Fraud Detection using Meta-Learning: Issues and Initials Results&lt;/td&gt;
&lt;td&gt;Apart from the finding like above (using balanced training), the paper talks about using metrics other than accuracy for model evaluation.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://journal.utem.edu.my/index.php/jtec/article/view/3571/2466&#34;&gt;http://journal.utem.edu.my/index.php/jtec/article/view/3571/2466&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Credit Card Fraud Detection Using Machine Learning As Data Mining Technique&lt;/td&gt;
&lt;td&gt;95% accuracy based on Naive based derivatives.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.jair.org/index.php/jair/article/view/10302/24590&#34;&gt;https://www.jair.org/index.php/jair/article/view/10302/24590&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;SMOTE: Synthetic Minority Over-sampling Technique&lt;/td&gt;
&lt;td&gt;Using SMOTE method as described in the paper is another alternative of getting around the skewness problem.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;3-research-on-available-solutions&#34;&gt;3. Research on available solutions:&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Airbnb&lt;/strong&gt;
&lt;a href=&#34;https://medium.com/airbnb-engineering/architecting-a-machine-learning-system-for-risk-941abbba5a60&#34;&gt;https://medium.com/airbnb-engineering/architecting-a-machine-learning-system-for-risk-941abbba5a60&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ways to mitigate potential bad actors to carry out different types of attacks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Product changes: 2FA, email verification, etc etc&lt;/li&gt;
&lt;li&gt;Anomaly detection: Scripted attacks that can cause anomaly&lt;/li&gt;
&lt;li&gt;heuristics/machine learning model based on different factors&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Framework&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fast and robust&lt;/li&gt;
&lt;li&gt;Agile (catch up game)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PMML: Predictive model markup language
Openscoring: encodes several common types of machine learning models&lt;/p&gt;
&lt;p&gt;They do not provide fraud detection as a service.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Paypal&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://venturebeat.com/2018/06/21/paypal-to-acquire-machine-learning-powered-fraud-detection-startup-simility/&#34;&gt;https://venturebeat.com/2018/06/21/paypal-to-acquire-machine-learning-powered-fraud-detection-startup-simility/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Paypal recently acquired Simility for fraud detection.&lt;/p&gt;
&lt;p&gt;Simility looks at various session, device, and behavioral bio-metrics and builds a profile for what constitutes “normal” user login behavior; if an anomaly is spotted, it can act to prevent the action.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.dropbox.com/s/ft3wu5ix15xukhc/Mobile%20Fintech%20Fraud.pdf?dl=0&#34;&gt;https://www.dropbox.com/s/ft3wu5ix15xukhc/Mobile%20Fintech%20Fraud.pdf?dl=0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stripe&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://stripe.com/us/radar&#34;&gt;https://stripe.com/us/radar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Even if a card is new to your business, there’s an 89% chance it’s been seen before on the Stripe network.&lt;/p&gt;
&lt;h2 id=&#34;4-machine-learning&#34;&gt;4. Machine Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Supervised learning was applied to the PCA data set discussed in the data sets section.&lt;/li&gt;
&lt;li&gt;Different ensemble machine learning algorithms were tested, rather than using one particular algorithm for modelling.&lt;/li&gt;
&lt;li&gt;Metrics like Precision, Recall, F1 score were used to evaluate the model and get a better understanding of True Positives, True Negatives, False Positive and False Negatives.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ayushsubedi/fraud_detection/blob/master/PCA_applied_dataset.ipynb&#34;&gt;Link to complete notebook&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Random Forest&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Accuracy score for Random Forest : 0.9538461538461539
Precision score Random Forest : 0.98
Recall score Random Forest : 0.9245283018867925
F1 score Random Forest : 0.9514563106796116
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Bagging&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Accuracy score for Bagging : 0.963076923076923
Precision score Bagging : 0.9867549668874173
Recall score Bagging : 0.9371069182389937
F1 score Bagging : 0.9612903225806452
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;AdaBoost&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Accuracy score for Ada Boost Classifier : 0.9446153846153846
Precision score Ada Boost Classifier : 0.9795918367346939
Recall score Ada Boost Classifier : 0.9056603773584906
F1 score Ada Boost Classifier : 0.9411764705882353
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;False Positives vs False Negatives&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://uc29b242eafa0323079c59de5fc0.previews.dropboxusercontent.com/p/thumb/AA6wvSlgpVw_UCQieCOAFCnCY7-pM3DqJHXKMIImW212qOFwUAphNe5k_qajpfHMLtsf6neHowUSn7aMPUrwwhlgxD3j9AfUAD7CsTD961sQ6gaht6kn2wfdJYm46GwQwW8DncIuzVtzwh8q169uEdp7Fw_0uBaBxCNJbQoqt-YVRI1f8nskTqVx_NrLJyVybhA5lW_dc02Iqhrql5kbv35sLLsCN_B9D0CdV3Dxdup3twa-YtsUF1xBqwXki83c3dtM4dnkbsNMiwk3w6MHXlu6T-Yyn-VuPVFOnNRWYapLlpWZMz-Q9F7TMz1ISka9udXN4jDlJF-gucN_RAECXwB0wpP5XZI35aWjvtyhXrPhubrOFzBHDQN_51eiGbuAvpceW6J8UKBaGzOZgsLl5jwK/p.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Birds, Plane, Superman</title>
      <link>https://ayushsubedi.github.io/posts/bird_plane_superman/</link>
      <pubDate>Fri, 02 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/bird_plane_superman/</guid>
      <description>&lt;h1 id=&#34;heading&#34;&gt;&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://4.bp.blogspot.com/-VRnyjR8-1FQ/W5EZ4WhvRpI/AAAAAAAAOPM/rNiNP_X9haIqqSt4692inhEUiucUuILewCLcBGAs/s400/001109.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;br&gt;
I have been experimenting with Deep Learning models in &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt; for a couple of weeks now. PyTorch is an open source python package that provides Tensor computation (similar to numpy) with GPU support. The dataset used for this particular blog post does no justice to the real-life usage of PyTorch for image classification. However, it serves as a general idea of how Transfer Learning can be used for more complicated image classification. Transfer learning, in a nutshell, is reusing a model developed for some other classification task, for your classification purposes. The dataset was created by scraping images from google image search.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Creating the dataset&lt;/strong&gt;&lt;br&gt;
For our dataset, we need images of birds, planes, and Superman. We will be using the &lt;a href=&#34;https://github.com/hellock/icrawler&#34;&gt;icrawler&lt;/a&gt; package to download the images from google image search.&lt;/p&gt;
&lt;p&gt;We repeat the same for birds and Superman. Once all the files have been downloaded, we will restructure the folders to contain our training, testing and validating samples. I am allocating 70% for training, 20% for validating and 10% for testing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1.bp.blogspot.com/--JBHptSr22k/W5EhKG9S0FI/AAAAAAAAOPY/b1JuDndQ6qAbXSgomBTZUZkjPRNEK-N-ACLcBGAs/s320/Capture.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Loading the data&lt;/strong&gt;&lt;br&gt;
PyTorch uses generators to read the data. Since datasets are usually large, it makes sense to not load everything in memory. Let&amp;rsquo;s import useful libraries that we will be using for classification.&lt;/p&gt;
&lt;p&gt;Now that we have imported useful libraries, we need to augment and normalize the images. Torchvision transforms is used to augment the training data with random scaling, rotations, mirroring and cropping. We do not need to rotate or flip our testing and validating sets. The data for each set will also be loaded with Torchivision&amp;rsquo;s DataLoader and ImageFolder.&lt;/p&gt;
&lt;p&gt;Let us visualize a few training images to understand the data augmentation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://3.bp.blogspot.com/-LNJ3iv_C5Zs/W5Elb_VXgmI/AAAAAAAAOPk/ThiNb-NKIUU5fFx-2oEaO4FzRcMTBvuwgCLcBGAs/s640/download%2B%25281%2529.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Loading a pre-trained model&lt;/strong&gt;
We will be using Densenet for our purposes.&lt;/p&gt;
&lt;p&gt;The pre-trained model&amp;rsquo;s classifier takes 1920 features as input. We need to be consistent with that. However, the output feature for our case is 3 (bird, plane, and Superman).&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s create our classifier and replace the model&amp;rsquo;s classifier.&lt;/p&gt;
&lt;p&gt;We are using ReLU activation function with random dropouts with a probability of 20% in the hidden layers. For the output layer, we are using LogSoftmax.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Training Criterion, Optimizer, and Decay&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;Model Training and Testing&lt;/strong&gt;&lt;br&gt;
Let us calculate the accuracy of the model without training it first.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://4.bp.blogspot.com/-heWEFxARMek/W5E49Y_TsPI/AAAAAAAAOPw/ha1h5ZBOs8ApPT0t5RcDsRWfvOXHwcnWACLcBGAs/s640/Capture.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The accuracy is pretty low at this time, which is expected. The &lt;em&gt;cuda&lt;/em&gt; parameter here is the boolean object passed for the availability of GPU hardware in the machine.&lt;/p&gt;
&lt;p&gt;Let us train the model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://4.bp.blogspot.com/-InDGqH9w1co/W5E520-6ANI/AAAAAAAAOP4/QJ_tiogtGmcdNkHB33ieKWk-UJ2AHfQqwCLcBGAs/s640/Capture.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since GPU is supported, the training took around 10 mins. The validation accuracy is almost 99%. Let us check the accuracy over training data again.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://3.bp.blogspot.com/-TwYWdmXYGyI/W5E7v95GTiI/AAAAAAAAOQE/FCIpQ7MPr4wXEFY_YqxsCl46ZdZlebg1ACLcBGAs/s640/Capture.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Image Preprocessing&lt;/strong&gt;
We declare a few functions to preprocess images and pass on the trained model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://3.bp.blogspot.com/-q3hQVfCnDmE/W5E9S5vSizI/AAAAAAAAOQQ/r1hP9xV1Tw4eoh6E5MzGbFGh7haNkn3BQCLcBGAs/s640/Untitled-1.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Predicting by passing an image&lt;/strong&gt;&lt;br&gt;
Since our model is ready and we have built functions that allows us to visualize, let us try it out on one of the sample images.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://3.bp.blogspot.com/-mv2wuvBJIcA/W5E_R-qFjZI/AAAAAAAAOQk/HDEZYXgYYM8kYGVvg5w5Y3KNHkUZs8KJQCLcBGAs/s640/Capture.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;So, that is it.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>