<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>sagemaker on Ayush Subedi</title>
    <link>https://ayushsubedi.github.io/tags/sagemaker/</link>
    <description>Recent content in sagemaker on Ayush Subedi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 01 Jan 2023 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ayushsubedi.github.io/tags/sagemaker/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>AWS Certified ML - Specialty exam (MLS-C01) - 4. Machine Learning Implementation and Operations</title>
      <link>https://ayushsubedi.github.io/posts/aws_ml_speciality_ml/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/aws_ml_speciality_ml/</guid>
      <description>&lt;h1 id=&#34;machine-learning-implementation-and-operations&#34;&gt;Machine Learning Implementation and Operations&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#section-intro-machine-learning-implementation-and-operations&#34;&gt;Section Intro: Machine Learning Implementation and Operations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemakers-inner-details-and-production-variants&#34;&gt;SageMaker&amp;rsquo;s Inner Details and Production Variants&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-on-the-edge-sagemaker-neo-and-iot-greengrass&#34;&gt;SageMaker On the Edge: SageMaker Neo and IoT Greengrass&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-security-encryption-at-rest-and-in-transit&#34;&gt;SageMaker Security: Encryption at Rest and In Transit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-security-vpcs-iam-logging-and-monitoring&#34;&gt;SageMaker Security: VPC&amp;rsquo;s, IAM, Logging, and Monitoring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-resource-management-instance-types-and-spot-training&#34;&gt;SageMaker Resource Management: Instance Types and Spot Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-resource-management-elastic-inference-automatic-scaling-azs&#34;&gt;SageMaker Resource Management: Elastic Inference, Automatic Scaling, AZ&amp;rsquo;s&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-serverless-inference-and-inference-recommender&#34;&gt;SageMaker Serverless Inference and Inference Recommender&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-inference-pipelines&#34;&gt;SageMaker Inference Pipelines&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This section covers building machine learning solutions for performance, availability, scalability, resiliency, and fault tolerance, recommending and implementing the appropriate machine learning services and features for a given problem, applying basic AWS security practices to machine learning solutions and deploying and operationalizing machine learning solutions.&lt;/p&gt;
&lt;h2 id=&#34;section-intro-machine-learning-implementation-and-operations&#34;&gt;Section Intro: Machine Learning Implementation and Operations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;scaling, productionalization and security&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemakers-inner-details-and-production-variants&#34;&gt;SageMaker&amp;rsquo;s Inner Details and Production Variants&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;all models in agemaker are hosted in docker containers&lt;/li&gt;
&lt;li&gt;the docker container is registered with ECR&lt;/li&gt;
&lt;li&gt;pre built deep learning, scikit learn and spark ml&lt;/li&gt;
&lt;li&gt;pre built tensorflow, mxnet, chainer, pytorch etc. horovod or parameter server is a way to distribute tensorflow training&lt;/li&gt;
&lt;li&gt;you can also use any script or algorithm within sagemaker&lt;/li&gt;
&lt;li&gt;the containers are isolated and contain all dependencies&lt;/li&gt;
&lt;li&gt;Dockerfile structure&lt;/li&gt;
&lt;li&gt;using your own image&lt;/li&gt;
&lt;li&gt;you can test muliple models on live traffic using Production Variants (Roll out variant weights)&lt;/li&gt;
&lt;li&gt;A/B test is posible&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-on-the-edge-sagemaker-neo-and-iot-greengrass&#34;&gt;SageMaker On the Edge: SageMaker Neo and IoT Greengrass&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Train once, run anywhere&lt;/li&gt;
&lt;li&gt;supports multiple architecture&lt;/li&gt;
&lt;li&gt;optimizes code for specific devices&lt;/li&gt;
&lt;li&gt;consists of a compiler and a runtime&lt;/li&gt;
&lt;li&gt;Neo compiled models can be deployed to an https endpoint, must be the same instance type used for compilation&lt;/li&gt;
&lt;li&gt;or you can deploy to iot greengrass&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-security-encryption-at-rest-and-in-transit&#34;&gt;SageMaker Security: Encryption at Rest and In Transit&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;IAM&lt;/li&gt;
&lt;li&gt;MFA&lt;/li&gt;
&lt;li&gt;SSL/TLS&lt;/li&gt;
&lt;li&gt;Cloudtrail to log API and user activity&lt;/li&gt;
&lt;li&gt;encryption&lt;/li&gt;
&lt;li&gt;AWS key mangement service is accepted by notebooks and all sagemaker jobs&lt;/li&gt;
&lt;li&gt;s3 can be encrypted as well&lt;/li&gt;
&lt;li&gt;All traffic supports TLS/SSL&lt;/li&gt;
&lt;li&gt;inter node training communication may be optionally encrypted&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-security-vpcs-iam-logging-and-monitoring&#34;&gt;SageMaker Security: VPC&amp;rsquo;s, IAM, Logging, and Monitoring&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;training jobs run in VPC&lt;/li&gt;
&lt;li&gt;private VPC&lt;/li&gt;
&lt;li&gt;s3 vpc endpoints&lt;/li&gt;
&lt;li&gt;IAM user permissions for CreateTrainingJob, CreateModel, CreateEndpointConfig, CreateTransformJob, CreateHyperParameterTuningJob, CreateNotebookInstance, UpdateNotebookInstance&lt;/li&gt;
&lt;li&gt;Predefined policies for AmazonSageMakerReadOnly, AmazonSageMakerFullAccess, AdministratorAccess, DataScientist&lt;/li&gt;
&lt;li&gt;cloudwatch can log, monitor and alarm on invocations and latency of endpoints, health of instance nodes, ground truth (active workers)&lt;/li&gt;
&lt;li&gt;cloudtrail records actions from users, roles, and services within Sagemaker: log files are delivered to s3 for auditing purposes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-resource-management-instance-types-and-spot-training&#34;&gt;SageMaker Resource Management: Instance Types and Spot Training&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;depends but usually gpu for training and cpu for inference&lt;/li&gt;
&lt;li&gt;EC2 spot training: checkpoints to s3 so training can resume&lt;/li&gt;
&lt;li&gt;can increate training time&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-resource-management-elastic-inference-automatic-scaling-azs&#34;&gt;SageMaker Resource Management: Elastic Inference, Automatic Scaling, AZ&amp;rsquo;s&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;accelerates deep learning inference at a fraction of a cost&lt;/li&gt;
&lt;li&gt;EI accelerator may be added alongside a CPU instance&lt;/li&gt;
&lt;li&gt;EI accelerators can also be applied to notebooks&lt;/li&gt;
&lt;li&gt;works with tensorflow, pytorch, mxnet, onnx may be used to export models to mxnet&lt;/li&gt;
&lt;li&gt;works with custom containers built with El-enabled Tensorflow, Pytorch or mxnet&lt;/li&gt;
&lt;li&gt;works with image classification and object detection built in algorithms&lt;/li&gt;
&lt;li&gt;Automatic scaling: can be used to define target metrics, min or max capacity, cooldown periods, works with cloudwatch, dynamically adjusts number of instances for a production variant, load test your configuration before using it.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-serverless-inference-and-inference-recommender&#34;&gt;SageMaker Serverless Inference and Inference Recommender&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;specify your container, memory requirement, concurrency requirements&lt;/li&gt;
&lt;li&gt;underlying capacity is automatically provisioned and scaled&lt;/li&gt;
&lt;li&gt;good for infrequent or unpredictable traffic, will scale down to zero when there are no requests&lt;/li&gt;
&lt;li&gt;chared based on usage&lt;/li&gt;
&lt;li&gt;monitor via cloudwatch&lt;/li&gt;
&lt;li&gt;Inference Recommender  recommends what instance type and configuration for your model&lt;/li&gt;
&lt;li&gt;automates load testing and model tuning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-inference-pipelines&#34;&gt;SageMaker Inference Pipelines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;linear sequence of 2-15 containers&lt;/li&gt;
&lt;li&gt;any combination of pre-trained built-in algorithms or your own algorithms in Docker&lt;/li&gt;
&lt;li&gt;combine pre-processing, predictions, post-processing&lt;/li&gt;
&lt;li&gt;Spark ML and scikit-learn&lt;/li&gt;
&lt;li&gt;chaining multiple inference containers into a pipeline of results&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>