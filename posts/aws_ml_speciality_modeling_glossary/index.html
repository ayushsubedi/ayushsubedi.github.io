<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
	
	<title>Ayush Subedi  | AWS Certified ML - Specialty exam (MLS-C01) - 3a. Modeling Glossary</title>
	<meta name="viewport" content="width=device-width,minimum-scale=1">
	<meta name="generator" content="Hugo 0.102.3" />
	
	
	<META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
	

		
	<title>Ayush Subedi</title>
	<meta name="title" content="Ayush Subedi">
	<meta name="description" content="… personal journey with mathematics, software engineering and data science">

	
	<meta property="og:type" content="website">
	<meta property="og:url" content="https://subedi.ml/">
	<meta property="og:title" content="Ayush Subedi">
	<meta property="og:description" content="… personal journey with mathematics, software engineering and data science">
	<meta property="og:image" content="https://subedi.ml/img/k.png">

	
	<meta property="twitter:card" content="summary_large_image">
	<meta property="twitter:url" content="https://subedi.ml/">
	<meta property="twitter:title" content="Ayush Subedi">
	<meta property="twitter:description" content="… personal journey with mathematics, software engineering and data science">
	<meta property="twitter:image" content="https://subedi.ml/img/k.png">

	
	
	<link href="/dist/app.css" rel="stylesheet">
	

	

	
	
<link rel="shortcut icon" href="/img/favicon.ico" type="image/png" />

	

	
	
	
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-177424799-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	
	
	



<link rel="stylesheet" href='https://ayushsubedi.github.io/lib/katex.min.css' integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">


<script defer src='https://ayushsubedi.github.io/lib/katex.min.js' integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>


<script defer src='https://ayushsubedi.github.io/lib/contrib/auto-render.min.js' integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
crossorigin="anonymous"
onload='renderMathInElement(document.body);'></script>


<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

	
	
</head>

<body class="bg-gray-100 text-gray-700 font-sans">
	<div class="p-6 sm:p-10 md:p-16 flex flex-wrap">
		<header class="w-full md:w-2/5 xl:w-1/2 md:pr-12 lg:pr-20 xl:pr-24 order-1 md:order-1 max-w-2xl">
			<div
				class="z-50 bg-gray-100 bg-opacity-75 bg-opacity-custom lg:min-w-0.7 max-w-xl md:float-right md:text-right leading-loose tracking-tight md:sticky md:top-0 pt-2">
				
<div>
	<h2>
		<a href="https://ayushsubedi.github.io" title="Ayush Subedi" class="heading font-cursive icon">Ayush Subedi</a>
	</h2>
</div>
<h1 class="pt-2">AWS Certified ML - Specialty exam (MLS-C01) - 3a. Modeling Glossary</h1>

<h3 class="text-java-700 font-normal leading-relaxed pt-2">My notes on the pre-requisite to the &#34;Modeling&#34; domain.</h3>

<div class="flex flex-wrap justify-end pt-2 "><div class="md:flex-grow-0 font-light">
	
	
	
	
	<a class="post-taxonomy-category text-medium-red-violet-600 hover:text-medium-red-violet-400"
		href='/categories/analytics'>analytics</a>&nbsp;&#47;
	
	<a class="post-taxonomy-category text-medium-red-violet-600 hover:text-medium-red-violet-400"
		href='/categories/machine-learning'>machine-learning</a>&nbsp;&#47;
	
	<a class="post-taxonomy-category text-medium-red-violet-600 hover:text-medium-red-violet-400"
		href='/categories/certification'>certification</a>&nbsp;&#47;
	
	<a class="post-taxonomy-category text-medium-red-violet-600 hover:text-medium-red-violet-400"
		href='/categories/aws'>AWS</a>&nbsp;&#47;
	
	<a class="post-taxonomy-category text-medium-red-violet-600 hover:text-medium-red-violet-400"
		href='/categories/aws-certified-ml-specialty-exam-mls-c01'>AWS Certified ML - Specialty exam (MLS-C01)</a>&nbsp;&#47;
	
	<a class="post-taxonomy-category text-medium-red-violet-600 hover:text-medium-red-violet-400"
		href='/categories/modeling'>Modeling</a>
	
	
	

	
	&nbsp;&nbsp;
	

	
	
	
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/certification'>certification</a>
	
	
	
</div><time class="text-eucalyptus-500 md:text-right md:flex-grow font-light pl-4"
		datetime="2023-01-01">2023-01-01</time>
</div>

<hr />

			</div>
		</header>
		<main role="main" class="w-full md:w-3/5 xl:w-1/2 max-w-3xl order-2 md:order-2 min-h-70vh pt-2 pb-4">
			

<article>
	<section class="mx-auto content">
		<div class="c-rich-text"><h1 id="modeling-glossary">Modeling Glossary</h1>
<ol>
<li><a href="#basic-machine-learning">Basic Machine Learning</a></li>
<li><a href="#confusion-matrix">Confusion Matrix</a></li>
<li><a href="#data">Data</a></li>
<li><a href="#design-of-experiments">Design of Experiments</a></li>
<li><a href="#game-theory">Game Theory</a></li>
<li><a href="#model-quality">Model Quality</a></li>
<li><a href="#non-parametric-tests">Non-Parametric Tests</a></li>
<li><a href="#optimization">Optimization</a></li>
<li><a href="#probability-based-models">Probability based models</a></li>
<li><a href="#regression">Regression</a></li>
<li><a href="#variable-selection">Variable Selection</a></li>
<li><a href="#misc">Misc</a></li>
</ol>
<h2 id="basic-machine-learning">Basic Machine Learning</h2>
<h3 id="algorithm">Algorithm</h3>
<ul>
<li>In the context of machine learning, an algorithm is a set of instructions that a computer follows in order to learn from data.</li>
<li>Machine learning algorithms take input data and use statistical analysis to predict an output value within an acceptable range.</li>
<li>The goal of a machine learning algorithm is to improve its prediction accuracy over time by adjusting the parameters of the model based on the input data.</li>
</ul>
<h3 id="change-detection">Change detection</h3>
<ul>
<li>Change detection is a process in which a system is able to identify changes in a given environment over time.</li>
<li>In the context of machine learning, change detection involves using algorithms to analyze data from a given environment in order to identify any changes that have occurred.</li>
<li>This can be useful in a variety of different applications, including monitoring changes in financial markets, detecting changes in customer behavior, or identifying changes in the physical environment.</li>
</ul>
<h3 id="classification">Classification</h3>
<ul>
<li>Classification is a supervised learning problem in which the model is trained to predict a discrete label or class for a given input data.</li>
<li>The goal is to predict the class or category that a new instance belongs to, based on the training data.</li>
<li>For example, a classifier could be trained to predict whether an email is spam or not spam, based on the contents of the email. The input data would be the contents of the email, and the output class would be either &ldquo;spam&rdquo; or &ldquo;not spam&rdquo;.</li>
<li>There are many different algorithms that can be used for classification, including logistic regression, support vector machines (SVMs), and decision trees. The choice of algorithm depends on the characteristics of the data and the desired complexity of the model.</li>
</ul>
<h3 id="classifier">Classifier</h3>
<ul>
<li>A classifier is a machine learning model that is trained to predict a discrete class or category for a given input data. Classifiers are used in a variety of applications, including spam filtering, image classification, and natural language processing.</li>
<li>There are many different types of classifiers, including logistic regression, support vector machines (SVMs), and decision trees. The choice of classifier depends on the characteristics of the data and the desired complexity of the model.</li>
<li>To train a classifier, the model is presented with a labeled dataset that includes input data and the corresponding correct class or category. The model then &ldquo;learns&rdquo; to predict the correct class by finding patterns in the training data. Once trained, the classifier can then be used to predict the class for new, unseen data.</li>
</ul>
<h3 id="cluster">Cluster</h3>
<ul>
<li>In the context of machine learning, a cluster refers to a group of data points that are similar to one another. Clustering is an unsupervised learning problem in which the goal is to divide the data into distinct groups, or clusters, such that the data points within each cluster are more similar to one another than they are to data points in other clusters.</li>
<li>There are many different algorithms that can be used for clustering, including k-means clustering and hierarchical clustering. The choice of algorithm depends on the characteristics of the data and the desired properties of the clusters.</li>
<li>Clustering can be used for a variety of purposes, including data compression, anomaly detection, and generating hypotheses for further testing. It is a useful tool for exploring and understanding the structure of a dataset.</li>
</ul>
<h3 id="cluster-center">Cluster center</h3>
<ul>
<li>In the context of clustering, a cluster center is a representative data point for a cluster. It is typically the mean or median of the points in the cluster, depending on the specific clustering algorithm being used.</li>
<li>In k-means clustering, for example, the cluster center is the mean of all the data points in the cluster. The k-means algorithm works by iteratively assigning each data point to the cluster with the closest cluster center and then updating the cluster center to be the mean of the points in the cluster.</li>
<li>In hierarchical clustering, the cluster center can be thought of as the point at the center of the cluster, which is determined by the specific linkage criterion being used.</li>
<li>The cluster center is used to represent the &ldquo;typical&rdquo; data point in a cluster, and can be useful for understanding the characteristics of the cluster and for visualization purposes.</li>
</ul>
<h3 id="clustering">Clustering</h3>
<ul>
<li>Clustering is an unsupervised learning problem in which the goal is to divide a dataset into distinct groups, or clusters, such that the data points within each cluster are more similar to one another than they are to data points in other clusters. Clustering is a useful tool for exploring and understanding the structure of a dataset, and can be used for a variety of purposes, including data compression, anomaly detection, and generating hypotheses for further testing.</li>
<li>There are many different algorithms that can be used for clustering, including k-means clustering, hierarchical clustering, and density-based clustering. The choice of algorithm depends on the characteristics of the data and the desired properties of the clusters.</li>
<li>In k-means clustering, for example, the goal is to partition the data into a specified number (k) of clusters by iteratively assigning each data point to the cluster with the closest cluster center and then updating the cluster center to be the mean of the points in the cluster. Hierarchical clustering, on the other hand, involves creating a hierarchy of clusters, where at each step, the two closest clusters are merged together. Density-based clustering algorithms, such as DBSCAN, identify clusters as areas of higher density surrounded by areas of lower density.</li>
</ul>
<h3 id="cusum">CUSUM</h3>
<ul>
<li>CUSUM is an acronym for &ldquo;Cumulative Sum.&rdquo; It is a statistical algorithm that is used to detect small shifts in the mean of a process over time. It is often used in quality control and reliability engineering to monitor processes and detect changes that may indicate a problem or deviation from the norm.</li>
<li>The CUSUM algorithm works by keeping track of a running total of the difference between the observed values and the expected or target value. When the running total exceeds a pre-determined threshold, it indicates that the process has shifted and may need to be corrected or investigated.</li>
<li>CUSUM charts are often used to visualize the performance of the CUSUM algorithm, with the running total being plotted on the y-axis and the time steps on the x-axis. The chart can then be used to identify when the running total exceeds the threshold and to identify any trends or patterns in the data.</li>
</ul>
<h3 id="deep-learning">Deep learning</h3>
<ul>
<li>Deep learning is a subfield of machine learning that is inspired by the structure and function of the brain, specifically the neural networks that make up the brain. It involves the use of artificial neural networks, which are computational models inspired by the structure and function of the brain, to learn from data and make decisions.</li>
<li>Deep learning algorithms learn by example, just like humans do. They learn by being presented with a large amount of labeled data and adjusting the internal parameters of the network to optimize performance on a specific task. The &ldquo;deep&rdquo; in deep learning refers to the fact that these algorithms typically have multiple layers of artificial neurons, with each layer learning to extract higher-level features of the data.</li>
<li>Deep learning has been successful in a wide range of applications, including image and speech recognition, natural language processing, and autonomous driving. It has revolutionized the field of machine learning and has enabled the development of many practical applications that were previously thought to be impossible.</li>
</ul>
<h3 id="dimension">Dimension</h3>
<ul>
<li>In the context of machine learning, a dimension refers to a particular feature or attribute of a dataset. For example, if you are working with a dataset that includes information about houses (such as price, number of bedrooms, square footage, and location), each of these features would be considered a separate dimension.</li>
<li>The number of dimensions in a dataset is often referred to as the &ldquo;dimensionality&rdquo; of the dataset. High-dimensional datasets, which have a large number of dimensions, can be difficult to work with and visualize, as it can be challenging to represent the relationships between all of the dimensions in a meaningful way.</li>
<li>In machine learning, techniques such as dimensionality reduction can be used to reduce the number of dimensions in a dataset, while still preserving the important information. This can be useful for tasks such as visualization and training machine learning models, which may be more efficient and effective on lower-dimensional data.</li>
</ul>
<h3 id="expectation-maximization-algorithm-em-algorithm">Expectation-maximization algorithm (EM algorithm)</h3>
<ul>
<li>The EM algorithm (Expectation-Maximization algorithm) is a widely used method for estimating the parameters of a statistical model when there is missing or incomplete data. It is an iterative algorithm that alternates between two steps: the expectation (E) step and the maximization (M) step.</li>
<li>In the E step, the algorithm estimates the expected value of the complete data likelihood function (a measure of the probability of the data given the model parameters) based on the current parameter values. In the M step, the algorithm updates the parameter values to maximize the expected complete data likelihood. The process is then repeated until convergence, at which point the parameter estimates are considered to be optimal.</li>
<li>The EM algorithm is widely used in a variety of applications, including machine learning, natural language processing, and bioinformatics. It is particularly useful when the data are incomplete or when the model is a mixture model (i.e., a model that consists of a mixture of different underlying distributions).</li>
</ul>
<h3 id="heuristic">Heuristic</h3>
<ul>
<li>In machine learning, a heuristic is a simplified, approximate solution to a problem that is used to quickly find a satisfactory answer. It is often used in situations where finding the optimal solution is computationally infeasible or impractical.</li>
<li>Heuristics are often used in machine learning as a way to quickly search through a large space of possible solutions and find a good, but not necessarily optimal, solution. They can be useful for tasks such as optimization, feature selection, and model selection.</li>
<li>Heuristics are often designed to be domain-specific and are based on the specific characteristics of the problem at hand. They can be useful for providing a rough estimate or approximation of the solution, but they may not always be reliable or accurate. In general, heuristics should be used with caution and should be validated against more rigorous methods where possible.</li>
</ul>
<h3 id="𝑘-means-algorithm">𝑘-means algorithm</h3>
<ul>
<li>The k-means algorithm is a method for clustering data into a specified number (k) of distinct clusters. It is an iterative algorithm that works by first randomly initializing k cluster centers, and then iteratively assigning each data point to the cluster with the closest cluster center and updating the cluster center to be the mean of the points in the cluster.</li>
<li>The k-means algorithm has the following steps:
<ul>
<li>Initialize k cluster centers randomly.</li>
<li>Assign each data point to the cluster with the closest cluster center.</li>
<li>Update the cluster centers to be the mean of the points in the cluster.</li>
<li>Repeat steps 2 and 3 until the cluster assignments stop changing or a maximum number of iterations is reached.</li>
</ul>
</li>
<li>The k-means algorithm is sensitive to the initial cluster assignments, so it is common to run the algorithm multiple times with different random initializations to ensure that the final clusters are stable. The algorithm is also sensitive to outliers and may produce suboptimal clusters if the data contain outliers.</li>
</ul>
<h3 id="𝑘-nearest-neighbor-knn">𝑘-Nearest-Neighbor (KNN)</h3>
<ul>
<li>The k-nearest neighbor (KNN) algorithm is a method for classifying objects based on the closest training examples in the feature space. It is a non-parametric method, which means that it does not make any assumptions about the underlying distribution of the data.</li>
<li>The KNN algorithm works by calculating the distance between the new data point and all the training data, and then selecting the k training points that are closest to the new data point. The class label of the new data point is then determined by majority vote among the k nearest neighbors.</li>
<li>The value of k is a hyperparameter of the KNN algorithm and must be chosen by the practitioner. A larger value of k will make the model more robust to noise, but a smaller value may be more sensitive to the underlying structure of the data.</li>
<li>KNN is a simple and effective method for classification, but it can be computationally expensive for large datasets, as it requires calculating the distance between the new data point and all the training examples.</li>
</ul>
<h3 id="kernel">Kernel</h3>
<ul>
<li>In the context of machine learning, a kernel is a function that takes in two inputs and returns a scalar value. Kernels are used in a variety of machine learning algorithms, including support vector machines (SVMs) and kernel principal component analysis (PCA).</li>
<li>In SVMs, kernels are used to define a similarity measure between two data points. The kernel function is applied to the data points to transform them into a higher-dimensional space, where it is then possible to find a linear separation between the classes. By using a kernel function, it is possible to learn a non-linear decision boundary in the original feature space using a linear classifier in the transformed space.</li>
<li>In kernel PCA, kernels are used to define a similarity measure between data points in the original space, and the resulting kernel matrix is used to perform PCA in the feature space. This allows for non-linear dimensionality reduction, which can be useful for data that is not linearly separable.</li>
<li>There are many different kernel functions that can be used, including linear kernels, polynomial kernels, and radial basis function (RBF) kernels. The choice of kernel depends on the characteristics of the data and the desired properties of the model.</li>
</ul>
<h3 id="margin">Margin</h3>
<ul>
<li>In the context of machine learning, the margin is the distance between the decision boundary (i.e., the line or hyperplane that separates the classes) and the nearest training data points. The margin is an important concept in certain types of algorithms, such as support vector machines (SVMs), where the goal is to find the decision boundary that has the largest margin.</li>
<li>In SVMs, the margin is the distance between the decision boundary and the closest data points from each class. The margin is maximized when the decision boundary is as far as possible from the closest data points from each class, which leads to a model that is more robust and generalizable to new data.</li>
<li>The margin can also be thought of as a measure of the confidence of the classifier. A larger margin indicates that the classifier is more confident in its predictions, as it is based on a wider separation between the classes.</li>
<li>The margin is an important consideration when training a machine learning model, as a model with a large margin is often preferred to a model with a small margin, as it is likely to be more robust and generalizable to new data.</li>
</ul>
<h3 id="machine-learning">Machine learning</h3>
<ul>
<li>Machine learning is a field of artificial intelligence that involves the use of computational models to learn from data and make predictions or decisions without being explicitly programmed. It involves the development of algorithms that can automatically improve their performance through experience.</li>
<li>There are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning.</li>
<li>In supervised learning, the goal is to learn a function that maps input data to output labels, based on a labeled training dataset. The model is trained on the training data and then evaluated on a separate test dataset to evaluate its performance. Examples of supervised learning tasks include classification and regression.</li>
<li>In unsupervised learning, the goal is to discover patterns or relationships in the data without any prior knowledge or labeled training data. Examples of unsupervised learning tasks include clustering and dimensionality reduction.</li>
<li>In reinforcement learning, the goal is to learn a policy that maximizes a reward signal. The model is trained by interacting with its environment and receiving feedback in the form of rewards or punishments. Reinforcement learning is used in a variety of applications, including robotics and control systems.</li>
<li>Machine learning has been successful in a wide range of applications, including image and speech recognition, natural language processing, and autonomous driving. It has revolutionized many fields and has enabled the development of practical applications that were previously thought to be impossible.</li>
</ul>
<h3 id="neural-network">Neural network</h3>
<ul>
<li>A neural network is a type of machine learning model inspired by the structure and function of the brain. It is composed of layers of interconnected &ldquo;neurons,&rdquo; which process and transmit information. Neural networks are able to learn and adapt to new data by adjusting the strengths of the connections between neurons.</li>
<li>The basic building block of a neural network is the neuron, which is a simple computational unit that receives input, processes it, and produces an output. The input is passed through multiple layers of neurons, with each layer learning to extract higher-level features of the data. The output of the final layer is the prediction or decision made by the neural network.</li>
<li>There are many different types of neural networks, including feedforward neural networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs). The choice of neural network architecture depends on the characteristics of the data and the desired properties of the model.</li>
<li>Neural networks have been successful in a wide range of applications, including image and speech recognition, natural language processing, and autonomous driving. They have revolutionized the field of machine learning and have enabled the development of many practical applications that were previously thought to be impossible.</li>
</ul>
<h3 id="supervised-learning">Supervised learning</h3>
<ul>
<li>Supervised learning is a type of machine learning in which the model is trained on a labeled dataset, where the correct output is provided for each example in the training set. The goal of supervised learning is to learn a function that can map input data to the correct output labels.</li>
<li>Supervised learning algorithms can be divided into two main categories: regression and classification.</li>
<li>In regression, the goal is to predict a continuous value, such as the price of a house or the likelihood of a customer churning. Examples of regression algorithms include linear regression and support vector regression.</li>
<li>In classification, the goal is to predict a discrete label or class, such as whether an email is spam or not spam. Examples of classification algorithms include logistic regression, k-nearest neighbors, and decision trees.</li>
<li>Supervised learning is the most widely used type of machine learning and has been successful in a wide range of applications, including image and speech recognition, natural language processing, and fraud detection. It requires a labeled dataset to train the model, which can be expensive and time-consuming to obtain.</li>
</ul>
<h3 id="support-vector-machine-svm">Support vector machine (SVM)</h3>
<ul>
<li>Support vector machine (SVM) is a type of supervised learning algorithm that can be used for classification or regression. It is based on the idea of finding a hyperplane in a high-dimensional space that maximally separates the classes.</li>
<li>In the case of classification, the goal is to find a hyperplane that separates the data points into different classes as well as possible. The SVM algorithm finds the hyperplane that has the largest margin, or distance, between the closest data points of each class. This maximizes the separation between the classes and leads to a more robust and generalizable model.</li>
<li>In the case of regression, the goal is to find a hyperplane that predicts the output value for a given input value. The SVM algorithm finds the hyperplane that minimizes the error between the predicted and actual values.</li>
<li>SVMs are effective in high-dimensional spaces and are widely used in a variety of applications, including image and speech recognition, natural language processing, and bioinformatics. They are also robust to noise and can handle datasets with a large number of features. However, they can be computationally expensive to train and are not well-suited for very large datasets.</li>
</ul>
<h3 id="unsupervised-learning">Unsupervised learning</h3>
<ul>
<li>Unsupervised learning is a type of machine learning in which the model is not given any labeled training data and must find patterns or relationships in the data on its own. The goal of unsupervised learning is to discover the underlying structure of the data, without any prior knowledge or assumptions.</li>
<li>Unsupervised learning algorithms can be divided into two main categories: clustering and dimensionality reduction.</li>
<li>In clustering, the goal is to group the data points into distinct clusters such that the points within each cluster are more similar to one another than they are to points in other clusters. Examples of clustering algorithms include k-means clustering and hierarchical clustering.</li>
<li>In dimensionality reduction, the goal is to reduce the number of dimensions (features) in the data while preserving as much of the information as possible. This can be useful for tasks such as visualization and feature selection. Examples of dimensionality reduction algorithms include principal component analysis (PCA) and t-SNE (t-distributed stochastic neighbor embedding).</li>
<li>Unsupervised learning is useful for exploring and understanding the structure of a dataset, and can be used for tasks such as anomaly detection and data compression. It does not require labeled data and can be used with data that has not been labeled or has incomplete labels. However, it can be more difficult to evaluate the performance of unsupervised learning algorithms, as there is no ground truth to compare the results to.</li>
</ul>
<h3 id="voronoi-diagram">Voronoi diagram</h3>
<ul>
<li>A Voronoi diagram is a graphical representation of the partitioning of a plane into regions based on the distance to a set of points. It is named after Russian mathematician Georgy Voronoi, who developed the concept in 1908.</li>
<li>In a Voronoi diagram, the plane is divided into a set of cells, with each cell corresponding to one of the input points. The points are called the &ldquo;generators&rdquo; of the Voronoi diagram. Each cell consists of all points that are closer to its generator than to any other generator. The boundary between cells is called a Voronoi edge, and the points where Voronoi edges intersect are called Voronoi vertices.</li>
<li>Voronoi diagrams have a wide range of applications, including computer graphics, image processing, and spatial analysis. They are used to model the spatial distribution of points and can be used to optimize the placement of facilities, such as warehouses or cell phone towers, to minimize the distance to the nearest facility. They are also used in computer games to determine the visibility of objects on the screen and in the design of efficient algorithms for solving problems in computational geometry.</li>
</ul>
<h2 id="confusion-matrix">Confusion Matrix</h2>
<h3 id="accuracy">Accuracy</h3>
<ul>
<li>Accuracy is a measure of how well a model correctly predicts the outcome of a given data sample. It is commonly used in classification problems, where the model is trying to predict a label for a given input.</li>
<li>The accuracy score is calculated by dividing the number of correct predictions made by the model by the total number of predictions made. This value is then expressed as a percentage. For example, if a model made 100 predictions and 75 of them were correct, the accuracy score would be 75%.</li>
<li>To calculate the accuracy score, you need a set of predictions made by the model and the corresponding true labels for those predictions. You can then compare the predictions to the true labels to see how many were correct.</li>
<li>Here is an example of how to calculate the accuracy score in Python:</li>
</ul>
<pre tabindex="0"><code>def accuracy_score(y_true, y_pred):
    # Calculate the number of correct predictions
    correct = sum(y_true == y_pred)
    # Calculate the total number of predictions
    total = len(y_true)
    # Calculate the accuracy score as a percentage
    return correct / total * 100
</code></pre><ul>
<li>Here, y_true is a list of the true labels and y_pred is a list of the predictions made by the model. The function first calculates the number of correct predictions and then divides that by the total number of predictions to get the accuracy as a decimal. It then multiplies that value by 100 to express the accuracy as a percentage.</li>
</ul>
<h3 id="confusion-matrix-1">Confusion matrix</h3>
<ul>
<li>
<p>A confusion matrix is a table that is used to evaluate the performance of a classification algorithm. It helps to visualize the correct and incorrect predictions made by the model and allows you to see which classes are being predicted accurately and which are not.</p>
</li>
<li>
<p>The rows of the matrix represent the actual classes of the samples and the columns represent the predicted classes. The diagonal elements of the matrix represent the number of samples that have been correctly classified, while the off-diagonal elements represent the number of misclassified samples.</p>
</li>
<li>
<p>Here is an example of a confusion matrix:</p>
</li>
</ul>
<pre tabindex="0"><code>              Predicted Positive    Predicted Negative
Actual Positive          TP                  FP
Actual Negative          FN                  TN
</code></pre><ul>
<li>In this example, TP (true positive) is the number of samples that are actually positive and have been correctly predicted as positive. TN (true negative) is the number of samples that are actually negative and have been correctly predicted as negative. FP (false positive) is the number of samples that are actually negative but have been predicted as positive. FN (false negative) is the number of samples that are actually positive but have been predicted as negative.</li>
<li>To calculate the values for the confusion matrix, you need a set of predictions made by the model and the corresponding true labels for those predictions. You can then compare the predictions to the true labels to see how many were correct and how many were incorrect.</li>
<li>Here is an example of how to calculate a confusion matrix in Python:</li>
</ul>
<pre tabindex="0"><code>from sklearn.metrics import confusion_matrix

y_true = [1, 0, 1, 1, 0, 1]
y_pred = [1, 1, 1, 1, 0, 0]

confusion_matrix(y_true, y_pred)
</code></pre><ul>
<li>This will output the following confusion matrix:</li>
</ul>
<pre tabindex="0"><code>array([[2, 1],
       [1, 2]])
</code></pre><h3 id="diagnostic-odds-ratio">Diagnostic odds ratio</h3>
<ul>
<li>The diagnostic odds ratio (DOR) is a measure of the accuracy of a diagnostic test. It is used to compare the accuracy of two or more diagnostic tests or to compare the accuracy of a diagnostic test to a reference standard.</li>
<li>The DOR is calculated as the ratio of the odds of a positive test result in patients with the condition being tested for to the odds of a positive test result in patients without the condition.</li>
<li>Here is the formula for calculating the DOR:</li>
</ul>
<pre tabindex="0"><code>DOR = (TP / FP) / (FN / TN)
</code></pre><ul>
<li>Where TP (true positive) is the number of samples that are actually positive and have been correctly predicted as positive, TN (true negative) is the number of samples that are actually negative and have been correctly predicted as negative, FP (false positive) is the number of samples that are actually negative but have been predicted as positive, and FN (false negative) is the number of samples that are actually positive but have been predicted as negative.</li>
<li>The DOR can range from 0 to infinity, with higher values indicating a more accurate diagnostic test. A DOR of 1 indicates that the test is no better than a coin flip, while a DOR of infinity indicates perfect accuracy.</li>
<li>To calculate the DOR, you need a set of predictions made by the diagnostic test and the corresponding true labels for those predictions. You can then use the formula above to calculate the DOR.</li>
<li>Here is an example of how to calculate the DOR in Python:</li>
</ul>
<pre tabindex="0"><code>def diagnostic_odds_ratio(y_true, y_pred):
    tp = sum((y_true == 1) &amp; (y_pred == 1))
    tn = sum((y_true == 0) &amp; (y_pred == 0))
    fp = sum((y_true == 0) &amp; (y_pred == 1))
    fn = sum((y_true == 1) &amp; (y_pred == 0))
    dor = (tp / fp) / (fn / tn)
    return dor
</code></pre><ul>
<li>Here, y_true is a list of the true labels and y_pred is a list of the predictions made by the diagnostic test. The function calculates the values for TP, TN, FP, and FN using boolean masks and then uses these values to calculate the DOR using the formula above.</li>
</ul>
<h3 id="fall-out">Fall out</h3>
<ul>
<li>Fallout (also known as false positive rate or type I error) is a measure of the performance of a diagnostic test or classification algorithm. It is the percentage of negative samples that are incorrectly classified as positive.</li>
<li>In the context of a diagnostic test, fallout represents the probability that a person without the condition being tested for will receive a positive test result. In the context of a classification algorithm, fallout represents the percentage of negative samples that are incorrectly classified as positive.</li>
<li>Here is the formula for calculating fallout:</li>
</ul>
<pre tabindex="0"><code>Fallout = FP / (FP + TN)
</code></pre><ul>
<li>Where FP (false positive) is the number of samples that are actually negative but have been predicted as positive, and TN (true negative) is the number of samples that are actually negative and have been correctly predicted as negative.</li>
<li>To calculate fallout, you need a set of predictions made by the diagnostic test or classification algorithm and the corresponding true labels for those predictions. You can then use the formula above to calculate the fallout.</li>
<li>Here is an example of how to calculate fallout in Python:</li>
</ul>
<pre tabindex="0"><code>def fallout(y_true, y_pred):
    fp = sum((y_true == 0) &amp; (y_pred == 1))
    tn = sum((y_true == 0) &amp; (y_pred == 0))
    fallout = fp / (fp + tn)
    return fallout
</code></pre><ul>
<li>Here, y_true is a list of the true labels and y_pred is a list of the predictions made by the diagnostic test or classification algorithm. The function calculates the values for FP and TN using boolean masks and then uses these values to calculate the fallout using the formula above.</li>
</ul>
<h3 id="false-negative-fn">False negative (FN)</h3>
<ul>
<li>A false negative (FN) is a prediction made by a diagnostic test or classification algorithm that is incorrect. It refers to a situation where the test or algorithm predicts a negative result for a sample that is actually positive.</li>
<li>In the context of a diagnostic test, a false negative means that the test failed to detect the presence of a condition in a person who actually has the condition. In the context of a classification algorithm, a false negative means that the algorithm failed to correctly classify a positive sample.</li>
<li>False negatives are often more serious than false positives, as they can have more serious consequences. For example, if a diagnostic test for a disease returns a false negative result, the person may not receive the necessary treatment and their condition may worsen.</li>
<li>To calculate the number of false negatives, you need a set of predictions made by the diagnostic test or classification algorithm and the corresponding true labels for those predictions. You can then compare the predictions to the true labels to see how many were incorrect.</li>
<li>Here is an example of how to calculate the number of false negatives in Python:</li>
</ul>
<pre tabindex="0"><code>def false_negatives(y_true, y_pred):
    fn = sum((y_true == 1) &amp; (y_pred == 0))
    return fn
</code></pre><ul>
<li>Here, y_true is a list of the true labels and y_pred is a list of the predictions made by the diagnostic test or classification algorithm. The function calculates the number of false negatives using a boolean mask that compares the true labels to the predictions</li>
</ul>
<h3 id="false-negative-rate">False negative rate</h3>
<ul>
<li>The false negative rate (FNR) is a measure of the performance of a diagnostic test or classification algorithm. It is the percentage of positive samples that are incorrectly classified as negative.</li>
<li>In the context of a diagnostic test, the false negative rate represents the probability that a person with the condition being tested for will receive a negative test result. In the context of a classification algorithm, the false negative rate represents the percentage of positive samples that are incorrectly classified as negative.</li>
<li>Here is the formula for calculating the false negative rate:</li>
</ul>
<pre tabindex="0"><code>FNR = FN / (FN + TP)
</code></pre><ul>
<li>Where FN (false negative) is the number of samples that are actually positive but have been predicted as negative, and TP (true positive) is the number of samples that are actually positive and have been correctly predicted as positive.</li>
<li>To calculate the false negative rate, you need a set of predictions made by the diagnostic test or classification algorithm and the corresponding true labels for those predictions. You can then use the formula above to calculate the false negative rate.</li>
<li>Here is an example of how to calculate the false negative rate in Python:</li>
</ul>
<pre tabindex="0"><code>def false_negative_rate(y_true, y_pred):
    fn = sum((y_true == 1) &amp; (y_pred == 0))
    tp = sum((y_true == 1) &amp; (y_pred == 1))
    fnr = fn / (fn + tp)
    return fnr
</code></pre><ul>
<li>Here, y_true is a list of the true labels and y_pred is a list of the predictions made by the diagnostic test or classification algorithm. The function calculates the values for FN and TP using boolean masks and then uses these values to calculate the false negative rate using the formula above.</li>
</ul>
<h3 id="false-positive-fp">False positive (FP)</h3>
<ul>
<li>A false positive (FP) is a prediction made by a diagnostic test or classification algorithm that is incorrect. It refers to a situation where the test or algorithm predicts a positive result for a sample that is actually negative.</li>
<li>In the context of a diagnostic test, a false positive means that the test detected the presence of a condition in a person who actually does not have the condition. In the context of a classification algorithm, a false positive means that the algorithm incorrectly classified a negative sample.</li>
<li>False positives can sometimes be less serious than false negatives, as they may lead to unnecessary follow-up tests or treatment. However, they can also be costly and cause anxiety for the person being tested.</li>
<li>To calculate the number of false positives, you need a set of predictions made by the diagnostic test or classification algorithm and the corresponding true labels for those predictions. You can then compare the predictions to the true labels to see how many were incorrect.</li>
<li>Here is an example of how to calculate the number of false positives in Python:</li>
</ul>
<pre tabindex="0"><code>def false_positives(y_true, y_pred):
    fp = sum((y_true == 0) &amp; (y_pred == 1))
    return fp
</code></pre><ul>
<li>Here, y_true is a list of the true labels and y_pred is a list of the predictions made by the diagnostic test or classification algorithm. The function calculates the number of false positives using a boolean mask that compares the true labels to the predictions.</li>
</ul>
<h3 id="false-positive-rate">False positive rate</h3>
<ul>
<li>In the context of diagnostic tests, the false positive rate is the probability that a patient with a negative disease status will receive a positive test result. In other words, it is the probability of a false alarm. A high false positive rate means that there is a high probability of a patient being told they have a disease when they actually do not. This can lead to unnecessary anxiety and further testing, and can also reduce the overall credibility of the diagnostic test.</li>
<li>The false positive rate is often considered in conjunction with the sensitivity and specificity of a diagnostic test. Sensitivity is the probability of a positive test result given that the patient actually has the disease, and specificity is the probability of a negative test result given that the patient does not have the disease. Together, these measures can give a more complete picture of the performance of a diagnostic test.</li>
</ul>
<h3 id="false-omission-rate">False omission rate</h3>
<ul>
<li>In the context of diagnostic tests, the false omission rate, also known as the false negative rate, is the probability that a patient with a positive disease status will receive a negative test result. A high false negative rate means that there is a high probability of a patient being told they do not have a disease when they actually do. This can have serious consequences, as the patient may not receive the necessary treatment.</li>
<li>The false negative rate is often considered in conjunction with the sensitivity and specificity of a diagnostic test. Sensitivity is the probability of a positive test result given that the patient actually has the disease, and specificity is the probability of a negative test result given that the patient does not have the disease. Together, these measures can give a more complete picture of the performance of a diagnostic test.</li>
<li>For example, consider a diagnostic test for a particular disease. The test has a sensitivity of 90%, meaning that it correctly identifies 90% of patients with the disease. It also has a specificity of 95%, meaning that it correctly identifies 95% of patients who do not have the disease. However, if the disease is relatively rare, the false negative rate may still be unacceptably high. For example, if the prevalence of the disease is 1%, and the test has a false negative rate of 10%, then out of 100 patients with the disease, the test will correctly identify only 81 of them (90% sensitivity), while 19 will be misdiagnosed as not having the disease (10% false negative rate). This could lead to a significant number of missed diagnoses.</li>
</ul>
<h3 id="hit-rate">Hit rate</h3>
<ul>
<li>Hit rate, also known as the hit ratio, is a measure of the accuracy of a classifier, predictor, or other machine learning model. It is the number of times the model correctly predicts the outcome (a &ldquo;hit&rdquo;) divided by the total number of predictions made. For example, if a model makes 100 predictions and is correct 70 times, the hit rate is 70%.</li>
<li>Hit rate is often used as a measure of performance for models that make binary predictions (e.g., &ldquo;positive&rdquo; or &ldquo;negative&rdquo;). In this case, a hit is a correct prediction of the positive or negative class, and the hit rate is the proportion of positive or negative predictions that are correct.</li>
<li>Hit rate is related to the true positive rate and the false positive rate, which are measures of the performance of a binary classifier. The true positive rate is the proportion of positive cases that are correctly classified as positive, while the false positive rate is the proportion of negative cases that are incorrectly classified as positive. Together, these measures can give a more complete picture of the performance of a classifier.</li>
</ul>
<h3 id="miss-rate">Miss rate</h3>
<ul>
<li>Miss rate, also known as the miss ratio or false negative rate, is a measure of the accuracy of a classifier, predictor, or other machine learning model. It is the number of times the model incorrectly predicts the outcome (a &ldquo;miss&rdquo;) divided by the total number of predictions made. For example, if a model makes 100 predictions and is incorrect 30 times, the miss rate is 30%.</li>
<li>Miss rate is often used as a measure of performance for models that make binary predictions (e.g., &ldquo;positive&rdquo; or &ldquo;negative&rdquo;). In this case, a miss is an incorrect prediction of the positive or negative class, and the miss rate is the proportion of positive or negative predictions that are incorrect.</li>
<li>Miss rate is related to the true positive rate and the false positive rate, which are measures of the performance of a binary classifier. The true positive rate is the proportion of positive cases that are correctly classified as positive, while the false positive rate is the proportion of negative cases that are incorrectly classified as positive. Together, these measures can give a more complete picture of the performance of a classifier.</li>
</ul>
<h3 id="negative-likelihood-ratio">Negative likelihood ratio</h3>
<ul>
<li>The negative likelihood ratio (NLR) is a measure of the performance of a diagnostic test or other classifier. It is the ratio of the probability of a negative test result given that the patient does not have the disease (specificity) to the probability of a negative test result given that the patient does have the disease (1 - sensitivity). The NLR is used to assess the ability of a test to rule out the presence of a disease.</li>
<li>The NLR can be calculated using the following formula: NLR = (1 - sensitivity) / specificity</li>
<li>A diagnostic test with a high NLR (greater than 1) is said to have a high negative predictive value, meaning that it is good at ruling out the presence of a disease. A test with a low NLR (less than 1) has a low negative predictive value, meaning that it is not good at ruling out the presence of a disease.</li>
<li>The NLR is often used in conjunction with the positive likelihood ratio (PLR), which is the ratio of the probability of a positive test result given that the patient has the disease (sensitivity) to the probability of a positive test result given that the patient does not have the disease (1 - specificity). The PLR is used to assess the ability of a test to detect the presence of a disease. Together, the NLR and PLR can give a more complete picture of the performance of a diagnostic test.</li>
</ul>
<h3 id="negative-predictive-value">Negative predictive value</h3>
<ul>
<li>The negative predictive value (NPV) is a measure of the performance of a diagnostic test or other classifier. It is the probability that a patient with a negative test result does not have the disease. The NPV is used to assess the ability of a test to rule out the presence of a disease.</li>
<li>The NPV can be calculated using the following formula: NPV = TN / (TN + FN)
where TN is the number of true negatives (patients with a negative test result who do not have the disease) and FN is the number of false negatives (patients with a negative test result who do have the disease).</li>
<li>A diagnostic test with a high NPV (close to 1) is said to have a high negative predictive value, meaning that it is good at ruling out the presence of a disease. A test with a low NPV (close to 0) has a low negative predictive value, meaning that it is not good at ruling out the presence of a disease.</li>
<li>The NPV is often used in conjunction with the positive predictive value (PPV), which is the probability that a patient with a positive test result does have the disease. The PPV is used to assess the ability of a test to detect the presence of a disease. Together, the NPV and PPV can give a more complete picture of the performance of a diagnostic test.</li>
</ul>
<h3 id="positive-likelihood-ratio">Positive likelihood ratio</h3>
<ul>
<li>The positive likelihood ratio (PLR) is a measure of the performance of a diagnostic test or other classifier. It is the ratio of the probability of a positive test result given that the patient has the disease (sensitivity) to the probability of a positive test result given that the patient does not have the disease (1 - specificity). The PLR is used to assess the ability of a test to detect the presence of a disease.</li>
<li>The PLR can be calculated using the following formula: PLR = sensitivity / (1 - specificity)</li>
<li>A diagnostic test with a high PLR (greater than 1) is said to have a high positive predictive value, meaning that it is good at detecting the presence of a disease. A test with a low PLR (less than 1) has a low positive predictive value, meaning that it is not good at detecting the presence of a disease.</li>
<li>The PLR is often used in conjunction with the negative likelihood ratio (NLR), which is the ratio of the probability of a negative test result given that the patient does not have the disease (specificity) to the probability of a negative test result given that the patient does have the disease (1 - sensitivity). The NLR is used to assess the ability of a test to rule out the presence of a disease. Together, the PLR and NLR can give a more complete picture of the performance of a diagnostic test.</li>
</ul>
<h3 id="positive-predictive-value">Positive predictive value</h3>
<ul>
<li>The positive predictive value (PPV) is a measure of the performance of a diagnostic test or other classifier. It is the probability that a patient with a positive test result does have the disease. The PPV is used to assess the ability of a test to detect the presence of a disease.</li>
<li>The PPV can be calculated using the following formula: PPV = TP / (TP + FP)
where TP is the number of true positives (patients with a positive test result who do have the disease) and FP is the number of false positives (patients with a positive test result who do not have the disease).</li>
<li>A diagnostic test with a high PPV (close to 1) is said to have a high positive predictive value, meaning that it is good at detecting the presence of a disease. A test with a low PPV (close to 0) has a low positive predictive value, meaning that it is not good at detecting the presence of a disease.</li>
<li>The PPV is often used in conjunction with the negative predictive value (NPV), which is the probability that a patient with a negative test result does not have the disease. The NPV is used to assess the ability of a test to rule out the presence of a disease. Together, the PPV and NPV can give a more complete picture of the performance of a diagnostic test.</li>
</ul>
<h3 id="precision">Precision</h3>
<ul>
<li>In the context of statistical hypothesis testing and machine learning, precision is a measure of the accuracy of a classifier, predictor, or other model. It is the number of true positive predictions made by the model divided by the total number of positive predictions made by the model. Precision is used to evaluate the performance of a model that makes binary predictions (e.g., &ldquo;positive&rdquo; or &ldquo;negative&rdquo;).</li>
<li>For example, consider a model that makes 100 predictions, of which 70 are positive and 30 are negative. If the model is correct in 60 of the positive predictions and all of the negative predictions, the precision of the model is 60/70 = 0.86. This means that of all the positive predictions made by the model, 86% are correct.</li>
<li>Precision is often used in conjunction with the recall, which is the number of true positive predictions made by the model divided by the total number of actual positive cases. Precision and recall are both used to evaluate the performance of a binary classifier, and can be balanced against each other to achieve the desired trade-off in a particular application.</li>
</ul>
<h3 id="recall">Recall</h3>
<ul>
<li>In the context of statistical hypothesis testing and machine learning, recall is a measure of the accuracy of a classifier, predictor, or other model. It is the number of true positive predictions made by the model divided by the total number of actual positive cases. Recall is used to evaluate the performance of a model that makes binary predictions (e.g., &ldquo;positive&rdquo; or &ldquo;negative&rdquo;).</li>
<li>For example, consider a model that makes 100 predictions, of which 70 are positive and 30 are negative. If the model is correct in 60 of the positive predictions and all of the negative predictions, and there are 80 actual positive cases, the recall of the model is 60/80 = 0.75. This means that of all the actual positive cases, 75% are correctly predicted by the model.</li>
<li>Recall is often used in conjunction with the precision, which is the number of true positive predictions made by the model divided by the total number of positive predictions made by the model. Precision and recall are both used to evaluate the performance of a binary classifier, and can be balanced against each other to achieve the desired trade-off in a particular application.</li>
</ul>
<h3 id="sensitivity">Sensitivity</h3>
<ul>
<li>Sensitivity, also known as the true positive rate or the recall, is a measure of the performance of a diagnostic test or other classifier. It is the probability of a positive test result given that the patient actually has the disease. Sensitivity is used to evaluate the ability of a test to detect the presence of a disease.</li>
<li>The sensitivity of a diagnostic test can be calculated using the following formula: sensitivity = TP / (TP + FN) where TP is the number of true positives (patients with a positive test result who do have the disease) and FN is the number of false negatives (patients with a negative test result who do have the disease).</li>
<li>A diagnostic test with a high sensitivity (close to 1) is said to have a high true positive rate, meaning that it is good at detecting the presence of a disease. A test with a low sensitivity (close to 0) has a low true positive rate, meaning that it is not good at detecting the presence of a disease.</li>
<li>Sensitivity is often used in conjunction with the specificity of a diagnostic test, which is the probability of a negative test result given that the patient does not have the disease. Together, sensitivity and specificity can give a more complete picture of the performance of a diagnostic test.</li>
</ul>
<h3 id="specificity">Specificity</h3>
<ul>
<li>Specificity, also known as the true negative rate, is a measure of the performance of a diagnostic test or other classifier. It is the probability of a negative test result given that the patient does not have the disease. Specificity is used to evaluate the ability of a test to rule out the presence of a disease.</li>
<li>The specificity of a diagnostic test can be calculated using the following formula: specificity = TN / (TN + FP) where TN is the number of true negatives (patients with a negative test result who do not have the disease) and FP is the number of false positives (patients with a positive test result who do not have the disease).</li>
<li>A diagnostic test with a high specificity (close to 1) is said to have a high true negative rate, meaning that it is good at ruling out the presence of a disease. A test with a low specificity (close to 0) has a low true negative rate, meaning that it is not good at ruling out the presence of a disease.</li>
<li>Specificity is often used in conjunction with the sensitivity of a diagnostic test, which is the probability of a positive test result given that the patient does have the disease. Together, sensitivity and specificity can give a more complete picture of the performance of a diagnostic test.</li>
</ul>
<h3 id="true-negative-tn">True negative (TN)</h3>
<ul>
<li>A true negative is a prediction made by a diagnostic test or other classifier that an event or condition is absent, and the event or condition is indeed absent. In the context of statistical hypothesis testing and machine learning, a true negative is a prediction made by a model that an instance belongs to the negative class, and the instance does indeed belong to the negative class.</li>
<li>True negatives are typically represented by the letter TN in performance metrics such as sensitivity, specificity, and the positive and negative predictive values. These metrics are used to evaluate the accuracy of a diagnostic test or other classifier. For example, the sensitivity of a test is the proportion of true positive predictions made by the test to the total number of actual positive cases, while the specificity of a test is the proportion of true negative predictions made by the test to the total number of actual negative cases.</li>
</ul>
<h3 id="true-positive-tp">True positive (TP)</h3>
<p>A true positive is a prediction made by a diagnostic test or other classifier that an event or condition is present, and the event or condition is indeed present. In the context of statistical hypothesis testing and machine learning, a true positive is a prediction made by a model that an instance belongs to the positive class, and the instance does indeed belong to the positive class.</p>
<p>True positives are typically represented by the letter TP in performance metrics such as sensitivity, specificity, and the positive and negative predictive values. These metrics are used to evaluate the accuracy of a diagnostic test or other classifier. For example, the sensitivity of a test is the proportion of true positive predictions made by the test to the total number of actual positive cases, while the specificity of a test is the proportion of true negative predictions made by the test to the total number of actual negative cases.</p>
<h2 id="data">Data</h2>
<h3 id="attribute">Attribute</h3>
<ul>
<li>In the context of data modeling and database design, an attribute is a property or characteristic of an entity, typically represented as a column in a database table. An attribute can be a simple data value (e.g., a string, integer, or date) or a complex data structure (e.g., an array or object).</li>
<li>For example, consider a database table that represents a collection of users. Each user in the table might have attributes such as name, email, and date of birth. These attributes can be used to describe the characteristics of each user in the table.</li>
<li>In the context of machine learning, an attribute is a feature or characteristic of a data instance that can be used for prediction or classification. For example, in a dataset of customer data, each customer might have attributes such as age, income, and location, which could be used to predict their purchasing behavior.</li>
<li>In both cases, the attributes of an entity or data instance are used to describe and differentiate it from other entities or instances in the same data set.</li>
</ul>
<h3 id="box-and-whisker-plot">Box and whisker plot</h3>
<ul>
<li>A box and whisker plot (also known as a box plot) is a graphical representation of a set of numerical data that summarizes several important features of the data using a simple and visually effective display. It is typically used to visualize the distribution of the data and to identify any outliers or unusual observations.</li>
<li>To create a box and whisker plot, the data is first sorted into numerical order. The middle 50% of the data is then represented by a box, which extends from the lower quartile (the 25th percentile) to the upper quartile (the 75th percentile). The lower and upper quartiles are the points that divide the data into four equal parts.</li>
<li>The median (the 50th percentile) is represented by a line inside the box. The median is the middle value of the data, such that half of the data is above it and half is below it.</li>
<li>The &ldquo;whiskers&rdquo; of the plot extend from the box to the minimum and maximum values of the data, unless there are outliers present, in which case the whiskers extend only to the most extreme data points that are not outliers. Outliers are data points that are significantly farther from the main body of the data than the rest of the data. They are typically plotted separately as individual points on the plot.</li>
<li>Box and whisker plots are useful for comparing the distributions of different sets of data, or for identifying patterns and trends in a single set of data.</li>
</ul>
<h3 id="categorical-data">Categorical data</h3>
<ul>
<li>Categorical data is data that can be divided into categories or groups. These categories are usually based on some shared characteristics or qualities. Categorical data can be either nominal, meaning the categories do not have any specific order or ranking, or ordinal, meaning the categories are ranked or ordered in some way.</li>
<li>Examples of categorical data include:
-Nominal data:
<ul>
<li>Gender (male, female)</li>
<li>Eye color (brown, blue, green)</li>
<li>Type of animal (cat, dog, bird)</li>
</ul>
</li>
<li>Ordinal data:
<ul>
<li>Educational degree (high school, bachelor&rsquo;s degree, master&rsquo;s degree)</li>
<li>Customer satisfaction ratings (very satisfied, satisfied, neutral, dissatisfied, very frustrated)</li>
<li>Military rank (private, sergeant, lieutenant, captain)</li>
</ul>
</li>
<li>Categorical data is often used in statistical analysis, and it is important to understand the type of data you are working with in order to choose the appropriate statistical techniques and analysis tools.</li>
</ul>
<h3 id="collective-outlier">Collective outlier</h3>
<ul>
<li>A collective outlier is a group of data points that are significantly different from the rest of the data. Collective outliers can occur when there is a group of data points that have a different distribution or pattern from the rest of the data. These data points may be the result of a measurement error, an unusual event, or a different process or population.</li>
<li>Collective outliers can be difficult to identify, as they may not stand out as clearly as individual outliers. It is important to carefully examine the data and consider the context in which it was collected to determine if a group of data points may be collective outliers.</li>
<li>There are several methods for detecting collective outliers, including visual inspection of the data, statistical tests, and machine learning algorithms. Once identified, it is important to determine the cause of the collective outliers and consider whether they should be included in the analysis or removed from the data.</li>
</ul>
<h3 id="contextual-outlier">Contextual outlier</h3>
<ul>
<li>A contextual outlier is a data point that is unusual or unexpected in the context in which it occurs, but may not be unusual if considered in a different context. Contextual outliers can occur when there are differences in the populations, processes, or environments being studied, or when the data is being collected for different purposes or using different methods.</li>
<li>For example, if you are studying the height of adult men and women, a data point representing the height of a 6-foot-tall woman might be considered a contextual outlier, as it is unusual compared to the rest of the data on women&rsquo;s height, but not necessarily unusual compared to the overall distribution of heights in the population.</li>
<li>It is important to consider the context in which the data was collected when identifying and analyzing contextual outliers. This can help to identify any underlying causes of the outlier and determine whether it is appropriate to include the outlier in the analysis or exclude it from the data.</li>
</ul>
<h3 id="covariate">Covariate</h3>
<ul>
<li>A covariate is a variable that is correlated with another variable and is included in a statistical model to control for its effect. Covariates are often used in statistical analysis to adjust for differences between groups or to better understand the relationship between two variables.</li>
<li>For example, in a study of the relationship between age and blood pressure, age might be included as a covariate to control for its effect on blood pressure. This is because age is known to be related to blood pressure, and including it as a covariate in the statistical model can help to isolate the relationship between blood pressure and other factors being studied.</li>
<li>In general, covariates are used to improve the accuracy and validity of statistical models by accounting for the influence of other variables that might confound the relationship being studied.</li>
</ul>
<h3 id="data-point">Data point</h3>
<ul>
<li>A data point is a single piece of data or a single observation in a dataset. Data points can represent a wide variety of things, depending on the context in which the data was collected. For example, a data point might represent a person&rsquo;s age, the number of sales made by a company in a given month, the temperature at a specific location on a given day, or the result of a laboratory experiment.</li>
<li>Data points are usually organized and stored in a dataset, which can be a table, spreadsheet, or other structured format. A dataset typically contains multiple data points, and each data point is often represented by a row in the dataset.</li>
<li>Data points are used in statistical analysis to understand patterns, trends, and relationships within the data. By examining individual data points and the relationships between them, it is possible to draw conclusions and make predictions about the population or system being studied.</li>
</ul>
<h3 id="detrending">Detrending</h3>
<ul>
<li>Detrending is the process of removing trends or long-term patterns from data in order to better understand short-term fluctuations or changes. Detrending is often used in time series analysis, where the goal is to identify and analyze patterns in data that occur over time.</li>
<li>There are several methods for detrending data, including:
<ul>
<li>Subtracting the mean: This method involves calculating the mean value of the data over a certain period of time, and then subtracting that value from each data point.</li>
<li>Fitting a trend line: This method involves fitting a line to the data using a statistical model, such as a linear or polynomial model, and then subtracting the predicted values from the actual data.</li>
<li>Differencing: This method involves subtracting each data point from the previous data point, which removes any trend that is present in the data.</li>
</ul>
</li>
<li>Detrending can help to identify and analyze shorter-term patterns or cycles in the data, and can be useful for forecasting or predicting future values. However, it is important to carefully consider the appropriateness of detrending for a particular dataset, as removing trends can also remove important information about the underlying process or system being studied.</li>
</ul>
<h3 id="eigenvalue">Eigenvalue</h3>
<ul>
<li>An eigenvalue is a special number that is associated with a linear transformation or matrix. In mathematics, a linear transformation is a function that maps one set of numbers (called vectors) to another set of numbers, in such a way that the transformation preserves certain properties of the original vectors. Matrices are used to represent linear transformations, and the eigenvalues of a matrix are a measure of its overall behavior or characteristics.</li>
<li>The eigenvalues of a matrix are the values that satisfy a particular equation involving the matrix and a vector. These values can be real numbers or complex numbers, and each matrix has a set of eigenvalues that are unique to that matrix.</li>
<li>Eigenvalues are used in a variety of mathematical and statistical contexts, including image processing, machine learning, and data analysis. They are often used to understand the behavior or characteristics of a matrix or linear transformation, and can be used to identify patterns or trends in data.</li>
</ul>
<h3 id="eigenvector">Eigenvector</h3>
<ul>
<li>An eigenvector is a special type of vector that is associated with a linear transformation or matrix. In mathematics, a vector is a set of numbers that can be used to represent quantities such as position, velocity, or force. A linear transformation is a function that maps one set of vectors to another set of vectors, in such a way that the transformation preserves certain properties of the original vectors. Matrices are used to represent linear transformations, and the eigenvectors of a matrix are vectors that are unchanged (up to a scale factor) by the matrix.</li>
<li>The eigenvectors of a matrix are the vectors that satisfy a particular equation involving the matrix and the vector. These vectors can have any number of dimensions, and each matrix has a set of eigenvectors that are unique to that matrix.</li>
<li>Eigenvectors are used in a variety of mathematical and statistical contexts, including image processing, machine learning, and data analysis. They are often used to understand the behavior or characteristics of a matrix or linear transformation, and can be used to identify patterns or trends in data.</li>
</ul>
<h3 id="feature">Feature</h3>
<ul>
<li>In the context of machine learning, features are pieces of data or characteristics that are used as inputs for a model. A machine learning model is a mathematical model that is trained to perform a specific task, such as classifying objects, predicting a numerical value, or generating text. In order to train a model, it is necessary to provide a set of input data, called features, along with the corresponding output data, called labels.</li>
<li>The choice of features can have a significant impact on the performance of a machine learning model. Good features should be relevant to the task being performed and should contain enough information to allow the model to make accurate predictions or decisions. In some cases, it may be necessary to transform or engineer the features in order to extract the relevant information or to improve the model&rsquo;s performance.</li>
<li>For example, in a machine learning model that is used to classify images of animals, the features might include the pixel values of the images, or characteristics such as the shape or color of the objects in the images. In a model that is used to predict the price of a house, the features might include characteristics of the house, such as the size, location, and age, as well as external factors such as the local housing market.</li>
</ul>
<h3 id="imputation">Imputation</h3>
<ul>
<li>Imputation is the process of estimating or replacing missing or incomplete data in a dataset. Missing data can occur for a variety of reasons, such as errors in data collection, missing values in a database, or respondents who do not answer certain questions in a survey. Imputation is often necessary in order to use the available data for statistical analysis or machine learning tasks.</li>
<li>There are several methods for imputing missing data, including:
<ul>
<li>Mean imputation: This method involves replacing missing values with the mean or average value of the data.</li>
<li>Median imputation: This method involves replacing missing values with the median value of the data.</li>
<li>Mode imputation: This method involves replacing missing values with the most frequent or common value in the data.</li>
<li>Regression imputation: This method involves using a statistical model, such as linear regression, to predict the missing values based on the other variables in the data.</li>
</ul>
</li>
<li>It is important to carefully consider the appropriate method for imputing missing data, as the choice of method can affect the accuracy and validity of the results.</li>
</ul>
<h3 id="observation">Observation</h3>
<ul>
<li>An observation is a single piece of data or a single measure of a variable. Observations can be collected in a variety of ways, depending on the context and the purpose of the study. For example, observations might be collected through experiments, surveys, or measurements.</li>
<li>Observations are used to collect and analyze data in order to understand patterns, trends, and relationships within the data. By examining individual observations and the relationships between them, it is possible to draw conclusions and make predictions about the population or system being studied.</li>
<li>Observations can be either qualitative, meaning they describe a characteristic or attribute of an object or phenomenon, or quantitative, meaning they represent a numerical measurement. Observations are usually organized and stored in a dataset, which can be a table, spreadsheet, or other structured format. A dataset typically contains multiple observations, and each observation is often represented by a row in the dataset.</li>
</ul>
<h3 id="pca">PCA</h3>
<ul>
<li>Principal component analysis (PCA) is a statistical technique that is used to reduce the dimensionality of a dataset by identifying and projecting the data onto a smaller set of orthogonal (uncorrelated) dimensions, called principal components.</li>
<li>PCA is often used as a preprocessing step for machine learning algorithms, as it can help to remove noise and redundancy from the data, and make the data easier to visualize and analyze. It can also help to identify patterns and trends in the data, and to identify the most important variables or features in the dataset.</li>
<li>To perform PCA, the data is first standardized, so that all of the variables have a mean of zero and a standard deviation of one. The data is then decomposed into a set of orthogonal principal components, which are ranked in order of their importance or variability in the data. The first principal component represents the direction in the data that has the highest variance, and the subsequent principal components represent directions that have decreasing variance.</li>
<li>PCA is a powerful tool for analyzing and understanding complex datasets, and it has a wide range of applications in fields such as machine learning, data mining, and image processing.</li>
</ul>
<h3 id="point-outlier">Point outlier</h3>
<ul>
<li>A point outlier is a data point that is significantly different from the rest of the data. Point outliers can occur when there is an unusual or unexpected measurement, an error in data collection, or a different process or population being studied.</li>
<li>Point outliers can be identified by visual inspection of the data, or by using statistical tests or machine learning algorithms. It is important to carefully consider the cause of the outlier and determine whether it is appropriate to include the outlier in the analysis or exclude it from the data.</li>
<li>In some cases, point outliers may be the result of errors or mistakes in data collection, and it may be appropriate to remove them from the data. In other cases, point outliers may represent unusual or unexpected events or observations, and it may be important to include them in the analysis in order to better understand the underlying process or system being studied.</li>
</ul>
<h3 id="predictor">Predictor</h3>
<ul>
<li>A predictor is a variable that is used to predict or estimate the value of another variable, called the response variable. In statistical analysis, predictor variables are often used to build models that can be used to make predictions or estimations about the response variable.</li>
<li>For example, in a study of the relationship between age and blood pressure, age might be used as a predictor variable to predict blood pressure. In this case, age would be considered a predictor because it is believed to have an effect on blood pressure, and the goal is to use it to predict or estimate blood pressure in a given population.</li>
<li>Predictor variables can be either continuous, meaning they can take on any value within a certain range, or categorical, meaning they belong to a specific category or group. The type of predictor variables and the relationship between them and the response variable can influence the choice of statistical techniques and models that are used to analyze the data.</li>
</ul>
<h3 id="quantitative-data">Quantitative data</h3>
<ul>
<li>Quantitative data is data that is numerical and can be measured or counted. Quantitative data is often used in statistical analysis to understand patterns, trends, and relationships within the data.</li>
<li>There are two main types of quantitative data: continuous data and discrete data. Continuous data can take on any value within a certain range, such as weight, height, or temperature. Discrete data can only take on specific values, such as the number of students in a class or the number of emails a person receives in a day.</li>
<li>Examples of quantitative data include:
<ul>
<li>Age</li>
<li>Income</li>
<li>Height</li>
<li>Weight</li>
<li>Temperature</li>
<li>Distance</li>
<li>Time</li>
<li>Sales revenue</li>
</ul>
</li>
<li>Quantitative data is often used in statistical analysis to understand patterns, trends, and relationships within the data. It can be analyzed using statistical techniques such as mean, median, mode, standard deviation, and correlation.</li>
</ul>
<h3 id="response">Response</h3>
<ul>
<li>In statistical analysis, the response (also known as the dependent variable) is the variable that is being predicted or estimated based on the values of one or more predictor variables (also known as independent variables).</li>
<li>For example, in a study of the relationship between age and blood pressure, blood pressure might be the response variable, and age might be a predictor variable. In this case, the goal might be to use age to predict or estimate blood pressure in a given population.</li>
<li>The response variable is often the main focus of statistical analysis, and the goal is usually to understand how the predictor variables influence the response variable. The choice of predictor variables and the relationship between them and the response variable can influence the choice of statistical techniques and models that are used to analyze the data.</li>
</ul>
<h3 id="scaling">Scaling</h3>
<ul>
<li>Scaling is the process of transforming data so that it is on the same scale or within the same range. Scaling is often necessary when comparing data from different sources or when the data has a wide range of values.</li>
<li>There are several methods for scaling data, including:
<ul>
<li>Min-Max scaling: This method scales the data to a specific range, such as 0 to 1, by</li>
<li>subtracting the minimum value from each data point and dividing by the range of the data.</li>
<li>Standardization: This method scales the data so that it has a mean of zero and standard deviation of one.</li>
<li>Z-score normalization: This method scales the data so that it has a mean of zero and</li>
<li>a standard deviation of one, and transforms it into a standard normal distribution.</li>
</ul>
</li>
<li>Scaling can be useful for improving the performance of machine learning algorithms, as it can help to prevent certain features from dominating the model due to their large scale. Scaling can also be useful for visualizing the data and comparing different variables or datasets.</li>
</ul>
<h3 id="standardization">Standardization</h3>
<ul>
<li>In the context of machine learning, standardization refers to the process of transforming data features so that they have zero mean and unit variance. This is often done to ensure that all features are on the same scale, which can be important for some machine learning algorithms to function properly.</li>
<li>For example, suppose that you have a dataset with two features, one that ranges from 0 to 100 and another that ranges from 0 to 1. Without standardization, the feature with a larger range will dominate the model. By standardizing the data, both features will be transformed to have the same scale, which can lead to better performance from the machine learning model.</li>
<li>Standardization is typically done by subtracting the mean of each feature from the feature values and dividing by the standard deviation of the feature. This ensures that the resulting feature values have zero mean and unit variance.</li>
</ul>
<h3 id="structured-data">Structured data</h3>
<ul>
<li>Structured data is data that is organized in a specific way and follows a clear set of rules. It is typically stored in a tabular form, with rows representing individual instances or observations and columns representing the attributes or features of the data. Structured data can be easily processed and analyzed by machines because it follows a well-defined format.</li>
<li>Examples of structured data include databases, spreadsheets, and tables in a relational database management system (RDBMS). Structured data is often contrasted with unstructured data, which does not follow a fixed format and is more difficult for machines to process and analyze.</li>
<li>In the context of machine learning, structured data refers to data that is organized in a way that can be easily fed into a machine learning model. This often involves formatting the data into a tabular form with rows representing individual observations and columns representing the features or attributes of the data. Machine learning algorithms are typically designed to work with structured data, so it is important to ensure that the data is properly structured before using it for training or testing a model.</li>
</ul>
<h3 id="time-series-data">Time series data</h3>
<ul>
<li>Time series data is a type of data that is collected over time at regular intervals. It is typically used to analyze trends and patterns in data over time. Time series data can be represented as a sequence of data points, where each data point represents the value of a particular variable at a specific time.</li>
<li>Examples of time series data include stock prices, weather data, and traffic data. Time series data can be used in a variety of applications, including financial forecasting, demand forecasting, and anomaly detection.</li>
<li>In the context of machine learning, time series data can be used to train models to make predictions about future values of a particular variable based on its past values. This can be done using techniques such as time series forecasting, which involves using machine learning algorithms to model the temporal dependencies in the data and make predictions about future values.</li>
<li>Time series data is often analyzed using specialized tools and techniques, such as autoregressive integrated moving average (ARIMA) models and long short-term memory (LSTM) neural networks.</li>
</ul>
<h3 id="unstructured-data">Unstructured data</h3>
<ul>
<li>Unstructured data is data that does not follow a specific format or structure. It is often unorganized and does not fit neatly into a traditional database or spreadsheet. Examples of unstructured data include natural language text, images, audio and video files, and social media posts.</li>
<li>Unstructured data is difficult for machines to process and analyze because it does not follow a fixed format. This makes it more challenging to extract insights and information from unstructured data compared to structured data, which is organized in a well-defined format and can be easily processed by machines.</li>
<li>In the context of machine learning, unstructured data can be used as input to train models, but it often requires preprocessing and feature engineering to extract relevant features that can be used by the model.</li>
<li>This can involve techniques such as natural language processing (NLP) for text data, image processing for image data, and audio processing for audio data. The extracted features can then be used to train machine learning models, which can be used to make predictions or classify the data in some way.</li>
</ul>
<h2 id="design-of-experiments">Design of Experiments</h2>
<h3 id="ab-testing">A/B testing</h3>
<ul>
<li>A/B testing, also known as split testing or bucket testing, is a statistical hypothesis testing procedure used to compare the results of two versions of a product or service. It is commonly used in the fields of marketing and user experience to determine which version is more effective.</li>
<li>In A/B testing, a random sample of users is selected and divided into two groups, referred to as the control group and the treatment group. The control group is exposed to the current version of the product or service, while the treatment group is exposed to the new version. The results of the two groups are then compared to determine if the new version is an improvement over the current version.</li>
<li>A/B testing is often used to test changes to websites, apps, and other products or services to determine their impact on user behavior. It is a powerful tool for making data-driven decisions because it allows you to measure the impact of a change in a controlled and statistically rigorous way.</li>
</ul>
<h3 id="analysis-of-variance">Analysis of Variance</h3>
<ul>
<li>Analysis of variance (ANOVA) is a statistical test used to compare the mean of a continuous variable between two or more groups. It is used to determine whether there is a significant difference between the means of the groups, and if so, where the difference lies.</li>
<li>ANOVA is based on the idea of partitioning the total variance in a dataset into different components, such as the variance within each group and the variance between groups. By comparing the size of these components, ANOVA can determine whether the differences between the group means are statistically significant or if they are likely due to random chance.</li>
<li>ANOVA can be used with both categorical and continuous independent variables, and it is a widely used tool in a variety of fields, including psychology, sociology, and economics. There are several different types of ANOVA tests, including one-way ANOVA, two-way ANOVA, and repeated measures ANOVA, which are used in different situations depending on the design of the study.</li>
</ul>
<h3 id="balanced-design">Balanced design</h3>
<ul>
<li>A balanced design is a type of experimental design in which the number of observations in each group is equal. Balanced designs are often used in experiments to ensure that the groups are comparable and that any differences between the groups can be attributed to the independent variable being tested.</li>
<li>For example, suppose that you are conducting an experiment to test the effectiveness of a new drug. You might use a balanced design by dividing the study participants into two groups: one group that receives the drug and another group that receives a placebo. By ensuring that the two groups are equal in size and composition, you can control for other factors that might influence the results and increase the reliability of your findings.</li>
<li>Balanced designs can be contrasted with unbalanced designs, in which the number of observations in each group is unequal. Unbalanced designs can be more prone to bias and may not be as reliable as balanced designs.</li>
</ul>
<h3 id="blocking">Blocking</h3>
<ul>
<li>In the context of experimental design, blocking refers to the process of dividing the study subjects into groups, or &ldquo;blocks,&rdquo; based on certain factors that could potentially affect the outcome of the experiment. The goal of blocking is to control for these factors and reduce the potential for extraneous variability in the results.</li>
<li>For example, suppose that you are conducting an experiment to test the effectiveness of a new teaching method. You might use blocking by dividing the students into groups based on their prior knowledge of the subject matter, in order to control for differences in their initial understanding. By ensuring that the groups are balanced with respect to this factor, you can increase the reliability of your findings and reduce the risk of confounding variables influencing the results.</li>
<li>Blocking is often used in conjunction with randomization, in which the subjects within each block are randomly assigned to the different treatment groups. This helps to further control for extraneous variables and increase the internal validity of the experiment.</li>
</ul>
<h3 id="control">Control</h3>
<ul>
<li>In the context of experimental design, a control group is a group of subjects that does not receive the treatment being tested. The control group is used for comparison with the experimental group, which does receive the treatment. By comparing the results of the two groups, researchers can determine the effect of the treatment on the outcome of interest.</li>
<li>The control group is an important element of experimental design because it helps to control for extraneous variables that might influence the results. For example, suppose that you are conducting an experiment to test the effectiveness of a new drug. By including a control group that does not receive the drug, you can control for other factors that might affect the outcome, such as the placebo effect or the natural course of the disease.</li>
<li>In order to be effective, the control group should be similar to the experimental group in all aspects except for the treatment being tested. This helps to ensure that any differences between the two groups can be attributed to the treatment, rather than other factors.</li>
</ul>
<h3 id="design-of-experiments-1">Design of experiments</h3>
<ul>
<li>The design of experiments (DOE) refers to the systematic and scientific approach to planning, conducting, analyzing, and interpreting experiments. It is a powerful tool for understanding the relationships between variables and for making informed decisions based on data.</li>
<li>The goal of DOE is to identify the key factors that affect the outcome of an experiment and to determine the optimal combination of these factors. This is typically done by manipulating the levels of the different variables and observing the resulting changes in the outcome.</li>
<li>There are many different types of experimental designs, including randomized controlled trials, cross-over designs, and factorial designs. The choice of design depends on the specific research question being addressed and the resources available for the experiment.</li>
<li>DOE is widely used in a variety of fields, including medicine, engineering, and the social sciences. It is an important tool for scientific research and for making data-driven decisions in a variety of settings.</li>
</ul>
<h3 id="exploitation">Exploitation</h3>
<ul>
<li>Exploitation refers to the act of using something or someone to achieve a benefit or gain, often in a way that is unfair or unethical. In the context of machine learning, exploitation can refer to the use of data or algorithms in ways that unfairly advantage certain individuals or groups, or that violate the privacy or autonomy of those whose data is being used.</li>
<li>For example, exploitation in machine learning could involve using sensitive personal data for purposes that were not disclosed to the individual when the data was collected, or using algorithms that are biased against certain groups. Such practices can lead to negative consequences for those affected by the exploitation, including loss of privacy, discrimination, or loss of opportunities.</li>
<li>It is important to be aware of the potential for exploitation in machine learning and to take steps to ensure that data and algorithms are used ethically and responsibly. This can involve adopting ethical principles and guidelines, such as those put forth by organizations like the Association for Computing Machinery (ACM) and the International Association for AI and Ethics (IAAIE).</li>
</ul>
<h3 id="exploration">Exploration</h3>
<ul>
<li>In the context of design of experiments (DOE), exploration refers to the process of systematically varying the levels of the input factors in order to better understand the response of the system being studied. Exploration is an important aspect of DOE because it helps to identify the important factors that influence the response, as well as the relationships between these factors and the response.</li>
<li>This information can be used to optimize the system by identifying the optimal levels of the input factors for a desired response. Exploration can be carried out using a variety of DOE techniques, such as factorial designs, response surface methodology, and DOE software tools.</li>
</ul>
<h3 id="factorial-design">Factorial design</h3>
<ul>
<li>A factorial design is a type of experimental design in which multiple levels of multiple input factors are tested simultaneously. This allows researchers to study the combined effect of multiple factors on a response, as well as the interaction between the factors.</li>
<li>Factorial designs are commonly used in DOE because they are efficient and can provide a lot of information about the system being studied. For example, if there are two factors being studied, each at two levels, a 2x2 factorial design would involve testing all four possible combinations of the factor levels. This allows researchers to see how the response changes as each factor is varied independently, as well as how the response changes when the factors are combined.</li>
<li>Factorial designs can have more than two factors and more than two levels per factor. The number of treatment combinations in a factorial design increases quickly as the number of factors and levels increases, so it is important to carefully plan the design to ensure that it is both practical and efficient.</li>
</ul>
<h3 id="fractional-factorial-design">Fractional factorial design</h3>
<ul>
<li>A fractional factorial design is a type of experimental design that is similar to a full factorial design, but involves testing only a fraction of the possible combinations of factor levels. This allows researchers to study the effects of multiple factors with a smaller number of experimental runs.</li>
<li>Fractional factorial designs are useful when there are a large number of factors that need to be studied, or when it is not practical or cost-effective to test all possible combinations of factor levels. However, because not all combinations of factor levels are tested, a fractional factorial design may not be as accurate as a full factorial design.</li>
<li>There are several types of fractional factorial designs, including two-level fractional factorial designs, which involve testing only a fraction of the possible combinations of two levels of each factor, and Plackett-Burman designs, which are a type of fractional factorial design that is commonly used to identify the important factors in a system.</li>
</ul>
<h3 id="full-factorial-design">Full factorial design</h3>
<ul>
<li>A full factorial design is a type of experimental design in which all possible combinations of the levels of multiple input factors are tested. This allows researchers to study the combined effect of multiple factors on a response, as well as the interaction between the factors.</li>
<li>Full factorial designs are commonly used in design of experiments (DOE) because they provide a lot of information about the system being studied. For example, if there are two factors being studied, each at two levels, a full factorial design would involve testing all four possible combinations of the factor levels. This allows researchers to see how the response changes as each factor is varied independently, as well as how the response changes when the factors are combined.</li>
<li>Full factorial designs can have more than two factors and more than two levels per factor. The number of treatment combinations in a full factorial design increases quickly as the number of factors and levels increases, so it is important to carefully plan the design to ensure that it is both practical and efficient.</li>
</ul>
<h3 id="multi-armed-bandit">Multi-armed bandit</h3>
<ul>
<li>A multi-armed bandit is a type of optimization problem that involves balancing the exploration of different options (the &ldquo;arms&rdquo; of the bandit) with the exploitation of the best option known so far. The goal is to maximize the reward over time by choosing the arm that is most likely to provide the highest reward at each step.</li>
<li>The multi-armed bandit problem is often used to model situations in which there is a trade-off between exploration and exploitation. For example, in online advertising, a website owner may need to choose which ads to display to a user. The website owner may not know which ad will be the most effective at converting the user into a customer, so they must balance the need to explore different ads with the need to exploit the most effective ad.</li>
<li>There are various algorithms that can be used to solve the multi-armed bandit problem, such as the epsilon-greedy algorithm and the upper confidence bound (UCB) algorithm. These algorithms use different approaches to balance exploration and exploitation, and can be modified to suit the specific needs of a given application.</li>
</ul>
<h3 id="response-surface">Response surface</h3>
<ul>
<li>Response surface methodology (RSM) is a statistical technique used to model and optimize the relationship between one or more input variables (also known as factors or independent variables) and an output variable (also known as the response). RSM involves designing experiments to study the response of a system to different levels of the input variables, and then fitting a mathematical model to the data to represent the relationship between the variables.</li>
<li>The response surface is the graphical representation of the response of the system as a function of the input variables. It is usually a two-dimensional plot showing the response as a function of two input variables, although it can also be a three-dimensional plot for systems with three or more input variables. The response surface can be used to identify the optimal combination of input variables that produce the desired response, as well as to understand the nature of the relationship between the variables.</li>
<li>RSM is commonly used in engineering and scientific research to optimize processes and products, and it can be applied to a wide range of systems and industries.</li>
</ul>
<h2 id="game-theory">Game Theory</h2>
<h3 id="cooperative-game-theory">Cooperative game theory</h3>
<ul>
<li>Cooperative game theory is a branch of game theory that studies situations in which multiple players can form coalitions and make binding agreements in order to achieve a common goal. In cooperative game theory, the players are assumed to be rational and to act in their own self-interest, but they are also able to communicate and make agreements with each other.</li>
<li>One important concept in cooperative game theory is the concept of the &ldquo;value&rdquo; of a game, which is the maximum payoff that can be achieved by the players if they cooperate. The value of a game can be determined using various solution concepts, such as the Shapley value, the nucleolus, and the core. These solution concepts provide a way to divide the value of the game among the players in a fair and stable way.</li>
<li>Cooperative game theory is used in a variety of fields, including economics, political science, and computer science. It is particularly useful for studying situations in which the players have conflicting interests, but may still be able to cooperate in order to achieve a mutually beneficial outcome.</li>
</ul>
<h3 id="game-theory-1">Game theory</h3>
<ul>
<li>Game theory is the study of mathematical models of strategic interactions between rational decision-makers. It has applications in a wide range of disciplines, including economics, political science, and psychology, as well as in biology and computer science.</li>
<li>In game theory, a &ldquo;game&rdquo; is defined as a situation in which multiple players, called &ldquo;players,&rdquo; have to make decisions that will affect the outcome of the game. Each player has a set of possible actions they can take, called a &ldquo;strategy,&rdquo; and a corresponding payoff that depends on the strategies chosen by all the players. The players are assumed to be rational and to act in their own self-interest, trying to maximize their payoff.</li>
<li>There are two main types of games in game theory: cooperative games and non-cooperative games. In cooperative games, the players can form coalitions and make binding agreements, while in non-cooperative games, the players act independently and cannot make agreements.</li>
<li>Game theory has been used to study a wide range of real-world situations, including auctions, negotiation, and voting systems. It has also been used to analyze strategic interactions in biology, such as predator-prey relationships and the evolution of social behavior.</li>
</ul>
<h3 id="mixed-strategyrandomized-strategy">Mixed strategy/randomized strategy</h3>
<ul>
<li>In game theory, a mixed strategy is a strategy in which a player randomly selects one of several pure strategies with a specified probability. A pure strategy is a strategy in which the player always chooses a particular action, while a mixed strategy allows the player to choose among several different actions with some probability.</li>
<li>Mixed strategies are often used to model situations in which a player has incomplete information about the other player&rsquo;s strategies or preferences, or in which the payoffs for each action are not fixed and may vary from one round of the game to the next.</li>
<li>In non-cooperative games, mixed strategies can be used to find a Nash equilibrium, which is a situation in which no player has an incentive to deviate from their current strategy given the strategies of the other players. In a Nash equilibrium, each player&rsquo;s mixed strategy is a best response to the mixed strategies of the other players.</li>
<li>Mixed strategies can also be used in cooperative games, although they are not always necessary to find a solution. In cooperative games, mixed strategies can be used to divide the value of the game among the players in a fair and stable way.</li>
</ul>
<h3 id="prisoners-dilemma">Prisoner&rsquo;s dilemma</h3>
<ul>
<li>The prisoner&rsquo;s dilemma is a classic example of a game used to illustrate the concept of game theory. It is a non-cooperative game that involves two players who must decide whether to cooperate with each other or to defect (i.e., not cooperate).</li>
<li>In the prisoner&rsquo;s dilemma, the players are assumed to be two prisoners who are being held in separate cells and are offered the following deal: if both prisoners defect, each one will serve a two-year prison sentence; if one defects and the other cooperates, the defector will go free while the cooperator will serve a three-year prison sentence; and if both cooperate, each one will serve a one-year prison sentence.</li>
<li>The prisoner&rsquo;s dilemma is interesting because the rational choice for each player, given the other player&rsquo;s choice, is to defect. However, if both players defect, they both end up with a worse outcome than if they had cooperated. This illustrates the concept of the &ldquo;prisoner&rsquo;s dilemma,&rdquo; in which individual rationality leads to a suboptimal outcome for both players.</li>
<li>The prisoner&rsquo;s dilemma has been used to model a wide range of real-world situations, including negotiations, international relations, and the evolution of social behavior.</li>
</ul>
<h3 id="pure-strategy">Pure strategy</h3>
<ul>
<li>In game theory, a pure strategy is a strategy in which a player always chooses a particular action. A pure strategy is contrasted with a mixed strategy, in which a player randomly selects one of several actions with a specified probability.</li>
<li>Pure strategies are often used to model situations in which a player has complete information about the other player&rsquo;s strategies or preferences, or in which the payoffs for each action are fixed and do not vary from one round of the game to the next.</li>
<li>In non-cooperative games, pure strategies can be used to find a Nash equilibrium, which is a situation in which no player has an incentive to deviate from their current strategy given the strategies of the other players. In a Nash equilibrium, each player&rsquo;s pure strategy is a best response to the pure strategies of the other players.</li>
<li>Pure strategies can also be used in cooperative games, although they are not always necessary to find a solution. In cooperative games, pure strategies can be used to divide the value of the game among the players in a fair and stable way.</li>
</ul>
<h3 id="sequential-game">Sequential game</h3>
<ul>
<li>A sequential game is a type of game in which the players take turns making decisions, and the actions of each player depend on the actions of the previous players. In a sequential game, the players have the opportunity to observe the actions of the other players before making their own decisions, which allows them to adjust their strategies based on the actions of the other players.</li>
<li>Sequential games can be either cooperative or non-cooperative. In cooperative sequential games, the players can communicate and make binding agreements with each other, while in non-cooperative sequential games, the players act independently and cannot make agreements.</li>
<li>There are several solution concepts that can be used to analyze sequential games, including the subgame perfect equilibrium, the backward induction solution, and the trembling hand perfect equilibrium. These solution concepts provide a way to predict the outcomes of sequential games and to understand the strategic interactions between the players.</li>
<li>Sequential games are often used to model real-world situations in which the players have the opportunity to observe and learn from each other&rsquo;s actions, such as in auctions and negotiations.</li>
</ul>
<h3 id="simultaneous-game">Simultaneous game</h3>
<ul>
<li>A simultaneous game is a type of game in which all of the players make their decisions at the same time, without knowing the decisions of the other players. In a simultaneous game, the players have to make their decisions based on their beliefs about the other players&rsquo; strategies or preferences, rather than on the actual actions of the other players.</li>
<li>Simultaneous games can be either cooperative or non-cooperative. In cooperative simultaneous games, the players can communicate and make binding agreements with each other, while in non-cooperative simultaneous games, the players act independently and cannot make agreements.</li>
<li>There are several solution concepts that can be used to analyze simultaneous games, including the Nash equilibrium, the correlated equilibrium, and the rationalizability concept. These solution concepts provide a way to predict the outcomes of simultaneous games and to understand the strategic interactions between the players.</li>
<li>Simultaneous games are often used to model real-world situations in which the players make their decisions simultaneously and do not have the opportunity to observe each other&rsquo;s actions, such as in auctions and political elections.</li>
</ul>
<h3 id="stable-equilibrium">Stable equilibrium</h3>
<ul>
<li>An equilibrium is a state in which no player has an incentive to change their behavior given the behavior of the other players. In game theory, an equilibrium is considered &ldquo;stable&rdquo; if it is the unique outcome of the game and if all the players are satisfied with the outcome.</li>
<li>There are several types of stable equilibria in game theory, including the Nash equilibrium, the correlated equilibrium, and the rationalizability concept. These solution concepts provide a way to predict the outcomes of games and to understand the strategic interactions between the players.</li>
<li>Stable equilibria are important because they provide a way to predict the behavior of players in strategic situations. They are often used to model real-world situations in which the players have conflicting interests and must make decisions that will affect the outcome of the game.</li>
<li>In order for an equilibrium to be stable, it must be the unique outcome of the game and all the players must be satisfied with the outcome. This means that if any player has an incentive to deviate from the equilibrium, the equilibrium is not stable.</li>
</ul>
<h3 id="zero-sum-game">Zero-sum game</h3>
<ul>
<li>A zero-sum game is a type of game in which the total gain or loss of the players is always zero. This means that the gain of one player is exactly balanced by the loss of the other player(s).</li>
<li>In a zero-sum game, the players are in direct competition with each other, and the outcome of the game depends on the relative skill of the players. If one player wins, the other player(s) must lose an equal amount.</li>
<li>Examples of zero-sum games include poker, chess, and the prisoner&rsquo;s dilemma. In these games, one player&rsquo;s gain is exactly offset by the other player&rsquo;s loss, so the total gain or loss of the players is always zero.</li>
<li>Zero-sum games are important in game theory because they provide a simple and well-defined framework for analyzing strategic interactions between players. They are also important in economics, where they are used to model situations in which the total resources available to the players are fixed and cannot be increased or decreased.</li>
</ul>
<h2 id="model-quality">Model Quality</h2>
<h3 id="akaike-information-criterion-aic">Akaike information criterion (AIC)</h3>
<ul>
<li>AIC stands for &ldquo;Akaike&rsquo;s Information Criterion.&rdquo; It is a statistical measure that is used to evaluate the quality of a statistical model. The AIC is based on the idea that the best model is the one that strikes the right balance between fit to the data and parsimony (i.e., simplicity).</li>
<li>The AIC is calculated as follows:</li>
</ul>
<pre tabindex="0"><code>AIC = 2k - 2ln(L)
</code></pre><p>where k is the number of parameters in the model and L is the maximum likelihood of the model. The AIC is a measure of the relative quality of a model, with lower values indicating a better model.</p>
<ul>
<li>The AIC is often used in model selection, where it is used to compare the relative quality of different models. It can also be used to compare the quality of nested models, where one model is a special case of another model.</li>
<li>The AIC is widely used in statistics and is particularly useful for comparing models with different numbers of parameters. It has been applied in a wide range of fields, including economics, engineering, and the natural sciences.</li>
</ul>
<h3 id="bayesian-information-criterion-bic">Bayesian Information criterion (BIC)</h3>
<ul>
<li>The Bayesian Information Criterion (BIC) is a statistical measure that is used to evaluate the quality of a statistical model. It is based on the idea that the best model is the one that strikes the right balance between fit to the data and parsimony (i.e., simplicity).</li>
<li>The BIC is calculated as follows:</li>
</ul>
<pre tabindex="0"><code>BIC = kln(n) - 2ln(L)
</code></pre><p>where k is the number of parameters in the model, n is the number of data points, and L is the maximum likelihood of the model. The BIC is a measure of the relative quality of a model, with lower values indicating a better model.</p>
<ul>
<li>The BIC is often used in model selection, where it is used to compare the relative quality of different models. It can also be used to compare the quality of nested models, where one model is a special case of another model.</li>
<li>The BIC is widely used in statistics and is particularly useful for comparing models with different numbers of parameters. It has been applied in a wide range of fields, including economics, engineering, and the natural sciences.</li>
</ul>
<h3 id="causation">Causation</h3>
<ul>
<li>Causation refers to the relationship between an event (the cause) and a second event (the effect), where the second event is the result of the first. In order for an event to be considered the cause of another event, it must be shown that there is a clear link between the two events and that the first event directly led to the second event.</li>
<li>There are several factors that are often used to establish causation, including the following:
<ul>
<li>Temporal precedence: The cause must occur before the effect.</li>
<li>Covariation: The cause and effect must vary together.</li>
<li>Control: When other variables are controlled for, the cause and effect should still be related.</li>
<li>Plausibility: The proposed cause must be scientifically plausible.</li>
</ul>
</li>
<li>Establishing causation can be challenging, particularly in complex systems where there may be multiple potential causes and it is difficult to control for all other variables. In these cases, it is often necessary to use statistical methods to assess the strength of the relationship between the cause and effect.</li>
</ul>
<h3 id="corrected-aic">Corrected AIC</h3>
<ul>
<li>Corrected AIC, also known as AICc, is a variant of Akaike&rsquo;s Information Criterion (AIC) that is used to evaluate the quality of a statistical model. Like the AIC, the AICc is based on the idea that the best model is the one that strikes the right balance between fit to the data and parsimony (i.e., simplicity).</li>
<li>The AICc is calculated as follows:</li>
</ul>
<pre tabindex="0"><code>AICc = AIC + (2k(k + 1)) / (n - k - 1)
</code></pre><p>where k is the number of parameters in the model, n is the number of data points, and AIC is Akaike&rsquo;s Information Criterion. The AICc is a measure of the relative quality of a model, with lower values indicating a better model.</p>
<ul>
<li>The AICc is often used in model selection, where it is used to compare the relative quality of different models. It is particularly useful for comparing models with small sample sizes, as it adjusts for the bias that can occur when using the AIC with small sample sizes.</li>
<li>The AICc is widely used in statistics and has been applied in a wide range of fields, including economics, engineering, and the natural sciences.</li>
</ul>
<h3 id="correlation">Correlation</h3>
<ul>
<li>Correlation is a statistical measure of the relationship between two variables. It is a way to describe the degree to which two variables are related to each other.</li>
<li>The correlation between two variables is usually represented by the correlation coefficient, which can range from -1 to 1. A correlation coefficient of -1 indicates a perfect negative correlation, meaning that as one variable increases, the other decreases. A correlation coefficient of 1 indicates a perfect positive correlation, meaning that as one variable increases, the other also increases. A correlation coefficient of 0 indicates no correlation.</li>
<li>Correlation does not imply causation, meaning that the presence of a correlation between two variables does not necessarily mean that one variable is causing the other. It is possible for two variables to be correlated without there being a causal relationship between them.</li>
<li>Correlation is an important statistical concept that is used in a wide range of fields, including economics, psychology, and the natural sciences. It is often used to understand the relationship between different variables and to predict future outcomes.</li>
</ul>
<h3 id="cross-validation">Cross-validation</h3>
<ul>
<li>Cross-validation is a method used to evaluate the performance of a statistical model. It involves dividing the data into a training set, which is used to train the model, and a test set, which is used to evaluate the model.</li>
<li>There are several types of cross-validation, including the following:
<ul>
<li>K-fold cross-validation: The data is divided into k folds, and the model is trained on k-1 folds and tested on the remaining fold. This process is repeated k times, with a different fold being used as the test set each time.</li>
<li>Leave-one-out cross-validation: The model is trained on all but one data point, and then tested on the left-out data point. This process is repeated for each data point, resulting in a model being trained and tested n times, where n is the number of data points.</li>
<li>Stratified cross-validation: The data is divided into folds such that the proportions of different classes in the folds are similar to the proportions in the entire dataset. This is useful when the classes are imbalanced.</li>
</ul>
</li>
<li>Cross-validation is a useful tool for evaluating the performance of a statistical model and for selecting the best model for a given dataset. It helps to ensure that the model is not overfitted to the training data and that it generalizes well to unseen data.</li>
</ul>
<h3 id="hypothesis-test">Hypothesis test</h3>
<ul>
<li>A hypothesis test is a statistical procedure used to test whether a hypothesis about a population parameter is true or false. It involves collecting data from a sample and using it to make a decision about the hypothesis.</li>
<li>The process of conducting a hypothesis test usually involves the following steps:
<ul>
<li>State the null hypothesis and the alternative hypothesis. The null hypothesis is the assumption that there is no relationship between the variables being tested, while the alternative hypothesis is the assumption that there is a relationship.</li>
<li>Select a sample and collect data. The sample should be representative of the population being studied.</li>
<li>Choose a test statistic and a critical value. The test statistic is a measure of the difference between the sample and the null hypothesis, while the critical value is a predetermined threshold that is used to decide whether to reject or accept the null hypothesis.</li>
<li>Calculate the p-value. The p-value is the probability of obtaining a test statistic as extreme as the one observed, given that the null hypothesis is true.</li>
<li>Make a decision. If the p-value is less than the critical value, the null hypothesis is rejected in favor of the alternative hypothesis. If the p-value is greater than the critical value, the null hypothesis is not rejected.</li>
</ul>
</li>
<li>Hypothesis tests are an important tool for making decisions about statistical relationships and are widely used in a variety of fields, including psychology, economics, and the natural sciences.</li>
</ul>
<h3 id="k-fold-cross-validation">k-fold cross-validation</h3>
<ul>
<li>K-fold cross-validation is a method used to evaluate the performance of a statistical model. It involves dividing the data into k folds (also known as &ldquo;subsets&rdquo;) and training the model k times, each time using a different fold as the test set and the remaining folds as the training set. The performance of the model is then averaged across the k iterations.</li>
<li>For example, in 5-fold cross-validation, the data is divided into 5 folds, and the model is trained and tested 5 times. Each time, a different fold is used as the test set, and the model is trained on the other 4 folds. The performance of the model is then averaged across the 5 iterations.</li>
<li>K-fold cross-validation is a useful tool for evaluating the performance of a model and for selecting the best model for a given dataset. It helps to ensure that the model is not overfitted to the training data and that it generalizes well to unseen data.</li>
<li>K-fold cross-validation is a widely used method in machine learning and is particularly useful for small datasets, where it can provide a more reliable estimate of model performance than other methods.</li>
</ul>
<h3 id="likelihood">Likelihood</h3>
<ul>
<li>In statistics, the likelihood of a model is a measure of how well the model fits the data. It is defined as the probability of observing the data given the model and a set of parameters.</li>
<li>The likelihood is often used to compare the fit of different models to the same data. A higher likelihood indicates a better fit, while a lower likelihood indicates a poorer fit.</li>
<li>The likelihood is often used in maximum likelihood estimation, a method used to estimate the parameters of a statistical model. In maximum likelihood estimation, the parameters of the model are chosen to maximize the likelihood of the model given the data.</li>
<li>The likelihood is an important concept in statistics that is used in a wide range of applications, including hypothesis testing, model selection, and statistical inference. It provides a way to evaluate the fit of a model to the data and to compare the fit of different models to the same data.</li>
</ul>
<h3 id="maximum-likelihood">Maximum likelihood</h3>
<ul>
<li>Maximum likelihood is a method used to estimate the parameters of a statistical model. It is based on the idea of finding the set of parameters that maximize the likelihood of the model given the data.</li>
<li>The likelihood of a model is a measure of how well the model fits the data. It is defined as the probability of observing the data given the model and a set of parameters. In maximum likelihood estimation, the parameters of the model are chosen to maximize the likelihood of the model given the data.</li>
<li>Maximum likelihood estimation has several desirable properties, including being asymptotically efficient (i.e., the estimators converge to the true values as the sample size increases) and being relatively easy to implement. It is widely used in a variety of fields, including economics, psychology, and the natural sciences.</li>
<li>Maximum likelihood estimation is often used in conjunction with other statistical methods, such as hypothesis testing and model selection, to make inferences about the underlying population from which the data were collected.</li>
</ul>
<h3 id="missing-data">Missing data</h3>
<ul>
<li>Missing data refers to data that is not available or that has not been collected. It is a common problem in statistical analysis and can occur for a variety of reasons, including errors in data collection, missing values in the data, and data that is not recorded.</li>
<li>Missing data can be a problem because it can bias the results of statistical analyses. For example, if the missing data is not randomly distributed, it can lead to sampling bias and affect the validity of the conclusions.</li>
<li>There are several approaches for dealing with missing data, including the following:
<ul>
<li>Complete case analysis: This involves removing any cases with missing data from the analysis. This is the simplest approach, but it can lead to biased results if the missing data is not missing at random.</li>
<li>Imputation: This involves replacing the missing values with estimates based on the available data. There are several methods for imputing missing data, including mean imputation, regression imputation, and multiple imputation.</li>
<li>Maximum likelihood: This involves using a statistical model to estimate the missing data based on the observed data.</li>
</ul>
</li>
<li>The best approach for dealing with missing data depends on the nature of the missing data and the goals of the analysis. It is important to carefully consider the implications of missing data and choose an appropriate approach to ensure the validity of the results.</li>
</ul>
<h3 id="random-effects">Random effects</h3>
<ul>
<li>In statistics, a random effect is a variable that is included in a statistical model to account for the fact that the data is a sample from a larger population. Random effects are used to model the variability between different groups or individuals in the population.</li>
<li>For example, consider a study that aims to investigate the relationship between diet and blood pressure. In this study, the researchers might collect data from several different groups of people, such as men and women, or people from different countries. If the researchers want to account for the fact that the data is a sample from a larger population, they might include a random effect for group in their statistical model. This would allow them to estimate the average effect of diet on blood pressure within each group, as well as the overall effect across all groups.</li>
<li>Random effects are often used in mixed-effects models, which are used to analyze data that has both fixed and random effects. They are an important tool for understanding the sources of variability in data and for making inferences about the population from which the data were collected.</li>
</ul>
<h3 id="real-effects">Real effects</h3>
<ul>
<li>In statistics, a real effect is a variable that is included in a statistical model to represent an underlying relationship or effect that is believed to exist in the population. Real effects are often used to test hypotheses about the relationships between variables and to estimate the strength and direction of those relationships.</li>
<li>For example, consider a study that aims to investigate the relationship between diet and blood pressure. In this study, the researchers might collect data from a sample of people and include a real effect for diet in their statistical model. This would allow them to estimate the average effect of diet on blood pressure in the population and to test whether this effect is statistically significant.</li>
<li>Real effects are often contrasted with random effects, which are used to account for the fact that the data is a sample from a larger population. While real effects represent underlying relationships in the population, random effects represent the variability between different groups or individuals in the population.</li>
</ul>
<h3 id="sum-of-squared-errors">Sum-of-squared errors</h3>
<ul>
<li>The sum of squared errors (SSE) is a measure of the deviation of a set of values from a predicted value. It is often used in statistical analysis to evaluate the fit of a model to a set of data.</li>
<li>The SSE is calculated as follows:</li>
</ul>
<pre tabindex="0"><code>SSE = ∑(observed value - predicted value)^2
</code></pre><p>where the sum is taken over all the data points.</p>
<ul>
<li>The SSE is a measure of the sum of the squared differences between the observed values and the predicted values. It is a common measure of the error or deviation of a set of values from a predicted value, and it is often used to compare the fit of different models to the same data.</li>
<li>In general, a smaller SSE indicates a better fit of the model to the data, while a larger SSE indicates a poorer fit. The SSE is often used in conjunction with other measures of fit, such as the coefficient of determination (R^2), to evaluate the quality of a statistical model.</li>
</ul>
<h3 id="test-datatest-set">Test data/test set</h3>
<ul>
<li>A test set is a set of data that is used to evaluate the performance of a statistical model. It is separate from the training set, which is used to fit the model, and is used to assess how well the model generalizes to new, unseen data.</li>
<li>The test set is often used to estimate the accuracy of the model, as well as other performance metrics such as precision, recall, and F1 score. It is a crucial step in the model development process, as it allows the model to be evaluated on data that it has not seen before and provides a way to assess the generalizability of the model.</li>
<li>The test set is usually chosen to be representative of the data that the model will encounter in real-world use. It is important to ensure that the test set is independent of the training set and that it is not used in any way to fit the model.</li>
<li>The test set is an important tool for evaluating the performance of a statistical model and for comparing the performance of different models. It is widely used in a variety of fields, including machine learning, data mining, and statistical analysis.</li>
</ul>
<h3 id="training-datatraining-set">Training data/training set</h3>
<ul>
<li>The training data or training set is a set of data that is used to fit a statistical model. It is used to learn the parameters of the model and to improve the model&rsquo;s ability to make predictions on new, unseen data.</li>
<li>The training set is usually a subset of the total dataset and is chosen to be representative of the data that the model will encounter in real-world use. It is important to ensure that the training set is representative of the data that the model will encounter in order to improve the model&rsquo;s ability to generalize to new data.</li>
<li>The training set is used to fit the model by adjusting the model&rsquo;s parameters to minimize the error between the predicted values and the observed values. Once the model has been trained on the training set, it can be evaluated on a separate test set to assess its performance on new data.</li>
<li>The training set is an important tool for building and evaluating statistical models and is widely used in a variety of fields, including machine learning, data mining, and statistical analysis.</li>
</ul>
<h3 id="validation-datavalidation-set">Validation data/validation set</h3>
<ul>
<li>The validation data or validation set is a set of data that is used to evaluate the performance of a statistical model. It is used to tune the model&rsquo;s hyperparameters and to select the best model among a set of candidates.</li>
<li>The validation set is usually a subset of the total dataset and is used to assess the model&rsquo;s ability to generalize to new, unseen data. It is important to ensure that the validation set is independent of the training set and is not used to fit the model in any way.</li>
<li>The validation set is used to compare the performance of different models and to select the best model based on a predetermined criterion, such as the accuracy of the model or the Akaike Information Criterion (AIC). Once the best model has been selected, it can be evaluated on a separate test set to assess its performance on new data.</li>
<li>The validation set is an important tool for building and evaluating statistical models and is widely used in a variety of fields, including machine learning, data mining, and statistical analysis.</li>
</ul>
<h2 id="non-parametric-tests">Non-Parametric Tests</h2>
<h3 id="mann-whitney-test">Mann-Whitney test</h3>
<ul>
<li>The Mann-Whitney test is a nonparametric statistical test used to compare the means of two independent samples. It is used when the data is not normally distributed or when the variances of the two samples are not equal.</li>
<li>The Mann-Whitney test is based on the ranks of the data rather than the raw data values. It involves ranking the data from the two samples and comparing the ranks of the observations from the two samples.</li>
<li>The Mann-Whitney test is used to test the hypothesis that the two samples come from the same population. If the null hypothesis is rejected, it indicates that there is a statistically significant difference between the means of the two samples.</li>
<li>The Mann-Whitney test is a widely used statistical test and is particularly useful when the assumptions of other tests, such as the t-test, are not met. It is an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn.</li>
</ul>
<h3 id="mcnemars-test">McNemar&rsquo;s test</h3>
<ul>
<li>McNemar&rsquo;s test is a statistical test used to compare the proportions of two dependent samples. It is used when the data is in the form of pairs, such as before and after measurements on the same group of individuals.</li>
<li>The McNemar&rsquo;s test is used to test the hypothesis that the proportions of the two samples are equal. It is based on the difference between the two proportions and is used to determine whether the difference is statistically significant.</li>
<li>The McNemar&rsquo;s test is a nonparametric test, which means that it does not assume that the data follows a specific distribution. It is often used when the assumptions of other tests, such as the chi-squared test, are not met.</li>
<li>The McNemar&rsquo;s test is an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. It is widely used in a variety of fields, including psychology, medicine, and the social sciences.</li>
</ul>
<h3 id="nonparametric-test">Nonparametric test</h3>
<ul>
<li>A nonparametric test is a statistical test that does not assume that the data follows a specific distribution. Nonparametric tests are often used when the assumptions of parametric tests, such as the t-test or the ANOVA test, are not met or when the sample size is too small to make such assumptions.</li>
<li>Nonparametric tests are based on the ranks or the frequencies of the data rather than the raw data values. They are often used to compare the means or proportions of two or more groups or to test for associations between variables.</li>
<li>Some examples of nonparametric tests include the Mann-Whitney test, the Wilcoxon signed-rank test, the Kruskal-Wallis test, the chi-squared test, and the McNemar&rsquo;s test.</li>
<li>Nonparametric tests are an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. They are widely used in a variety of fields, including psychology, medicine, and the social sciences.</li>
</ul>
<h3 id="paired-samples">Paired samples</h3>
<ul>
<li>Paired samples are two sets of measurements that are taken on the same group of individuals or units. Paired samples are often used in statistical analysis to compare the means or proportions of the two samples and to test for statistical significance.</li>
<li>Paired samples are often used when the two samples are dependent, meaning that the measurements in one sample are related to the measurements in the other sample. For example, paired samples might be used to compare the scores of the same group of individuals on two different tests, or to compare the blood pressure of the same group of individuals before and after a treatment.</li>
<li>Paired samples can be analyzed using parametric or nonparametric statistical tests, depending on the assumptions of the data. Some examples of statistical tests for paired samples include the paired t-test, the Wilcoxon signed-rank test, and the McNemar&rsquo;s test.</li>
<li>Paired samples are an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. They are widely used in a variety of fields, including psychology, medicine, and the social sciences.</li>
</ul>
<h3 id="parametric-test">Parametric test</h3>
<ul>
<li>A parametric test is a statistical test that assumes that the data follows a specific distribution, such as the normal distribution. Parametric tests are based on the parameters of the distribution and are used to test hypotheses about the population means or proportions.</li>
<li>Parametric tests are often more powerful than nonparametric tests, which means that they can detect smaller differences between the samples. However, they are also more sensitive to violations of the assumptions of the test, such as normality and homoscedasticity.</li>
<li>Some examples of parametric tests include the t-test, the ANOVA test, and the linear regression model.</li>
<li>Parametric tests are an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. They are widely used in a variety of fields, including psychology, medicine, and the social sciences.</li>
</ul>
<h3 id="wilcoxon-signed-rank-test-one-sample">Wilcoxon signed rank test (one sample)</h3>
<ul>
<li>The Wilcoxon signed-rank test is a nonparametric statistical test used to compare the median of a single sample to a hypothesized value. It is used when the data are not normally distributed or when the sample size is small.</li>
<li>The Wilcoxon signed-rank test is based on the ranks of the differences between the observations and the hypothesized value. It involves ranking the differences and testing the hypothesis that the median of the ranked differences is equal to zero.</li>
<li>The Wilcoxon signed-rank test is used to test the hypothesis that the median of the sample is equal to the hypothesized value. If the null hypothesis is rejected, it indicates that there is a statistically significant difference between the median of the sample and the hypothesized value.</li>
<li>The Wilcoxon signed-rank test is an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. It is widely used in a variety of fields, including psychology, medicine, and the social sciences.</li>
</ul>
<h3 id="wilcoxon-signed-rank-test">Wilcoxon signed rank test</h3>
<ul>
<li>The Wilcoxon signed-rank test is a nonparametric statistical test used to compare the means of two related or dependent samples. It is used when the data are not normally distributed or when the variances of the two samples are not equal.</li>
<li>The Wilcoxon signed-rank test is based on the ranks of the differences between the observations in the two samples. It involves ranking the differences and testing the hypothesis that the median of the ranked differences is equal to zero.</li>
<li>The Wilcoxon signed-rank test is used to test the hypothesis that the means of the two samples are equal. If the null hypothesis is rejected, it indicates that there is a statistically significant difference between the means of the two samples.</li>
<li>The Wilcoxon signed-rank test is an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. It is widely used in a variety of fields, including psychology, medicine, and the social sciences.</li>
</ul>
<h2 id="optimization">Optimization</h2>
<h3 id="approximate-dynamic-program">Approximate dynamic program</h3>
<h3 id="arc">Arc</h3>
<h3 id="assignment-problem">Assignment problem</h3>
<h3 id="bellmans-equation">Bellman&rsquo;s equation</h3>
<h3 id="binary-integer-program">Binary integer program</h3>
<h3 id="binary-variable">Binary variable</h3>
<h3 id="chance-constraint">Chance constraint</h3>
<h3 id="clique">Clique</h3>
<h3 id="concave-function">Concave function</h3>
<h3 id="constant">Constant</h3>
<h3 id="constraint">Constraint</h3>
<h3 id="convex-function">Convex function</h3>
<h3 id="convex-optimization-model">Convex optimization model</h3>
<h3 id="convex-quadratic-function">Convex quadratic function</h3>
<h3 id="convex-quadratic-program">Convex quadratic program</h3>
<h3 id="convex-set">Convex set</h3>
<h3 id="decision">Decision</h3>
<h3 id="diet-problem">Diet problem</h3>
<h3 id="dynamic-programming">Dynamic programming</h3>
<h3 id="edge">Edge</h3>
<h3 id="feasible-solution">Feasible solution</h3>
<h3 id="fixed-charge">Fixed charge</h3>
<h3 id="flow">Flow</h3>
<h3 id="global-optimummaximumminimum">Global optimum/maximum/minimum</h3>
<h3 id="graph">Graph</h3>
<h3 id="greedy-algorithm">Greedy algorithm</h3>
<h3 id="improving-direction">Improving direction</h3>
<h3 id="initialization">Initialization</h3>
<h3 id="integer-program">Integer program</h3>
<h3 id="iterate">Iterate</h3>
<h3 id="linear-equation">Linear equation</h3>
<h3 id="linear-function">Linear function</h3>
<h3 id="linear-inequality">Linear inequality</h3>
<h3 id="linear-program">Linear program</h3>
<h3 id="local-optimummaximumminimum">Local optimum/maximum/minimum</h3>
<h3 id="louvain-algorithm">Louvain algorithm</h3>
<h3 id="markov-decision-process">Markov decision process</h3>
<h3 id="mathematical-programming">Mathematical programming</h3>
<h3 id="maximization-problem">Maximization problem</h3>
<h3 id="maximum-flow-problem">Maximum flow problem</h3>
<h3 id="minimization-problem">Minimization problem</h3>
<h3 id="modularity">Modularity</h3>
<h3 id="most-optimal">Most optimal</h3>
<h3 id="network">Network</h3>
<h3 id="network-optimization-problem">Network optimization problem</h3>
<h3 id="node">Node</h3>
<h3 id="non-convex-program">Non-convex program</h3>
<h3 id="non-negativity-constraints">Non-negativity constraints</h3>
<h3 id="objective-function">Objective function</h3>
<h3 id="optimal">Optimal</h3>
<h3 id="optimal-solution">Optimal solution</h3>
<h3 id="optimization-1">Optimization</h3>
<h3 id="robust-solution">Robust solution</h3>
<h3 id="scenario">Scenario</h3>
<h3 id="shortest-path-problem">Shortest path problem</h3>
<h3 id="solution-in-the-optimization-sense">Solution (in the optimization sense)</h3>
<h3 id="state">State</h3>
<h3 id="step-size">Step size</h3>
<h3 id="stochastic-dynamic-program">Stochastic dynamic program</h3>
<h3 id="stochastic-optimization">Stochastic optimization</h3>
<h3 id="uncertainty">Uncertainty</h3>
<h3 id="variable-optimization-sense">Variable (optimization sense)</h3>
<h3 id="variable-statistics-sense">Variable (statistics sense)</h3>
<h3 id="vertex">Vertex</h3>
<h2 id="probability-based-models">Probability based models</h2>
<h3 id="action">Action</h3>
<h3 id="arrival-rate">Arrival rate</h3>
<h3 id="balking">Balking</h3>
<h3 id="bayes-theorembayes-rule">Bayes&rsquo; theorem/Bayes&rsquo; rule</h3>
<h3 id="continuous-time-simulation">Continuous-time simulation</h3>
<h3 id="decision-point">Decision point</h3>
<h3 id="deterministic-simulation">Deterministic simulation</h3>
<h3 id="discrete-event-simulation">Discrete-event simulation</h3>
<h3 id="empirical-bayes-model">Empirical Bayes model</h3>
<h3 id="entity">Entity</h3>
<h3 id="fifo">FIFO</h3>
<h3 id="interarrival-time">Interarrival time</h3>
<h3 id="kendall-notation">Kendall notation</h3>
<h3 id="lifo">LIFO</h3>
<h3 id="markov-chain">Markov chain</h3>
<h3 id="memoryless-markov-chain">Memoryless (Markov chain)</h3>
<h3 id="module">Module</h3>
<h3 id="queue">Queue</h3>
<h3 id="queuing">Queuing</h3>
<h3 id="replication">Replication</h3>
<h3 id="resource">Resource</h3>
<h3 id="service-rate">Service rate</h3>
<h3 id="simulation">Simulation</h3>
<h3 id="steady-state">Steady state</h3>
<h3 id="stochastic-simulation">Stochastic simulation</h3>
<h3 id="transition-matrix">Transition matrix</h3>
<h3 id="transition-probability">Transition probability</h3>
<h3 id="validation-of-simulation">Validation (of simulation)</h3>
<h2 id="probability-distributions">Probability distributions</h2>
<h3 id="bernoulli-distribution">Bernoulli distribution</h3>
<h3 id="bias">Bias</h3>
<h3 id="binomial-distribution">Binomial distribution</h3>
<h3 id="distribution-fitting">Distribution-fitting</h3>
<h3 id="exponential-distribution">Exponential distribution</h3>
<h3 id="geometric-distribution">Geometric distribution</h3>
<h3 id="iid">iid</h3>
<h3 id="independent">Independent</h3>
<h3 id="independent-and-identically-distributed-iid">Independent and identically distributed (iid)</h3>
<h3 id="lower-tail">Lower tail</h3>
<h3 id="memoryless-distribution">Memoryless (distribution)</h3>
<h3 id="normal-distribution">Normal distribution</h3>
<h3 id="poisson-distribution">Poisson distribution</h3>
<h3 id="q-q-plot">Q-Q plot</h3>
<h3 id="tails">Tail(s)</h3>
<h3 id="upper-tail">Upper tail</h3>
<h3 id="weibull-distribution">Weibull distribution</h3>
<h2 id="regression">Regression</h2>
<h3 id="adjusted-r-squaredadjusted-r2">Adjusted R-squared/Adjusted R2</h3>
<h3 id="area-under-curveauc">Area under curve/AUC</h3>
<h3 id="bayesian-regression">Bayesian regression</h3>
<h3 id="box-cox-transformation">Box-Cox transformation</h3>
<h3 id="branching">Branching</h3>
<h3 id="cart">CART</h3>
<h3 id="classification-tree">Classification tree</h3>
<h3 id="concordance-index">Concordance index</h3>
<h3 id="decision-tree">Decision tree</h3>
<h3 id="earth">Earth</h3>
<h3 id="elastic-net">Elastic net</h3>
<h3 id="forest">Forest</h3>
<h3 id="interaction-term">Interaction term</h3>
<h3 id="𝑘-nearest-neighbor-regression">𝑘-Nearest-Neighbor regression</h3>
<h3 id="knot">Knot</h3>
<h3 id="lassolasso-regression">Lasso/Lasso regression</h3>
<h3 id="leaf">Leaf</h3>
<h3 id="linear-regression">Linear regression</h3>
<h3 id="logistic-regression">Logistic regression</h3>
<h3 id="logit-model">Logit model</h3>
<h3 id="mars">MARS</h3>
<h3 id="multi-adaptive-regression-splines-mars">Multi-adaptive regression splines (MARS)</h3>
<h3 id="p-value">p-value</h3>
<h3 id="p-value-fishing">p-value fishing</h3>
<h3 id="poisson-regression">Poisson regression</h3>
<h3 id="pruning">Pruning</h3>
<h3 id="pseudo-r-squaredpseudo-r2">Pseudo-R-squared/Pseudo-R2</h3>
<h3 id="r-squaredr2">R-squared/R2</h3>
<h3 id="random-forest">Random forest</h3>
<h3 id="receiver-operating-characteristic-curve-roc-curve">Receiver operating characteristic curve (ROC curve)</h3>
<h3 id="regression-1">Regression</h3>
<h3 id="regression-splines">Regression splines</h3>
<h3 id="regression-tree">Regression tree</h3>
<h3 id="ridge-regression">Ridge regression</h3>
<h3 id="roc-curve">ROC curve</h3>
<h3 id="root">Root</h3>
<h3 id="spline-regression">Spline regression</h3>
<h3 id="transformation">Transformation</h3>
<h3 id="tree">Tree</h3>
<h2 id="time-series-models">Time series models</h2>
<h3 id="additive-seasonality">Additive seasonality</h3>
<h3 id="arima">ARIMA</h3>
<h3 id="autoregression">Autoregression</h3>
<h3 id="autoregressive-integrated-moving-average-arima">Autoregressive integrated moving average (ARIMA)</h3>
<h3 id="differencing">Differencing</h3>
<h3 id="double-exponential">Double exponential</h3>
<h3 id="smoothing">smoothing</h3>
<h3 id="exponential-smoothing">Exponential smoothing</h3>
<h3 id="garch">GARCH</h3>
<h3 id="generalized-autoregressive-conditional-heteroscedasticity-garch">Generalized autoregressive conditional heteroscedasticity (GARCH)</h3>
<h3 id="holt-winters-method">Holt-Winters method</h3>
<h3 id="moving-average">Moving average</h3>
<h3 id="multiplicative-seasonality">Multiplicative seasonality</h3>
<h3 id="seasonalitycycles">Seasonality/cycles</h3>
<h3 id="seasonality-lengthcycle-length">Seasonality length/cycle length</h3>
<h3 id="single-exponential-smoothing">Single exponential smoothing</h3>
<h3 id="smoothing-1">Smoothing</h3>
<h3 id="smoothing-constant">Smoothing constant</h3>
<h3 id="stationary-process">Stationary process</h3>
<h3 id="trend">Trend</h3>
<h3 id="triple-exponential-smoothing">Triple exponential smoothing</h3>
<h3 id="winters-method">Winters&rsquo; method</h3>
<h2 id="variable-selection">Variable Selection</h2>
<h3 id="backward-elimination">Backward elimination</h3>
<h3 id="elastic-net-1">Elastic net</h3>
<h3 id="forward-selection">Forward selection</h3>
<h3 id="lassolasso-regression-1">Lasso/Lasso regression</h3>
<h3 id="overfitting">Overfitting</h3>
<h3 id="regularization">Regularization</h3>
<h3 id="ridge-regression-1">Ridge regression</h3>
<h3 id="simplicity-of-a-model">Simplicity (of a model)</h3>
<h3 id="stepwise-regression">Stepwise regression</h3>
<h3 id="variable-selection-1">Variable selection</h3>
<h2 id="misc">Misc</h2>
<h3 id="1-norm">1-norm</h3>
<h3 id="2-norm">2-norm</h3>
<h3 id="convex-hull-of-a-set-of-points">Convex hull (of a set of points)</h3>
<h3 id="descriptive-analytics">Descriptive analytics</h3>
<h3 id="distance">Distance</h3>
<h3 id="elbow-diagram">Elbow diagram</h3>
<h3 id="error-per-data-point">Error (per data point)</h3>
<h3 id="error-total-over-data-set">Error (total over data set)</h3>
<h3 id="euclidian-distancestraight--line-distance">Euclidian distance/straight- line distance</h3>
<h3 id="fitting">Fitting</h3>
<h3 id="heteroscedasticity">Heteroscedasticity</h3>
<h3 id="infinity-norm">Infinity-norm</h3>
<h3 id="linear-combination">Linear combination</h3>
<h3 id="manhattan-distance">Manhattan distance</h3>
<h3 id="minkowski-distance-of-order-𝑝">Minkowski distance (of order 𝑝)</h3>
<h3 id="model-mathematical">Model (mathematical)</h3>
<h3 id="multiplier">Multiplier</h3>
<h3 id="normdistance-norm">Norm/distance norm</h3>
<h3 id="order-of-magnitude">Order of magnitude</h3>
<h3 id="orthogonal">Orthogonal</h3>
<h3 id="outlier">Outlier</h3>
<h3 id="overfitting-1">Overfitting</h3>
<h3 id="𝑝-norm">𝑝-norm</h3>
<h3 id="parameter">Parameter</h3>
<h3 id="perturbation">Perturbation</h3>
<h3 id="prediction">Prediction</h3>
<h3 id="predictive-analytics">Predictive analytics</h3>
<h3 id="prescriptive-analytics">Prescriptive analytics</h3>
<h3 id="rectilinear-distance">Rectilinear distance</h3>
<h3 id="threshold">Threshold</h3>
<h3 id="transformation-1">Transformation</h3>
</div>
	</section>

</article>

		</main>
		<aside role="contentinfo"
			class="w-full md:w-2/5 xl:w-1/2 md:pr-12 lg:pr-20 xl:pr-24 order-4 md:order-3 md:sticky md:bottom-0 self-end max-w-2xl">
			<div class="md:float-right md:text-right leading-loose tracking-tight md:mb-2">
				
	<div class="md:max-w-xs  flex flex-col md:items-end">
	<ul class="font-serif flex-grow-0 flex justify-between flex-wrap md:flex-col">
	
	
	<li class="px-1 md:px-0">
		<a href="/posts/" title="Posts page" 
			class="font-medium text-medium-red-violet-600 hover:text-medium-red-violet-400" >
			Posts
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/resume/" title="Resume page" >
			Resume
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/certifications/" title="Certifications page" >
			Certifications
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/publications/" title="Publications page" >
			Publications
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/tags/" title="Tags page" >
			Tags
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/categories/" title="Categories page" >
			Categories
		</a>
	</li>
	
	
	
	
	<div id="fastSearch" class="m-0">
		<input id="searchInput" type="text" size=10 
			class="bg-gray-100 focus:outline-none border-b border-gray-100 focus:border-eucalyptus-300 md:text-right
			placeholder-java-500 min-w-0 max-w-xxxs"
			placeholder="search" />
		<ul id="searchResults" class="bg-gray-200 px-2 divide-y divide-gray-400">
		</ul>
	</div>
	
</ul>
	

<div class="flex flex-wrap-reverse md:justify-end content-end md:content-start justify-start items-start md:flex-col  max-h-16">
	
	<a href='https://github.com/ayushsubedi' target="_blank" class="github icon pl-1 text-eucalyptus-400 hover:text-java-400" title="github link" rel="noopener"
		aria-label="follow on github——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path fill-rule="nonzero" d="M5.883 18.653c-.3-.2-.558-.455-.86-.816a50.32 50.32 0 0 1-.466-.579c-.463-.575-.755-.84-1.057-.949a1 1 0 0 1 .676-1.883c.752.27 1.261.735 1.947 1.588-.094-.117.34.427.433.539.19.227.33.365.44.438.204.137.587.196 1.15.14.023-.382.094-.753.202-1.095C5.38 15.31 3.7 13.396 3.7 9.64c0-1.24.37-2.356 1.058-3.292-.218-.894-.185-1.975.302-3.192a1 1 0 0 1 .63-.582c.081-.024.127-.035.208-.047.803-.123 1.937.17 3.415 1.096A11.731 11.731 0 0 1 12 3.315c.912 0 1.818.104 2.684.308 1.477-.933 2.613-1.226 3.422-1.096.085.013.157.03.218.05a1 1 0 0 1 .616.58c.487 1.216.52 2.297.302 3.19.691.936 1.058 2.045 1.058 3.293 0 3.757-1.674 5.665-4.642 6.392.125.415.19.879.19 1.38a300.492 300.492 0 0 1-.012 2.716 1 1 0 0 1-.019 1.958c-1.139.228-1.983-.532-1.983-1.525l.002-.446.005-.705c.005-.708.007-1.338.007-1.998 0-.697-.183-1.152-.425-1.36-.661-.57-.326-1.655.54-1.752 2.967-.333 4.337-1.482 4.337-4.66 0-.955-.312-1.744-.913-2.404a1 1 0 0 1-.19-1.045c.166-.414.237-.957.096-1.614l-.01.003c-.491.139-1.11.44-1.858.949a1 1 0 0 1-.833.135A9.626 9.626 0 0 0 12 5.315c-.89 0-1.772.119-2.592.35a1 1 0 0 1-.83-.134c-.752-.507-1.374-.807-1.868-.947-.144.653-.073 1.194.092 1.607a1 1 0 0 1-.189 1.045C6.016 7.89 5.7 8.694 5.7 9.64c0 3.172 1.371 4.328 4.322 4.66.865.097 1.201 1.177.544 1.748-.192.168-.429.732-.429 1.364v3.15c0 .986-.835 1.725-1.96 1.528a1 1 0 0 1-.04-1.962v-.99c-.91.061-1.662-.088-2.254-.485z"/>
    </g>
</svg>

		</div>
	</a>
	
	<a href='https://www.instagram.com/ayushsube/' target="_blank" class="instagram icon pl-1 text-eucalyptus-400 hover:text-java-400" title="instagram link" rel="noopener"
		aria-label="follow on instagram——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path fill-rule="nonzero" d="M12 9a3 3 0 1 0 0 6 3 3 0 0 0 0-6zm0-2a5 5 0 1 1 0 10 5 5 0 0 1 0-10zm6.5-.25a1.25 1.25 0 0 1-2.5 0 1.25 1.25 0 0 1 2.5 0zM12 4c-2.474 0-2.878.007-4.029.058-.784.037-1.31.142-1.798.332-.434.168-.747.369-1.08.703a2.89 2.89 0 0 0-.704 1.08c-.19.49-.295 1.015-.331 1.798C4.006 9.075 4 9.461 4 12c0 2.474.007 2.878.058 4.029.037.783.142 1.31.331 1.797.17.435.37.748.702 1.08.337.336.65.537 1.08.703.494.191 1.02.297 1.8.333C9.075 19.994 9.461 20 12 20c2.474 0 2.878-.007 4.029-.058.782-.037 1.309-.142 1.797-.331.433-.169.748-.37 1.08-.702.337-.337.538-.65.704-1.08.19-.493.296-1.02.332-1.8.052-1.104.058-1.49.058-4.029 0-2.474-.007-2.878-.058-4.029-.037-.782-.142-1.31-.332-1.798a2.911 2.911 0 0 0-.703-1.08 2.884 2.884 0 0 0-1.08-.704c-.49-.19-1.016-.295-1.798-.331C14.925 4.006 14.539 4 12 4zm0-2c2.717 0 3.056.01 4.122.06 1.065.05 1.79.217 2.428.465.66.254 1.216.598 1.772 1.153a4.908 4.908 0 0 1 1.153 1.772c.247.637.415 1.363.465 2.428.047 1.066.06 1.405.06 4.122 0 2.717-.01 3.056-.06 4.122-.05 1.065-.218 1.79-.465 2.428a4.883 4.883 0 0 1-1.153 1.772 4.915 4.915 0 0 1-1.772 1.153c-.637.247-1.363.415-2.428.465-1.066.047-1.405.06-4.122.06-2.717 0-3.056-.01-4.122-.06-1.065-.05-1.79-.218-2.428-.465a4.89 4.89 0 0 1-1.772-1.153 4.904 4.904 0 0 1-1.153-1.772c-.248-.637-.415-1.363-.465-2.428C2.013 15.056 2 14.717 2 12c0-2.717.01-3.056.06-4.122.05-1.066.217-1.79.465-2.428a4.88 4.88 0 0 1 1.153-1.772A4.897 4.897 0 0 1 5.45 2.525c.638-.248 1.362-.415 2.428-.465C8.944 2.013 9.283 2 12 2z"/>
    </g>
</svg>

		</div>
	</a>
	
	<a href='https://www.linkedin.com/in/ayush-subedi/' target="_blank" class="linkedin icon pl-1 text-eucalyptus-400 hover:text-java-400" title="linkedin link" rel="noopener"
		aria-label="follow on linkedin——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path d="M12 9.55C12.917 8.613 14.111 8 15.5 8a5.5 5.5 0 0 1 5.5 5.5V21h-2v-7.5a3.5 3.5 0 0 0-7 0V21h-2V8.5h2v1.05zM5 6.5a1.5 1.5 0 1 1 0-3 1.5 1.5 0 0 1 0 3zm-1 2h2V21H4V8.5z"/>
    </g>
</svg>

		</div>
	</a>
	
	<a href='mailto:ayush.subedi@gmail.com' target="_blank" class="mail icon pl-1 text-eucalyptus-400 hover:text-java-400" title="mail link" rel="noopener"
		aria-label="follow on mail——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path d="M3 3h18a1 1 0 0 1 1 1v16a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1zm17 4.238l-7.928 7.1L4 7.216V19h16V7.238zM4.511 5l7.55 6.662L19.502 5H4.511z"/>
    </g>
</svg>
		</div>
	</a>
	
	<a href='https://twitter.com/ayushsubs' target="_blank" class="twitter icon pl-1 text-eucalyptus-400 hover:text-java-400" title="twitter link" rel="noopener"
		aria-label="follow on twitter——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path fill-rule="nonzero" d="M15.3 5.55a2.9 2.9 0 0 0-2.9 2.847l-.028 1.575a.6.6 0 0 1-.68.583l-1.561-.212c-2.054-.28-4.022-1.226-5.91-2.799-.598 3.31.57 5.603 3.383 7.372l1.747 1.098a.6.6 0 0 1 .034.993L7.793 18.17c.947.059 1.846.017 2.592-.131 4.718-.942 7.855-4.492 7.855-10.348 0-.478-1.012-2.141-2.94-2.141zm-4.9 2.81a4.9 4.9 0 0 1 8.385-3.355c.711-.005 1.316.175 2.669-.645-.335 1.64-.5 2.352-1.214 3.331 0 7.642-4.697 11.358-9.463 12.309-3.268.652-8.02-.419-9.382-1.841.694-.054 3.514-.357 5.144-1.55C5.16 15.7-.329 12.47 3.278 3.786c1.693 1.977 3.41 3.323 5.15 4.037 1.158.475 1.442.465 1.973.538z"/>
    </g>
</svg>

		</div>
	</a>
	
</div>
	<div class="text-sm text-gray-500 leading-tight a-gray">
		
		<br />
		21166 words in this page.
	</div>
</div>

			</div>
		</aside>
		<footer class="w-full md:w-3/5 xl:w-1/2 order-3 max-w-3xl md:order-4 pt-2">
			
<hr class="" />
<div class="flex flex-wrap justify-between pb-2 leading-loose font-serif">
    
    <a class="flex-grow-0" href="/posts/aws_ml_speciality_modeling/">
        <svg class="fill-current inline-block h-4 w-4" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24"
            height="24">
            <path fill="none" d="M0 0h24v24H0z" />
            <path d="M7.828 11H20v2H7.828l5.364 5.364-1.414 1.414L4 12l7.778-7.778 1.414 1.414z" /></svg>
        AWS Certified ML - Specialty exam (MLS-C01) - 3b. Modeling
    </a>
    
    
    <a class="flex-grow-0" href="/posts/aws_ml_speciality_eda/">
        AWS Certified ML - Specialty exam (MLS-C01) - 2. Exploratory Data Analysis
        <svg class="fill-current inline-block h-4 w-4" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24"
            height="24">
            <path fill="none" d="M0 0h24v24H0z" />
            <path d="M16.172 11l-5.364-5.364 1.414-1.414L20 12l-7.778 7.778-1.414-1.414L16.172 13H4v-2z" /></svg></a>
    
</div>
<div >



<div class="font-serif pb-2 flex align-start leading-loose">
	<span class="heading pr-6 leading-loose">Related</span>
	<span >
		
			<a href="/posts/aws_ml_speciality_data_engineering/">AWS Certified ML - Specialty exam (MLS-C01) - 1. Data Engineering</a>&nbsp;&nbsp;&#47;&nbsp;
		
			<a href="/posts/aws_ml_speciality_modeling/">AWS Certified ML - Specialty exam (MLS-C01) - 3b. Modeling</a>&nbsp;&nbsp;&#47;&nbsp;
		
			<a href="/posts/aws_ml_speciality_ml/">AWS Certified ML - Specialty exam (MLS-C01) - 4. Machine Learning Implementation and Operations</a>&nbsp;&nbsp;&#47;&nbsp;
		
			<a href="/posts/tfidf/">Term Frequecy Inverse Document Frequency (TFIDF)</a>
		
</span>
</div>

</div>
<hr />
<div class="pb-2">
    
</div>
<hr />

		</footer>
		

<script src="/dist/app.js"></script>


<script src="/lib/fuse.min.js"></script> 
<script src="/lib/fastsearch.js"></script>

	</div>
</body>

</html>