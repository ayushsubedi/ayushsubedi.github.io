<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
	
	<title>Ayush Subedi  | Curriculum on LLM: A Building-Block Approach</title>
	<meta name="viewport" content="width=device-width,minimum-scale=1">
	<meta name="generator" content="Hugo 0.128.2">
	
	
	<META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
	

	<meta name="title" content="Ayush Subedi">
	<meta name="description" content="… personal journey with mathematics, software engineering and data science">

	
	<meta property="og:type" content="website">
	<meta property="og:url" content="https://ayushsubedi.github.io/">
	<meta property="og:title" content="Ayush Subedi">
	<meta property="og:description" content="… personal journey with mathematics, software engineering and data science">
	<meta property="og:image" content="https://ayushsubedi.github.io/img/k.png">

	
	<meta property="twitter:card" content="summary_large_image">
	<meta property="twitter:url" content="https://ayushsubedi.github.io/">
	<meta property="twitter:title" content="Ayush Subedi">
	<meta property="twitter:description" content="… personal journey with mathematics, software engineering and data science">
	<meta property="twitter:image" content="https://ayushsubedi.github.io/img/k.png">

	
	
	<link href="/dist/app.css" rel="stylesheet">
	

	

	
	
<link rel="shortcut icon" href="/img/favicon.ico" type="image/png" />

	

	

	
	



<link rel="stylesheet" href='https://ayushsubedi.github.io/lib/katex.min.css' integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">


<script defer src='https://ayushsubedi.github.io/lib/katex.min.js' integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>


<script defer src='https://ayushsubedi.github.io/lib/contrib/auto-render.min.js' integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
crossorigin="anonymous"
onload='renderMathInElement(document.body);'></script>


<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

	
	
</head>

<body class="bg-gray-100 text-gray-700 font-sans">
	<div class="p-6 sm:p-10 md:p-16 flex flex-wrap">
		<header class="w-full md:w-2/5 xl:w-1/2 md:pr-12 lg:pr-20 xl:pr-24 order-1 md:order-1 max-w-2xl">
			<div
				class="z-50 bg-gray-100 bg-opacity-75 bg-opacity-custom lg:min-w-0.7 max-w-xl md:float-right md:text-right leading-loose tracking-tight md:sticky md:top-0 pt-2">
				
<div>
	<h2>
		<a href="https://ayushsubedi.github.io/" title="Ayush Subedi" class="heading font-cursive icon">Ayush Subedi</a>
	</h2>
</div>
<h1 class="pt-2">Curriculum on LLM: A Building-Block Approach</h1>

<h3 class="text-java-700 font-normal leading-relaxed pt-2">An up to date curriculum on LLM for someone who comes from Vision/pre Transformer world.</h3>

<div class="flex flex-wrap justify-end pt-2 "><div class="md:flex-grow-0 font-light">
	
	
	
	
	<a class="post-taxonomy-category text-medium-red-violet-600 hover:text-medium-red-violet-400"
		href='/categories/llm'>llm</a>&nbsp;&#47;
	
	<a class="post-taxonomy-category text-medium-red-violet-600 hover:text-medium-red-violet-400"
		href='/categories/genai'>genai</a>&nbsp;&#47;
	
	<a class="post-taxonomy-category text-medium-red-violet-600 hover:text-medium-red-violet-400"
		href='/categories/curriculum'>curriculum</a>
	
	
	

	
	&nbsp;&nbsp;
	

	
	
	
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/llm'>llm</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/genai'>genai</a>
	
	
	
</div><time class="text-eucalyptus-500 md:text-right md:flex-grow font-light pl-4"
		datetime="2025-01-01">2025-01-01</time>
</div>

<hr />

			</div>
		</header>
		<main role="main" class="w-full md:w-3/5 xl:w-1/2 max-w-3xl order-2 md:order-2 min-h-70vh pt-2 pb-4">
			

<article>
	<section class="mx-auto content">
		<div class="c-rich-text"><h1 id="curriculum-on-llm-a-building-block-approach">Curriculum on LLM: A Building-Block Approach</h1>
<h2 id="1-foundational-models">1. Foundational Models</h2>
<h3 id="what-is-a-foundational-model">What is a foundational model?</h3>
<p>A <strong>foundational model</strong> is defined as a pre-trained neural network that serves as the basis for a wide range of downstream tasks. These models are generally pre-trained on vast corpora of data and can be fine-tuned for specific applications. In mathematical terms, a foundational model $M$ can be represented as a mapping:</p>
<p>$$ M: X \to Y $$</p>
<p>where $X$ is the input space and $Y$ is the output space. For language models, $X$ represents the set of tokenized inputs (such as words or subwords), and $Y$ represents the possible outputs (such as the predicted token). The underlying model architecture is typically a deep neural network, where parameters $\theta$ are learned through optimization methods such as stochastic gradient descent (SGD).</p>
<h4 id="historical-context-of-foundational-models">Historical context of foundational models</h4>
<p>The historical evolution of foundational models can be traced back to early neural network architectures like <strong>word embeddings</strong> (e.g., Word2Vec, GloVe), which provided dense representations of words. These embeddings were foundational in shifting from simpler, task-specific models to more complex, pre-trained models that could be adapted for a wide range of tasks.</p>
<h4 id="characteristics-of-foundational-models">Characteristics of foundational models</h4>
<ul>
<li>
<p><strong>Pre-training on large datasets</strong>: Foundational models are typically pre-trained on large, diverse datasets. For example, models like GPT and BERT are trained on corpora like Wikipedia, BooksCorpus, and other web-based text.</p>
</li>
<li>
<p><strong>Transferability</strong>: These models are designed to be fine-tuned on smaller, task-specific datasets. The pre-trained weights provide a strong starting point for learning task-specific representations.</p>
</li>
<li>
<p><strong>Scalability</strong>: Foundational models scale effectively with increased data and computational power, often demonstrating improved performance as model size increases.</p>
</li>
</ul>
<h4 id="differences-from-traditional-machine-learning-models">Differences from traditional machine learning models</h4>
<p>Traditional machine learning models, such as decision trees or SVMs, are often trained on task-specific data and typically lack the generalization power exhibited by foundational models. In contrast, foundational models are <strong>pre-trained on large, generic datasets</strong> and are capable of adapting to a wide range of downstream tasks through transfer learning.</p>
<h3 id="evolution-of-foundational-models-in-ai">Evolution of foundational models in AI</h3>
<h4 id="early-milestones-in-foundational-models">Early milestones in foundational models</h4>
<p>Early deep learning models like <strong>word embeddings</strong> served as the initial foray into pre-training. These embeddings were learned using shallow neural networks and then applied to downstream tasks.</p>
<p>With the advent of <strong>deep neural networks</strong> and the introduction of <strong>transformer models</strong>, foundational models were able to leverage vast amounts of unstructured data (e.g., text, images) and achieve remarkable performance in various domains.</p>
<h4 id="transition-from-task-specific-to-general-purpose-models">Transition from task-specific to general-purpose models</h4>
<p>Previously, machine learning models were designed for specific tasks such as classification, regression, or language translation. However, with the advent of foundational models, a paradigm shift occurred, moving towards <strong>general-purpose models</strong>. This shift was marked by the introduction of models like <strong>BERT</strong> and <strong>GPT</strong>, which can be fine-tuned for various tasks beyond their initial pre-training objective.</p>
<h4 id="integration-of-foundational-models-in-various-industries">Integration of foundational models in various industries</h4>
<p>Foundational models have been integrated across various industries, such as healthcare, finance, and entertainment. Their ability to understand and generate text, code, and images has led to their widespread adoption in fields ranging from <strong>natural language processing</strong> (NLP) to <strong>computer vision</strong>.</p>
<h3 id="key-examples-gpt-bert-etc">Key examples (GPT, BERT, etc.)</h3>
<h4 id="gpt-family-evolution-from-gpt-1-to-gpt-4">GPT family: Evolution from GPT-1 to GPT-4</h4>
<p>The <strong>GPT family</strong> of models, introduced by OpenAI, represents a series of autoregressive transformer models. These models are trained to predict the next token in a sequence of text. Mathematically, the autoregressive nature of GPT can be described as:</p>
<p>$$ P(w_1, w_2, \dots, w_T) = \prod_{t=1}^{T} P(w_t | w_1, \dots, w_{t-1}) $$</p>
<p>where $w_t$ represents the $t$-th token in the sequence, and $P(w_t | w_1, \dots, w_{t-1})$ is the probability distribution of the next token, conditioned on the previous tokens.</p>
<ul>
<li><strong>GPT-1</strong>: Introduced the transformer architecture and demonstrated that large-scale pre-training could improve performance on various downstream NLP tasks.</li>
<li><strong>GPT-2</strong>: Increased the model size and training data, achieving significant improvements in text generation.</li>
<li><strong>GPT-3</strong>: With 175 billion parameters, GPT-3 showed substantial advances in few-shot learning, where the model could generalize to tasks with minimal task-specific data.</li>
<li><strong>GPT-4</strong>: Represents the latest iteration, with further improvements in language understanding and generation.</li>
</ul>
<h4 id="bert-and-its-variations-roberta-distilbert">BERT and its variations (RoBERTa, DistilBERT)</h4>
<p><strong>BERT</strong> (Bidirectional Encoder Representations from Transformers) introduced the concept of bidirectional context for pre-trained language models. BERT is trained by predicting missing words in a sentence, using the following objective function:</p>
<p>$$
L_{\text{masked}} = - \sum_{i} \log P(w_i | \hat{w}_i)
$$</p>
<p>where $w_i$ is the $i$-th token, and $\hat{w}_i$ is the masked token.</p>
<ul>
<li><strong>RoBERTa</strong>: A variant of BERT that improves on training by using more data, longer training, and dynamic masking.</li>
<li><strong>DistilBERT</strong>: A smaller, faster version of BERT that retains much of the original model’s performance.</li>
</ul>
<h4 id="comparison-between-autoregressive-and-autoencoding-models">Comparison between autoregressive and autoencoding models</h4>
<ul>
<li>
<p><strong>Autoregressive models</strong> (e.g., GPT) predict tokens sequentially, conditioning on previous tokens. They are effective for generative tasks, such as text generation.</p>
</li>
<li>
<p><strong>Autoencoding models</strong> (e.g., BERT) are trained to predict missing tokens in a sequence, allowing for bidirectional context understanding. These models excel at tasks requiring contextual comprehension, such as classification and question answering.</p>
</li>
</ul>
<h3 id="applications-of-foundational-models">Applications of foundational models</h3>
<ul>
<li>
<p><strong>Language generation and translation</strong>: Models like GPT and T5 have revolutionized the field of natural language generation and translation. These models are used for text generation, machine translation, summarization, and more.</p>
</li>
<li>
<p><strong>Image captioning and visual question answering</strong>: Vision-language models like <strong>CLIP</strong> and <strong>DALL-E</strong> have enabled systems to generate captions for images and answer questions about visual content.</p>
</li>
<li>
<p><strong>Biomedical research and drug discovery</strong>: Foundational models are being used to analyze large biomedical datasets, predict protein structures, and assist in drug discovery.</p>
</li>
<li>
<p><strong>Personalized recommendations and search engines</strong>: Models like BERT and GPT are employed to power personalized search engines, recommendation systems, and conversational agents.</p>
</li>
</ul>
<h3 id="ethical-implications-and-considerations">Ethical implications and considerations</h3>
<ul>
<li>
<p><strong>Bias in training data and output</strong>: Foundational models can inherit biases present in their training data. These biases can lead to unfair or discriminatory outputs. Techniques like <strong>bias mitigation</strong> and <strong>adversarial training</strong> are being explored to address these issues.</p>
</li>
<li>
<p><strong>Environmental concerns of large-scale training</strong>: Training large foundational models requires substantial computational resources, leading to concerns about their carbon footprint. Methods to optimize training efficiency and reduce energy consumption are a key area of ongoing research.</p>
</li>
<li>
<p><strong>Misinformation and responsible AI usage</strong>: The ability of models like GPT-3 to generate coherent and convincing text poses risks in the context of misinformation and fake news. Ensuring responsible usage and implementing safeguards is crucial for the responsible deployment of these technologies.</p>
</li>
</ul>
<h2 id="sources">Sources:</h2>
</div>
	</section>

</article>

		</main>
		<aside role="contentinfo"
			class="w-full md:w-2/5 xl:w-1/2 md:pr-12 lg:pr-20 xl:pr-24 order-4 md:order-3 md:sticky md:bottom-0 self-end max-w-2xl">
			<div class="md:float-right md:text-right leading-loose tracking-tight md:mb-2">
				
	<div class="md:max-w-xs  flex flex-col md:items-end">
	<ul class="font-serif flex-grow-0 flex justify-between flex-wrap md:flex-col">
	
	
	<li class="px-1 md:px-0">
		<a href="/posts/" title="Posts page" 
			class="font-medium text-medium-red-violet-600 hover:text-medium-red-violet-400" >
			Posts
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/resume/" title="Resume page" >
			Resume
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/certifications/" title="Certifications page" >
			Certifications
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/publications/" title="Publications page" >
			Publications
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/tags/" title="Tags page" >
			Tags
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/categories/" title="Categories page" >
			Categories
		</a>
	</li>
	
	
	
	
	<div id="fastSearch" class="m-0">
		<input id="searchInput" type="text" size=10 
			class="bg-gray-100 focus:outline-none border-b border-gray-100 focus:border-eucalyptus-300 md:text-right
			placeholder-java-500 min-w-0 max-w-xxxs"
			placeholder="search" />
		<ul id="searchResults" class="bg-gray-200 px-2 divide-y divide-gray-400">
		</ul>
	</div>
	
</ul>
	

<div class="flex flex-wrap-reverse md:justify-end content-end md:content-start justify-start items-start md:flex-col  max-h-16">
	
	<a href='https://github.com/ayushsubedi' target="_blank" class="github icon pl-1 text-eucalyptus-400 hover:text-java-400" title="github link" rel="noopener"
		aria-label="follow on github——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path fill-rule="nonzero" d="M5.883 18.653c-.3-.2-.558-.455-.86-.816a50.32 50.32 0 0 1-.466-.579c-.463-.575-.755-.84-1.057-.949a1 1 0 0 1 .676-1.883c.752.27 1.261.735 1.947 1.588-.094-.117.34.427.433.539.19.227.33.365.44.438.204.137.587.196 1.15.14.023-.382.094-.753.202-1.095C5.38 15.31 3.7 13.396 3.7 9.64c0-1.24.37-2.356 1.058-3.292-.218-.894-.185-1.975.302-3.192a1 1 0 0 1 .63-.582c.081-.024.127-.035.208-.047.803-.123 1.937.17 3.415 1.096A11.731 11.731 0 0 1 12 3.315c.912 0 1.818.104 2.684.308 1.477-.933 2.613-1.226 3.422-1.096.085.013.157.03.218.05a1 1 0 0 1 .616.58c.487 1.216.52 2.297.302 3.19.691.936 1.058 2.045 1.058 3.293 0 3.757-1.674 5.665-4.642 6.392.125.415.19.879.19 1.38a300.492 300.492 0 0 1-.012 2.716 1 1 0 0 1-.019 1.958c-1.139.228-1.983-.532-1.983-1.525l.002-.446.005-.705c.005-.708.007-1.338.007-1.998 0-.697-.183-1.152-.425-1.36-.661-.57-.326-1.655.54-1.752 2.967-.333 4.337-1.482 4.337-4.66 0-.955-.312-1.744-.913-2.404a1 1 0 0 1-.19-1.045c.166-.414.237-.957.096-1.614l-.01.003c-.491.139-1.11.44-1.858.949a1 1 0 0 1-.833.135A9.626 9.626 0 0 0 12 5.315c-.89 0-1.772.119-2.592.35a1 1 0 0 1-.83-.134c-.752-.507-1.374-.807-1.868-.947-.144.653-.073 1.194.092 1.607a1 1 0 0 1-.189 1.045C6.016 7.89 5.7 8.694 5.7 9.64c0 3.172 1.371 4.328 4.322 4.66.865.097 1.201 1.177.544 1.748-.192.168-.429.732-.429 1.364v3.15c0 .986-.835 1.725-1.96 1.528a1 1 0 0 1-.04-1.962v-.99c-.91.061-1.662-.088-2.254-.485z"/>
    </g>
</svg>

		</div>
	</a>
	
	<a href='https://www.instagram.com/ayushsube_fit/' target="_blank" class="instagram icon pl-1 text-eucalyptus-400 hover:text-java-400" title="instagram link" rel="noopener"
		aria-label="follow on instagram——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path fill-rule="nonzero" d="M12 9a3 3 0 1 0 0 6 3 3 0 0 0 0-6zm0-2a5 5 0 1 1 0 10 5 5 0 0 1 0-10zm6.5-.25a1.25 1.25 0 0 1-2.5 0 1.25 1.25 0 0 1 2.5 0zM12 4c-2.474 0-2.878.007-4.029.058-.784.037-1.31.142-1.798.332-.434.168-.747.369-1.08.703a2.89 2.89 0 0 0-.704 1.08c-.19.49-.295 1.015-.331 1.798C4.006 9.075 4 9.461 4 12c0 2.474.007 2.878.058 4.029.037.783.142 1.31.331 1.797.17.435.37.748.702 1.08.337.336.65.537 1.08.703.494.191 1.02.297 1.8.333C9.075 19.994 9.461 20 12 20c2.474 0 2.878-.007 4.029-.058.782-.037 1.309-.142 1.797-.331.433-.169.748-.37 1.08-.702.337-.337.538-.65.704-1.08.19-.493.296-1.02.332-1.8.052-1.104.058-1.49.058-4.029 0-2.474-.007-2.878-.058-4.029-.037-.782-.142-1.31-.332-1.798a2.911 2.911 0 0 0-.703-1.08 2.884 2.884 0 0 0-1.08-.704c-.49-.19-1.016-.295-1.798-.331C14.925 4.006 14.539 4 12 4zm0-2c2.717 0 3.056.01 4.122.06 1.065.05 1.79.217 2.428.465.66.254 1.216.598 1.772 1.153a4.908 4.908 0 0 1 1.153 1.772c.247.637.415 1.363.465 2.428.047 1.066.06 1.405.06 4.122 0 2.717-.01 3.056-.06 4.122-.05 1.065-.218 1.79-.465 2.428a4.883 4.883 0 0 1-1.153 1.772 4.915 4.915 0 0 1-1.772 1.153c-.637.247-1.363.415-2.428.465-1.066.047-1.405.06-4.122.06-2.717 0-3.056-.01-4.122-.06-1.065-.05-1.79-.218-2.428-.465a4.89 4.89 0 0 1-1.772-1.153 4.904 4.904 0 0 1-1.153-1.772c-.248-.637-.415-1.363-.465-2.428C2.013 15.056 2 14.717 2 12c0-2.717.01-3.056.06-4.122.05-1.066.217-1.79.465-2.428a4.88 4.88 0 0 1 1.153-1.772A4.897 4.897 0 0 1 5.45 2.525c.638-.248 1.362-.415 2.428-.465C8.944 2.013 9.283 2 12 2z"/>
    </g>
</svg>

		</div>
	</a>
	
	<a href='https://www.linkedin.com/in/ayush-subedi/' target="_blank" class="linkedin icon pl-1 text-eucalyptus-400 hover:text-java-400" title="linkedin link" rel="noopener"
		aria-label="follow on linkedin——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path d="M12 9.55C12.917 8.613 14.111 8 15.5 8a5.5 5.5 0 0 1 5.5 5.5V21h-2v-7.5a3.5 3.5 0 0 0-7 0V21h-2V8.5h2v1.05zM5 6.5a1.5 1.5 0 1 1 0-3 1.5 1.5 0 0 1 0 3zm-1 2h2V21H4V8.5z"/>
    </g>
</svg>

		</div>
	</a>
	
	<a href='mailto:ayush.subedi@gmail.com' target="_blank" class="mail icon pl-1 text-eucalyptus-400 hover:text-java-400" title="mail link" rel="noopener"
		aria-label="follow on mail——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path d="M3 3h18a1 1 0 0 1 1 1v16a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1zm17 4.238l-7.928 7.1L4 7.216V19h16V7.238zM4.511 5l7.55 6.662L19.502 5H4.511z"/>
    </g>
</svg>
		</div>
	</a>
	
	<a href='https://public.tableau.com/app/profile/ayush3339' target="_blank" class="tableau icon pl-1 text-eucalyptus-400 hover:text-java-400" title="tableau link" rel="noopener"
		aria-label="follow on tableau——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M2 13H8V21H2V13ZM9 3H15V21H9V3ZM16 8H22V21H16V8Z"/></svg>
		</div>
	</a>
	
	<a href='https://twitter.com/ayushsubs' target="_blank" class="twitter icon pl-1 text-eucalyptus-400 hover:text-java-400" title="twitter link" rel="noopener"
		aria-label="follow on twitter——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path fill-rule="nonzero" d="M15.3 5.55a2.9 2.9 0 0 0-2.9 2.847l-.028 1.575a.6.6 0 0 1-.68.583l-1.561-.212c-2.054-.28-4.022-1.226-5.91-2.799-.598 3.31.57 5.603 3.383 7.372l1.747 1.098a.6.6 0 0 1 .034.993L7.793 18.17c.947.059 1.846.017 2.592-.131 4.718-.942 7.855-4.492 7.855-10.348 0-.478-1.012-2.141-2.94-2.141zm-4.9 2.81a4.9 4.9 0 0 1 8.385-3.355c.711-.005 1.316.175 2.669-.645-.335 1.64-.5 2.352-1.214 3.331 0 7.642-4.697 11.358-9.463 12.309-3.268.652-8.02-.419-9.382-1.841.694-.054 3.514-.357 5.144-1.55C5.16 15.7-.329 12.47 3.278 3.786c1.693 1.977 3.41 3.323 5.15 4.037 1.158.475 1.442.465 1.973.538z"/>
    </g>
</svg>

		</div>
	</a>
	
</div>
	<div class="text-sm text-gray-500 leading-tight a-gray">
		
		<br />
		1048 words in this page.
	</div>
</div>

			</div>
		</aside>
		<footer class="w-full md:w-3/5 xl:w-1/2 order-3 max-w-3xl md:order-4 pt-2">
			
<hr class="" />
<div class="flex flex-wrap justify-between pb-2 leading-loose font-serif">
    
    <a class="flex-grow-0" href="/posts/segment_anything/">
        <svg class="fill-current inline-block h-4 w-4" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24"
            height="24">
            <path fill="none" d="M0 0h24v24H0z" />
            <path d="M7.828 11H20v2H7.828l5.364 5.364-1.414 1.414L4 12l7.778-7.778 1.414 1.414z" /></svg>
        [Paper Exploration] In-Depth Analysis of the Segment Anything Model (SAM)
    </a>
    
    
</div>
<div >



</div>
<hr />
<div class="pb-2">
    
</div>
<hr />

		</footer>
		

<script src="/dist/app.js"></script>


<script src="/lib/fuse.min.js"></script> 
<script src="/lib/fastsearch.js"></script>

	</div>
</body>

</html>