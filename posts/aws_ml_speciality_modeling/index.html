<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
	
	<title>Ayush Subedi  | AWS Certified ML - Specialty exam (MLS-C01) - 3b. Modeling</title>
	<meta name="viewport" content="width=device-width,minimum-scale=1">
	<meta name="generator" content="Hugo 0.102.3" />
	
	
	<META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
	

		
	<title>Ayush Subedi</title>
	<meta name="title" content="Ayush Subedi">
	<meta name="description" content="… personal journey with mathematics, software engineering and data science">

	
	<meta property="og:type" content="website">
	<meta property="og:url" content="https://subedi.ml/">
	<meta property="og:title" content="Ayush Subedi">
	<meta property="og:description" content="… personal journey with mathematics, software engineering and data science">
	<meta property="og:image" content="https://subedi.ml/img/k.png">

	
	<meta property="twitter:card" content="summary_large_image">
	<meta property="twitter:url" content="https://subedi.ml/">
	<meta property="twitter:title" content="Ayush Subedi">
	<meta property="twitter:description" content="… personal journey with mathematics, software engineering and data science">
	<meta property="twitter:image" content="https://subedi.ml/img/k.png">

	
	
	<link href="/dist/app.css" rel="stylesheet">
	

	

	
	
<link rel="shortcut icon" href="/img/favicon.ico" type="image/png" />

	

	
	
	
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-177424799-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	
	
	



<link rel="stylesheet" href='https://ayushsubedi.github.io/lib/katex.min.css' integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">


<script defer src='https://ayushsubedi.github.io/lib/katex.min.js' integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>


<script defer src='https://ayushsubedi.github.io/lib/contrib/auto-render.min.js' integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
crossorigin="anonymous"
onload='renderMathInElement(document.body);'></script>


<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

	
	
</head>

<body class="bg-gray-100 text-gray-700 font-sans">
	<div class="p-6 sm:p-10 md:p-16 flex flex-wrap">
		<header class="w-full md:w-2/5 xl:w-1/2 md:pr-12 lg:pr-20 xl:pr-24 order-1 md:order-1 max-w-2xl">
			<div
				class="z-50 bg-gray-100 bg-opacity-75 bg-opacity-custom lg:min-w-0.7 max-w-xl md:float-right md:text-right leading-loose tracking-tight md:sticky md:top-0 pt-2">
				
<div>
	<h2>
		<a href="https://ayushsubedi.github.io" title="Ayush Subedi" class="heading font-cursive icon">Ayush Subedi</a>
	</h2>
</div>
<h1 class="pt-2">AWS Certified ML - Specialty exam (MLS-C01) - 3b. Modeling</h1>

<h3 class="text-java-700 font-normal leading-relaxed pt-2">My notes on the &#34;Modeling&#34; domain. Notes created with the help of ChatGPT</h3>

<div class="flex flex-wrap justify-end pt-2 "><div class="md:flex-grow-0 font-light">
	
	
	
	
	<a class="post-taxonomy-category text-medium-red-violet-600 hover:text-medium-red-violet-400"
		href='/categories/aws-certified-ml-specialty-exam-mls-c01'>AWS Certified ML - Specialty exam (MLS-C01)</a>
	
	
	

	
	&nbsp;&nbsp;
	

	
	
	
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/analytics'>analytics</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/machine-learning'>machine-learning</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/certification'>certification</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/aws'>AWS</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/certification'>certification</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/deepar-in-sagemaker'>deepar-in-sagemaker</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/sagemaker-debugger'>sagemaker-debugger</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/sagemaker-canvas'>sagemaker-canvas</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/amazon-comprehend'>amazon-comprehend</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/amazon-translate'>amazon-translate</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/modeling'>Modeling</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/amazon-transcribe'>amazon-transcribe</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/amazon-polly'>amazon-polly</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/amazon-rekognition'>amazon-rekognition</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/amazon-forecast'>amazon-forecast</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/amazon-lex'>amazon-lex</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/amazon-personalize'>amazon-personalize</a>
	
	
	
</div><time class="text-eucalyptus-500 md:text-right md:flex-grow font-light pl-4"
		datetime="2023-12-07">2023-12-07</time>
</div>

<hr />

			</div>
		</header>
		<main role="main" class="w-full md:w-3/5 xl:w-1/2 max-w-3xl order-2 md:order-2 min-h-70vh pt-2 pb-4">
			

<article>
	<section class="mx-auto content">
		<div class="c-rich-text"><h1 id="modeling">Modeling</h1>
<ol>
<li><a href="#activation-functions">Activation Functions</a></li>
<li><a href="#convolutional-neural-network">Convolutional Neural Network</a></li>
<li><a href="#recurrent-neural-networks">Recurrent Neural Networks</a></li>
<li><a href="#modern-nlp-with-bert-and-gpt-and-transfer-learning">Modern NLP with BERT and GPT, and Transfer Learning</a></li>
<li><a href="#deep-learning-on-ec2-and-emr">Deep Learning on EC2 and EMR</a></li>
<li><a href="#tuning-neural-networks">Tuning Neural Networks</a></li>
<li><a href="#regularization-techniques-for-neural-networks-dropout-early-stopping">Regularization Techniques for Neural Networks (Dropout, Early Stopping)</a></li>
<li><a href="#l1-and-l2-regularization">L1 and L2 Regularization</a></li>
<li><a href="#grief-with-gradients-the-vanishing-gradient-problem">Grief with Gradients The Vanishing Gradient problem</a></li>
<li><a href="#the-confusion-matrix">The Confusion Matrix</a></li>
<li><a href="#precision-recall-f1-auc-and-more">Precision, Recall, F1, AUC, and more</a></li>
<li><a href="#ensemble-methods-bagging-and-boosting">Ensemble Methods Bagging and Boosting</a></li>
<li><a href="#introducing-amazon-sagemaker">Introducing Amazon SageMaker</a></li>
<li><a href="#linear-learner-in-sagemaker">Linear Learner in SageMaker</a></li>
<li><a href="#xgboost-in-sagemaker">XGBoost in SageMaker</a></li>
<li><a href="#seq2seq-in-sagemaker">Seq2Seq in SageMaker</a></li>
<li><a href="#deepar-in-sagemaker">DeepAR in SageMaker</a></li>
<li><a href="#blazingtext-in-sagemaker">BlazingText in SageMaker</a></li>
<li><a href="#object2vec-in-sagemaker">Object2Vec in SageMaker</a></li>
<li><a href="#object-detection-in-sagemaker">Object Detection in SageMaker</a></li>
<li><a href="#image-classification-in-sagemaker">Image Classification in SageMaker</a></li>
<li><a href="#semantic-segmentation-in-sagemaker">Semantic Segmentation in SageMaker</a></li>
<li><a href="#random-cut-forest-in-sagemaker">Random Cut Forest in SageMaker</a></li>
<li><a href="#neural-topic-model-in-sagemaker">Neural Topic Model in SageMaker</a></li>
<li><a href="#latent-dirichlet-allocation-lda-in-sagemaker">Latent Dirichlet Allocation (LDA) in SageMaker</a></li>
<li><a href="#k-nearest-neighbors-knn-in-sagemaker">K-Nearest-Neighbors (KNN) in SageMaker</a></li>
<li><a href="#k-means-clustering-in-sagemaker">K-Means Clustering in SageMaker</a></li>
<li><a href="#principal-component-analysis-pca-in-sagemaker">Principal Component Analysis (PCA) in SageMaker</a></li>
<li><a href="#factorization-machines-in-sagemaker">Factorization Machines in SageMaker</a></li>
<li><a href="#ip-insights-in-sagemaker">IP Insights in SageMaker</a></li>
<li><a href="#reinforcement-learning-in-sagemaker">Reinforcement Learning in SageMaker</a></li>
<li><a href="#automatic-model-tuning">Automatic Model Tuning</a></li>
<li><a href="#apache-spark-with-sagemaker">Apache Spark with SageMaker</a></li>
<li><a href="#sagemaker-studio-and-sagemaker-experiments">SageMaker Studio, and SageMaker Experiments</a></li>
<li><a href="#sagemaker-debugger">SageMaker Debugger</a></li>
<li><a href="#sagemaker-autopilot-automl">SageMaker Autopilot / AutoML</a></li>
<li><a href="#sagemaker-model-monitor">SageMaker Model Monitor</a></li>
<li><a href="#other-recent-features-jumpstart-data-wrangler-features-store-edge-manager">Other recent features (JumpStart, Data Wrangler, Features Store, Edge Manager)</a></li>
<li><a href="#sagemaker-canvas">SageMaker Canvas</a></li>
<li><a href="#bias-measures-in-sagemaker-canvas">Bias Measures in SageMaker Canvas</a></li>
<li><a href="#sagemaker-training-compiler">SageMaker Training Compiler</a></li>
<li><a href="#amazon-comprehend">Amazon Comprehend</a></li>
<li><a href="#amazon-translate">Amazon Translate</a></li>
<li><a href="#amazon-transcribe">Amazon Transcribe</a></li>
<li><a href="#amazon-polly">Amazon Polly</a></li>
<li><a href="#amazon-rekognition">Amazon Rekognition</a></li>
<li><a href="#amazon-forecast">Amazon Forecast</a></li>
<li><a href="#amazon-forecast-algorithms">Amazon Forecast Algorithms</a></li>
<li><a href="#amazon-lex">Amazon Lex</a></li>
<li><a href="#amazon-personalize">Amazon Personalize</a></li>
<li><a href="#lightning-round-textract-deeplens-deepracher-lookout-and-monitron">Lightning round! TexTract, DeepLens, DeepRacher, Lookout, and Monitron</a></li>
<li><a href="#torchserve-aws-neuron-and-aws-panorama">TorchServe, AWS Neuron, and AWS Panorama</a></li>
<li><a href="#deep-composer-fraud-detection-codeguru-and-contact-lens">Deep Composer, Fraud Detection, CodeGuru, and Contact Lens</a></li>
<li><a href="#amazon-kendra-and-amazon-augmented-ai-a2i">Amazon Kendra and Amazon Augmented AI (A2I)</a></li>
</ol>
<p>This section covers framing business problems as machine learning problems, selecting the appropriate model(s) for a given machine learning problem, training machine learning models, performing hyperparameter optimization, and evaluate machine learning models.</p>
<h2 id="deeplearning-frameworks">Deeplearning Frameworks</h2>
<ul>
<li>Tensorflow/Keras (Google)</li>
<li>PyTorch (Meta)</li>
<li>MXNet (Apache, and therefore AWS leans towards this)</li>
<li>Scikit-Learn (for simple DL)</li>
</ul>
<h2 id="activation-functions">Activation Functions</h2>
<ul>
<li>Apply a non linear transformation</li>
<li>Given the input, what should by output be</li>
<li>Can be applied in between layers, or in the output layer</li>
<li>Step Function, Sigmoid, TanH, ReLU, Leaky ReLU</li>
<li>Binary Step Function is either on or off, cannot handle multiple classification, vertical slopes do not work with calculus</li>
<li>Sigmoid: 0 to 1</li>
<li>TanH: -1 to 1</li>
<li>For Sigmoid and TanH there is a vanishing gradient problem (value changes slowly for high or low value)</li>
<li>Sigmoid and TanH are computationally expensive</li>
<li>ReLu: fast to compute, for inputs that are zero or negative, it is a linear function (dying relu problem)</li>
<li>Leaky ReLU solves this</li>
<li>Parametric ReLU, slope in the negative part is learned via backpropagation, complicated</li>
<li>Exponential Linear Unit (ELU)</li>
<li>Maxout: usually not worth the effort</li>
<li>Softmax: usually the final layer of a classification model</li>
<li>RNN&rsquo;s do well with Tanh</li>
<li>Sigmoid if more that one classification is required for the same thing</li>
<li>For everything else, start with ReLU</li>
</ul>
<h2 id="convolutional-neural-network">Convolutional Neural Network</h2>
<ul>
<li>CNN vs MLP (Multilayer perceptron)</li>
<li>They have convolutional layers</li>
<li>Some filters may detect edges, lines, shapes etc. and deeper layers can detect objects</li>
<li>Feature location invariant, Shift Invariant, Space Invariant Artificial Neural Networks</li>
<li>Image and video recognition, recommender systems, image classification, image segmentations,</li>
<li>Machine translation, Sentence Classification, Sentiment analysis</li>
<li>AlexNet, LeNet, GoogLeNet, ResNet as an example</li>
<li>source data must be of appropriate dimensions</li>
</ul>
<h2 id="recurrent-neural-networks">Recurrent Neural Networks</h2>
<ul>
<li>deals with sequences in time (predict stock prices, understand words in a sentence, translation etc)</li>
<li>time series data, sequence of arbitrary length</li>
<li>captions for images, order matters</li>
<li>structure and context is relevant</li>
<li>machine generated music</li>
<li>past behaviour of neuron impacts the future</li>
<li>Sequence to Sequence: predict stock prices based on series of historic data</li>
<li>Sequence to vector: words in a sentence to sentiment</li>
<li>Vector to sequence: create captions from an image</li>
<li>Encoder -&gt; Decoder: Sequence -&gt; vector -&gt; sequence, machine translation</li>
<li>Backpropogation through time</li>
<li>Ends up looking like a really really deep neural network</li>
<li>Therefore, we use truncated backpropagation through time</li>
<li>State from earlier time steps get diluted over time, Long Short-Term memory cell LSTM cell</li>
<li>GRU cell: Gated Recurrent Unit, Simplified LSTM which performs almost as well</li>
<li>Traning RNN&rsquo;s is hard, very sensitive to topologies, choice of hyperparameters, very resource intensive, a wrong choice can lead to a RNN that does not converge at all.</li>
</ul>
<h2 id="modern-nlp-with-bert-and-gpt-and-transfer-learning">Modern NLP with BERT and GPT, and Transfer Learning</h2>
<ul>
<li>Transformer deep learning architectures</li>
<li>BERT, RoBERTa, T5, GPT2, GPT3, etc</li>
<li>DistilBERT: uses knowledge distillation to reduce model size by 40%</li>
<li>BERT: Bi-directional Encoder Representations from Transformers</li>
<li>GPT: Generative Pre-trained Transformer</li>
<li>Transfer Learning</li>
<li>Model zoos: hugging face offer pre trained models to start with</li>
<li>Hugging face DLC (deep learning containers)</li>
<li>Transfer Learning, retrain=True vs False</li>
</ul>
<h2 id="deep-learning-on-ec2emr">Deep Learning on EC2/EMR</h2>
<ul>
<li>EMR supports Apache MXNet and GPU instance types</li>
<li>Appropriate instance types for deep learning</li>
<li>P3, P2, G3</li>
<li>Deep Learning AMI&rsquo;s</li>
</ul>
<h2 id="tuning-neural-networks">Tuning Neural Networks</h2>
<ul>
<li>Neural nets are trained by gradient descent or sth similar</li>
<li>We start at some random point, and sample different solutions seeking to minimize some cost functions, over many epochs</li>
<li>how far apart these samples are is the learning rate</li>
<li>learning rate is an example of a hyperparameter</li>
<li>batch size is also a hyperparameter, smaller batch size can work out of local minima</li>
<li>small batch size tend to not get stuck in local minima</li>
<li>large batch sizes can converge on the wrong solution at random</li>
<li>large learning rates can overshoot the correct solution</li>
<li>small learning rates increate training time</li>
</ul>
<h2 id="regularization-techniques-for-neural-networks-dropout-early-stopping">Regularization Techniques for Neural Networks (Dropout, Early Stopping)</h2>
<ul>
<li>Regularization helps with avoiding overfitting</li>
<li>build simple model, dropout, early stopping can also help with avoiding overfitting</li>
</ul>
<h2 id="l1-and-l2-regularization">L1 and L2 Regularization</h2>
<ul>
<li>L1: sum of abs value of weights: perform feature selection, computationally inefficient, sparse output</li>
<li>L2: sum of square of weights, all features considered but weighted, computationally efficient, dence output</li>
</ul>
<h2 id="grief-with-gradients-the-vanishing-gradient-problem">Grief with Gradients The Vanishing Gradient problem</h2>
<ul>
<li>vanishing gradient propogate to deeper layer</li>
<li>slope is approaching zero</li>
<li>it could be the local miminum or global where the convergence is happening</li>
<li>long short term memory RNN can be used</li>
<li>resnet also helps with vanishing gradient problem</li>
<li>better activation function (relu is a good choice)</li>
</ul>
<h2 id="the-confusion-matrix">The Confusion Matrix</h2>
<ul>
<li>sometimes accuracy does not tell the whole story</li>
<li>TP, TN, FP, FN</li>
<li>Confusion matrix shows this</li>
<li>multi class confusion matrix: heatmap</li>
</ul>
<h2 id="precision-recall-f1-auc-and-more">Precision, Recall, F1, AUC, and more</h2>
<ul>
<li>Precision/Correct Positives/Percent of relevant results: when you are a lot about false positives: TP/(TP+FP)</li>
<li>Recall/Sensitivity/True Positive Rate:  TP/(TP + FN): when you care about false negatives</li>
<li>F1 score: harmonic mean of Precision and Recall</li>
<li>Specificity: TN/(TN+FP)</li>
<li>RMSE, AMSE, etc.</li>
<li>ROC curve: Receiver Operating Characteristic Curve: Plot of true positive rate (recall) vs false positive rate at various threshold setting.</li>
<li>AUC curve: area under the ROC curve.</li>
</ul>
<h2 id="ensemble-methods-bagging-and-boosting">Ensemble Methods Bagging and Boosting</h2>
<ul>
<li>Bagging: Generate N new training sets by random sampling with replacement, each resampled model can be trained in parallel</li>
<li>Boosting: Observations are weighted, training is sequential</li>
<li>XGBoost is the latest hotness, boosting generally yields better accuracy, bagging avoids overfitting, bagging is easier to parallelize</li>
</ul>
<h2 id="introducing-amazon-sagemaker">Introducing Amazon SageMaker</h2>
<ul>
<li>built to handle the entire machine learning workflow</li>
<li>deploy model, evaluate results in production, fetch, clean and prepare data, train and evaluate a model</li>
<li>training data will be in s3, sagemakaker docker EC2 for inference</li>
<li>spins as many hosts, spins as many endpoints</li>
<li>Sagemaker notebook: notebook instance on EC2, has access to s3, scikit learn, spark, tensorflow, ability to deploy trained models for making predictions at scale</li>
<li>hyperparameter tuning from notebook</li>
<li>Sagemaker console</li>
<li>Data comes from S3, ideal format is RecordIO/Protobuf/csv</li>
<li>Can also ingest from Athena, EMR, Redshift, Amazon Keyspaces DB</li>
<li>Apache Spark integrates with Sagemaker</li>
<li>Scikit learn, numpy, pandas all work</li>
<li>Create training job</li>
<li>save your trained model to s3</li>
<li>can be deployed using persistent endpoint for making individual predictions on demand</li>
<li>or batch transform to get prediction for and entire dataset</li>
<li>inference pipelines</li>
<li>sagemaker neo for deploying to edge devices</li>
<li>elastic inference for accelerating deep learning models</li>
<li>automatic scaling of endpoints as needed</li>
</ul>
<h2 id="linear-learner-in-sagemaker">Linear Learner in SageMaker</h2>
<ul>
<li>Linear learer can handle both classification and regression</li>
<li>can do classification using Linear Learner threshold</li>
<li>as long as a line will fit</li>
<li>RecordIO wrapped protobuf float32, or csv (first column assumed to be the label)</li>
<li>File or pipe mode both supported</li>
<li>pipe mode will be more efficient</li>
<li>if s3 is taking to long to train, pipe is a simple optimization</li>
<li>training data should be normalized</li>
<li>input data should be shuffled</li>
<li>uses SGD</li>
<li>multiple models are optimized in parallel</li>
<li>tune l1, l2 regularization</li>
<li>balance multiclass weights: give each class equal importance in loss functions</li>
<li>learning rate, mini batch size, l1 regualization</li>
<li>multi gpu does not help</li>
</ul>
<h2 id="xgboost-in-sagemaker">XGBoost in SageMaker</h2>
<ul>
<li>eXtreme gradient boosting</li>
<li>boosted group of decision trees</li>
<li>gradient descent</li>
<li>winning a lot of kaggle competitions</li>
<li>fast</li>
<li>classification/regression</li>
<li>CSV/libsvm/recordIO-protobuf/parquet</li>
<li>models are searilized/deserialized with pickle</li>
<li>can use as a framework withing notebooks</li>
<li>or as a built in sagemaker algorithm</li>
<li>subsample (prevent overfitting)</li>
<li>ETA (step size shrinkage, prevents overfitting)</li>
<li>Gamma (minimul loss reduction to create a partition)</li>
<li>Alpha (L1 regularization term, larger = more conservative)</li>
<li>Lambda (L2 regularization term, larger = more conservative)</li>
<li>eval_metric: Optimize on AUC, example: if you care about false positives more than accuracy</li>
<li>scale_pos_weight: adjusts balance of positive and negative weights, helpful for unbalanced classes</li>
<li>max_depth : too high may overfit</li>
<li>Xgboost with cpu: M5 is a good choice (optimize for memory and not compute)</li>
<li>Xgboost with gpu: tree_method hyperparameter: gpu_hist, cheaper and faster, P3 is good choice</li>
</ul>
<h2 id="seq2seq-in-sagemaker">Seq2Seq in SageMaker</h2>
<ul>
<li>sequence to sequence (example machine translation, text summarization, speech to text)</li>
<li>implemented with RNN&rsquo;s and CNN&rsquo;s with attention</li>
<li>RecordIO-Protobuf tokens must be integers</li>
<li>start with tokenized text files</li>
<li>convert to protobuf using sample code</li>
<li>must provide training data, validation data and vocabulary files</li>
<li>training machine translation can take days, pretrained models are available</li>
<li>public training datasets are avaialable for specific translation tasks</li>
<li>batch_size, optimizer_type, learning_rate, num_layers_encoder, num_layers_decoder, can optimize on accuracy, bleu score (compares against multiple reference translations), perplexity (cross-entropy)</li>
<li>cannot be parallelized</li>
<li>can only use gpu instance</li>
<li>can use multi gpu within an instance machine</li>
</ul>
<h2 id="deepar-in-sagemaker">DeepAR in SageMaker</h2>
<ul>
<li>Forecasting one dimensional time series data</li>
<li>uses rnn&rsquo;s</li>
<li>allows you to train the same model over several related time series</li>
<li>finds frequencies and seasonality</li>
<li>json lines format, Gzip or Parquet</li>
<li>each record must contain, start and target</li>
<li>each record can contain dynamic features and categorical features</li>
<li>always include entire time series for training, testing and inference</li>
<li>use entire dataset as test set</li>
<li>do not use very large values for prediction (&gt;400)</li>
<li>train on many time series</li>
<li>contect length, epochs, mini batch size, learning rate, num cells</li>
<li>can use cpu or gpu</li>
<li>single or multi machine</li>
<li>cpu only for inferene</li>
<li>may need larger instances for tuning</li>
</ul>
<h2 id="blazingtext-in-sagemaker">BlazingText in SageMaker</h2>
<ul>
<li>Text classification: predict labels for a sentence, useful in web searches, information retrieal, supervised</li>
<li>Word2vec: creates a vector representation of workds</li>
<li>semantically similar words are represented by vectors close to each otehr</li>
<li>this is called a word embedding</li>
<li>it is useful for nlp, but is not an nlp algorithm itself</li>
<li>it only works on individual words, not sentences or documents</li>
<li>for supervised mode, one sentence per line, first word in the sentence is the string <em>label</em> followed by the label</li>
<li>Also, &ldquo;augmented manifest text format&rdquo;</li>
<li>Word3vec just wants a text file with one training sentence per line</li>
<li>There are multiple modes:</li>
<li>Cbow (Continuous Bag of Words)</li>
<li>Skip-gram</li>
<li>Batch skip-gram (Distributed computation over many CPU nodes)</li>
<li>Word2vec: mode, learning rate, window size, verctor dim, negative samples</li>
<li>Text classification: epochs, learning rate, word ngrams, vector dim</li>
<li>For cbow and skipgram, recommend a single ml.p3.2xlarge, any single CPU or single GPU instance will work</li>
<li>for batch_skipgram, can use single or multiple CPU instances</li>
<li>for text classification C5 recommended if less than 2GB training data, for larger datasets use a single GPU instance ml.p2.xlarge or ml.p3.2xlarge</li>
</ul>
<h2 id="object2vec-in-sagemaker">Object2Vec in SageMaker</h2>
<ul>
<li>creates low-dimensional dense embeddings of high-dimensional objects</li>
<li>compute nearest neighbors of objects</li>
<li>visualize clusters</li>
<li>genre prediction</li>
<li>recommendations</li>
<li>data must be tokenized into integers</li>
<li>training data consists of pairs of tokens and or sequenses of tokens</li>
<li>process data into json lines and shuffle it</li>
<li>train with two input channels, two encoders, and a comparator</li>
<li>encoder choices: average-pooled embeddings, cnn&rsquo;s, bidirectional lstm</li>
<li>comparator is followed by feed-fowrard neural network</li>
<li>usual suspect: dropout, early stopping, epochs, learning rate, bbatch size, layers, activation function, optimizer, weight decay</li>
<li>Enc1_network, enc2_network</li>
<li>instance types: can only train on a single machine (cpu or gpu, multi-gpu ok)</li>
<li>inference: use ml.p2.2xlarge</li>
</ul>
<h2 id="object-detection-in-sagemaker">Object Detection in SageMaker</h2>
<ul>
<li>identify all objects in an image with bounding box</li>
<li>detects and classifies objects with a single deep neural network</li>
<li>classes are accompanied by confidence scores</li>
<li>can train from scratch, or use pretrained models based on imagenet</li>
<li>recodrio or image format</li>
<li>with image format, supply a json file for annotation data for each image</li>
<li>takes and image input, outputs all instances of objects in teh imagte with categories and confidence scores</li>
<li>uses cnn with single shot multibox detector ssd algorithm, the base being vgg-16 or resnet-50</li>
<li>transfer learning mode/incrementatl training: use pretrained model for the base network instead of random inintial weights</li>
<li>uses flip, rescale, and jitter internally to avoid overfitting</li>
<li>mini batch size, learning rate, optimizer</li>
<li>gpu instances for training</li>
<li>multi gpu multi machines</li>
<li>for inference cpu is enough</li>
</ul>
<h2 id="image-classification-in-sagemaker">Image Classification in SageMaker</h2>
<ul>
<li>assign one or more labels to an image</li>
<li>does not tell you where objects are</li>
<li>mxnet recordio (not protobuf)</li>
<li>raw jpg or png</li>
<li>.lst files to associate image index and class</li>
<li>augmented manifest image format enables pipe mode</li>
<li>resnet cnn under the hood</li>
<li>full training mode</li>
<li>transfer learning mode</li>
<li>default image is 224 224 3</li>
<li>bbatch size, learning rate, optimizer</li>
<li>weight decay, beta 1, beta 2, eps, gamma</li>
<li>gpu instance fr training</li>
<li>cpu or gpu for inference</li>
</ul>
<h2 id="semantic-segmentation-in-sagemaker">Semantic Segmentation in SageMaker</h2>
<ul>
<li>pixel level object classificaion</li>
<li>different from image classification</li>
<li>useful for self driving vehicles, medical imaging, robot sensing</li>
<li>produces a semantic mask</li>
<li>jpg or img with annotations</li>
<li>augmented manifest image format supported for pipe mode</li>
<li>jpg images accepted for inference</li>
<li>mxnet gluon and gluon cv</li>
<li>fully convolution network, pyramid scene parsing, deeplabv3</li>
<li>resnet50, renet101, both rained on imagenet</li>
<li>incremental training, or scratch</li>
<li>epochs, learning rate, batch size, optimizer, algorithm, backbone</li>
<li>only gpu for training (p2 or p3), and only on one maching</li>
<li>cpu or gpu for inference</li>
</ul>
<h2 id="random-cut-forest-in-sagemaker">Random Cut Forest in SageMaker</h2>
<ul>
<li>anomaly detection</li>
<li>unsupervised</li>
<li>detect unexpected spikes in time series data</li>
<li>breaks in periodicity</li>
<li>unclassifiable data points</li>
<li>assigns and anamoly score to each data points</li>
<li>recordio protobuf or csv</li>
<li>can use file or pipe mode on either</li>
<li>optional test channel for computation</li>
<li>creates a forest of trees where each tree is a partition of the training data, looks at expected change in complexity of the tree as a result of adding a point into it</li>
<li>data is sampled randomly and then trained</li>
<li>rcf shows up in kinesis analytics as well, it can work on streaming data as well.</li>
<li>num_trees, num_samples_per_tree (should be chosen such that 1/num_samples_per_tree approximates the ratio of anomalous to normal data)</li>
<li>does not take advantage of gpu</li>
<li>ml.c5.xl for inference</li>
</ul>
<h2 id="neural-topic-model-in-sagemaker">Neural Topic Model in SageMaker</h2>
<ul>
<li>organize documents into topics</li>
<li>classify or summarize documents based on topics</li>
<li>it is not just tf/idf</li>
<li>unsupervised: algorithm is neural variational inference</li>
<li>four data channels, train, validation, test and auxiliary</li>
<li>record io or csv</li>
<li>words muyst be tokenized into integers</li>
<li>file or pipe mode</li>
<li>you define how many topics you want, these topics are latent representation based on top ranking words</li>
<li>one of two modelling algorithms sagemaker offers</li>
<li>batch size, num_topics</li>
<li>gpu or cpu</li>
</ul>
<h2 id="latent-dirichlet-allocation-lda-in-sagemaker">Latent Dirichlet Allocation (LDA) in SageMaker</h2>
<ul>
<li>latent dirichlet allocation</li>
<li>another topic modeling algorithm but not based on deep learning</li>
<li>unsupervised: topics are unlabeled, they are just grouping of documents with a shared subseet of words</li>
<li>can be used for other purposes as well</li>
<li>train channel, optional test channel</li>
<li>protobuf or csv</li>
<li>each document has counts for every word in vocabulary</li>
<li>pipe mode: only supported with proto</li>
<li>unsupervised, generates however many topics you specify</li>
<li>per-word log likelyhood</li>
<li>num_topics, alpha0</li>
<li>cpu single instance, cannot parallelize</li>
</ul>
<h2 id="k-nearest-neighbors-knn-in-sagemaker">K-Nearest-Neighbors (KNN) in SageMaker</h2>
<ul>
<li>simple classification or regression algorithm</li>
<li>classification: k closest points</li>
<li>regression: average values</li>
<li>train channel, test channel emits accuracy or MSE</li>
<li>protobuf or csv (first column is label)</li>
<li>data is sampled, sagemaker includes dimensionality reduction stage, build an index for looking up neighbors, serialize the model, query the model for given K</li>
<li>hyperparameter K, sample_size</li>
<li>cpu or gpu</li>
<li>cpu or gpu for inference</li>
</ul>
<h2 id="k-means-clustering-in-sagemaker">K-Means Clustering in SageMaker</h2>
<ul>
<li>unsupervised clustering</li>
<li>divide data into k groups, where members of a group are as similar as possible to each other</li>
<li>web scale k means clustering</li>
<li>training input: train channel, train in shardedbys3key and testing: fullyreplicated</li>
<li>recordio or csv</li>
<li>file or pipemode</li>
<li>every ovservation is mapped to n-dimensional space</li>
<li>works to optimize the center of k clusters</li>
<li>algorithm: k means++ tries to make initial clusters far away, lloyd&rsquo;s method</li>
<li>mini_batch_size, extra_center_factor, init_method</li>
<li>cpu or gpu, but cpu recommended</li>
<li>only one gpu per instance used on gpu</li>
</ul>
<h2 id="principal-component-analysis-pca-in-sagemaker">Principal Component Analysis (PCA) in SageMaker</h2>
<ul>
<li>dimensionality reduction</li>
<li>unsupervised</li>
<li>covariance matrix is created, then SVD</li>
<li>two modesL regular: for sparse matrix, randomized: for large number of observations and features</li>
<li>algorithm_mode and subtract_mean</li>
<li>gpu or cpu</li>
</ul>
<h2 id="factorization-machines-in-sagemaker">Factorization Machines in SageMaker</h2>
<ul>
<li>dealing with sparse data</li>
<li>item recommendations</li>
<li>supervised: classification or regression</li>
<li>limited to pair-wise interactions</li>
<li>protobuf with float32</li>
<li>bias, factors, and linear terms</li>
<li>cpu or gpu, cpu recommended</li>
</ul>
<h2 id="ip-insights-in-sagemaker">IP Insights in SageMaker</h2>
<ul>
<li>finding fishy behaviour</li>
<li>unsupervised learning of ip address</li>
<li>identifies suspicious behaviour from ip addresses</li>
<li>user names, account ids, not need to pre process</li>
<li>training channel, optional validation (computes auc score)</li>
<li>csv only (entity, ips)</li>
<li>neural network to learn latent vector representations of entities and ip addresses</li>
<li>entities are hashed and embedded</li>
<li>automatically generates negative samples during training by randomly pairing entities and ips</li>
<li>num_entity vectors, vector_dim, epochs, learning rate, batch size</li>
<li>cpu or gpu</li>
<li>gpu recommended</li>
<li>multiple gpu can be used withing an instance</li>
</ul>
<h2 id="reinforcement-learning-in-sagemaker">Reinforcement Learning in SageMaker</h2>
<ul>
<li>agent and environment</li>
<li>supply chain management, hvac systems, industrial robots, dialog systems, autonomous vehicles</li>
<li>yields fast on-line performance once the space has been explored</li>
<li>Q learning: environment, actions, state/action part</li>
<li>uses a deep learning framework with tensorflow and mxnet</li>
<li>supports intel coach and ray rllib toolkits</li>
<li>custom, open-source or commercial environments supported</li>
<li>can distribute trining and environment rollout</li>
<li>multi core and multi instance</li>
</ul>
<h2 id="automatic-model-tuning">Automatic Model Tuning</h2>
<ul>
<li>define the hyperparameters you care about</li>
<li>sagemaker spins up a hyperparameter tuning job that trains as many combinations as you will allow</li>
<li>it learns as it goes, so it does not have to try every possible combination</li>
<li>intelligent</li>
<li>do not optimize too many hyperparameters at once</li>
<li>limit your ranges to as samall range</li>
<li>use logarithmic scales</li>
<li>do not run too many training jobs concurently</li>
<li>make sure training jobs running on multiple instance report the correct objective metric in the end</li>
</ul>
<h2 id="apache-spark-with-sagemaker">Apache Spark with SageMaker</h2>
<ul>
<li>apache spark allows for preprocessing and also has mllib</li>
<li>combination of sagemaker and spark is possible</li>
<li>preprocess with spark, and instead of using mllib, you can use sagemaker estimator, you can use kmeans, pca, xgboost</li>
<li>sagemakermodel, can be used to make inferences</li>
<li>connect notebook to a remote emr</li>
<li>fit, transform in sagemaker</li>
</ul>
<h2 id="sagemaker-studio-and-sagemaker-experiments">SageMaker Studio, and SageMaker Experiments</h2>
<ul>
<li>visual ide</li>
<li>sagemaker notebooks</li>
<li>sagemaker experiments</li>
</ul>
<h2 id="sagemaker-debugger">SageMaker Debugger</h2>
<ul>
<li>saves internal model state at periodical intervals</li>
<li>gradients/tensors over time is saved</li>
<li>define rules for detecting unwanted conditions while training</li>
<li>a debug job is run for each rule</li>
<li>logs and fires a cloudwatch event when the rule is hit</li>
<li>sagemaker studio debugger dashboards</li>
<li>auto generated training reports</li>
<li>built in rules: monitor system bottlenecks, profile model framework operations, debug model parameters</li>
<li>supported framewords and algorithms: tensorflow, pytorch, mxnet, xgboost, sagemaker generic estimator</li>
<li>debugger api&rsquo;s available in github</li>
<li>smdebug is the library</li>
<li>Sagemaker debugger insights dashboard</li>
<li>profiler report, hardware system metrics, framework metrics</li>
<li>built in actions to receive notifications or stop training</li>
<li>profiling system resource usage and training</li>
</ul>
<h2 id="sagemaker-autopilot--automl">SageMaker Autopilot / AutoML</h2>
<ul>
<li>automates algorithm selection, data preprocessing, model tuning</li>
<li>it does all the trial and error for you</li>
<li>automl</li>
<li>automatic model creation</li>
<li>model leaderboard</li>
<li>ranks</li>
<li>can add in human guidance</li>
<li>human in the loop</li>
<li>with or without code in sagemaker studio</li>
<li>problem types: binary/multiclass classification</li>
<li>linear learner, xgboost, mlp</li>
<li>data must be tabular csv</li>
<li>autopilot explainability</li>
<li>integrates with sagemaker clarify</li>
<li>transparency on how models arrive at predictions</li>
<li>feature attributions: uses shap baselines/shapley values, research from cooperative game theory, assigns each feature an importance value for a give prediction</li>
</ul>
<h2 id="sagemaker-model-monitor">SageMaker Model Monitor</h2>
<ul>
<li>get alery on quality deviations on your deployed models via cloudwatch</li>
<li>visualize data drift</li>
<li>detect anomalies and outliers</li>
<li>detect new features</li>
<li>no code required</li>
<li>data is stored in s3, monitoring jobs are scheduled via a monitoring schedule, metrics are emitted to cloudwatch, integrates with quicksight, tensorboard etc.</li>
<li>drift in statistical properties of the features</li>
<li>drift in model quality</li>
<li>bias drift</li>
<li>feature attribution drift</li>
</ul>
<h2 id="other-recent-features-jumpstart-data-wrangler-features-store-edge-manager">Other recent features (JumpStart, Data Wrangler, Features Store, Edge Manager)</h2>
<ul>
<li>jumpstart: one click models and algorithms from model zoos: 150 open source models in nlp, object detection, image classification etc</li>
<li>data wrangler: import transform analayze and export data withing sagemaker studio</li>
<li>feature studio: find, discover and share features in studio:online and offline modes</li>
<li>sagemaker edge manager: software agent for edge devices, models optimized with agemaker neo, collects and samples data for monitoring, labeling and retraining</li>
</ul>
<h2 id="sagemaker-canvas">SageMaker Canvas</h2>
<ul>
<li>no code machine learning for business analysts</li>
<li>upload csv data, select a column to predict, build it and make predictions</li>
<li>can also join datasets</li>
<li>classification or regressions</li>
<li>automatic data cleaning, missing values, outlier and duplicates</li>
<li>share models and datasets with sagemaker studio</li>
<li>import from redshift is possible</li>
<li>time series must be enabled via IAM</li>
<li>vpc</li>
<li>a little expensive</li>
</ul>
<h2 id="bias-measures-in-sagemaker-clarify">Bias Measures in SageMaker Clarify</h2>
<ul>
<li>class imbalance</li>
<li>difference in proportions of labels</li>
<li>kullback-leibler divergence, jensen-shannon divergence</li>
<li>lp-norm</li>
<li>total variation distance</li>
<li>kolmogorov-smirnov</li>
<li>conditional demographic disparity</li>
</ul>
<h2 id="sagemaker-training-compiler">SageMaker Training Compiler</h2>
<ul>
<li>integrates into AWS deep learning containers</li>
<li>compile and optimize training jobs on gpu</li>
<li>can accelerate training up to 50%</li>
<li>converts models into hardware-optimized instructions</li>
<li>tested with hugging face transformers library, or bring your own model</li>
<li>ensure gpu instance are used in ml.p3, ml.p4</li>
<li>pytorch models must use pytorch xla&rsquo;s model save function</li>
<li>enable dubug flask in compiler_config parameter to enable debugging</li>
</ul>
<h2 id="amazon-comprehend">Amazon Comprehend</h2>
<ul>
<li>nlp and text analytics</li>
<li>input social media, emails, web pages, documents, transcripts, medical records (comprehend medical)</li>
<li>extract key phrases, entities, sentiment, language, syntax, topics, and document classifications</li>
</ul>
<h2 id="amazon-translate">Amazon Translate</h2>
<ul>
<li>translates text</li>
<li>uses deep learning</li>
<li>supports custom terminology for proper names</li>
</ul>
<h2 id="amazon-transcribe">Amazon Transcribe</h2>
<ul>
<li>speech to text</li>
<li>speaker identification</li>
<li>channel identification</li>
<li>language identification</li>
<li>custom vocabularies</li>
</ul>
<h2 id="amazon-polly">Amazon Polly</h2>
<ul>
<li>text to speech</li>
<li>polly is parrot</li>
<li>lexicons</li>
<li>ssml (speech synthesis markup language)</li>
<li>speech marks</li>
</ul>
<h2 id="amazon-rekognition">Amazon Rekognition</h2>
<ul>
<li>compute vision</li>
<li>object and scene detection</li>
<li>image moderation, facial analysis, celebrity recognition, face comparison, text in image, video analysis</li>
<li>kinesis video stream h.264 encoded, 5-30 fps</li>
<li>can use lambda to trigger image analysis upon upload</li>
</ul>
<h2 id="amazon-forecast">Amazon Forecast</h2>
<ul>
<li>fully managed service to deliver highly accurate forecasts with ml</li>
<li>automl chooses the best model for your time series data</li>
<li>arima, deepar, ets, npts, prophet</li>
<li>works with any time series</li>
<li>inventory planning, financial planning, resource planning, based on dataset groups, predictors and forecasts</li>
</ul>
<h2 id="amazon-forecast-algorithms">Amazon Forecast Algorithms</h2>
<ul>
<li>cnnqr: convolutional neural network quantile regression, best for large datasets with hundreds of time series, accepts related historical time series data and metadata</li>
<li>deepar+ : recurrent neural network, best for large datasets, accepts related forward-looking time series and metadata</li>
<li>prophet: additive model with non linear trends and seasonality</li>
<li>npts: non parametric time series: good for sparse data</li>
<li>arima: simple datasets</li>
<li>ets: exponential smoothing</li>
</ul>
<h2 id="amazon-lex">Amazon Lex</h2>
<ul>
<li>chatbot engine</li>
<li>lambda to fulfill intent from text</li>
<li>can deploy to aws mobile sdk, facebook messenger, slack, twilio</li>
</ul>
<h2 id="amazon-personalize">Amazon Personalize</h2>
<ul>
<li>fully managed recommendation engine</li>
<li>api access: feed in data, provide schema in avro, javascript or sdk, get recommendations, get personalized ranking</li>
<li>real time or batch recommendations</li>
<li>recommendations for new users and new items</li>
<li>contextual recommendations</li>
<li>similar items</li>
<li>datasets, recipes, solutions, compaignhs</li>
<li>hidden_dimensions, bptt, recency_mask, min/max_user_history_length_percentile, exploration_weight, exploration_item_age_cut_off</li>
<li>necessary to maintain recency</li>
<li>bucket policy</li>
<li>data ingestion: per gb, training per training hour, inference per tps-hour, batch recommendations: per user or per item</li>
</ul>
<h2 id="lightning-round-textract-deeplens-deepracher-lookout-and-monitron">Lightning round! TexTract, DeepLens, DeepRacher, Lookout, and Monitron</h2>
<ul>
<li>TexTract: ocr with forms, fields, tables support</li>
<li>DeepLens: deep learning enabled video camera, integrated with rekognition, sagemaker, polly, tensorflow, mxnet, caffe</li>
<li>DeepRacer: reinforcement learning powered 1/18 scale race car</li>
<li>Lookout: equipment, metrics and vision: detect defects in silicon wafers, circuit boards etc.</li>
<li>Monitron: end to end system for monitoring equipment and predictive maintenance</li>
</ul>
<h2 id="torchserve-aws-neuron-and-aws-panorama">TorchServe, AWS Neuron, and AWS Panorama</h2>
<ul>
<li>TorchServe: model serving framework for pytorch</li>
<li>AWS Neuron: ml inferentia chip, Ec2 inf1 instance type</li>
<li>Panorama: computer vision at the edge</li>
</ul>
<h2 id="deep-composer-fraud-detection-codeguru-and-contact-lens">Deep Composer, Fraud Detection, CodeGuru, and Contact Lens</h2>
<ul>
<li>DeepComposer: ai powered keyboard</li>
<li>fraud detection: upload your own data</li>
<li>Codeguru: automated code reviews, finds lines of code that hurt performance</li>
<li>contact lens: for customer support call centers, ingests audio, sentiment analysis</li>
<li>finds utterances that correlate with successful calls</li>
<li>categorize calls automatically</li>
<li>measure talk speed and interruptions</li>
<li>theme detection: discovers emerging issues</li>
</ul>
<h2 id="amazon-kendra-and-amazon-augmented-ai-a2i">Amazon Kendra and Amazon Augmented AI (A2I)</h2>
<ul>
<li>Enterprise search with natural languate</li>
<li>combines data from sharepoint, intranet, sharing services, jdbc, s4 into one searchable repo</li>
<li>ml powered, uses thumbs up/down</li>
<li>relevance tuning, boost strength of document freshness</li>
<li>Kendra: Alexa&rsquo;s sister</li>
<li>AugmentedAI: human review of ml predictions, mechanical turk workforce or vendors</li>
<li>integrated into textract and rekognition</li>
<li>integrates with sagemaker</li>
</ul>
</div>
	</section>

</article>

		</main>
		<aside role="contentinfo"
			class="w-full md:w-2/5 xl:w-1/2 md:pr-12 lg:pr-20 xl:pr-24 order-4 md:order-3 md:sticky md:bottom-0 self-end max-w-2xl">
			<div class="md:float-right md:text-right leading-loose tracking-tight md:mb-2">
				
	<div class="md:max-w-xs  flex flex-col md:items-end">
	<ul class="font-serif flex-grow-0 flex justify-between flex-wrap md:flex-col">
	
	
	<li class="px-1 md:px-0">
		<a href="/posts/" title="Posts page" 
			class="font-medium text-medium-red-violet-600 hover:text-medium-red-violet-400" >
			Posts
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/resume/" title="Resume page" >
			Resume
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/certifications/" title="Certifications page" >
			Certifications
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/publications/" title="Publications page" >
			Publications
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/ml_glossary/" title="ML Glossary page" >
			ML Glossary
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/tags/" title="Tags page" >
			Tags
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/categories/" title="Categories page" >
			Categories
		</a>
	</li>
	
	
	
	
	<div id="fastSearch" class="m-0">
		<input id="searchInput" type="text" size=10 
			class="bg-gray-100 focus:outline-none border-b border-gray-100 focus:border-eucalyptus-300 md:text-right
			placeholder-java-500 min-w-0 max-w-xxxs"
			placeholder="search" />
		<ul id="searchResults" class="bg-gray-200 px-2 divide-y divide-gray-400">
		</ul>
	</div>
	
</ul>
	

<div class="flex flex-wrap-reverse md:justify-end content-end md:content-start justify-start items-start md:flex-col  max-h-16">
	
	<a href='https://github.com/ayushsubedi' target="_blank" class="github icon pl-1 text-eucalyptus-400 hover:text-java-400" title="github link" rel="noopener"
		aria-label="follow on github——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path fill-rule="nonzero" d="M5.883 18.653c-.3-.2-.558-.455-.86-.816a50.32 50.32 0 0 1-.466-.579c-.463-.575-.755-.84-1.057-.949a1 1 0 0 1 .676-1.883c.752.27 1.261.735 1.947 1.588-.094-.117.34.427.433.539.19.227.33.365.44.438.204.137.587.196 1.15.14.023-.382.094-.753.202-1.095C5.38 15.31 3.7 13.396 3.7 9.64c0-1.24.37-2.356 1.058-3.292-.218-.894-.185-1.975.302-3.192a1 1 0 0 1 .63-.582c.081-.024.127-.035.208-.047.803-.123 1.937.17 3.415 1.096A11.731 11.731 0 0 1 12 3.315c.912 0 1.818.104 2.684.308 1.477-.933 2.613-1.226 3.422-1.096.085.013.157.03.218.05a1 1 0 0 1 .616.58c.487 1.216.52 2.297.302 3.19.691.936 1.058 2.045 1.058 3.293 0 3.757-1.674 5.665-4.642 6.392.125.415.19.879.19 1.38a300.492 300.492 0 0 1-.012 2.716 1 1 0 0 1-.019 1.958c-1.139.228-1.983-.532-1.983-1.525l.002-.446.005-.705c.005-.708.007-1.338.007-1.998 0-.697-.183-1.152-.425-1.36-.661-.57-.326-1.655.54-1.752 2.967-.333 4.337-1.482 4.337-4.66 0-.955-.312-1.744-.913-2.404a1 1 0 0 1-.19-1.045c.166-.414.237-.957.096-1.614l-.01.003c-.491.139-1.11.44-1.858.949a1 1 0 0 1-.833.135A9.626 9.626 0 0 0 12 5.315c-.89 0-1.772.119-2.592.35a1 1 0 0 1-.83-.134c-.752-.507-1.374-.807-1.868-.947-.144.653-.073 1.194.092 1.607a1 1 0 0 1-.189 1.045C6.016 7.89 5.7 8.694 5.7 9.64c0 3.172 1.371 4.328 4.322 4.66.865.097 1.201 1.177.544 1.748-.192.168-.429.732-.429 1.364v3.15c0 .986-.835 1.725-1.96 1.528a1 1 0 0 1-.04-1.962v-.99c-.91.061-1.662-.088-2.254-.485z"/>
    </g>
</svg>

		</div>
	</a>
	
	<a href='https://www.instagram.com/ayushsube/' target="_blank" class="instagram icon pl-1 text-eucalyptus-400 hover:text-java-400" title="instagram link" rel="noopener"
		aria-label="follow on instagram——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path fill-rule="nonzero" d="M12 9a3 3 0 1 0 0 6 3 3 0 0 0 0-6zm0-2a5 5 0 1 1 0 10 5 5 0 0 1 0-10zm6.5-.25a1.25 1.25 0 0 1-2.5 0 1.25 1.25 0 0 1 2.5 0zM12 4c-2.474 0-2.878.007-4.029.058-.784.037-1.31.142-1.798.332-.434.168-.747.369-1.08.703a2.89 2.89 0 0 0-.704 1.08c-.19.49-.295 1.015-.331 1.798C4.006 9.075 4 9.461 4 12c0 2.474.007 2.878.058 4.029.037.783.142 1.31.331 1.797.17.435.37.748.702 1.08.337.336.65.537 1.08.703.494.191 1.02.297 1.8.333C9.075 19.994 9.461 20 12 20c2.474 0 2.878-.007 4.029-.058.782-.037 1.309-.142 1.797-.331.433-.169.748-.37 1.08-.702.337-.337.538-.65.704-1.08.19-.493.296-1.02.332-1.8.052-1.104.058-1.49.058-4.029 0-2.474-.007-2.878-.058-4.029-.037-.782-.142-1.31-.332-1.798a2.911 2.911 0 0 0-.703-1.08 2.884 2.884 0 0 0-1.08-.704c-.49-.19-1.016-.295-1.798-.331C14.925 4.006 14.539 4 12 4zm0-2c2.717 0 3.056.01 4.122.06 1.065.05 1.79.217 2.428.465.66.254 1.216.598 1.772 1.153a4.908 4.908 0 0 1 1.153 1.772c.247.637.415 1.363.465 2.428.047 1.066.06 1.405.06 4.122 0 2.717-.01 3.056-.06 4.122-.05 1.065-.218 1.79-.465 2.428a4.883 4.883 0 0 1-1.153 1.772 4.915 4.915 0 0 1-1.772 1.153c-.637.247-1.363.415-2.428.465-1.066.047-1.405.06-4.122.06-2.717 0-3.056-.01-4.122-.06-1.065-.05-1.79-.218-2.428-.465a4.89 4.89 0 0 1-1.772-1.153 4.904 4.904 0 0 1-1.153-1.772c-.248-.637-.415-1.363-.465-2.428C2.013 15.056 2 14.717 2 12c0-2.717.01-3.056.06-4.122.05-1.066.217-1.79.465-2.428a4.88 4.88 0 0 1 1.153-1.772A4.897 4.897 0 0 1 5.45 2.525c.638-.248 1.362-.415 2.428-.465C8.944 2.013 9.283 2 12 2z"/>
    </g>
</svg>

		</div>
	</a>
	
	<a href='https://www.linkedin.com/in/ayush-subedi/' target="_blank" class="linkedin icon pl-1 text-eucalyptus-400 hover:text-java-400" title="linkedin link" rel="noopener"
		aria-label="follow on linkedin——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path d="M12 9.55C12.917 8.613 14.111 8 15.5 8a5.5 5.5 0 0 1 5.5 5.5V21h-2v-7.5a3.5 3.5 0 0 0-7 0V21h-2V8.5h2v1.05zM5 6.5a1.5 1.5 0 1 1 0-3 1.5 1.5 0 0 1 0 3zm-1 2h2V21H4V8.5z"/>
    </g>
</svg>

		</div>
	</a>
	
	<a href='mailto:ayush.subedi@gmail.com' target="_blank" class="mail icon pl-1 text-eucalyptus-400 hover:text-java-400" title="mail link" rel="noopener"
		aria-label="follow on mail——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path d="M3 3h18a1 1 0 0 1 1 1v16a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1zm17 4.238l-7.928 7.1L4 7.216V19h16V7.238zM4.511 5l7.55 6.662L19.502 5H4.511z"/>
    </g>
</svg>
		</div>
	</a>
	
	<a href='https://public.tableau.com/app/profile/ayush3339' target="_blank" class="tableau icon pl-1 text-eucalyptus-400 hover:text-java-400" title="tableau link" rel="noopener"
		aria-label="follow on tableau——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M2 13H8V21H2V13ZM9 3H15V21H9V3ZM16 8H22V21H16V8Z"/></svg>
		</div>
	</a>
	
	<a href='https://twitter.com/ayushsubs' target="_blank" class="twitter icon pl-1 text-eucalyptus-400 hover:text-java-400" title="twitter link" rel="noopener"
		aria-label="follow on twitter——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path fill-rule="nonzero" d="M15.3 5.55a2.9 2.9 0 0 0-2.9 2.847l-.028 1.575a.6.6 0 0 1-.68.583l-1.561-.212c-2.054-.28-4.022-1.226-5.91-2.799-.598 3.31.57 5.603 3.383 7.372l1.747 1.098a.6.6 0 0 1 .034.993L7.793 18.17c.947.059 1.846.017 2.592-.131 4.718-.942 7.855-4.492 7.855-10.348 0-.478-1.012-2.141-2.94-2.141zm-4.9 2.81a4.9 4.9 0 0 1 8.385-3.355c.711-.005 1.316.175 2.669-.645-.335 1.64-.5 2.352-1.214 3.331 0 7.642-4.697 11.358-9.463 12.309-3.268.652-8.02-.419-9.382-1.841.694-.054 3.514-.357 5.144-1.55C5.16 15.7-.329 12.47 3.278 3.786c1.693 1.977 3.41 3.323 5.15 4.037 1.158.475 1.442.465 1.973.538z"/>
    </g>
</svg>

		</div>
	</a>
	
</div>
	<div class="text-sm text-gray-500 leading-tight a-gray">
		
		<br />
		4127 words in this page.
	</div>
</div>

			</div>
		</aside>
		<footer class="w-full md:w-3/5 xl:w-1/2 order-3 max-w-3xl md:order-4 pt-2">
			
<hr class="" />
<div class="flex flex-wrap justify-between pb-2 leading-loose font-serif">
    
    <a class="flex-grow-0" href="/posts/aws_ml_speciality_ml/">
        <svg class="fill-current inline-block h-4 w-4" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24"
            height="24">
            <path fill="none" d="M0 0h24v24H0z" />
            <path d="M7.828 11H20v2H7.828l5.364 5.364-1.414 1.414L4 12l7.778-7.778 1.414 1.414z" /></svg>
        AWS Certified ML - Specialty exam (MLS-C01) - 4. Machine Learning Implementation and Operations
    </a>
    
    
    <a class="flex-grow-0" href="/posts/aws_ml_speciality_eda/">
        AWS Certified ML - Specialty exam (MLS-C01) - 2. Exploratory Data Analysis
        <svg class="fill-current inline-block h-4 w-4" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24"
            height="24">
            <path fill="none" d="M0 0h24v24H0z" />
            <path d="M16.172 11l-5.364-5.364 1.414-1.414L20 12l-7.778 7.778-1.414-1.414L16.172 13H4v-2z" /></svg></a>
    
</div>
<div >



<div class="font-serif pb-2 flex align-start leading-loose">
	<span class="heading pr-6 leading-loose">Related</span>
	<span >
		
			<a href="/posts/aws_ml_speciality_data_engineering/">AWS Certified ML - Specialty exam (MLS-C01) - 1. Data Engineering</a>&nbsp;&nbsp;&#47;&nbsp;
		
			<a href="/posts/aws_ml_speciality_ml/">AWS Certified ML - Specialty exam (MLS-C01) - 4. Machine Learning Implementation and Operations</a>&nbsp;&nbsp;&#47;&nbsp;
		
			<a href="/posts/aws_ml_speciality_eda/">AWS Certified ML - Specialty exam (MLS-C01) - 2. Exploratory Data Analysis</a>&nbsp;&nbsp;&#47;&nbsp;
		
			<a href="/posts/school_satisfaction_and_school_ranking/">Enhancing Decision-Making for Parents and Authorities, A Comprehensive Analysis and Mapping of School Performance in New York City</a>&nbsp;&nbsp;&#47;&nbsp;
		
			<a href="/posts/tfidf/">Term Frequecy Inverse Document Frequency (TFIDF)</a>&nbsp;&nbsp;&#47;&nbsp;
		
			<a href="/posts/airlines_delay/">Flight delay prediction and exploration in the United States</a>&nbsp;&nbsp;&#47;&nbsp;
		
			<a href="/ml_glossary/">Machine Learning Glossary</a>&nbsp;&nbsp;&#47;&nbsp;
		
			<a href="/posts/beginners_trap/">The Beginner&#39;s Trap in analytics</a>&nbsp;&nbsp;&#47;&nbsp;
		
			<a href="/posts/ride_hailing_analytics/">Analytics for Ride Hailing Services</a>
		
</span>
</div>

</div>
<hr />
<div class="pb-2">
    
</div>
<hr />

		</footer>
		

<script src="/dist/app.js"></script>


<script src="/lib/fuse.min.js"></script> 
<script src="/lib/fastsearch.js"></script>

	</div>
</body>

</html>