<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
	
	<title>Ayush Subedi  | Deterministic Optimization</title>
	<meta name="viewport" content="width=device-width,minimum-scale=1">
	<meta name="generator" content="Hugo 0.102.3" />
	
	
	<META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
	

		
	<title>Ayush Subedi</title>
	<meta name="title" content="Ayush Subedi">
	<meta name="description" content="… personal journey with mathematics, software engineering and data science">

	
	<meta property="og:type" content="website">
	<meta property="og:url" content="https://subedi.ml/">
	<meta property="og:title" content="Ayush Subedi">
	<meta property="og:description" content="… personal journey with mathematics, software engineering and data science">
	<meta property="og:image" content="https://subedi.ml/img/k.png">

	
	<meta property="twitter:card" content="summary_large_image">
	<meta property="twitter:url" content="https://subedi.ml/">
	<meta property="twitter:title" content="Ayush Subedi">
	<meta property="twitter:description" content="… personal journey with mathematics, software engineering and data science">
	<meta property="twitter:image" content="https://subedi.ml/img/k.png">

	
	
	<link href="/dist/app.css" rel="stylesheet">
	

	

	
	
<link rel="shortcut icon" href="/img/favicon.ico" type="image/png" />

	

	
	
	
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-177424799-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	
	
	



<link rel="stylesheet" href='https://ayushsubedi.github.io/lib/katex.min.css' integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">


<script defer src='https://ayushsubedi.github.io/lib/katex.min.js' integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>


<script defer src='https://ayushsubedi.github.io/lib/contrib/auto-render.min.js' integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
crossorigin="anonymous"
onload='renderMathInElement(document.body);'></script>


<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

	
	
</head>

<body class="bg-gray-100 text-gray-700 font-sans">
	<div class="p-6 sm:p-10 md:p-16 flex flex-wrap">
		<header class="w-full md:w-2/5 xl:w-1/2 md:pr-12 lg:pr-20 xl:pr-24 order-1 md:order-1 max-w-2xl">
			<div
				class="z-50 bg-gray-100 bg-opacity-75 bg-opacity-custom lg:min-w-0.7 max-w-xl md:float-right md:text-right leading-loose tracking-tight md:sticky md:top-0 pt-2">
				
<div>
	<h2>
		<a href="https://ayushsubedi.github.io" title="Ayush Subedi" class="heading font-cursive icon">Ayush Subedi</a>
	</h2>
</div>
<h1 class="pt-2">Deterministic Optimization</h1>

<h3 class="text-java-700 font-normal leading-relaxed pt-2">Optimization is the process of adjusting a system to achieve the best possible performance or outcome. Deterministic (non-stochastic) optimization is a mathematical approach to finding the best solution to a problem by systematically searching the solution space for the optimal outcome. The optimization process is based on a set of deterministic (i.e., non-random) rules and algorithms, and the result of the optimization process is unique and repeatable.</h3>

<div class="flex flex-wrap justify-end pt-2 "><div class="md:flex-grow-0 font-light">
	
	
	
	
	<a class="post-taxonomy-category text-medium-red-violet-600 hover:text-medium-red-violet-400"
		href='/categories/gatech'>gatech</a>
	
	
	

	
	&nbsp;&nbsp;
	

	
	
	
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/optimization'>optimization</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/linear-algebra'>linear-algebra</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/objective-function'>objective-function</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/cost-function'>cost-function</a>&nbsp;&#47;
	
	<a class="post-taxonomy-tag text-eucalyptus-500"
		href='/tags/integral'>integral</a>
	
	
	
</div><time class="text-eucalyptus-500 md:text-right md:flex-grow font-light pl-4"
		datetime="2023-01-16">2023-01-16</time>
</div>

<hr />

			</div>
		</header>
		<main role="main" class="w-full md:w-3/5 xl:w-1/2 max-w-3xl order-2 md:order-2 min-h-70vh pt-2 pb-4">
			

<article>
	<section class="mx-auto content">
		<div class="c-rich-text"><h1 id="deterministic-optimization">Deterministic Optimization</h1>
<h2 id="introduction-to-optimization-models">Introduction to Optimization Models</h2>
<h3 id="generic-form-of-optimization-problem">Generic form of optimization problem:</h3>
<p>$min$ $f(x)$ $s.t.$ $x \in X $</p>
<h3 id="example-designing-a-box">Example: Designing a box:</h3>
<p><strong>Given a $1$ feet by $1$ feet piece of cardboard, cut out corners and fold to make a box of maximum volume:</strong><br/>
<strong>Decision:</strong> $x$ = how much to cut from each of the corners?<br/>
<strong>Alternatives:</strong> $0&lt;=x&lt;=1/2$<br/>
<strong>Best:</strong> Maximize volume: $V(x) = x(1-2x)^2$ ($x$ is the height and $(1-2x)^2$ is the base, and their product is the volume)<br/>
<strong>Optimization formulation:</strong> $max$ $x(1-2x)^2$ subject to $0&lt;=x&lt;=1/2$ (which are the constraints in this case)<br/></p>
<iframe src="https://www.desmos.com/calculator/ily45jyfsv?embed" width="100%" height="500" style="border: 1px solid #ccc" frameborder=0></iframe>
<h3 id="example-data-fitting">Example: Data Fitting:</h3>
<p><strong>Given $N$ data points $(y_1, x_1)&hellip;(y_N, x_N)$ where $y_i$ belongs to $\mathbb{R}$ and $x_i$ belongs to $\mathbb{R}^n$, for all $i = 1..N$, find a line $y = a^Tx+b$ that best fits the data.</strong><br/>
<strong>Decision</strong>: A vector $a$ that belongs to $\mathbb{R}^n$ and a scalar $b$ that belongs to $\mathbb{R}$<br/>
<strong>Alternatives</strong>: All $n$-dimensional vectors and scalars<br/>
<strong>Best</strong>: Minimise the sum of squared errors<br/>
<strong>Optimization formulation</strong>:
$\begin{array}{ll}\min &amp; \sum_{i=1}^N\left(y_i-a^{\top} x_i-b\right)^2 \ \text { s.t. } &amp; a \in \mathbb{R}^n, b \in \mathbb{R}\end{array}$</p>
<h3 id="example-product-mix">Example: Product Mix:</h3>
<p><strong>A firm make $n$ different products using $m$ types of resources. Each unit of product $i$ generates $p_i$ dollars of profit, and requires $r_{ij}$ units of resource $j$. The firm has $u_j$ units of resource $j$ available. How much of each product should the firm make to maximize profits?</strong><br/>
<strong>Decision</strong>: how much of each product to make<br/>
<strong>Alternatives</strong>: defined by the resource limits<br/>
<strong>Best</strong>: Maximize profits<br/>
<strong>Optimization formulation:</strong> <br/>
Sum notation: $\begin{array}{lll}\max &amp; \sum_{i=1}^n p_i x_i \ \text { s.t. } &amp; \sum_{i=1}^n r_{i j} x_i \leq u_j &amp; \forall j=1, \ldots, m \ &amp; x_i \geq 0 &amp; \forall i=1, \ldots, n\end{array}$ <br/>
Matrix notation: $\begin{array}{cl}\max &amp; p^{\top} x \ \text { s.t. } &amp; R x \leq u \ &amp; x \geq 0\end{array}$</p>
<h3 id="example-project-investment">Example: Project investment</h3>
<p><strong> A firm is considering investing in $n$ different R&amp;D projects. Project $j$ requires an investment of $c_j$ dollars and promises a return of $r_j$ dollars. The firm has a budget of $B$ dollars. Which projects should the firm invest in?</strong><br/>
<strong>Decision</strong>: Whether or not to invest in project<br/>
<strong>Alternatives</strong>: Defined by budget<br/>
<strong>Best</strong>: Maximize return on investment<br/>
Sum notation: $\begin{aligned} \max &amp; \sum_{j=1}^n r_j x_j \ \text { s.t. } &amp; \sum_{j=1}^n c_j x_j \leq B \ &amp; x_j \in{0,1} \forall j=1, \ldots, n\end{aligned}$ <br/>
Matrix notation: $\begin{aligned} \max  &amp; r^{\top} x \ \text { s.t. } &amp; c^{\top} x \leq B \ &amp; x \in{0,1}^n\end{aligned}$</p>
<h2 id="mathematical-ingredients-of-an-optimization-model">Mathematical ingredients of an optimization model:</h2>
<ul>
<li>Encode decisions/actions as <strong>decision variables</strong> whose values we are seeking</li>
<li>Identify the relevant <strong>problem data</strong></li>
<li>Express <strong>constraints</strong> on the values of the decision variables as mathematical relationships (inequalities) between the variables and problem data</li>
<li>Express the <strong>objective function</strong> as a function of the decision variables and the problem data.</li>
</ul>
<p>Minimize or Maximize an objective function of decision variable subject to constraints on the values of the decision variables.</p>
<pre tabindex="0"><code>min or max f(x1, x2, .... , xn)
subject to gi(x1, x2, ...., ) &lt;= bi     i = 1,....,m 
        xj is continuous or discrete    j = 1,....,n
</code></pre><h3 id="the-problem-setting">The problem setting</h3>
<ul>
<li>Finite number of decision variables</li>
<li>A single objective function of decision variables and problem data
<ul>
<li>Multiple objective functions are handled by either taking a weighted combination of them or by optimizing one of the objectives while ensuring the other objectives meet target requirements.</li>
</ul>
</li>
<li>The constraints are defined by a finite number of inequalities or equalities involving functions of the decision variables and problem data</li>
<li>There may be domain restrictions (continious or discrete) on some of the variables</li>
<li>The functions defining the objective and constraints are algebraic (typically with rational coeffiicients)</li>
</ul>
<h3 id="minimization-vs-maximization">Minimization vs Maximization</h3>
<ul>
<li>Without the loss of generality, it is sufficient to consider a minimization objective since maximization of objective function is minimization of the negation of the objective function</li>
</ul>
<h3 id="program-vs-optimization">Program vs optimization</h3>
<ul>
<li>A program or mathematical program is an optimization problem with a finite number of variables and constraints written out using explicit mathematical (algebraic) expressions</li>
<li>The word program means plan/planning</li>
<li>Early application of optimization arose in planning resource allocations and gave rise to programming to mean optimization (predates computer programming)</li>
</ul>
<h2 id="classification-of-optimization-problems">Classification of optimization problems</h2>
<ul>
<li>The tractability of a large scale optimization problem depends on the structure of the functions that make up the objective and constraints, and the domain restrictions on the variables.</li>
</ul>
<table>
<thead>
<tr>
<th>Functions</th>
<th>Variable domains</th>
<th>Problem Type</th>
<th>Difficulty</th>
</tr>
</thead>
<tbody>
<tr>
<td>All linear</td>
<td>Continuous variables</td>
<td>Linear Program</td>
<td>Easy</td>
</tr>
<tr>
<td>Some nonlinear</td>
<td>Continuous variables</td>
<td>Nonlinear Program or Nonlinear Optimization Problem</td>
<td>Easy/Difficult</td>
</tr>
<tr>
<td>Linear/nonlinear</td>
<td>Some discrete</td>
<td>Integer Problem or Discrete Optimization Problem</td>
<td>Difficult</td>
</tr>
</tbody>
</table>
<h3 id="subclasses-of-nlp-non-linear-problem">Subclasses of NLP (Non Linear Problem)</h3>
<ul>
<li>Unconstrained optimization: No constraints or simple bound constraints on the variables (Box design example above)</li>
<li>Quadrative programming: objectives and constraints involve quadratic functions (Data fitting example above), subset of NLP</li>
</ul>
<h3 id="subclasses-of-ip-integer-programming">Subclasses of IP (Integer Programming)</h3>
<ul>
<li>Mixed Integer Linear Program
<ul>
<li>All linear functions</li>
<li>Some variables are continous and some are discrete</li>
</ul>
</li>
<li>Mixed Integer Nonlinear Program (MINLP)
<ul>
<li>Some nonlinear functions</li>
<li>Some variables are continous and some are discrete</li>
</ul>
</li>
<li>Mixed Integer Quadratic Program (MIQLP)
<ul>
<li>Nonlinear functions are quadratic</li>
<li>Some variables are continuous and some are discrete</li>
<li>subset of MINLP</li>
</ul>
</li>
</ul>
<h3 id="why-and-how-to-classify">Why and how to classify?</h3>
<ul>
<li>Important to recognize the type of an optimization problem:
<ul>
<li>to formulate problems to be amenable to certain solution methods</li>
<li>to anticipate the difficulty of solving the problem</li>
<li>to know which solution methods to use</li>
<li>to design customized solution methods</li>
</ul>
</li>
<li>how to classify:
<ul>
<li>check domain restriction on variables</li>
<li>check the structure of the functions involved</li>
</ul>
</li>
</ul>
<h3 id="portfolio-optimization-problem">Portfolio Optimization Problem</h3>
<ul>
<li>Identify basic portfolio optimization and associated issues</li>
<li>Examine the Markowitz Portfolio Optimization approach
<ul>
<li>Markowitz Principle: Select a portfolio that attempts to maximize the expected return and minimize the variance of returns (risk)</li>
</ul>
</li>
<li>For multi objective problem (like defined by the Markowitz Principle), two objectives can be combined:
<ul>
<li>Maximize Expected Return - $\lambda$*risk</li>
<li>Maximize Expected Return subject to risk &lt;= s_max (constraint on risk)</li>
<li>Minimize Risk subject to return &gt;= r_min (threshold on expected returns)</li>
</ul>
</li>
<li>Optimization Problem Statement</li>
</ul>
<pre tabindex="0"><code>Given $1000, how much should we invest in each of the three stocks MSFT, V and WMT so as to :
- have a one month expected return of at least a given threshold
- minimize the risk(variance) of the portfolio return
</code></pre><ul>
<li>Decision: investment in each stock</li>
<li>alternatives: any investment that meets the budget and the minimum expected return requirement</li>
<li>best: minimize variance</li>
<li>Key trade-off: How much of the detail of the actual problem to consider while maintaining computational tractability of the mathematical model?</li>
<li>Requires making simplifying assumptions, either because some of the problem characteristics are not well-defined mathematically, or because we wish to develop a model that can actually be solved</li>
<li>Need to exercise great caution in these assumptions and not loose sight of the true underlying problem</li>
<li>Assumptions:
<ul>
<li>No transaction cost</li>
<li>Stocks does not need to be bought in blocks (any amount &gt;=0 is fine)</li>
</ul>
</li>
<li>Optimization Process: Decision Problem -&gt; Model -&gt; Data Collection -&gt; Model Solution -&gt; Analysis -&gt; Problem solution</li>
<li>No clear cut recipe</li>
<li>Lots of feedbacks and iterations</li>
<li>Approximations and assumptions involved in each stage</li>
<li>Success requires good understanding of the actual problem (domain knowledge is important)</li>
</ul>
<h2 id="notes-from-linear-algebra">Notes from Linear Algebra</h2>
<ul>
<li>A vector is a mathematical object that has both a magnitude (size) and a direction. Vectors are often used to represent physical quantities such as velocity or force. In two-dimensional space, a vector is represented by an ordered pair of numbers (x, y), and in three-dimensional space, it is represented by an ordered triple (x, y, z). Vectors can be added and subtracted, and multiplied by a scalar (a single number). They also have properties such as the dot product and cross product. In computer science and programming, a vector is also a data structure that can store multiple values of the same type.</li>
<li>The vectors $x$ and $y$ are orthogonal if $x^Ty=0$, they make an acute angle if $x^Ty&gt;0$ and an obtuse angle if $x^Ty&lt;0$</li>
<li>Also, $x^Ty=||x||.||y||cos\theta$</li>
<li>A set of vectors are linearly independent if none of the vectors can be written as a linear combination of the others. That is the unique solution to the system of equations. There can be at most $n$ linearly independent vectors in $R^n$</li>
<li>Any collection of $n$ linearly independent vectors in $R$ defines a basis (or a coordinate system) of $R^n$, any vector in $R^n$ can be written as a linear combination of the basis vectors  The unit vectors $e^1= [1, 0, &hellip;0]^T$, $e^2= [0, 1, &hellip;0]^T$,&hellip;,$e^n= [0, 0, &hellip;1]^T$, define the standard basis for $R^n$</li>
<li>The rank of a matrix is a measure of the &ldquo;nondegeneracy&rdquo; of the matrix and it is one of the most important concepts in linear algebra. It is defined as the dimension of the vector space spanned by its columns or rows. Intuitively, it represents the number of linearly independent columns or rows in the matrix. $row rank = column rank = rank(A)$. $A$ is full rank if $rank(A) = min(m, n)$</li>
<li>A system of equations has a solution when the equations are consistent, meaning that there is at least one set of values for the variables that satisfies all of the equations. If the equations are inconsistent, meaning that there is no set of values that satisfies all of the equations, then the system of equations has no solution.</li>
<li>An affine function is a function that is defined as a linear combination of variables, with the addition of a constant term. An affine function can be written as:</li>
</ul>
<pre tabindex="0"><code>f(x) = a_1x_1 + a_2x_2 + ... + a_nx_n + b
</code></pre><p>Where x_1, x_2, &hellip;, x_n are the input variables, a_1, a_2, &hellip;, a_n are the coefficients, and b is a constant term. An affine function is a generalization of a linear function, which does not have the constant term.</p>
<iframe width="100%" height ="1024" src="https://www.aerostudents.com/courses/linear-algebra/linearAlgebraFullVersion.pdf#toolbar=0"></iframe>
<h2 id="notes-from-multivariate-calculus">Notes from Multivariate Calculus</h2>
<h3 id="hessian-matrix">Hessian matrix</h3>
<ul>
<li>The Hessian matrix is a square matrix of second-order partial derivatives of a scalar-valued function of multiple variables.</li>
<li>The Hessian matrix of a scalar-valued function f(x) of n variables x = (x1, x2, &hellip;, xn) is defined as the matrix of second-order partial derivatives of f with respect to x, with the i-th row and j-th column containing the second partial derivative of f with respect to xi and xj.</li>
<li>The Hessian matrix is often used in optimization, for example, to find the local minima or maxima of a function. A point where the Hessian is positive definite is a local minimum, while a point where the Hessian is negative definite is a local maximum. If the Hessian is positive semi-definite, it&rsquo;s a saddle point.</li>
<li>It is important to notice that the Hessian Matrix is symmetric, therefore it has real eigenvalues and it is diagonalisable.</li>
</ul>
<h3 id="taylor-approximation">Taylor Approximation</h3>
<p>The Taylor series of a real or complex-valued function f (x) that is infinitely differentiable at a real or complex number a is the power series.
Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a differentiable function and $\mathbf{x}^0 \in \mathbb{R}^n$.</p>
<ul>
<li>First order Taylor&rsquo;s approximation of $f$ at $\mathbf{x}^0$ :
$$
f(\mathbf{x}) \approx f\left(\mathbf{x}^0\right)+\nabla f\left(\mathbf{x}^0\right)^{\top}\left(\mathbf{x}-\mathbf{x}^0\right)
$$</li>
<li>Second order Taylor&rsquo;s approximation of $f$ at $\mathbf{x}^0$ :
$$
f(\mathbf{x}) \approx f\left(\mathbf{x}^0\right)+\nabla f\left(\mathbf{x}^0\right)^{\top}\left(\mathbf{x}-\mathbf{x}^0\right)+\frac{1}{2}\left(\mathbf{x}-\mathbf{x}^0\right)^{\top} \nabla^2 f\left(\mathbf{x}^0\right)\left(\mathbf{x}-\mathbf{x}^0\right)
$$
`</li>
</ul>
<h3 id="sets-in-optimization-problems">Sets in Optimization Problems</h3>
<ul>
<li>A set is closed if it includes its boundary points.</li>
<li>Intersection of closed sets is closed.</li>
<li>Typically, if none of inequalities are strict, then the set is closed.</li>
<li>A set is convex if a line segment connecting two points in the set lies entirely in the set.</li>
<li>A set is bounded if it can be enclosed in a large enough (hyper)-sphere or a box.</li>
<li>A set that is both bounded and closed is called compact.
<ul>
<li>$R^2$ is closed but not bounded</li>
<li>$x^2+y^2&lt;1$ is bounded but not closed</li>
<li>$x+y&gt;=1$ is closed but not bounded</li>
<li>$x^2+y^2&lt;=1$ is closed and bounded (compact)</li>
</ul>
</li>
</ul>
<iframe src="https://www.desmos.com/calculator/49e59msg7u?embed" width="100%" height="500" style="border: 1px solid #ccc" frameborder=0></iframe>
<h2 id="convex-function">Convex Function</h2>
<ul>
<li>A function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is convex if
$$
f(\lambda \mathbf{x}+(1-\lambda) \mathbf{y}) \leq \lambda f(\mathbf{x})+(1-\lambda) f(\mathbf{y}) \quad \forall \mathbf{x}, \mathbf{y} \in \mathbb{R}^n \text { and } \lambda \in[0,1]
$$</li>
<li>Function value at the average is less than the average of the function values</li>
<li>This also implies that $a^Tx+b$ is convex (and concave)</li>
<li>For a convex function the first order Taylor&rsquo;s approximation is a global under estimator</li>
<li>A convex optimization problem has a convex objective and convex set of solutions.</li>
<li>Linear programs (LPs) can be seen as a special case of convex optimization problems. In an LP, the objective function and constraints are linear, which means that the feasible region defined by the constraints is a convex set. As a result, the optimal solution to an LP is guaranteed to be at a vertex (corner) of the feasible region, which makes it a convex optimization problem.</li>
<li>A twice differentiable univariate function is convex if the Hessian matrix is positive semi definite.</li>
<li>A positive semi-definite (PSD) matrix is a matrix that is symmetric and has non-negative eigenvalues. In the context of a Hessian matrix, it represents the second-order partial derivatives of a multivariate function and reflects the curvature of the function. If the Hessian is PSD, it indicates that the function is locally convex, meaning that it has a minimum value in the vicinity of that point. On the other hand, if the Hessian is not PSD, the function may have a saddle point or be locally non-convex. The PSD property of a Hessian matrix is important in optimization, as it guarantees the existence of a minimum value for the function.</li>
<li>Sylvester&rsquo;s criterion is a method for determining if a matrix is positive definite or positive semi-definite. The criterion states that a real symmetric matrix is positive definite if and only if all of its leading principal minors (i.e. determinants of the submatrices formed by taking the first few rows and columns of the matrix) are positive. If all the leading principal minors are non-negative, then the matrix is positive semi-definite.</li>
</ul>
<h3 id="operations-preserving-convexity">Operations preserving convexity</h3>
<ul>
<li>Nonnegative weighted sum of convex functions is convex, i.e. if $f_i$ is convex and $\alpha_i \geq 0$ for all $i=1, \ldots, m$, then $g(\mathbf{x})=\sum_{i=1}^m \alpha_i f_i(\mathbf{x})$ is convex.</li>
<li>Maximum of convex functions is convex.</li>
<li>Composition: Let $f: \mathbb{R}^m \rightarrow \mathbb{R}$ be a convex function, and $g_i: \mathbb{R}^n \rightarrow \mathbb{R}$ be convex for all $i=1, \ldots, m$. Then the composite function
$$
h(\mathbf{x})=f\left(g_1(\mathbf{x}), g_2(\mathbf{x}), \ldots, g_m(\mathbf{x})\right)
$$
is convex if either $f$ is nondecreasing or if each $q_i$ is a linear function.</li>
</ul>
<h3 id="convex-optimization-problem">Convex Optimization Problem</h3>
<ul>
<li>An optimization problem (in minimization) form is a convex optimization problem, if the objective function is a convex function and constraint set is a convex set.</li>
<li>The problem $min$ ${f(x) :  x \in X}$ is a convex optimization problem if $f$ is a convex function and $X$ is a convex set.</li>
<li>To check if a given problem is convex, we can check convexity of each cnstraint separately. (This is a sufficient test, not necessary).</li>
</ul>
<h3 id="sufficient-and-necessary">Sufficient and necessary</h3>
<ul>
<li>In mathematical logic, the terms &ldquo;sufficient&rdquo; and &ldquo;necessary&rdquo; are used to describe the relationship between two conditions.</li>
<li>A condition A is considered &ldquo;sufficient&rdquo; for a condition B if whenever condition A is true, condition B is also guaranteed to be true. In other words, if A is sufficient for B, then having A implies having B.</li>
<li>A condition B is considered &ldquo;necessary&rdquo; for a condition A if whenever condition B is false, condition A is also guaranteed to be false. In other words, if B is necessary for A, then not having B implies not having A.</li>
<li>Together, &ldquo;necessary and sufficient&rdquo; means that the two conditions are equivalent, in the sense that if one is true, then the other must also be true, and if one is false, then the other must also be false. In mathematical terms, A is necessary and sufficient for B if and only if (A if and only if B).</li>
<li>&ldquo;being a male is a necessary condition for being a brother, but it is not sufficient — while being a male sibling is a necessary and sufficient condition for being a brother&rdquo;</li>
</ul>
<h3 id="epigraph-of-a-function">Epigraph of a function</h3>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Epigraph_convex.svg/660px-Epigraph_convex.svg.png" alt=""></p>
<ul>
<li>An epigraph of a function is a graphical representation of the function&rsquo;s domain and range. It is formed by the region above the graph of the function and the line x = a for some value of a. The epigraph represents all possible values of the function for all values of x greater than or equal to a. It is used in optimization problems to visualize the feasible region for the optimization variable.</li>
<li>A function (in black) is convex if and only if the region above its graph (in green) is a convex set. This region is the function&rsquo;s epigraph.</li>
</ul>
<h2 id="outcomes-of-optimization">Outcomes of Optimization</h2>
<h3 id="possible-outcomes-of-optimization">Possible Outcomes of Optimization</h3>
<ul>
<li>Any $x \in X$ is a feasible solution of the optimization problem (P)</li>
<li>Feasible solution = A solution that satisfies all the constraints</li>
<li>An unbounded problem must be feasible</li>
<li>An optimization problem is unbounded, if there are feasible solutions with arbitrarily small objective values.</li>
<li>If $X=\emptyset$ then no feasible solutions exist, and the problem (P) is said to be infeasible.</li>
<li>The problem $\min {3x+ 2y: x+ y&lt;=1,x&gt;=2,y&gt;=2}$ is infeasible</li>
<li>An optimization problem can have 4 possible outcomes. The outcome can be infeasible, unbounded, have no optimal solution, have one optimal solution, or have multiple optimal solutions</li>
</ul>
<h3 id="existence-of-optimal-solutions">Existence of Optimal Solutions</h3>
<ul>
<li>The Weierstrass extreme value theorem asserts that if you minimize a continuous function over a closed and bounded set in $R_n$, then the minimum will be achieved at some point in the set.</li>
<li>Sufficient conditions: if the constraint set is bounded and non empty (feasible), then continuity and closedness guarantees an optimal solution exist.</li>
</ul>
<h3 id="local-and-global-optimal-solutions">Local and Global Optimal Solutions</h3>
<ul>
<li>Local optimal solutions are also global optimal solutions for convex optimization problems</li>
<li>Every global optimal solution is a local optimal solution, but not vice versa</li>
<li>The objective function value at different local optimal solutions may be different</li>
<li>The objective funtion value at all global solutions must be the same</li>
</ul>
<h3 id="idea-of-improving-search">Idea of Improving Search</h3>
<ul>
<li>Most optimization algorithms are based on the paradigm of improving search:
<ul>
<li>Start from a feasible solution</li>
<li>Move to a new feasible solution with a better objective value, Stop if not possible</li>
<li>Repeat step 2</li>
</ul>
</li>
<li>In general, we are only able to look in the &ldquo;neighbourhood&rdquo; of the current solution in search of a better feasible solution (solutions that are within a small positive distance from the current solution)</li>
<li>The move direction and step size should ensure that the new point is fesible and has an improved objective function value</li>
<li>The improving search is better for local solutions, but for convex, in principal it can be used to find global solutions (by definition)</li>
</ul>
<h2 id="optimality-certificates">Optimality Certificates</h2>
<h3 id="optimality-certificates-and-relaxations">Optimality Certificates and Relaxations</h3>
<ul>
<li>A certificate or a stopping condition is an easily checkable condition such that if the current solution satisfies this condition then it is guaranteed to be optimal or near optimal</li>
<li>Lower bound allows us to get upper bound on the gap</li>
<li>For two optimization problem (P) $min$ $f(x)$ $s.t.$ $x \in X $ and (Q) $min$ $g(x)$ $s.t.$ $x \in Y $, Problem (Q) is a relaxation of P if $X \subseteq Y$ and $f(x) &gt;= g(x) \forall x \in X $</li>
<li>obtained by enlarging the feasible region and underapproximating the objective function. we do not have to do both of those (see equals to sign)</li>
<li>Relaxation in optimization refers to the process of modifying or loosening the constraints of an optimization problem to make it easier to solve. The relaxed problem provides an approximate solution, which can then be improved by tightening the constraints.</li>
<li>Optimal value of the relaxation provides a lower bound on the original problem</li>
<li>If the relaxation is infeasible then clearly the original problem is also infeasible</li>
<li>Suppose only the constraints are relaxed, then if a solution to the relaxation is fesible to the original problem then it must be an optimal solution to the original problem</li>
</ul>
<h3 id="lagrangian-relaxation-and-duality">Lagrangian Relaxation and Duality</h3>
<ul>
<li>Lagrangian relaxation is a method used in optimization to solve a difficult problem by relaxing some of its constraints and instead optimizing a modified objective function known as the Lagrangian function. The Lagrangian function is constructed by adding a penalty term for each constraint to the original objective function. The penalty term is multiplied by a non-negative Lagrange multiplier that represents the slack in the constraint. By choosing appropriate values for the multipliers, the relaxed problem can be made to approximate the original problem.</li>
<li>The dual problem attempts to find the relaxation with the tightest bound</li>
<li>Weak duality: dual optimal value &lt;= original optimal value</li>
<li>Some times we get strong duality</li>
</ul>
<h2 id="unconstrained-optimization-derivative-based">Unconstrained Optimization: Derivative Based</h2>
<h3 id="optimality-conditions">Optimality Conditions</h3>
<ul>
<li>Unconstrained, that is the constraints are only $x \in R^n$ and twice differentiable</li>
<li>If a solution is a local optimal solution of an unconstrained problem, then the gradient vanishes at the point (First order optimality condition)</li>
<li>Hessian is a positive semidefinite (Second order optimality condition)</li>
<li>The conditions are necessary but not sufficient</li>
<li>For example for, $f(x)=x^3$, at point 0, both of the conditions are satisfied. However, it is neither a local min or max</li>
<li>A sufficient (but not necessary) condition would be the gradient vanishing at the point, and is the Hessian is positive definite.</li>
</ul>
<h3 id="gradient-descent">Gradient Descent</h3>
<ul>
<li>The gradient descent method moves from one iteration to the next by moving along the negative of the gradient direction in order to minimize the function.</li>
<li>Gradient descent is a optimization algorithm used to minimize the error of a machine learning model. It is an iterative method that updates the model parameters in the direction of the negative gradient of the cost function with respect to the parameters. The gradient indicates the direction of steepest increase in the cost function and the descent refers to moving in the direction of negative gradient to find the minimum of the cost function. The learning rate determines the size of the steps taken to reach the minimum and the algorithm stops when the change in cost is below a certain threshold or when a maximum number of iterations is reached.</li>
<li>Let $x^k$ be the current iterate, and we want to chose a downhill direction $d^k$ and a step size $a$ such that $f(x^k+ad^k)&lt;f(x^k)$</li>
<li>By Taylor&rsquo;s expansion, $f(x^k+ad^k) \approx f(x^k) + a \nabla f(x^k)^Td_k$</li>
<li>So we want $\nabla f(x^k)^Td_k &lt; 0$. The steepest descent direction is $d^k = - \nabla f(x^k) $</li>
<li>Step size can be identified using a line search. That is, define a function $g(a) := f(x^k + ad^k)$. Choose $a$ to minimize $g$. It can also be a small fixed step size.</li>
</ul>
<h3 id="newtons-method">Newton&rsquo;s Method</h3>
<ul>
<li>Newton&rsquo;s Method is a second-order optimization algorithm that is used to find the minimum of a function. It is an iterative method that updates the parameters by using the gradient of the function (first derivative) and the Hessian matrix (second derivative) to find the direction of the local minimum. The algorithm starts with an initial guess for the parameters and iteratively updates them using the Newton-Raphson formula until the change in the parameters is below a certain threshold or a maximum number of iterations is reached. Newton&rsquo;s Method is faster and more precise than gradient descent for well-behaved functions, but it can be sensitive to poor initialization and can get stuck in local minima.</li>
<li>$x^{k+1} $ = $x^k$ - $[\nabla^2$ $f(x_k)]^{-1}$ $ \nabla f(x^k)$</li>
<li>If started close enough to local minimum and the Hessian is positive definite, then the method has quadratic convergence</li>
<li>Not guarenteed to converge. The Newton direction may not be improving at all.</li>
<li>If the Hessian is singular ( or close to singular) at some iteration, we cannot proceed.</li>
<li>Computing gradient as well as the Hessian and its inverse is expensive</li>
</ul>
<h3 id="quasi-netwon-methods">Quasi-Netwon Methods</h3>
<ul>
<li>Blend of gradient descent and Newton&rsquo;s method.</li>
<li>Avoids computation of Hessian and its inverse</li>
<li>$x^{k+1} $ = $x^k$ - $a_k H_k$ $ \nabla f(x^k)$, where $H_k$ is an approximation of $[\nabla^2$ $f(x_k)]^{-1}$ and $a_k$ is determined by line search</li>
</ul>
<h2 id="unconstrained-optimization-derivative-free">Unconstrained Optimization: Derivative Free</h2>
<h3 id="methods-for-univariate-functions">Methods for Univariate Functions</h3>
<ul>
<li>Golden Section Search: Start with an initial interval $[x_l, x_u]$ containing the minima, and successively narrow this interval</li>
<li>Golden Section Search is an optimization algorithm used to find the minimum of a unimodal function, i.e., a function with a single minimum. The method is based on the idea of dividing an interval that contains the minimum into three sections, with the middle section being proportional to the golden ratio. The algorithm iteratively narrows down the interval by selecting the section that contains the minimum and discards the other sections. The process continues until the interval is sufficiently small and the minimum can be approximated with a desired accuracy. Golden Section Search is a bracketing method, which means it only requires the function to be unimodal and does not require the derivative or any other information about the function. It is a simple and efficient method for finding the minimum of unimodal functions, but it is slower than more sophisticated optimization methods for functions with multiple minima or more complex structures.</li>
<li>Step 0: Set $x_1 = x_u - a(x_u-x_l)$ and $x_2=x_l+a(x_u-x_l)$</li>
<li>Step 1: If $(x_u-x_l) &lt;= \epsilon$ stop and return $x^* = 0.5(x_l+x_u)$ as the minima</li>
<li>Example of how to use scipy.optimize.minimize to minimize a scalar function:</li>
</ul>
<pre tabindex="0"><code>import numpy as np
from scipy.optimize import minimize

def objective_function(x):
    return x**2 + 5*np.sin(x)

x0 = np.array([1.0]) # Initial guess
result = minimize(objective_function, x0, method=&#39;BFGS&#39;)
print(&#34;Minimum at:&#34;, result.x)
</code></pre><h3 id="methods-for-multivariate-function">Methods for Multivariate Function</h3>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/d/de/Nelder-Mead_Himmelblau.gif" alt=""></p>
<ul>
<li>The Nelder-Mead method is a optimization algorithm used to minimize a scalar function of several variables. It is a derivative-free method, meaning that it does not require the gradient of the objective function to be calculated. It works by constructing a simplex (a set of vertices) in the high-dimensional space defined by the input variables, and then iteratively modifying the vertices to find the minimum.</li>
<li>Here&rsquo;s an example of how to use scipy.optimize.minimize with the Nelder-Mead method:</li>
</ul>
<pre tabindex="0"><code>import numpy as np
from scipy.optimize import minimize

def objective_function(x):
    return x**2 + 5*np.sin(x)

x0 = np.array([1.0]) # Initial guess
result = minimize(objective_function, x0, method=&#39;Nelder-Mead&#39;)
print(&#34;Minimum at:&#34;, result.x)
</code></pre><ul>
<li>Nelder-Mead method is a numerical algorithm for minimizing a multivariate function using only function evaluations</li>
<li>It is not guaranteed to converge but often works well.</li>
</ul>
<h2 id="linear-optimization-modeling---network-flow-problems">Linear Optimization Modeling - Network Flow Problems</h2>
<h3 id="introduction-to-lp-modeling">Introduction to LP Modeling</h3>
<ul>
<li>A linear program is composed of:
<ul>
<li>Variables $x=(x_1,x_2,x_3&hellip;,x_n)$</li>
<li>Linear objective function $f(x_1,x_2,x_3&hellip;,x_n)=\sum_{i=1}^n c_i x_i = c^Tx$</li>
<li>Linear constraints: $&gt;=, &lt;= or =$</li>
</ul>
</li>
</ul>
<h3 id="optimal-transportation-problem">Optimal Transportation Problem</h3>
<ul>
<li>The transportation problem is a type of linear programming problem that deals with finding the optimal assignment of resources to meet a set of demands. The problem is typically framed as a network flow problem, where the goal is to find the maximum flow from a set of sources to a set of destinations.</li>
<li>In a transportation problem, the goal is to find the least cost way to transport a given amount of goods from a set of sources (e.g. factories) to a set of destinations (e.g. warehouses) subject to certain constraints such as limited supply at the sources and limited demand at the destinations. The cost of transporting a unit of goods from a source to a destination is represented by a cost matrix, which is usually obtained through market research or historical data.</li>
<li>There are various algorithms that can be used to solve transportation problems, including the North-West Corner Method, the Minimum Cost Method (also known as the Vogel&rsquo;s Approximation Method), and the Modified Distribution Method. The most popular algorithm for solving transportation problems is the Iterative Proportional Fitting (IPF) algorithm, also known as the MODI (Modified Distribution) method.</li>
<li>The transportation problem is an important optimization problem with numerous real-world applications, including supply chain management, distribution systems, and logistics planning.</li>
<li>There are $m$ suppliers, $n$ customers. Supplier $i$ can supply up to $s_i$ units of supply, and customer $j$ has $d_j$ units of demand. It costs $c_{ij}$ to transport a unit of product from supplier $i$ to customer $j$. We want to find a transportation schedule to satisfy all the demand within minimum transportation cost.</li>
<li>Formulation 1: $\begin{array}{ll}\min &amp; \sum_{i=1}^m \sum_{j=1}^n c_{i j} x_{i j} \ \text { s.t. } &amp; \sum_{i=1}^m x_{i j}=d_j, \quad \forall j \ &amp; \sum_{j=1}^n x_{i j} \leq s_i, \quad \forall i \ &amp; x_{i j} \geq 0, \quad \forall i, j .\end{array}$</li>
<li>Formulation 2: $\begin{array}{ll}\min &amp; \sum_{i=1}^m \sum_{j=1}^n c_{i j} x_{i j} \ \text { s.t. } &amp; \sum_{i=1}^m x_{i j}&gt;=d_j, \quad \forall j \ &amp; \sum_{j=1}^n x_{i j} \leq s_i, \quad \forall i \ &amp; x_{i j} \geq 0, \quad \forall i, j .\end{array}$</li>
<li>But &gt;= inequality in the second formulation will be satisfied as = at optimal solution, thus, the two formulations are equivalent</li>
<li>The graphs here are bipartite</li>
</ul>
<h3 id="maximum-flow-problem">Maximum Flow Problem</h3>
<ul>
<li>The maximum flow problem is a classical problem in network flow theory that aims to find the maximum amount of flow that can be sent from a source node to a sink node in a network, subject to capacity constraints on the edges. The maximum flow problem is a special case of the more general minimum cut problem, which aims to find the minimum capacity of a cut that separates the source and the sink in the network.</li>
<li>A network in this context is represented as a graph, where the nodes represent the vertices and the edges represent the capacities of the arcs. The source node is where the flow originates, and the sink node is where the flow terminates. The capacity constraints on the edges determine the maximum amount of flow that can be sent through a particular edge.</li>
<li>There are several algorithms that can be used to solve the maximum flow problem, including the Ford-Fulkerson algorithm, the Edmonds-Karp algorithm, and the push-relabel algorithm. These algorithms work by finding augmenting paths in the residual network, which is a network derived from the original network that represents the remaining capacities of the edges after some flow has already been sent. The algorithms continue to find augmenting paths until no more can be found, at which point the maximum flow has been found.</li>
<li>The maximum flow problem has many real-world applications, including traffic flow in transportation networks, the allocation of bandwidth in communication networks, and the distribution of resources in supply chain networks.</li>
<li>The graphs here are directed</li>
<li>$\begin{array}{ll}\max &amp; b_s \ \end{array}$</li>
<li>$\begin{array}{ll} \text { s.t. } &amp; \sum_{k \in O(i)} x_{i k}-\sum_{j \in I(i)} x_{j i}=b_i \quad \forall i \ &amp; b_t=-b_s \ &amp; b_i=0, \quad \forall i \neq s, t \ &amp; 0 \leq x_{i j} \leq u_{i j}, \quad \forall(i, j) \in \mathcal{A} .\end{array}$</li>
</ul>
<h3 id="minimum-cut-problem">Minimum Cut Problem</h3>
<ul>
<li>The Maximum Cut Problem is a well-known optimization problem in computer science and mathematics. The goal of the problem is to divide a given graph into two sets of vertices such that the sum of the weights of the edges between the two sets is as large as possible.</li>
<li>Formally, given a graph G = (V,E) with a weight function w : E → R, the maximum cut problem is to find a partition of the vertices into two sets S and T such that the sum of the weights of the edges between S and T is maximized.</li>
<li>The problem is NP-hard, meaning that finding the optimal solution is computationally infeasible for large graphs. However, there are approximate algorithms that can find near-optimal solutions, such as semidefinite programming, spectral methods, and local search algorithms.</li>
<li>The maximum cut problem has a wide range of applications, including network design, image and signal processing, and machine learning.</li>
<li>Minimum cut = Maximum flow</li>
</ul>
<h3 id="shortest-path-problem">Shortest Path Problem</h3>
<ul>
<li>The Maximum Cut Problem is a well-known optimization problem in computer science and mathematics. The goal of the problem is to divide a given graph into two sets of vertices such that the sum of the weights of the edges between the two sets is as large as possible.</li>
<li>Formally, given a graph G = (V,E) with a weight function w : E → R, the maximum cut problem is to find a partition of the vertices into two sets S and T such that the sum of the weights of the edges between S and T is maximized.</li>
<li>The problem is NP-hard, meaning that finding the optimal solution is computationally infeasible for large graphs. However, there are approximate algorithms that can find near-optimal solutions, such as semidefinite programming, spectral methods, and local search algorithms.</li>
<li>The maximum cut problem has a wide range of applications, including network design, image and signal processing, and machine learning.</li>
<li>Shortest Path Problem is a Flow problem if we are shipping 1 unit of flow from $s$ to all other nodes</li>
<li>$\begin{array}{ll}\min &amp; \sum_{(i, j) \in \mathcal{A}} c_{i j} x_{i j} \ \end{array}$</li>
<li>$\begin{array}{ll}{ s.t. } &amp; \sum_{k \in O(i)} x_{i k}-\sum_{j \in I(i)} x_{j i}=-1 \forall i \neq s \ &amp; \sum_{k \in O(s)} x_{s k}-\sum_{j \in I(s)} x_{j s}=n-1 \ &amp; x_{i j} \geq 0, \quad \forall(i, j) \in \mathcal{A} .\end{array}$</li>
</ul>
<h3 id="lp-model-for-market-clearing">LP model for market clearing:</h3>
<img src="/img/op.png" width="300" height="200">
<h3 id="rosenbrock-function">Rosenbrock function</h3>
<p>The Rosenbrock function is a widely used test function in optimization and is often used as a performance test for optimization algorithms. Here&rsquo;s a simple code to plot the Rosenbrock function in Python using Matplotlib:</p>
<pre tabindex="0"><code>import numpy as np
import matplotlib.pyplot as plt

def rosenbrock(x, y):
    return (1-x)**2 + 100*(y-x**2)**2

x = np.linspace(-2, 2, 400)
y = np.linspace(-1, 3, 400)
X, Y = np.meshgrid(x, y)
Z = rosenbrock(X, Y)

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection=&#39;3d&#39;)
ax.plot_surface(X, Y, Z, cmap=&#39;viridis&#39;)
ax.set_xlabel(&#39;X axis&#39;)
ax.set_ylabel(&#39;Y axis&#39;)
ax.set_zlabel(&#39;Z axis&#39;)
plt.show()
</code></pre><p><img src="/img/rosenbrock.png" alt=""></p>
</div>
	</section>

</article>

		</main>
		<aside role="contentinfo"
			class="w-full md:w-2/5 xl:w-1/2 md:pr-12 lg:pr-20 xl:pr-24 order-4 md:order-3 md:sticky md:bottom-0 self-end max-w-2xl">
			<div class="md:float-right md:text-right leading-loose tracking-tight md:mb-2">
				
	<div class="md:max-w-xs  flex flex-col md:items-end">
	<ul class="font-serif flex-grow-0 flex justify-between flex-wrap md:flex-col">
	
	
	<li class="px-1 md:px-0">
		<a href="/posts/" title="Posts page" 
			class="font-medium text-medium-red-violet-600 hover:text-medium-red-violet-400" >
			Posts
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/resume/" title="Resume page" >
			Resume
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/certifications/" title="Certifications page" >
			Certifications
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/publications/" title="Publications page" >
			Publications
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/ml_glossary/" title="ML Glossary page" >
			ML Glossary
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/tags/" title="Tags page" >
			Tags
		</a>
	</li>
	
	<li class="px-1 md:px-0">
		<a href="/categories/" title="Categories page" >
			Categories
		</a>
	</li>
	
	
	
	
	<div id="fastSearch" class="m-0">
		<input id="searchInput" type="text" size=10 
			class="bg-gray-100 focus:outline-none border-b border-gray-100 focus:border-eucalyptus-300 md:text-right
			placeholder-java-500 min-w-0 max-w-xxxs"
			placeholder="search" />
		<ul id="searchResults" class="bg-gray-200 px-2 divide-y divide-gray-400">
		</ul>
	</div>
	
</ul>
	

<div class="flex flex-wrap-reverse md:justify-end content-end md:content-start justify-start items-start md:flex-col  max-h-16">
	
	<a href='https://github.com/ayushsubedi' target="_blank" class="github icon pl-1 text-eucalyptus-400 hover:text-java-400" title="github link" rel="noopener"
		aria-label="follow on github——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path fill-rule="nonzero" d="M5.883 18.653c-.3-.2-.558-.455-.86-.816a50.32 50.32 0 0 1-.466-.579c-.463-.575-.755-.84-1.057-.949a1 1 0 0 1 .676-1.883c.752.27 1.261.735 1.947 1.588-.094-.117.34.427.433.539.19.227.33.365.44.438.204.137.587.196 1.15.14.023-.382.094-.753.202-1.095C5.38 15.31 3.7 13.396 3.7 9.64c0-1.24.37-2.356 1.058-3.292-.218-.894-.185-1.975.302-3.192a1 1 0 0 1 .63-.582c.081-.024.127-.035.208-.047.803-.123 1.937.17 3.415 1.096A11.731 11.731 0 0 1 12 3.315c.912 0 1.818.104 2.684.308 1.477-.933 2.613-1.226 3.422-1.096.085.013.157.03.218.05a1 1 0 0 1 .616.58c.487 1.216.52 2.297.302 3.19.691.936 1.058 2.045 1.058 3.293 0 3.757-1.674 5.665-4.642 6.392.125.415.19.879.19 1.38a300.492 300.492 0 0 1-.012 2.716 1 1 0 0 1-.019 1.958c-1.139.228-1.983-.532-1.983-1.525l.002-.446.005-.705c.005-.708.007-1.338.007-1.998 0-.697-.183-1.152-.425-1.36-.661-.57-.326-1.655.54-1.752 2.967-.333 4.337-1.482 4.337-4.66 0-.955-.312-1.744-.913-2.404a1 1 0 0 1-.19-1.045c.166-.414.237-.957.096-1.614l-.01.003c-.491.139-1.11.44-1.858.949a1 1 0 0 1-.833.135A9.626 9.626 0 0 0 12 5.315c-.89 0-1.772.119-2.592.35a1 1 0 0 1-.83-.134c-.752-.507-1.374-.807-1.868-.947-.144.653-.073 1.194.092 1.607a1 1 0 0 1-.189 1.045C6.016 7.89 5.7 8.694 5.7 9.64c0 3.172 1.371 4.328 4.322 4.66.865.097 1.201 1.177.544 1.748-.192.168-.429.732-.429 1.364v3.15c0 .986-.835 1.725-1.96 1.528a1 1 0 0 1-.04-1.962v-.99c-.91.061-1.662-.088-2.254-.485z"/>
    </g>
</svg>

		</div>
	</a>
	
	<a href='https://www.instagram.com/ayushsube/' target="_blank" class="instagram icon pl-1 text-eucalyptus-400 hover:text-java-400" title="instagram link" rel="noopener"
		aria-label="follow on instagram——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path fill-rule="nonzero" d="M12 9a3 3 0 1 0 0 6 3 3 0 0 0 0-6zm0-2a5 5 0 1 1 0 10 5 5 0 0 1 0-10zm6.5-.25a1.25 1.25 0 0 1-2.5 0 1.25 1.25 0 0 1 2.5 0zM12 4c-2.474 0-2.878.007-4.029.058-.784.037-1.31.142-1.798.332-.434.168-.747.369-1.08.703a2.89 2.89 0 0 0-.704 1.08c-.19.49-.295 1.015-.331 1.798C4.006 9.075 4 9.461 4 12c0 2.474.007 2.878.058 4.029.037.783.142 1.31.331 1.797.17.435.37.748.702 1.08.337.336.65.537 1.08.703.494.191 1.02.297 1.8.333C9.075 19.994 9.461 20 12 20c2.474 0 2.878-.007 4.029-.058.782-.037 1.309-.142 1.797-.331.433-.169.748-.37 1.08-.702.337-.337.538-.65.704-1.08.19-.493.296-1.02.332-1.8.052-1.104.058-1.49.058-4.029 0-2.474-.007-2.878-.058-4.029-.037-.782-.142-1.31-.332-1.798a2.911 2.911 0 0 0-.703-1.08 2.884 2.884 0 0 0-1.08-.704c-.49-.19-1.016-.295-1.798-.331C14.925 4.006 14.539 4 12 4zm0-2c2.717 0 3.056.01 4.122.06 1.065.05 1.79.217 2.428.465.66.254 1.216.598 1.772 1.153a4.908 4.908 0 0 1 1.153 1.772c.247.637.415 1.363.465 2.428.047 1.066.06 1.405.06 4.122 0 2.717-.01 3.056-.06 4.122-.05 1.065-.218 1.79-.465 2.428a4.883 4.883 0 0 1-1.153 1.772 4.915 4.915 0 0 1-1.772 1.153c-.637.247-1.363.415-2.428.465-1.066.047-1.405.06-4.122.06-2.717 0-3.056-.01-4.122-.06-1.065-.05-1.79-.218-2.428-.465a4.89 4.89 0 0 1-1.772-1.153 4.904 4.904 0 0 1-1.153-1.772c-.248-.637-.415-1.363-.465-2.428C2.013 15.056 2 14.717 2 12c0-2.717.01-3.056.06-4.122.05-1.066.217-1.79.465-2.428a4.88 4.88 0 0 1 1.153-1.772A4.897 4.897 0 0 1 5.45 2.525c.638-.248 1.362-.415 2.428-.465C8.944 2.013 9.283 2 12 2z"/>
    </g>
</svg>

		</div>
	</a>
	
	<a href='https://www.linkedin.com/in/ayush-subedi/' target="_blank" class="linkedin icon pl-1 text-eucalyptus-400 hover:text-java-400" title="linkedin link" rel="noopener"
		aria-label="follow on linkedin——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path d="M12 9.55C12.917 8.613 14.111 8 15.5 8a5.5 5.5 0 0 1 5.5 5.5V21h-2v-7.5a3.5 3.5 0 0 0-7 0V21h-2V8.5h2v1.05zM5 6.5a1.5 1.5 0 1 1 0-3 1.5 1.5 0 0 1 0 3zm-1 2h2V21H4V8.5z"/>
    </g>
</svg>

		</div>
	</a>
	
	<a href='mailto:ayush.subedi@gmail.com' target="_blank" class="mail icon pl-1 text-eucalyptus-400 hover:text-java-400" title="mail link" rel="noopener"
		aria-label="follow on mail——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path d="M3 3h18a1 1 0 0 1 1 1v16a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1zm17 4.238l-7.928 7.1L4 7.216V19h16V7.238zM4.511 5l7.55 6.662L19.502 5H4.511z"/>
    </g>
</svg>
		</div>
	</a>
	
	<a href='https://twitter.com/ayushsubs' target="_blank" class="twitter icon pl-1 text-eucalyptus-400 hover:text-java-400" title="twitter link" rel="noopener"
		aria-label="follow on twitter——Opens in a new window">
		
		<div class="fill-current h-8 w-8">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <g>
        <path fill="none" d="M0 0h24v24H0z"/>
        <path fill-rule="nonzero" d="M15.3 5.55a2.9 2.9 0 0 0-2.9 2.847l-.028 1.575a.6.6 0 0 1-.68.583l-1.561-.212c-2.054-.28-4.022-1.226-5.91-2.799-.598 3.31.57 5.603 3.383 7.372l1.747 1.098a.6.6 0 0 1 .034.993L7.793 18.17c.947.059 1.846.017 2.592-.131 4.718-.942 7.855-4.492 7.855-10.348 0-.478-1.012-2.141-2.94-2.141zm-4.9 2.81a4.9 4.9 0 0 1 8.385-3.355c.711-.005 1.316.175 2.669-.645-.335 1.64-.5 2.352-1.214 3.331 0 7.642-4.697 11.358-9.463 12.309-3.268.652-8.02-.419-9.382-1.841.694-.054 3.514-.357 5.144-1.55C5.16 15.7-.329 12.47 3.278 3.786c1.693 1.977 3.41 3.323 5.15 4.037 1.158.475 1.442.465 1.973.538z"/>
    </g>
</svg>

		</div>
	</a>
	
</div>
	<div class="text-sm text-gray-500 leading-tight a-gray">
		
		<br />
		5532 words in this page.
	</div>
</div>

			</div>
		</aside>
		<footer class="w-full md:w-3/5 xl:w-1/2 order-3 max-w-3xl md:order-4 pt-2">
			
<hr class="" />
<div class="flex flex-wrap justify-between pb-2 leading-loose font-serif">
    
    <a class="flex-grow-0" href="/posts/human_computer_interaction/">
        <svg class="fill-current inline-block h-4 w-4" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24"
            height="24">
            <path fill="none" d="M0 0h24v24H0z" />
            <path d="M7.828 11H20v2H7.828l5.364 5.364-1.414 1.414L4 12l7.778-7.778 1.414 1.414z" /></svg>
        Human-Computer Interaction
    </a>
    
    
</div>
<div >



<div class="font-serif pb-2 flex align-start leading-loose">
	<span class="heading pr-6 leading-loose">Related</span>
	<span >
		
			<a href="/posts/ride_hailing_analytics/">Analytics for Ride Hailing Services</a>
		
</span>
</div>

</div>
<hr />
<div class="pb-2">
    
</div>
<hr />

		</footer>
		

<script src="/dist/app.js"></script>


<script src="/lib/fuse.min.js"></script> 
<script src="/lib/fastsearch.js"></script>

	</div>
</body>

</html>