[{"categories":["analytics","group-project","machine-learning","random-forest","XGBoost","data-viz","georgia-tech","capstone-project"],"contents":"","date":"2022-12-15T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/airlines_delay/","section":"posts","tags":["correlation","dimensionality reduction","PCA","Random Forest","Deep Learning","XGBoost","AdaBoost"],"title":"Flight delay prediction and exploration in the United States"},{"categories":["analytics","data-viz","georgia-tech","capstone-project"],"contents":"","date":"2022-04-30T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/mushrooms/","section":"posts","tags":["correlation","dimensionality reduction","PCA","MCA","t-SNE","Correspondence Analysis","Contingency Table","Cramers V","Decision Tree"],"title":"Detecting the edibility of Mushrooms in the wild"},{"categories":null,"contents":"Certifications Name Issuing Organization Issue date Expiry date Credential URL D3.js Essential Training for Data Scientists Linkedin Learning September 12, 2022 NA link Academy Accreditation - Databricks Lakehouse Fundamentals Databricks August 24, 2022 NA link Mathematical Thinking with Terence Tao Masterclass July, 2022 NA Predictive Analytics Essential Training: Estimating and Ensuring ROI Linkedin Learning June, 2022 NA link Big Data with PySpark Track Datacamp March 4, 2022 NA link Building Recommendations Engines with PySpark Datacamp March 4, 2022 NA link Machine Learning with PySpark Datacamp March 2, 2022 NA link Feature Engineering with PySpark Datacamp March 1, 2022 NA link Cleaning Data with PySpark Datacamp Feb 16, 2022 NA link Big Data Fundamentals with PySpark Datacamp Jan 10, 2022 NA link Introduction to PySpark Datacamp Jan 05, 2022 NA link Intermediate SQL Datacamp July 20, 2021 NA link Joining Data in SQL Datacamp July 13, 2021 NA link Introduction to SQL Datacamp July 08, 2021 NA link Reporting with R Markdown Datacamp June 15, 2021 NA link Introduction to Data Visualization with ggplot2 Datacamp June 14, 2021 NA link Joining Data with dplyr Datacamp June 07, 2021 NA link Data Manipulation with dplyr Datacamp June 01, 2021 NA link Introduction to the Tidyverse Datacamp June 01, 2021 NA link Intermediate R Datacamp May 28, 2021 NA link Introduction to R Datacamp May 27, 2021 NA link Intro to AI Ethics Kaggle May 20, 2021 NA link Computer Vision Kaggle February 28, 2021 NA link Natural Language Processing Kaggle February 23, 2021 NA link Intro to Deep Learning Kaggle January 11, 2021 NA link Geospatial Analysis Kaggle August 25, 2020 NA link Machine Learning Explainability Kaggle August 25, 2020 NA link Intermediate Machine Learning Kaggle August 24, 2020 NA link Advanced SQL Kaggle August 20, 2020 NA link Intro to SQL Kaggle August 19, 2020 NA link Data Visualization Kaggle August 18, 2020 NA link Data Cleaning Kaggle August 17, 2020 NA link Pandas Kaggle August 11, 2020 NA link Intro to Machine Learning Kaggle August 7, 2020 NA link Python Kaggle August 3, 2020 NA link AWS Fundamentals in AWS Machine Learning Udacity July 5, 2020 NA link ","date":"2022-01-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/certifications/","section":"","tags":null,"title":"Certifications"},{"categories":["georgia tech","data viz","analytics"],"contents":"The Beginner\u0026rsquo;s Trap The stages in the analytics process is filled with moments of success and failures. There are some instant gratifications during the process, where a begginer like myself might construe a non success as success, due to some kind of judgement error.\nThis project will go through one of those instances and discuss some of the things I need to keep in mind so that I (and people new to analytics) do not make these mistakes.\nThe dataset is from https://www.kaggle.com/sanjeetsinghnaik/most-expensive-footballers-2021\nSince learning is a never ending process, I will be updating the notebook (or creating a variation of it) as I discover more best practices. So, consider this to be an excerise on SOTA (State of the art) POC (proof of concept).\nSteps involved:\nIntroduction Exploratory Data Analysis Feature Engineering Linear Regression Decision Tree Regression Validation Conclusion 1. Introduction The description of the dataset from Kaggle:\nThis file consists of data of Top 500 Most Expensive Footballer In 2021. The data is according to the prices listed in transfer market along with data like goals, assists, matches, age, etc.\nThe question we are trying to answer is:\n** Based on different predictors (goals, assists, matches, age etc.) is it possible to predict the market value of footballers? **\nThe source of the image above is the Kaggle repository hosting the dataset.\nImports import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set(rc={\u0026#39;figure.figsize\u0026#39;:(12, 6)}) from matplotlib.ticker import MaxNLocator import numpy as np from sklearn.linear_model import LinearRegression from sklearn.tree import DecisionTreeRegressor from sklearn.model_selection import cross_val_score The dataset is in the file players_data.csv\ndf = pd.read_csv(\u0026#39;players_data.csv\u0026#39;) Let us take a look at the columns. We have a few ways of doing this but I always found using list easy.\nlist(df) ['Name', 'Position', 'Age', 'Markey Value In Millions(£)', 'Country', 'Club', 'Matches', 'Goals', 'Own Goals', 'Assists', 'Yellow Cards', 'Second Yellow Cards', 'Red Cards', 'Number Of Substitute In', 'Number Of Substitute Out'] Since we know the dataset is a ranking of 500 highest paid footballers, the expected rows should be 500. And the number of columns is 15\ndf.shape (500, 15) Let\u0026rsquo;s look at the first few rows.\ndf.head() Name Position Age Markey Value In Millions(£) Country Club Matches Goals Own Goals Assists Yellow Cards Second Yellow Cards Red Cards Number Of Substitute In Number Of Substitute Out 0 Kylian Mbappé Centre-Forward 22 144.0 France Paris Saint-Germain 16 7 0 11 3 0 0 0 8 1 Erling Haaland Centre-Forward 21 135.0 Norway Borussia Dortmund 10 13 0 4 1 0 0 0 1 2 Harry Kane Centre-Forward 28 108.0 England Tottenham Hotspur 16 7 0 2 2 0 0 2 2 3 Jack Grealish Left Winger 26 90.0 England Manchester City 15 2 0 3 1 0 0 2 8 4 Mohamed Salah Right Winger 29 90.0 Egypt Liverpool FC 15 15 0 6 1 0 0 0 3 Let\u0026rsquo;s look at the last few rows.\ndf.tail() Name Position Age Markey Value In Millions(£) Country Club Matches Goals Own Goals Assists Yellow Cards Second Yellow Cards Red Cards Number Of Substitute In Number Of Substitute Out 495 Giorgian de Arrascaeta Attacking Midfield 27 16.2 Uruguay Clube de Regatas do Flamengo 0 0 0 0 0 0 0 0 0 496 Ayoze Pérez Second Striker 28 16.2 Spain Leicester City 8 1 0 3 0 0 1 2 5 497 Alex Meret Goalkeeper 24 16.2 Italy SSC Napoli 5 0 0 0 0 0 0 0 0 498 Duje Caleta-Car Centre-Back 25 16.2 Croatia Olympique Marseille 8 0 0 0 2 0 0 0 2 499 Aritz Elustondo Centre-Back 27 16.2 Spain Real Sociedad 15 3 0 1 4 0 0 1 1 2. Exploratory Data Analysis Let\u0026rsquo;s use Pandas built in function to generate descriptive statistics\ndf.describe() Age Markey Value In Millions(£) Matches Goals Own Goals Assists Yellow Cards Second Yellow Cards Red Cards Number Of Substitute In Number Of Substitute Out count 500.000000 500.000000 500.000000 500.000000 500.000000 500.00000 500.000000 500.000000 500.000000 500.000000 500.000000 mean 24.968000 31.537800 12.396000 2.160000 0.030000 1.51200 1.592000 0.036000 0.046000 2.394000 3.744000 std 3.165916 17.577697 4.342453 2.880102 0.170758 1.85276 1.445585 0.186477 0.209695 2.517825 3.293046 min 16.000000 16.200000 0.000000 0.000000 0.000000 0.00000 0.000000 0.000000 0.000000 0.000000 0.000000 25% 23.000000 19.800000 10.000000 0.000000 0.000000 0.00000 0.000000 0.000000 0.000000 0.000000 1.000000 50% 25.000000 25.200000 13.000000 1.000000 0.000000 1.00000 1.000000 0.000000 0.000000 2.000000 3.000000 75% 27.000000 36.000000 16.000000 3.000000 0.000000 2.00000 2.000000 0.000000 0.000000 3.250000 6.000000 max 36.000000 144.000000 24.000000 23.000000 1.000000 12.00000 7.000000 1.000000 1.000000 13.000000 20.000000 Let us look into the data types for each of the columns\ndf.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 500 entries, 0 to 499 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Name 500 non-null object 1 Position 500 non-null object 2 Age 500 non-null int64 3 Markey Value In Millions(£) 500 non-null float64 4 Country 500 non-null object 5 Club 500 non-null object 6 Matches 500 non-null int64 7 Goals 500 non-null int64 8 Own Goals 500 non-null int64 9 Assists 500 non-null int64 10 Yellow Cards 500 non-null int64 11 Second Yellow Cards 500 non-null int64 12 Red Cards 500 non-null int64 13 Number Of Substitute In 500 non-null int64 14 Number Of Substitute Out 500 non-null int64 dtypes: float64(1), int64(10), object(4) memory usage: 58.7+ KB Count of missing values\ndf.isna().sum() Name 0 Position 0 Age 0 Markey Value In Millions(£) 0 Country 0 Club 0 Matches 0 Goals 0 Own Goals 0 Assists 0 Yellow Cards 0 Second Yellow Cards 0 Red Cards 0 Number Of Substitute In 0 Number Of Substitute Out 0 dtype: int64 df.isna().sum().sum() 0 We do not have any values missing from the dataset.\ndf.groupby(\u0026#39;Club\u0026#39;).size().sort_values(ascending=False).head(10) Club Manchester United 19 Manchester City 18 Chelsea FC 16 Tottenham Hotspur 16 Real Madrid 16 Paris Saint-Germain 16 RB Leipzig 15 Arsenal FC 15 Liverpool FC 15 Atlético de Madrid 15 dtype: int64 ax = sns.countplot(x=\u0026#39;Club\u0026#39;, data=df, order=df.Club.value_counts().iloc[:10].index) ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) ax.yaxis.set_major_locator(MaxNLocator(integer=True)) ax.set(xlabel=\u0026#39;Club\u0026#39;, ylabel=\u0026#39;Count of players\u0026#39;) plt.title(\u0026#34;Top 10 Clubs based on player representation (count)\u0026#34;) plt.show() club_grouped = df.groupby([\u0026#39;Club\u0026#39;])[[\u0026#39;Markey Value In Millions(£)\u0026#39;]].sum().sort_values(by=\u0026#39;Markey Value In Millions(£)\u0026#39;, ascending=False).head(10) club_grouped Markey Value In Millions(£) Club Manchester City 940.5 Paris Saint-Germain 775.8 Manchester United 760.5 Chelsea FC 709.2 Bayern Munich 685.8 Liverpool FC 681.3 Atlético de Madrid 616.5 Real Madrid 594.0 Tottenham Hotspur 536.4 Juventus FC 450.0 ax = sns.barplot(x=club_grouped.index, y=club_grouped[\u0026#39;Markey Value In Millions(£)\u0026#39;]) ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) ax.set(xlabel=\u0026#39;Club\u0026#39;, ylabel=\u0026#39;Markey Value In Millions(£)\u0026#39;) plt.title(\u0026#34;Top 10 Clubs based on total market value\u0026#34;) plt.show() df.groupby(\u0026#39;Country\u0026#39;).size().sort_values(ascending=False).head(10) Country England 67 France 58 Spain 52 Brazil 41 Germany 29 Portugal 26 Italy 26 Argentina 22 Netherlands 17 Belgium 14 dtype: int64 ax = sns.countplot(x=\u0026#39;Country\u0026#39;, data=df, order=df.Country.value_counts().iloc[:10].index) ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) ax.yaxis.set_major_locator(MaxNLocator(integer=True)) ax.set(xlabel=\u0026#39;Country\u0026#39;, ylabel=\u0026#39;Count of players\u0026#39;) plt.title(\u0026#34;Top 10 Country based on player representation (count)\u0026#34;) plt.show() country_grouped = df.groupby([\u0026#39;Country\u0026#39;])[[\u0026#39;Markey Value In Millions(£)\u0026#39;]].sum().sort_values(by=\u0026#39;Markey Value In Millions(£)\u0026#39;, ascending=False).head(10) country_grouped Markey Value In Millions(£) Country England 2248.2 France 1895.4 Spain 1565.1 Brazil 1275.3 Germany 1005.3 Portugal 890.1 Italy 854.1 Argentina 650.7 Netherlands 571.5 Belgium 522.9 ax = sns.barplot(x=country_grouped.index, y=country_grouped[\u0026#39;Markey Value In Millions(£)\u0026#39;]) ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) ax.set(xlabel=\u0026#39;Country\u0026#39;, ylabel=\u0026#39;Markey Value In Millions(£)\u0026#39;) plt.title(\u0026#34;Top 10 Country based on total market value\u0026#34;) plt.show() df.groupby(\u0026#39;Position\u0026#39;).size().sort_values(ascending=False) Position Centre-Back 87 Central Midfield 74 Centre-Forward 70 Right Winger 48 Left Winger 46 Attacking Midfield 41 Defensive Midfield 41 Right-Back 30 Left-Back 23 Goalkeeper 19 Left Midfield 8 Second Striker 8 Right Midfield 5 dtype: int64 ax = sns.countplot(x=\u0026#39;Position\u0026#39;, data=df, order=df.Position.value_counts().iloc[:10].index) ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) ax.yaxis.set_major_locator(MaxNLocator(integer=True)) ax.set(xlabel=\u0026#39;Position\u0026#39;, ylabel=\u0026#39;Count of players\u0026#39;) plt.title(\u0026#34;Top 10 Position based on player representation (count)\u0026#34;) plt.show() position_grouped = df.groupby([\u0026#39;Position\u0026#39;])[[\u0026#39;Markey Value In Millions(£)\u0026#39;]].sum().sort_values(by=\u0026#39;Markey Value In Millions(£)\u0026#39;, ascending=False).head(10) position_grouped Markey Value In Millions(£) Position Centre-Back 2583.9 Central Midfield 2421.9 Centre-Forward 2369.7 Left Winger 1647.0 Right Winger 1461.6 Attacking Midfield 1332.0 Defensive Midfield 1275.3 Right-Back 784.8 Left-Back 704.7 Goalkeeper 585.9 ax = sns.barplot(x=position_grouped.index, y=position_grouped[\u0026#39;Markey Value In Millions(£)\u0026#39;]) ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) ax.set(xlabel=\u0026#39;Position\u0026#39;, ylabel=\u0026#39;Markey Value In Millions(£)\u0026#39;) plt.title(\u0026#34;Top 10 Position based on total market value\u0026#34;) plt.show() 3. Feature Engineering Since we only have 500 rows of data, 10 different positions, let us aggregate some of these positions together.\nI created a simple position mapping and transformed it so that it was possible to use map\nposition_mapping = { \u0026#34;forward\u0026#34; : [\u0026#39;Centre-Forward\u0026#39;, \u0026#39;Second Striker\u0026#39;], \u0026#34;midfield\u0026#34; : [\u0026#39;Central Midfield\u0026#39;, \u0026#39;Attacking Midfield\u0026#39;, \u0026#39;Defensive Midfield\u0026#39;, \u0026#39;Left Midfield\u0026#39;, \u0026#39;Right Midfield\u0026#39;, \u0026#39;Right Winger\u0026#39;, \u0026#39;Left Winger\u0026#39;], \u0026#34;back\u0026#34; : [\u0026#39;Centre-Back\u0026#39;, \u0026#39;Right-Back\u0026#39;, \u0026#39;Left-Back\u0026#39;] } The only position we have not mapped here is Goalkeeper, which we will remove later on.\nmapping_dict = {} for key in position_mapping: for item in position_mapping[key]: mapping_dict[item] = key mapping_dict {'Centre-Forward': 'forward', 'Second Striker': 'forward', 'Central Midfield': 'midfield', 'Attacking Midfield': 'midfield', 'Defensive Midfield': 'midfield', 'Left Midfield': 'midfield', 'Right Midfield': 'midfield', 'Right Winger': 'midfield', 'Left Winger': 'midfield', 'Centre-Back': 'back', 'Right-Back': 'back', 'Left-Back': 'back'} I could have just typed in the mapping_dict directly but this felt more natural.\ndf[\u0026#39;Position_Agg\u0026#39;] = df.Position.map(mapping_dict) df Name Position Age Markey Value In Millions(£) Country Club Matches Goals Own Goals Assists Yellow Cards Second Yellow Cards Red Cards Number Of Substitute In Number Of Substitute Out Position_Agg 0 Kylian Mbappé Centre-Forward 22 144.0 France Paris Saint-Germain 16 7 0 11 3 0 0 0 8 forward 1 Erling Haaland Centre-Forward 21 135.0 Norway Borussia Dortmund 10 13 0 4 1 0 0 0 1 forward 2 Harry Kane Centre-Forward 28 108.0 England Tottenham Hotspur 16 7 0 2 2 0 0 2 2 forward 3 Jack Grealish Left Winger 26 90.0 England Manchester City 15 2 0 3 1 0 0 2 8 midfield 4 Mohamed Salah Right Winger 29 90.0 Egypt Liverpool FC 15 15 0 6 1 0 0 0 3 midfield ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 495 Giorgian de Arrascaeta Attacking Midfield 27 16.2 Uruguay Clube de Regatas do Flamengo 0 0 0 0 0 0 0 0 0 midfield 496 Ayoze Pérez Second Striker 28 16.2 Spain Leicester City 8 1 0 3 0 0 1 2 5 forward 497 Alex Meret Goalkeeper 24 16.2 Italy SSC Napoli 5 0 0 0 0 0 0 0 0 NaN 498 Duje Caleta-Car Centre-Back 25 16.2 Croatia Olympique Marseille 8 0 0 0 2 0 0 0 2 back 499 Aritz Elustondo Centre-Back 27 16.2 Spain Real Sociedad 15 3 0 1 4 0 0 1 1 back 500 rows × 16 columns\nNow we can remove goalkeepers (which are NaNs because of missing mapping)\ndf.dropna(inplace=True) Let\u0026rsquo;s create similar charts for the new column as well\ndf.groupby(\u0026#39;Position_Agg\u0026#39;).size().sort_values(ascending=False) Position_Agg midfield 263 back 140 forward 78 dtype: int64 ax = sns.countplot(x=\u0026#39;Position_Agg\u0026#39;, data=df, order=df.Position_Agg.value_counts().iloc[:10].index) ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) ax.yaxis.set_major_locator(MaxNLocator(integer=True)) ax.set(xlabel=\u0026#39;Position\u0026#39;, ylabel=\u0026#39;Count of players\u0026#39;) plt.title(\u0026#34;Top 10 Position based on player representation\u0026#34;) plt.show() position_agg_grouped = df.groupby([\u0026#39;Position_Agg\u0026#39;])[[\u0026#39;Markey Value In Millions(£)\u0026#39;]].sum().sort_values(by=\u0026#39;Markey Value In Millions(£)\u0026#39;, ascending=False).head(10) position_agg_grouped Markey Value In Millions(£) Position_Agg midfield 8460.0 back 4073.4 forward 2649.6 ax = sns.barplot(x=position_agg_grouped.index, y=position_agg_grouped[\u0026#39;Markey Value In Millions(£)\u0026#39;]) ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) ax.set(xlabel=\u0026#39;Position\u0026#39;, ylabel=\u0026#39;Markey Value In Millions(£)\u0026#39;) plt.title(\u0026#34;Top 10 Position based on total market value\u0026#34;) plt.show() 4. Linear Regression First of all, let us only focus on the midfield players. Midfield is the largest group of players we have, and it might make more sense to use regression the same group of players.\ndf_mid = df[df.Position_Agg==\u0026#39;midfield\u0026#39;] df_mid.reset_index(drop=True, inplace=True) list(df_mid) ['Name', 'Position', 'Age', 'Markey Value In Millions(£)', 'Country', 'Club', 'Matches', 'Goals', 'Own Goals', 'Assists', 'Yellow Cards', 'Second Yellow Cards', 'Red Cards', 'Number Of Substitute In', 'Number Of Substitute Out', 'Position_Agg'] df_mid = df_mid[[\u0026#39;Age\u0026#39;, \u0026#39;Goals\u0026#39;, \u0026#39;Assists\u0026#39;, \u0026#39;Markey Value In Millions(£)\u0026#39;]] df_mid = df_mid.apply(lambda x: x/x.max(), axis=0) df_mid Age Goals Assists Markey Value In Millions(£) 0 0.764706 0.133333 0.3 1.00 1 0.852941 1.000000 0.6 1.00 2 0.882353 0.200000 0.1 1.00 3 0.852941 0.200000 0.3 1.00 4 0.617647 0.000000 0.0 0.90 ... ... ... ... ... 258 0.676471 0.066667 0.1 0.18 259 0.764706 0.000000 0.0 0.18 260 0.823529 0.000000 0.1 0.18 261 0.764706 0.200000 0.1 0.18 262 0.794118 0.000000 0.0 0.18 263 rows × 4 columns\nX = df_mid.iloc[:, :-1] y = df_mid.iloc[:, -1] linear_regressor = LinearRegression().fit(X, y) linear_regressor.score(X, y) 0.06775382996413992 linear_regressor.predict(X) array([0.37728587, 0.61138884, 0.3870485 , 0.40721923, 0.28447775, 0.326131 , 0.4879743 , 0.40387802, 0.36451519, 0.34205403, 0.4380145 , 0.42209148, 0.26836585, 0.40466779, 0.3113309 , 0.39576034, 0.35685413, 0.39922502, 0.36962481, 0.31978173, 0.3962891 , 0.36759539, 0.3706756 , 0.41769948, 0.37093661, 0.47298534, 0.36451519, 0.34021348, 0.45451087, 0.33229141, 0.30798969, 0.39103519, 0.34179302, 0.30058964, 0.34866431, 0.5442388 , 0.41146694, 0.34021348, 0.31415009, 0.32823257, 0.38036608, 0.34637388, 0.34663489, 0.3274428 , 0.37473444, 0.34716365, 0.33871282, 0.49826569, 0.30904047, 0.32718179, 0.30798969, 0.30058964, 0.38370729, 0.45300347, 0.54202051, 0.32823257, 0.38573671, 0.408009 , 0.39110734, 0.45484402, 0.39549933, 0.38167787, 0.39437641, 0.31415009, 0.42385989, 0.35174451, 0.3144111 , 0.4689265 , 0.32921121, 0.39523832, 0.33255242, 0.37296602, 0.31644052, 0.3422429 , 0.33589363, 0.36451519, 0.35174451, 0.36680562, 0.32410158, 0.37525645, 0.40616845, 0.39136835, 0.32181115, 0.36248577, 0.28447775, 0.32076037, 0.42183047, 0.31106989, 0.31212067, 0.31952073, 0.36222476, 0.38986769, 0.33995247, 0.30596027, 0.36346441, 0.32718179, 0.30058964, 0.36019534, 0.35711514, 0.3484033 , 0.4241209 , 0.30032863, 0.41567006, 0.36969696, 0.32384057, 0.36556597, 0.37525645, 0.36988582, 0.31670153, 0.40669721, 0.31741916, 0.36444305, 0.34401131, 0.33890169, 0.40590744, 0.31336032, 0.36654461, 0.32947222, 0.31336032, 0.34637388, 0.36248577, 0.30596027, 0.35711514, 0.35711514, 0.32410158, 0.36346441, 0.40079782, 0.38265651, 0.34761353, 0.37859767, 0.35200552, 0.29829921, 0.39294789, 0.30261906, 0.30058964, 0.3144111 , 0.38573671, 0.35050486, 0.40151545, 0.34100325, 0.31336032, 0.37735802, 0.27373649, 0.3144111 , 0.330523 , 0.27576591, 0.26836585, 0.29521901, 0.41638769, 0.4781671 , 0.40571857, 0.43238286, 0.31873095, 0.39726774, 0.34637388, 0.42288125, 0.3113309 , 0.42175832, 0.32286194, 0.35050486, 0.32286194, 0.30596027, 0.43257173, 0.30058964, 0.45052417, 0.35174451, 0.39313676, 0.38802714, 0.34126426, 0.40079782, 0.3678564 , 0.40721923, 0.34558411, 0.34558411, 0.36222476, 0.3366834 , 0.47095592, 0.38900577, 0.38960668, 0.3706756 , 0.4166487 , 0.34663489, 0.30569926, 0.29724843, 0.35482471, 0.36962481, 0.48876408, 0.29829921, 0.33255242, 0.3422429 , 0.28984838, 0.30904047, 0.34408345, 0.40590744, 0.30904047, 0.31336032, 0.34637388, 0.33589363, 0.31670153, 0.36248577, 0.34558411, 0.3274428 , 0.33897383, 0.3854757 , 0.47167354, 0.28650717, 0.28447775, 0.3484033 , 0.29521901, 0.30261906, 0.30366984, 0.35685413, 0.32384057, 0.32718179, 0.40676935, 0.38678749, 0.33995247, 0.28984838, 0.3144111 , 0.29521901, 0.30596027, 0.31873095, 0.31336032, 0.3113309 , 0.30798969, 0.31873095, 0.33484285, 0.33890169, 0.35174451, 0.39005656, 0.37191524, 0.31670153, 0.41893913, 0.3113309 , 0.40827001, 0.40086996, 0.34021348, 0.35940557, 0.37525645, 0.34126426, 0.38494693, 0.39136835, 0.41488029, 0.35606436, 0.31415009, 0.36943595, 0.31846994, 0.38141686, 0.36864618, 0.28447775, 0.31538974, 0.29521901, 0.36969696, 0.30904047, 0.330523 , 0.39850739, 0.31336032, 0.30596027, 0.32181115, 0.3113309 , 0.33484285, 0.36556597, 0.31670153]) 5. Decision Tree Regressor decision_tree_regressor = DecisionTreeRegressor(random_state=42) decision_tree_regressor.fit(X, y) DecisionTreeRegressor(random_state=42) decision_tree_regressor.predict(X) array([1. , 1. , 1. , 0.625 , 0.4075 , 0.9 , 0.9 , 0.9 , 0.66666667, 0.85 , 0.85 , 0.85 , 0.525 , 0.8 , 0.326 , 0.8 , 0.475 , 0.7 , 0.46 , 0.7 , 0.7 , 0.7 , 0.465 , 0.7 , 0.7 , 0.7 , 0.66666667, 0.5 , 0.65 , 0.65 , 0.45 , 0.65 , 0.6 , 0.396 , 0.6 , 0.6 , 0.6 , 0.5 , 0.38333333, 0.5 , 0.55 , 0.33 , 0.39 , 0.385 , 0.5 , 0.5 , 0.5 , 0.5 , 0.28 , 0.35 , 0.45 , 0.396 , 0.5 , 0.48 , 0.47 , 0.5 , 0.365 , 0.45 , 0.45 , 0.45 , 0.45 , 0.45 , 0.45 , 0.38333333, 0.42 , 0.3175 , 0.29 , 0.42 , 0.4 , 0.4 , 0.31 , 0.4 , 0.4 , 0.31 , 0.31 , 0.66666667, 0.3175 , 0.4 , 0.35 , 0.30666667, 0.4 , 0.3 , 0.28 , 0.3 , 0.4075 , 0.35 , 0.35 , 0.35 , 0.35 , 0.35 , 0.295 , 0.35 , 0.275 , 0.256 , 0.325 , 0.35 , 0.396 , 0.35 , 0.31666667, 0.275 , 0.35 , 0.33 , 0.33 , 0.25 , 0.26 , 0.25 , 0.30666667, 0.32 , 0.23 , 0.3 , 0.3 , 0.3 , 0.3 , 0.25 , 0.26 , 0.245 , 0.3 , 0.3 , 0.245 , 0.33 , 0.3 , 0.256 , 0.31666667, 0.31666667, 0.35 , 0.325 , 0.275 , 0.3 , 0.3 , 0.3 , 0.3 , 0.25 , 0.28 , 0.24 , 0.396 , 0.29 , 0.365 , 0.26 , 0.27 , 0.27 , 0.245 , 0.27 , 0.26 , 0.29 , 0.22 , 0.25 , 0.525 , 0.2075 , 0.25 , 0.25 , 0.25 , 0.25 , 0.21666667, 0.25 , 0.33 , 0.25 , 0.326 , 0.25 , 0.25 , 0.26 , 0.25 , 0.256 , 0.25 , 0.396 , 0.25 , 0.3175 , 0.25 , 0.25 , 0.225 , 0.275 , 0.25 , 0.625 , 0.24 , 0.24 , 0.295 , 0.24 , 0.24 , 0.24 , 0.23 , 0.465 , 0.23 , 0.39 , 0.22 , 0.22 , 0.22 , 0.46 , 0.22 , 0.25 , 0.31 , 0.31 , 0.21 , 0.28 , 0.22 , 0.26 , 0.28 , 0.245 , 0.33 , 0.31 , 0.23 , 0.3 , 0.24 , 0.385 , 0.21 , 0.2 , 0.2 , 0.2 , 0.4075 , 0.275 , 0.2075 , 0.24 , 0.2 , 0.475 , 0.26 , 0.35 , 0.2 , 0.2 , 0.275 , 0.21 , 0.29 , 0.2075 , 0.256 , 0.21666667, 0.245 , 0.326 , 0.45 , 0.21666667, 0.19 , 0.25 , 0.3175 , 0.2 , 0.2 , 0.23 , 0.2 , 0.326 , 0.2 , 0.2 , 0.5 , 0.2 , 0.30666667, 0.225 , 0.2 , 0.3 , 0.2 , 0.19 , 0.38333333, 0.18 , 0.18 , 0.18 , 0.18 , 0.4075 , 0.18 , 0.2075 , 0.25 , 0.28 , 0.22 , 0.18 , 0.245 , 0.256 , 0.28 , 0.326 , 0.19 , 0.25 , 0.23 ]) decision_tree_regressor.score(X, y) 0.7410570792901311 6. Validation from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42) decision_tree_regressor = DecisionTreeRegressor(random_state=42) decision_tree_regressor.fit(X_train, y_train) DecisionTreeRegressor(random_state=42) decision_tree_regressor.score(X_train, y_train) 0.7325965837309238 decision_tree_regressor.score(X_test, y_test) -1.0994712392955237 7. Conclusion The Trap The trap here is using same data for training and validation. As a beginner, it is common to look into a few models (here we looked into Linear Regression and Decision Tree Regressor), and concluded one is better than the other because the latter had a good determination coefficient score.\nHowever, after validation we realize that the Decision Tree Regressor is up to no good.\nhttps://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor.score\nThe score is negative, and based on the documentation above:\nThe best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse)\nThis is just a very basic example of misjudgement on my side. As a peer reviewer, you might feel like this is something very trivial. Finally, I just demonstrated one example here. There are pleanty of these in analytics.\n","date":"2021-12-10T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/beginners_trap/","section":"posts","tags":["cse-6040","analytics","eda"],"title":"The Beginner's Trap in analytics"},{"categories":["georgia tech","data viz","analytics","tootle"],"contents":"Analytics for Ride Hailing Services Introduction to Ride Hailing At present, it is pretty common to hail a ride to get from one place to the other at a tap of a button. Almost all major cities in the world have some sort of ride-hailing service. Uber, Lyft, Didi, Ola, Gojek, etc. are some examples of service providers that come to mind. Additionally, the service is also proliferating to smaller cities and has become commonplace in many parts of the world. Analytics is a key component in making sure the service is provided efficiently. All of the aforementioned companies invest heavily in data science and analytics to be competitive and to provide better services.\nFor this post, I will focus on Ride-Hailing services (not Ride Sharing services). See the difference here.\nPredominantly, ride-hailing functions as a Gig Economy. The drivers (sometimes referred to as partners, captains, etc.) are mostly independent contractors who bring their own vehicle and work at their own time and are paid based on their time commitment. This variability requires monitoring, sophisticated algorithms, good incentives, competitive pricing to passengers, etc. which is also common in other gig economy jobs. In most cases, the analytics models that will be built for one gig economy can be tweaked to fit another one as well.\nLet\u0026rsquo;s look at a few components of Ride-hailing that will be relevant for how we frame our models and the data we use.\nFor this post, \u0026ldquo;passengers\u0026rdquo; are referred to as service requesters/receivers and \u0026ldquo;drivers\u0026rdquo; are referred to as service providers.\nComponents of the problem Balancing act: Supply and Demand, and Chicken and Egg Problem There is a balancing act that all of these ride-sharing platforms need to perform to be efficient. A healthy ratio between driver and passenger (to go more granular, for a segment of geographic area at a given time) is very important. The balancing act is even crucial when a ride-hailing service decides to introduce itself to a new city (especially one that is new to ride-hailing).\nIf an area has more drivers than demand from passengers, the drivers might not get ride requests causing them to lose interest and find a different job or move to a different competition. If an area has more passengers than a supply of drivers, the passengers might not get their ride requests accepted causing them to move onto another (direct/indirect) competition. From an analytics perspective, this is a difficult problem to solve. However, good analytics can also be a competitive advantage here.\nPricing Pricing is a by-product of the balancing act described above. The pricing must be competitive enough to lure the supply and the demand pool. The driver should feel like the pricing justifies the time, effort, and resources supplied. The passenger should feel the amount paid for the service justifies the service received.\nFew ride-hailing services opt-out for transparent and fixed payment (i.e the price is only dictated by the distance to destination), while some have complex pricing strategies to stand out, lure passengers or drivers, and manage supply and demand effectively.\nDynamic Pricing Some ride-hailing services implement dynamic pricing as a way to balance the chicken and egg problem described above. This is a large-scale, complex analytics problem involving several variables. Additionally, driver bonuses, discounts, and referrals might constitute the pricing strategy as well.\nCredit: Forbes\nCompetition (Direct and Indirect) Direct Competition (Passenger) other ride hailing services Direct Competition (Driver) other ride-hailing services Indirect Competition (Passenger) public transportation taxi/cab etc. Indirect Competition (Driver) other employment opportunities Credit: Forbes\nDescriptive analysis Before we build complex models, it is essential to understand how the business/service is performing. These descriptive analyses will lay the foundation for us when we build complex and combined models later on.\nRide Completion/Cancellation rate What is the ride completion rate? To be more granular, what is the ride completion rate at a geographic segment of the city at a particular time? What is the ride cancellation rate? Similar to before, what is the ride cancellation rate at a geographic segment of the city at a particular time? Why do passengers cancel rides? Is cancellation more prominent in one area compared to the other? Is this dependent on the time of the day? DATA passenger_id driver_id latitude (pickup, drop) longitude (pickup, drop) timestamps (requested, accepted, picked up, dropped, canceled) completion_status cancellation_reason Late arrival rate What is the late arrival rate? what is the late arrival rate at a geographic segment of the city at a particular time? Is the late arrival rate prominent for some time of the day or for a particular geographical area? DATA passenger_id driver_id latitude (pickup, drop) longitude (pickup, drop) timestamps (requested, accepted, picked up, dropped, canceled) completion_status cancellation_reason Activation, Acquisition, Retention, Referral, Revenue What does the pirate metric funnel look like? Is there a specific area where the business should focus to improve business/efficiency? Is the funnel leaking somewhere? What is the passenger/driver churn rate? DATA passenger_id/driver_id timestamps (created_date, last_ride_date) total_amount_spent_on_platform / total_money_made total_rides num_of_referrals acquisition_channel Credit: hygger.io\nChannels What is the acquisition rate from different marketing channels for drivers or for passengers? What marketing channel is more apt/effective for different demography/user segments? Can we use the multi-arm bandits model to identify a balance between exploration and exploitation to test on different channels? DATA passenger_id/driver_id timestamps (created_date) acquisition_channel total_amount_spent_on_platform / total_money_made passenger/driver demographic information (age, gender, etc.) User Analysis What does the demography (social, cultural, economic) of the driver look like? What does the demography (social, cultural, economic) of the passenger look like? What does the demography of the city look like? What does the demography of the segment that uses the service the most look like? DATA passenger_id/driver_id timestamps (created_date) acquisition_channel total_amount_spent_on_platform / total_money_made passenger/driver/city demographic information (age, gender etc.) Driver Ranking/Driver Performance How is a driver performing? (this could be based on multiple factors including customer rating, and other factors) Based on the index for performance, what is the rank of a driver? What is the rank of a driver among a segment of drivers? (this will be useful for priority queue for driver dispatching) DATA driver_id timestamps average_rating rides_complete_rate last_ride_date Predictive analysis If we are looking to make the system more efficient, it is also very important to understand what the future holds.\nGrowth in rides What is the number of expected daily rides next day/week/month/year? What is the expected revenue for the next day/week/month/year? Is there a daily/weekly/monthly seasonality? DATA timestamp (daily) num_of_ride (completed rides or ride requests) Passenger growth What is the number of expected passenger growth next day/week/month/year? Is there a daily/weekly/monthly seasonality? DATA timestamp (daily) num_of_unique_passengers (acquisition or ride request) Driver growth What is the number of expected driver growth next day/week/month/year? Is there a daily/weekly/monthly seasonality? DATA timestamp (daily) num_of_unique_drivers (acquisition or ride request) Churn over the period of time What is the expected churn in the next day/week/month/year? Is there a daily/weekly/monthly seasonality? DATA timestamp (passenger acquisition) passenger\u0026rsquo;s number of rides each month (grouped acquisition to present) Prescriptive analysis Descriptive and Predictive analysis will help us move towards prescriptive analysis, especially for optimization models. These models will help the service provider in decision making, especially with regards to an increase in efficiency for drivers and passengers.\nRatio of drivers to passengers What is the ideal ratio of the passenger to the driver to maximize rides completion rate? Given Voronoi clustering for geographic indexing based on geographic hotspots (other indexing methods are more efficient like h3 developed by Uber, but Voronoi can be used to build something similar as well.) rides data (requested, canceled, completed) passenger data (raw data and data after descriptive analysis performed: Pirate metrics etc.) driver data Use Optimization with constraints: num_of_rides should be greater than a threshold (comes from future rides data) with objective functions: maximize rides completion rate for each geographic segment to find an optimal driver to passenger ratio Notes\nRegression (or logistic regression if we only care about a healthy/unhealthy ratio) can also be used to do something similar as well. Additionally, the result from the model can also be used to model advertisement campaigns for the future if we find the number of driver or passenger (in a particular geographic area) need to be increased for a stable ratio. This is an important indicator because it allows the service provider to focus on growth while keeping this indicator at a healthy level. Dynamic pricing What should the dynamic/surge pricing be at a given time? Given ratio of the driver to passenger paying capacity of passengers (based on descriptive analysis of users, useful for capping at some multiplier so that it does not go wild) number of requests in the queue in a geographic segment competition surge at the moment number of requests completed in the geographic segment (and neighboring segment) in last x minutes (arbitrary but can be defined by waiting for time analysis from descriptive analysis) geographic location information (grid-based on Voronoi for the availability of drivers in other cells) number of drivers that will be free (complete a ride soon or are predicted to come online soon) in the grid or neighboring grids Use Linear regression to find ideal dynamic pricing multiplier Notes\nThe cap might/might not be necessary, and that might be another analytics problem altogether. There have been some cases where a natural disaster/terrorist attack increased surge multiplier to an exorbitant number causing massive backlash. grid above refers to one unit of Voronoi based geographic segmentation It is necessary to study the correlation of some of the predictors mentioned above. Ride Dispatching What is a robust ride dispatching mechanism that will increase passengers and drivers? Given Drivers in Geographic Grid (and neighboring Grid) Driver Rating/Driver Ranking Geographic Grid Pickup/Drop location (distance and Grid) Use Optimization with constraints: the probability of each driver getting ride should be close to 1, waiting time should be less than some threshold for the request to be accepted or not accepted (which comes from descriptive analysis), the time between request dispatching (time window a driver gets before the request is passed on to a different driver, also comes from descriptive analysis) should be equal to the acceptable waiting time divided by some constant (integer) with objective functions: maximize rides completion rate for each geographic segment ** Notes **\nQueuing models can also be here to identify correct values for the dispatching system (waiting time, dynamic geographic grid, etc.). However, there is a need to check the distribution of different events (booking created, booking accepted, waiting time, etc.) Conclusion It seems analytics is extremely relevant in all aspects of ride-hailing. In this project, I merely covered a few use cases, with one or two relevant models. Even with this brief exploration, I can conclude that analytics can lead to better outcomes for both drivers and passengers.\n","date":"2021-12-02T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/ride_hailing_analytics/","section":"posts","tags":["analytics in ride sharing","tootle","nepal","analytics","isye-6501","ride hailing","ride sharing","regression","optimization","eda"],"title":"Analytics for Ride Hailing Services"},{"categories":["british college"],"contents":"Business Intelligence (British College, 2021 Fall) Objectives To ensure students understand the concepts of business analytics and data visualization To develop students\u0026rsquo; knowledge, understanding. and skills in the real life scenario tackling real world business problems using data To ensure students learn practical skills leveraging few essential tools Outcomes Understand the essentials of business intelligence, statistics and the corresponding terminologies. Be able to create data visualizations using spreadsheets/tableau. Be able to interpret real world problems using data. Be familiar with the steps involved in the BI process. Be able to interact competently on the topic of BI. Have had some hands-on experience in using BI techniques. Tools Google Sheets/Excel, Tableau Github Basic Python and Pandas (if participants want) Weekly Breakdown Week # Topics Covered Week 1 Introduction to BI (Definition, Context, Process, Use Cases) Week 2 Decision Making and Decision Support Systems\nDiscuss the decision making challenges faced by different managers\nDiscuss various DSS and its evolution Week 3 Introduction to Business Metrics, KPIs\nBecome familiar with business metrics used by business analysts in the area of marketing, sales, growth, engagement, and financial analysis\nCalculate and interpret key performance metrics Week 4 Business statistics\nBasic statistics\nTypes of visualization\nInterpreting visualization\nGood visualization Week 5 Descriptive Analysis with Excel/Google Sheets\nFundamentals of data and statistics with use case Week 6 Descriptive Analysis with Excel/Google Sheets\nFundamentals of data and statistics with use case Week 7 Time Series Analysis, Simple Regression, Trends, Seasonality concepts with relevant instances Week 8 Design Principles\nTableau Basics\nTelling stories with Tableau Week 9 Workshop on building Tableau Dashboard Week 10 Current trends in BI, ending notes, complete projects, wrap up Potpourri Terminilogies (will be added thoughout the course) A/B testing ACID compliancy Ad Hoc Agile Anonymization API AWS Quicksight, Tableau, Power BI, Datastudio Backend Batch processing vs streaming Biases Big Data Bots Brand Awareness+Equity+Loyalty Call to action Channel Churn Cloud Columnar Database Competitive Analysis Conversion Correlation CRO (Conversion Rate Optimization) Customer Lifetime Value Dashboard Data Engineering Data Lake Data Mart Data Pipeline (Luigi, Apache airflow) Data Scraping Data Storytelling Data Warehouse Data Wrangling Data Cleansing, Referential integrity, Domain integrity, Entity integrity Database Demand Generation Demography Denormalization DevOps Dimensions and measures Discrete, Continious, Categorical, Ordinal Data Domain knowledge ETL EDA Forecasting Funnels Git Goals Grant funding Hadoop, Spark etc Heuristics Impressions In-Memory BI Infographic Joins KPIs Leads Lookalike audiences Metadata Metrics Mission Near Real Time Normal distribution Normalization Net Promoter Score OKRs OLAP OLTP outlier Pandas, Tidyverse, PySpark Pareto Pirate Metrics Qualatitive data Quantitative data Real Time RDBMS Roadmap ROI Runway period SaaS Schema Seasonality Single source of truth Semi structured data Structured data Spreadsheets Snapshot SQL Surveys Synthetic Data Trend Unstructured data Unique Value Preposition Valuation View Vision Volume Velocity Variety Veracity Variability Don\u0026rsquo;t worry, some of the acronyms confuse everyone Source: https://www.perdoo.com/resources/merkel-asks-what-does-okr-mean/\nDataset eHamroPasalmandu.com (we will be using a fake dataset for this fake ecommerce startup in nepal, see the datasets folder) Tables clients table items table transactions table merged (complete transactions table) clients table sample client_id name gender dob email phone channel first_contact lat lon location_name created_at 100000000 Kaushal Bashyal Male 14/03/2003 kaushal.bashyal@fakeemail.com 9841791565 Google Search browser 27.7768 85.3622 Golfutar Main Rd 48:09.8 100000001 Sona Hayanju Female 03/07/1996 sona.hayanju@fakeemail.com 9841685812 Other browser 27.6954 85.3447 ACE Institute Of Management 46:03.5 100000002 Suman Pokherel Male 04/08/2005 suman.pokherel@fakeemail.com 9841526187 Facebook/Ads browser 27.659 85.368 Changathali Rd 05:06.3 100000003 Samita Yogol Female 22/08/2001 samita.yogol@fakeemail.com 9841131562 Facebook/Ads browser 27.7126 85.283 Ring Road 56:28.8 100000004 Mahima Marasaini Female 20/03/1985 mahima.marasaini@fakeemail.com 9841859344 Other app 27.7137 85.3245 Bhagawati Marg 06:58.6 items table sample item_id item_name price category image_url inventory 10000 iPhone 12 (256GB) 152900 Phone https://dummyimage.com/600x400/000/fff\u0026amp;text=iPhone+12+(256GB) 36 10001 VivoBook 14 X415JA (14” FHD, Intel i7-1065G7, Intel UHD, 8GB, 512GB SSD) 115000 Laptop https://dummyimage.com/600x400/000/fff\u0026amp;text=VivoBook+14+X415JA+(14”+FHD,+Intel+i7-1065G7,+Intel+UHD,+8GB,+512GB+SSD) 39 10002 Helios 300 2020 (15.6″ FHD 144Hz, Core i7-10750H, GTX 1660 Ti, 16GB, 512GB SSD) 190000 Laptop https://dummyimage.com/600x400/000/fff\u0026amp;text=Helios+300+2020+(15.6″+FHD+144Hz,+Core+i7-10750H,+GTX+1660+Ti,+16GB,+512GB+SSD) 27 10003 Sony Alpha A6500 (With 18-135mm zoom lens) 245000 Camera https://dummyimage.com/600x400/000/fff\u0026amp;text=Sony+Alpha+A6500+(With+18-135mm+zoom+lens) 71 10004 iPhone SE 2 (256GB) 96000 Phone https://dummyimage.com/600x400/000/fff\u0026amp;text=iPhone+SE+2+(256GB) 24 transactions table sample created_at item_id client_id 2020-09-01 09:49:12.285421 10142 100000015 2020-09-01 16:44:09.888896 10113 100000010 2020-09-01 22:40:07.678792 10105 100000010 2020-09-02 18:48:22.620671 10177 100000022 2020-09-02 11:04:38.703035 10039 100000019 merged table sample item_id client_id created_at name gender dob email phone channel first_contact lat lon location_name created_at_client item_name price category image_url inventory 10001 100000127 2020-09-06 16:03:41.236993 Pralhad Biskiwakarma Male 2002-05-25 pralhad.biskiwakarma@fakeemail.com 9841968059 Word of Mouth app 27.712 85.322 Hattisar Sadak 2020-09-06 04:41:23.065429 VivoBook 14 X415JA (14” FHD, Intel i7-1065G7, Intel UHD, 8GB, 512GB SSD) 115000 Laptop https://dummyimage.com/600x400/000/fff\u0026amp;text=VivoBook+14+X415JA+(14”+FHD,+Intel+i7-1065G7,+Intel+UHD,+8GB,+512GB+SSD) 39 10001 100000262 2021-03-03 04:08:21.342722 Prazol Harlalka Male 1999-08-22 prazol.harlalka@fakeemail.com 9841213863 Word of Mouth app 27.7415 85.3127 G4 Futsal 2020-09-13 21:25:24.950483 VivoBook 14 X415JA (14” FHD, Intel i7-1065G7, Intel UHD, 8GB, 512GB SSD) 115000 Laptop https://dummyimage.com/600x400/000/fff\u0026amp;text=VivoBook+14+X415JA+(14”+FHD,+Intel+i7-1065G7,+Intel+UHD,+8GB,+512GB+SSD) 39 10001 100000901 2020-11-03 06:43:52.725155 Pramod Shapakota Male 1979-09-14 pramod.shapakota@fakeemail.com 9841852187 Google Search browser 27.7244 85.322 Lazimpat Rd 1079 2020-10-09 13:10:51.153269 VivoBook 14 X415JA (14” FHD, Intel i7-1065G7, Intel UHD, 8GB, 512GB SSD) 115000 Laptop https://dummyimage.com/600x400/000/fff\u0026amp;text=VivoBook+14+X415JA+(14”+FHD,+Intel+i7-1065G7,+Intel+UHD,+8GB,+512GB+SSD) 39 10001 100002190 2020-11-25 08:56:35.801396 Samita Panjiyar Female 1987-10-20 samita.panjiyar@fakeemail.com 9841003522 Word of Mouth browser 27.7057 85.3337 Maiti Devi Marg 2020-11-20 05:14:59.540079 VivoBook 14 X415JA (14” FHD, Intel i7-1065G7, Intel UHD, 8GB, 512GB SSD) 115000 Laptop https://dummyimage.com/600x400/000/fff\u0026amp;text=VivoBook+14+X415JA+(14”+FHD,+Intel+i7-1065G7,+Intel+UHD,+8GB,+512GB+SSD) 39 10001 100002773 2021-04-29 08:08:06.426326 Kusum Katiwada Female 2003-03-01 kusum.katiwada@fakeemail.com 9841201400 Google Search browser 27.7224 85.3101 Rayamajhi Marga 2020-12-07 01:10:09.243962 VivoBook 14 X415JA (14” FHD, Intel i7-1065G7, Intel UHD, 8GB, 512GB SSD) 115000 Laptop https://dummyimage.com/600x400/000/fff\u0026amp;text=VivoBook+14+X415JA+(14”+FHD,+Intel+i7-1065G7,+Intel+UHD,+8GB,+512GB+SSD) 39 Find the complete course here: https://github.com/ayushsubedi/bi\n","date":"2021-10-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/british_bi/","section":"posts","tags":["teaching","curriculum","syllabus"],"title":"Business Intelligence for Nepal"},{"categories":["british college","analytics"],"contents":"eHamroPasalmandu.com eHamroPasalmandu is a fictional eCommerse based in Nepal. If it were operating for a year, this is what the data would look like (kind of, sort of).\nSome \u0026ldquo;real world\u0026rdquo; patterns here: Upward Trend Weekly seasonality Holiday System shutdowns Pirate metrics Pattens usually visible in Nepali startups (demography and distributions) Customer Churn others Raw sources http://www.studentsoftheworld.info/penpals/stats.php?Pays=NEP https://raw.githubusercontent.com/amitness/Thar/master/surnames_en.csv https://www.gadgetbytenepal.com/ Clients table For rest of the data generation, go to https://github.com/ayushsubedi/eHamroPasalmandu\nfrom faker import Faker import pandas as pd import random import matplotlib.pyplot as plt import datetime df = pd.read_csv(\u0026#39;../datasets/nepali_first_name_gender.csv\u0026#39;) filename = \u0026#39;../datasets/lastnames.txt\u0026#39; with open(filename) as f: content = f.readlines() content = [x.strip()[:-1] for x in content] last_names = [x.capitalize() for x in content] first_name_male = [x.capitalize() for x in list(df[df.gender==\u0026#39;M\u0026#39;].first_name)] first_name_female = [x.capitalize() for x in list(df[df.gender==\u0026#39;F\u0026#39;].first_name)] [random.choice(first_name_male) + \u0026#34; \u0026#34; + random.choice(last_names) for x in range(10)] ['Anjan Bhul', 'Ayus Kaliraj', 'Manish Praveen', 'Yubaraj Kareem', 'Bhakta Bagala', 'Sabin Rajbansi', 'Susshanzt Tinkari', 'Sajit Rasaili', 'Dipak Kahrel', 'Sam Remimagar'] total_customer = 30000 male_percent = 58 male = {\u0026#39;name\u0026#39;: [random.choice(first_name_male) + \u0026#34; \u0026#34; + random.choice(last_names) for x in range(int(total_customer*male_percent/100))]} female = {\u0026#39;name\u0026#39;: [random.choice(first_name_female) + \u0026#34; \u0026#34; + random.choice(last_names) for x in range(int(total_customer*(100-male_percent)/100))]} df_male = pd.DataFrame(data=male) df_male[\u0026#39;gender\u0026#39;] = \u0026#39;Male\u0026#39; df_female = pd.DataFrame(data=female) df_female[\u0026#39;gender\u0026#39;] = \u0026#39;Female\u0026#39; df = df_male.append(df_female) df.loc[df.sample(frac=.04).index, \u0026#39;gender\u0026#39; ] = \u0026#39;Other\u0026#39; df.loc[df.sample(frac=.02).index, \u0026#39;gender\u0026#39; ] = \u0026#39;Prefer not to say\u0026#39; df.gender.value_counts() Male 15670 Female 11145 Other 2091 Prefer not to say 1094 Name: gender, dtype: int64 def email_gen(name): name = name.lower() return (name.split(\u0026#39; \u0026#39;)[0]+\u0026#39;.\u0026#39;+name.split(\u0026#39; \u0026#39;)[1]+\u0026#39;@fakeemail.com\u0026#39;) df[\u0026#39;email\u0026#39;] = df.name.apply(email_gen) df name gender email 0 Suman Chyhetri Male suman.chyhetri@fakeemail.com 1 Susshanzt Achrya Male susshanzt.achrya@fakeemail.com 2 Asutosh Awal Male asutosh.awal@fakeemail.com 3 Aditya Madwari Male aditya.madwari@fakeemail.com 4 Pradeep raj nepal Lamichahne Male pradeep.raj@fakeemail.com ... ... ... ... 12595 Prati Bhatt Other prati.bhatt@fakeemail.com 12596 Sneha Jyakhwo Female sneha.jyakhwo@fakeemail.com 12597 Shambhav Shai Female shambhav.shai@fakeemail.com 12598 Soneeya Vonjon Female soneeya.vonjon@fakeemail.com 12599 Yahnaa Bishwas Female yahnaa.bishwas@fakeemail.com 30000 rows × 3 columns\ndf[\u0026#39;phone\u0026#39;] = [random.randint(9841000000, 9842000000) for x in range(df.shape[0])] df name gender email phone 0 Suman Chyhetri Male suman.chyhetri@fakeemail.com 9841137209 1 Susshanzt Achrya Male susshanzt.achrya@fakeemail.com 9841064824 2 Asutosh Awal Male asutosh.awal@fakeemail.com 9841162884 3 Aditya Madwari Male aditya.madwari@fakeemail.com 9841298742 4 Pradeep raj nepal Lamichahne Male pradeep.raj@fakeemail.com 9841204451 ... ... ... ... ... 12595 Prati Bhatt Other prati.bhatt@fakeemail.com 9841152214 12596 Sneha Jyakhwo Female sneha.jyakhwo@fakeemail.com 9841550701 12597 Shambhav Shai Female shambhav.shai@fakeemail.com 9841067695 12598 Soneeya Vonjon Female soneeya.vonjon@fakeemail.com 9841045051 12599 Yahnaa Bishwas Female yahnaa.bishwas@fakeemail.com 9841352244 30000 rows × 4 columns\ndf[\u0026#39;channel\u0026#39;] = \u0026#39;Word of Mouth\u0026#39; df.loc[df.sample(frac=.5).index, \u0026#39;channel\u0026#39; ] = \u0026#39;Facebook/Ads\u0026#39; df.loc[df.sample(frac=.2).index, \u0026#39;channel\u0026#39; ] = \u0026#39;Google Search\u0026#39; df.loc[df.sample(frac=.1).index, \u0026#39;channel\u0026#39; ] = \u0026#39;Other\u0026#39; df name gender email phone channel 0 Suman Chyhetri Male suman.chyhetri@fakeemail.com 9841137209 Facebook/Ads 1 Susshanzt Achrya Male susshanzt.achrya@fakeemail.com 9841064824 Google Search 2 Asutosh Awal Male asutosh.awal@fakeemail.com 9841162884 Facebook/Ads 3 Aditya Madwari Male aditya.madwari@fakeemail.com 9841298742 Facebook/Ads 4 Pradeep raj nepal Lamichahne Male pradeep.raj@fakeemail.com 9841204451 Other ... ... ... ... ... ... 12595 Prati Bhatt Other prati.bhatt@fakeemail.com 9841152214 Word of Mouth 12596 Sneha Jyakhwo Female sneha.jyakhwo@fakeemail.com 9841550701 Facebook/Ads 12597 Shambhav Shai Female shambhav.shai@fakeemail.com 9841067695 Facebook/Ads 12598 Soneeya Vonjon Female soneeya.vonjon@fakeemail.com 9841045051 Facebook/Ads 12599 Yahnaa Bishwas Female yahnaa.bishwas@fakeemail.com 9841352244 Facebook/Ads 30000 rows × 5 columns\ndf.channel.value_counts() Facebook/Ads 11414 Google Search 8131 Other 5318 Word of Mouth 5137 Name: channel, dtype: int64 df[\u0026#39;first_contact\u0026#39;] = \u0026#39;app\u0026#39; df.loc[df.sample(frac=.676).index, \u0026#39;first_contact\u0026#39; ] = \u0026#39;browser\u0026#39; df name gender email phone channel first_contact 0 Suman Chyhetri Male suman.chyhetri@fakeemail.com 9841137209 Facebook/Ads browser 1 Susshanzt Achrya Male susshanzt.achrya@fakeemail.com 9841064824 Google Search browser 2 Asutosh Awal Male asutosh.awal@fakeemail.com 9841162884 Facebook/Ads browser 3 Aditya Madwari Male aditya.madwari@fakeemail.com 9841298742 Facebook/Ads browser 4 Pradeep raj nepal Lamichahne Male pradeep.raj@fakeemail.com 9841204451 Other browser ... ... ... ... ... ... ... 12595 Prati Bhatt Other prati.bhatt@fakeemail.com 9841152214 Word of Mouth browser 12596 Sneha Jyakhwo Female sneha.jyakhwo@fakeemail.com 9841550701 Facebook/Ads app 12597 Shambhav Shai Female shambhav.shai@fakeemail.com 9841067695 Facebook/Ads browser 12598 Soneeya Vonjon Female soneeya.vonjon@fakeemail.com 9841045051 Facebook/Ads browser 12599 Yahnaa Bishwas Female yahnaa.bishwas@fakeemail.com 9841352244 Facebook/Ads browser 30000 rows × 6 columns\ndf[\u0026#39;first_contact\u0026#39;].value_counts() browser 25780 app 4220 Name: first_contact, dtype: int64 df = df.sample(frac=1) df name gender email phone channel first_contact 8250 Kaushal Bashyal Male kaushal.bashyal@fakeemail.com 9841791565 Google Search browser 10606 Sona Hayanju Female sona.hayanju@fakeemail.com 9841685812 Other browser 5076 Suman Pokherel Male suman.pokherel@fakeemail.com 9841526187 Facebook/Ads browser 1824 Samita Yogol Female samita.yogol@fakeemail.com 9841131562 Facebook/Ads browser 6878 Mahima Marasaini Female mahima.marasaini@fakeemail.com 9841859344 Other app ... ... ... ... ... ... ... 9653 Ansu Adhikary Female ansu.adhikary@fakeemail.com 9841430787 Facebook/Ads browser 3735 Lune Bisht Female lune.bisht@fakeemail.com 9841969651 Other browser 1387 Rohit Gajurel Other rohit.gajurel@fakeemail.com 9841940773 Google Search browser 2197 Kusum J.b.rana Female kusum.j.b.rana@fakeemail.com 9841904502 Google Search browser 14831 Satish Rangata Male satish.rangata@fakeemail.com 9841982263 Word of Mouth app 30000 rows × 6 columns\nfrom timeseries_generator import LinearTrend, Generator, WhiteNoise, RandomFeatureFactor, WeekdayFactor import pandas as pd start=\u0026#34;09-01-2020\u0026#34; end=\u0026#34;08-31-2021\u0026#34; # setting up a linear tren lt = LinearTrend(coef=5.0, offset=1., col_name=\u0026#34;my_linear_trend\u0026#34;) weekday_factor = WeekdayFactor( col_name=\u0026#34;weekend_boost_factor\u0026#34;, factor_values={4: 1.1, 5: 1.2, 6: 1.2} # Here we assign a factor of 1.1 to Friday, and 1.2 to Sat/Sun ) # weekday_factor.plot(start_date=start, end_date=end) g = Generator(factors={lt, weekday_factor}, features=None, date_range=pd.date_range(start=start, end=end)) g.generate() # update by adding some white noise to the generator wn = WhiteNoise(stdev_factor=0.06) g.update_factor(wn) g.generate() g.plot() data = g.generate() data[\u0026#39;rows\u0026#39;] = (10*data.value).astype(int) data.rows.sum() 17326 data date base_amount my_linear_trend white_noise weekend_boost_factor total_factor value rows 0 2020-09-01 1.0 2.000000 1.033445 1.0 2.066891 2.066891 20 1 2020-09-02 1.0 2.013699 0.956114 1.0 1.925326 1.925326 19 2 2020-09-03 1.0 2.027397 0.906669 1.0 1.838179 1.838179 18 3 2020-09-04 1.0 2.041096 0.957273 1.1 2.149275 2.149275 21 4 2020-09-05 1.0 2.054795 0.962236 1.2 2.372637 2.372637 23 ... ... ... ... ... ... ... ... ... 360 2021-08-27 1.0 6.931507 0.986548 1.1 7.522090 7.522090 75 361 2021-08-28 1.0 6.945205 0.981777 1.2 8.182375 8.182375 81 362 2021-08-29 1.0 6.958904 1.021142 1.2 8.527237 8.527237 85 363 2021-08-30 1.0 6.972603 1.005213 1.0 7.008952 7.008952 70 364 2021-08-31 1.0 6.986301 1.063856 1.0 7.432417 7.432417 74 365 rows × 8 columns\ndef nepali_holiday(row): date = datetime.datetime.strptime(str(row.date), \u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;) num = row.rows if date\u0026gt;datetime.datetime(2020, 10, 16) and date\u0026lt;datetime.datetime(2020, 10, 25): num = int (num*1.2) if date\u0026gt;datetime.datetime(2020, 12, 24) and date\u0026lt;datetime.datetime(2021, 1, 2): num = int (num*1.4) if date\u0026gt;datetime.datetime(2021, 1, 4) and date\u0026lt;datetime.datetime(2021, 1, 7): num = num - num if date\u0026gt;datetime.datetime(2021, 2, 13) and date\u0026lt;datetime.datetime(2021, 2, 15): num = int (num*1.4) if date\u0026gt;datetime.datetime(2021, 7, 1): num = int(num * 2) return num data[\u0026#39;rows\u0026#39;] = data.apply(nepali_holiday, axis=1) # [\u0026#34;2020-09-01\u0026#34;]*100 + [\u0026#34;2020-09-02\u0026#34;] * 200 from datetime import datetime as dt import datetime def create_list(row): return [str(row.date)]*row.rows final_dates = (data.apply(create_list, axis=1).sum()) clients = df.head(len(final_dates)) clients.reset_index(inplace = True, drop=True) clients[\u0026#39;created_at\u0026#39;] = final_dates clients name gender email phone channel first_contact created_at 0 Kaushal Bashyal Male kaushal.bashyal@fakeemail.com 9841791565 Google Search browser 2020-09-01 00:00:00 1 Sona Hayanju Female sona.hayanju@fakeemail.com 9841685812 Other browser 2020-09-01 00:00:00 2 Suman Pokherel Male suman.pokherel@fakeemail.com 9841526187 Facebook/Ads browser 2020-09-01 00:00:00 3 Samita Yogol Female samita.yogol@fakeemail.com 9841131562 Facebook/Ads browser 2020-09-01 00:00:00 4 Mahima Marasaini Female mahima.marasaini@fakeemail.com 9841859344 Other app 2020-09-01 00:00:00 ... ... ... ... ... ... ... ... 21638 Bibek Barakoti Male bibek.barakoti@fakeemail.com 9841288299 Facebook/Ads browser 2021-08-31 00:00:00 21639 Ansu Huzdar Female ansu.huzdar@fakeemail.com 9841694532 Google Search browser 2021-08-31 00:00:00 21640 Sumit Sanjel Male sumit.sanjel@fakeemail.com 9841638790 Google Search browser 2021-08-31 00:00:00 21641 Prapti Mushyakhow Female prapti.mushyakhow@fakeemail.com 9841359950 Facebook/Ads browser 2021-08-31 00:00:00 21642 Prazol Swarnakar Male prazol.swarnakar@fakeemail.com 9841429598 Facebook/Ads browser 2021-08-31 00:00:00 21643 rows × 7 columns\nclients.gender.value_counts() Male 11306 Female 8040 Other 1500 Prefer not to say 797 Name: gender, dtype: int64 clients.channel.value_counts() Facebook/Ads 8247 Google Search 5849 Other 3827 Word of Mouth 3720 Name: channel, dtype: int64 clients[\u0026#39;first_contact\u0026#39;].value_counts() browser 18601 app 3042 Name: first_contact, dtype: int64 def random_times(input_time): random_hour = random.uniform(0, 23) return dt.strptime(input_time, \u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;) + datetime.timedelta(hours=random_hour) clients[\u0026#39;created_at\u0026#39;] = clients[\u0026#39;created_at\u0026#39;].apply(random_times) clients.sort_values(\u0026#39;created_at\u0026#39;).reset_index(drop=True, inplace=True) clients[\u0026#39;client_id\u0026#39;] = clients.index + 100000000 clients name gender email phone channel first_contact created_at client_id 0 Kaushal Bashyal Male kaushal.bashyal@fakeemail.com 9841791565 Google Search browser 2020-09-01 10:48:09.763278 100000000 1 Sona Hayanju Female sona.hayanju@fakeemail.com 9841685812 Other browser 2020-09-01 11:46:03.489814 100000001 2 Suman Pokherel Male suman.pokherel@fakeemail.com 9841526187 Facebook/Ads browser 2020-09-01 14:05:06.316626 100000002 3 Samita Yogol Female samita.yogol@fakeemail.com 9841131562 Facebook/Ads browser 2020-09-01 20:56:28.775488 100000003 4 Mahima Marasaini Female mahima.marasaini@fakeemail.com 9841859344 Other app 2020-09-01 16:06:58.593803 100000004 ... ... ... ... ... ... ... ... ... 21638 Bibek Barakoti Male bibek.barakoti@fakeemail.com 9841288299 Facebook/Ads browser 2021-08-31 21:01:37.440162 100021638 21639 Ansu Huzdar Female ansu.huzdar@fakeemail.com 9841694532 Google Search browser 2021-08-31 01:11:26.657379 100021639 21640 Sumit Sanjel Male sumit.sanjel@fakeemail.com 9841638790 Google Search browser 2021-08-31 20:41:11.488311 100021640 21641 Prapti Mushyakhow Female prapti.mushyakhow@fakeemail.com 9841359950 Facebook/Ads browser 2021-08-31 18:17:39.263566 100021641 21642 Prazol Swarnakar Male prazol.swarnakar@fakeemail.com 9841429598 Facebook/Ads browser 2021-08-31 02:46:50.362896 100021642 21643 rows × 8 columns\ndef date_gen(start, end): start_date = datetime.date(start, 1, 1) end_date = datetime.date(end, 12, 31) time_between_dates = end_date - start_date days_between_dates = time_between_dates.days random_number_of_days = random.randrange(days_between_dates) random_date = start_date + datetime.timedelta(days=random_number_of_days) return str(random_date) dob = [date_gen(1975, 1990) for x in range(clients.shape[0]//5)] + \\ [date_gen(1991, 1995) for x in range(clients.shape[0]//5)] + \\ [date_gen(1996, 2000) for x in range(clients.shape[0]//2)] + \\ [date_gen(2001, 2005) for x in range(clients.shape[0]//2)] + \\ [date_gen(2006, 2008) for x in range(clients.shape[0]//5)] random.shuffle(dob) clients[\u0026#39;dob\u0026#39;] = dob[:clients.shape[0]] clients name gender email phone channel first_contact created_at client_id dob 0 Kaushal Bashyal Male kaushal.bashyal@fakeemail.com 9841791565 Google Search browser 2020-09-01 10:48:09.763278 100000000 2003-03-14 1 Sona Hayanju Female sona.hayanju@fakeemail.com 9841685812 Other browser 2020-09-01 11:46:03.489814 100000001 1996-07-03 2 Suman Pokherel Male suman.pokherel@fakeemail.com 9841526187 Facebook/Ads browser 2020-09-01 14:05:06.316626 100000002 2005-08-04 3 Samita Yogol Female samita.yogol@fakeemail.com 9841131562 Facebook/Ads browser 2020-09-01 20:56:28.775488 100000003 2001-08-22 4 Mahima Marasaini Female mahima.marasaini@fakeemail.com 9841859344 Other app 2020-09-01 16:06:58.593803 100000004 1985-03-20 ... ... ... ... ... ... ... ... ... ... 21638 Bibek Barakoti Male bibek.barakoti@fakeemail.com 9841288299 Facebook/Ads browser 2021-08-31 21:01:37.440162 100021638 1999-10-05 21639 Ansu Huzdar Female ansu.huzdar@fakeemail.com 9841694532 Google Search browser 2021-08-31 01:11:26.657379 100021639 2004-03-30 21640 Sumit Sanjel Male sumit.sanjel@fakeemail.com 9841638790 Google Search browser 2021-08-31 20:41:11.488311 100021640 1997-07-11 21641 Prapti Mushyakhow Female prapti.mushyakhow@fakeemail.com 9841359950 Facebook/Ads browser 2021-08-31 18:17:39.263566 100021641 2006-05-08 21642 Prazol Swarnakar Male prazol.swarnakar@fakeemail.com 9841429598 Facebook/Ads browser 2021-08-31 02:46:50.362896 100021642 1975-05-24 21643 rows × 9 columns\nclients name gender email phone channel first_contact created_at client_id dob 0 Kaushal Bashyal Male kaushal.bashyal@fakeemail.com 9841791565 Google Search browser 2020-09-01 10:48:09.763278 100000000 2003-03-14 1 Sona Hayanju Female sona.hayanju@fakeemail.com 9841685812 Other browser 2020-09-01 11:46:03.489814 100000001 1996-07-03 2 Suman Pokherel Male suman.pokherel@fakeemail.com 9841526187 Facebook/Ads browser 2020-09-01 14:05:06.316626 100000002 2005-08-04 3 Samita Yogol Female samita.yogol@fakeemail.com 9841131562 Facebook/Ads browser 2020-09-01 20:56:28.775488 100000003 2001-08-22 4 Mahima Marasaini Female mahima.marasaini@fakeemail.com 9841859344 Other app 2020-09-01 16:06:58.593803 100000004 1985-03-20 ... ... ... ... ... ... ... ... ... ... 21638 Bibek Barakoti Male bibek.barakoti@fakeemail.com 9841288299 Facebook/Ads browser 2021-08-31 21:01:37.440162 100021638 1999-10-05 21639 Ansu Huzdar Female ansu.huzdar@fakeemail.com 9841694532 Google Search browser 2021-08-31 01:11:26.657379 100021639 2004-03-30 21640 Sumit Sanjel Male sumit.sanjel@fakeemail.com 9841638790 Google Search browser 2021-08-31 20:41:11.488311 100021640 1997-07-11 21641 Prapti Mushyakhow Female prapti.mushyakhow@fakeemail.com 9841359950 Facebook/Ads browser 2021-08-31 18:17:39.263566 100021641 2006-05-08 21642 Prazol Swarnakar Male prazol.swarnakar@fakeemail.com 9841429598 Facebook/Ads browser 2021-08-31 02:46:50.362896 100021642 1975-05-24 21643 rows × 9 columns\nlocation = pd.read_csv(\u0026#34;../datasets/location.csv\u0026#34;) location = location[[\u0026#39;lat\u0026#39;, \u0026#39;lon\u0026#39;, \u0026#39;name\u0026#39;]] location_list = [location.sample().values.tolist() for x in range(clients.shape[0])] location_df = pd.DataFrame(location_list) location_df 0 0 [27.7768, 85.3622, Golfutar Main Rd] 1 [27.6954, 85.3447, ACE Institute Of Management] 2 [27.659, 85.368, Changathali Rd] 3 [27.7126, 85.283, Ring Road] 4 [27.7137, 85.3245, Bhagawati Marg] ... ... 21638 [27.6787, 85.3103, Jhamsikhel Marg] 21639 [27.7305, 85.3405, Banshidhar Marg] 21640 [27.7206, 85.3194, Kumari Mai Marg] 21641 [27.7351, 85.3056, Gongabu New Buspark] 21642 [27.6962, 85.3393, BanaGanga Marga] 21643 rows × 1 columns\nlocation_df[[\u0026#39;lat\u0026#39;,\u0026#39;lon\u0026#39;, \u0026#39;location_name\u0026#39;]] = pd.DataFrame(location_df[0].tolist(), index= location_df.index) location_df[[\u0026#39;lat\u0026#39;, \u0026#39;lon\u0026#39;, \u0026#39;location_name\u0026#39;]] lat lon location_name 0 27.7768 85.3622 Golfutar Main Rd 1 27.6954 85.3447 ACE Institute Of Management 2 27.6590 85.3680 Changathali Rd 3 27.7126 85.2830 Ring Road 4 27.7137 85.3245 Bhagawati Marg ... ... ... ... 21638 27.6787 85.3103 Jhamsikhel Marg 21639 27.7305 85.3405 Banshidhar Marg 21640 27.7206 85.3194 Kumari Mai Marg 21641 27.7351 85.3056 Gongabu New Buspark 21642 27.6962 85.3393 BanaGanga Marga 21643 rows × 3 columns\nclients name gender email phone channel first_contact created_at client_id dob 0 Kaushal Bashyal Male kaushal.bashyal@fakeemail.com 9841791565 Google Search browser 2020-09-01 10:48:09.763278 100000000 2003-03-14 1 Sona Hayanju Female sona.hayanju@fakeemail.com 9841685812 Other browser 2020-09-01 11:46:03.489814 100000001 1996-07-03 2 Suman Pokherel Male suman.pokherel@fakeemail.com 9841526187 Facebook/Ads browser 2020-09-01 14:05:06.316626 100000002 2005-08-04 3 Samita Yogol Female samita.yogol@fakeemail.com 9841131562 Facebook/Ads browser 2020-09-01 20:56:28.775488 100000003 2001-08-22 4 Mahima Marasaini Female mahima.marasaini@fakeemail.com 9841859344 Other app 2020-09-01 16:06:58.593803 100000004 1985-03-20 ... ... ... ... ... ... ... ... ... ... 21638 Bibek Barakoti Male bibek.barakoti@fakeemail.com 9841288299 Facebook/Ads browser 2021-08-31 21:01:37.440162 100021638 1999-10-05 21639 Ansu Huzdar Female ansu.huzdar@fakeemail.com 9841694532 Google Search browser 2021-08-31 01:11:26.657379 100021639 2004-03-30 21640 Sumit Sanjel Male sumit.sanjel@fakeemail.com 9841638790 Google Search browser 2021-08-31 20:41:11.488311 100021640 1997-07-11 21641 Prapti Mushyakhow Female prapti.mushyakhow@fakeemail.com 9841359950 Facebook/Ads browser 2021-08-31 18:17:39.263566 100021641 2006-05-08 21642 Prazol Swarnakar Male prazol.swarnakar@fakeemail.com 9841429598 Facebook/Ads browser 2021-08-31 02:46:50.362896 100021642 1975-05-24 21643 rows × 9 columns\nclients = pd.concat([clients, location_df[[\u0026#39;lat\u0026#39;, \u0026#39;lon\u0026#39;, \u0026#39;location_name\u0026#39;]]], axis=1) list(clients) ['name', 'gender', 'email', 'phone', 'channel', 'first_contact', 'created_at', 'client_id', 'dob', 'lat', 'lon', 'location_name'] clients[ [\u0026#39;client_id\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;gender\u0026#39;, \u0026#39;dob\u0026#39;, \u0026#39;email\u0026#39;, \u0026#39;phone\u0026#39;, \u0026#39;channel\u0026#39;, \u0026#39;first_contact\u0026#39;, \u0026#39;lat\u0026#39;, \u0026#39;lon\u0026#39;, \u0026#39;location_name\u0026#39;, \u0026#39;created_at\u0026#39;] ].to_csv(\u0026#39;../datasets/clients.csv\u0026#39;, index=False) Find the analysis notebooks here: https://github.com/ayushsubedi/eHamroPasalmandu\n","date":"2021-09-12T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/synthetic_data_generation/","section":"posts","tags":["synthetic","python"],"title":"Synthetic Data Generation: eHamroPasalmandu.com"},{"categories":["open source","pet projects","proof of concept"],"contents":"Twitter Sentiment Analysis Getting data from twitter Using Tweepy (uses official twitter API) easy several limitations check https://developer.twitter.com/en/docs/rate-limits Using Twint (unofficial) a little difficult less limitation amazing for large volume Analysis Vader sentiment using python package https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664 Naive bayes for sentiment analysis (a little of DIY) https://www.datacamp.com/community/tutorials/simplifying-sentiment-analysis-python Spacy package Gensim package etc. Installation Clone the repo git clone https://github.com/cloudfactory/sentiment_analysis_twitter_starter_code\nCD into the cloned directory and create a virtualenv python -m venv env\nEnable virtualenv source env/bin/activate\nInstall dependency packages from requirements.txt pip install -r requirements.txt\nOper jupyter lab session jupyter-lab\nA simple twitter sentiment analysis poc import tweepy import json from tweepy import OAuthHandler import pandas as pd Full documentation here: https://docs.tweepy.org/en/stable/client.html#tweets\nAccess keys Apply at twitter developer and receive these: API_SECRET_KEY = \u0026#34;fill this in\u0026#34; API_KEY = \u0026#34;fill this in\u0026#34; ACCESS_TOKEN = \u0026#34;fill this in\u0026#34; ACCESS_TOKEN_SECRET = \u0026#34;fill this in\u0026#34; class TwitterClient(object): \u0026#39;\u0026#39;\u0026#39; Twitter Client \u0026#39;\u0026#39;\u0026#39; def __init__(self): \u0026#39;\u0026#39;\u0026#39; Class constructor or initialization method. \u0026#39;\u0026#39;\u0026#39; # read keys from the secret credentials file api_key = API_KEY api_secret =API_SECRET_KEY access_token = ACCESS_TOKEN access_token_secret = ACCESS_TOKEN_SECRET try: self.auth = OAuthHandler(api_key, api_secret) self.auth.set_access_token(access_token, access_token_secret) self.api = tweepy.API(self.auth) except: print(\u0026#39;Error: Authentication error\u0026#39;) def get_tweets(self): tweet = self.api.user_timeline(screen_name =\u0026#39;kathmandupost\u0026#39;, count=20) return tweet raw = TwitterClient().get_tweets() df = pd.json_normalize([r._json for r in raw]) df.head() created_at id id_str text truncated source in_reply_to_status_id in_reply_to_status_id_str in_reply_to_user_id in_reply_to_user_id_str ... user.profile_text_color user.profile_use_background_image user.has_extended_profile user.default_profile user.default_profile_image user.following user.follow_request_sent user.notifications user.translator_type user.withheld_in_countries 0 Tue Oct 26 01:20:38 +0000 2021 1452807348309868564 1452807348309868564 EDITORIAL: Protect paddy farmers\\n\\nEase of ac... True \u0026lt;a href=\"https://about.twitter.com/products/tw... None None None None ... 333333 True False False False False False False none [] 1 Tue Oct 26 00:45:00 +0000 2021 1452798380527161348 1452798380527161348 Despite some opposition, Oli appears to have p... True \u0026lt;a href=\"https://about.twitter.com/products/tw... None None None None ... 333333 True False False False False False False none [] 2 Mon Oct 25 23:15:00 +0000 2021 1452775731050782726 1452775731050782726 Over 100,000 doses of Pfizer-BioNtech vaccine ... True \u0026lt;a href=\"https://about.twitter.com/products/tw... None None None None ... 333333 True False False False False False False none [] 3 Mon Oct 25 21:45:00 +0000 2021 1452753082111209475 1452753082111209475 Congress may appoint deputy Speaker, leaving s... True \u0026lt;a href=\"https://about.twitter.com/products/tw... None None None None ... 333333 True False False False False False False none [] 4 Mon Oct 25 20:15:00 +0000 2021 1452730433033015296 1452730433033015296 Everything you need to know about the Covid-19... True \u0026lt;a href=\"https://about.twitter.com/products/tw... None None None None ... 333333 True False False False False False False none [] 5 rows × 70 columns\ndf.text 0 EDITORIAL: Protect paddy farmers\\n\\nEase of ac... 1 Despite some opposition, Oli appears to have p... 2 Over 100,000 doses of Pfizer-BioNtech vaccine ... 3 Congress may appoint deputy Speaker, leaving s... 4 Everything you need to know about the Covid-19... 5 United States to provide 100,620 doses of Pfiz... 6 Paddy damage by freak rains estimated at Rs8.2... 7 Dalit representatives complain of social discr... 8 Consult Delhi for census in Kalapani, census b... 9 Supreme Court justices to boycott full court m... 10 London expands vehicle levy to improve air qua... 11 ‘Children are going to die’, UN agency warns a... 12 EDITORIAL: Railblock ahead\\n\\nDelay in operati... 13 Nepal reports 673 new Covid-19 cases, 13 death... 14 Whether it’s supply or demand, oil era heads f... 15 Justices to decide their further step after me... 16 Climate change: what are the economic stakes?\\... 17 Oslo opens museum to “The Scream” painter Munc... 18 Everything you need to know about the Covid-19... 19 Fauci says vaccines for kids between 5-11 like... Name: text, dtype: object Sentiment analysis from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer def sentiment_scores(sentence): sid_obj = SentimentIntensityAnalyzer() return sid_obj.polarity_scores(sentence) df[\u0026#39;sentiment_scores\u0026#39;] = df.text.apply(sentiment_scores) df[[\u0026#39;text\u0026#39;, \u0026#39;sentiment_scores\u0026#39;]] text sentiment_scores 0 EDITORIAL: Protect paddy farmers\\n\\nEase of ac... {'neg': 0.0, 'neu': 0.779, 'pos': 0.221, 'comp... 1 Despite some opposition, Oli appears to have p... {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound... 2 Over 100,000 doses of Pfizer-BioNtech vaccine ... {'neg': 0.0, 'neu': 0.872, 'pos': 0.128, 'comp... 3 Congress may appoint deputy Speaker, leaving s... {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound... 4 Everything you need to know about the Covid-19... {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound... 5 United States to provide 100,620 doses of Pfiz... {'neg': 0.0, 'neu': 0.865, 'pos': 0.135, 'comp... 6 Paddy damage by freak rains estimated at Rs8.2... {'neg': 0.276, 'neu': 0.724, 'pos': 0.0, 'comp... 7 Dalit representatives complain of social discr... {'neg': 0.134, 'neu': 0.753, 'pos': 0.113, 'co... 8 Consult Delhi for census in Kalapani, census b... {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound... 9 Supreme Court justices to boycott full court m... {'neg': 0.164, 'neu': 0.582, 'pos': 0.255, 'co... 10 London expands vehicle levy to improve air qua... {'neg': 0.145, 'neu': 0.683, 'pos': 0.173, 'co... 11 ‘Children are going to die’, UN agency warns a... {'neg': 0.277, 'neu': 0.723, 'pos': 0.0, 'comp... 12 EDITORIAL: Railblock ahead\\n\\nDelay in operati... {'neg': 0.113, 'neu': 0.887, 'pos': 0.0, 'comp... 13 Nepal reports 673 new Covid-19 cases, 13 death... {'neg': 0.17, 'neu': 0.83, 'pos': 0.0, 'compou... 14 Whether it’s supply or demand, oil era heads f... {'neg': 0.061, 'neu': 0.939, 'pos': 0.0, 'comp... 15 Justices to decide their further step after me... {'neg': 0.076, 'neu': 0.762, 'pos': 0.162, 'co... 16 Climate change: what are the economic stakes?\\... {'neg': 0.0, 'neu': 0.901, 'pos': 0.099, 'comp... 17 Oslo opens museum to “The Scream” painter Munc... {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound... 18 Everything you need to know about the Covid-19... {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound... 19 Fauci says vaccines for kids between 5-11 like... {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound... sentiments = pd.concat([df, df.sentiment_scores.apply(pd.Series)], axis=1)[[\u0026#39;text\u0026#39;, \u0026#39;neg\u0026#39;, \u0026#39;neu\u0026#39;, \u0026#39;pos\u0026#39;]] sentiments text neg neu pos 0 EDITORIAL: Protect paddy farmers\\n\\nEase of ac... 0.000 0.779 0.221 1 Despite some opposition, Oli appears to have p... 0.000 1.000 0.000 2 Over 100,000 doses of Pfizer-BioNtech vaccine ... 0.000 0.872 0.128 3 Congress may appoint deputy Speaker, leaving s... 0.000 1.000 0.000 4 Everything you need to know about the Covid-19... 0.000 1.000 0.000 5 United States to provide 100,620 doses of Pfiz... 0.000 0.865 0.135 6 Paddy damage by freak rains estimated at Rs8.2... 0.276 0.724 0.000 7 Dalit representatives complain of social discr... 0.134 0.753 0.113 8 Consult Delhi for census in Kalapani, census b... 0.000 1.000 0.000 9 Supreme Court justices to boycott full court m... 0.164 0.582 0.255 10 London expands vehicle levy to improve air qua... 0.145 0.683 0.173 11 ‘Children are going to die’, UN agency warns a... 0.277 0.723 0.000 12 EDITORIAL: Railblock ahead\\n\\nDelay in operati... 0.113 0.887 0.000 13 Nepal reports 673 new Covid-19 cases, 13 death... 0.170 0.830 0.000 14 Whether it’s supply or demand, oil era heads f... 0.061 0.939 0.000 15 Justices to decide their further step after me... 0.076 0.762 0.162 16 Climate change: what are the economic stakes?\\... 0.000 0.901 0.099 17 Oslo opens museum to “The Scream” painter Munc... 0.000 1.000 0.000 18 Everything you need to know about the Covid-19... 0.000 1.000 0.000 19 Fauci says vaccines for kids between 5-11 like... 0.000 1.000 0.000 Depending on the threshold you create, you can also label something as positive or negative sentiment now ","date":"2021-08-11T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/sentiment_analysis/","section":"posts","tags":["twitter","sentiment","python"],"title":"Sentiment Analysis starter code"},{"categories":["open source","pet projects","proof of concept"],"contents":"Nepal Vaccine Progress Twitter Bot https://twitter.com/NepalVaccine Data Source: https://github.com/owid/covid-19-data/blob/master/public/data/vaccinations/country_data/Nepal.csv\nThis is a Twitter bot that shows progress on covid vaccination (full and partial) in Nepal.\nInstallation for contribution Clone the repository https://github.com/ayushsubedi/vaccine_progress_bot_nepal/ CD into the cloned directory and create a virtualenv python -m venv env Enable virtualenv .\\env\\Scripts\\activate Enable virtualenv (windows) .\\env\\Scripts\\activate Install dependency packages from requirements.txt pip install -r requirements.txt Run flask app source FLASK_APP=\u0026#34;app.py\u0026#34; flask run Run flask app (windows) $env:FLASK_APP=\u0026#34;app.py\u0026#34; flask run .env Add Twitter keys and BasicAuth keys to the .env_sample file. Once complete rename it to .env\n","date":"2021-07-20T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/vaccine_progress/","section":"posts","tags":["flask","python","bot"],"title":"Nepal Vaccine Progress Twitter Bot"},{"categories":["pet projects","data viz","analytics"],"contents":"The Emojis of Kathmandu 50K tweets between 2012 to 2021 (containing emojis, and geocoded to Kathmandu) show that we really love 😂\nRecently though, we have been 😂 less and 🙏 more, 😁 less and ❤️ more. Just goes to show Kathmandu is healing from Covid.\nGithub repo: https://github.com/ayushsubedi/emojan\nCreate the dataset using twint import twint import pandas as pd import nest_asyncio nest_asyncio.apply() emoji_list = [\u0026#39;😁\u0026#39;, \u0026#39;😒\u0026#39;, \u0026#39;😊\u0026#39;, \u0026#39;😈\u0026#39;, \u0026#39;🇳🇵\u0026#39;, \u0026#39;😌\u0026#39;, \u0026#39;☕\u0026#39;, \u0026#39;👶\u0026#39;, \u0026#39;👍\u0026#39;, \u0026#39;😷\u0026#39;, \u0026#39;👌\u0026#39;, \u0026#39;🌞\u0026#39;, \u0026#39;😑\u0026#39;, \u0026#39;😉\u0026#39;, \u0026#39;😍\u0026#39;, \u0026#39;☺️\u0026#39;, \u0026#39;😴\u0026#39;, \u0026#39;😱\u0026#39;, \u0026#39;🙏\u0026#39;, \u0026#39;😘\u0026#39;, \u0026#39;🙌\u0026#39;, \u0026#39;😔\u0026#39;, \u0026#39;😋\u0026#39;, \u0026#39;😂\u0026#39;, \u0026#39;😩\u0026#39;, \u0026#39;💕\u0026#39;, \u0026#39;😎\u0026#39;, \u0026#39;😭\u0026#39;, \u0026#39;😳\u0026#39;, \u0026#39;😇\u0026#39;, \u0026#39;😏\u0026#39;, \u0026#39;😜\u0026#39;, \u0026#39;☕️\u0026#39;, \u0026#39;✌️\u0026#39;, \u0026#39;🙈\u0026#39;, \u0026#39;❤️\u0026#39;, \u0026#39;😄\u0026#39;, \u0026#39;💞\u0026#39;] for emoji in emoji_list: c = twint.Config() c.Search = emoji c.Pandas = True c.Store_csv = True c.Output = emoji c.Hide_output= True c.Near= \u0026#34;kathmandu\u0026#34; c.Since=\u0026#34;2010-01-01\u0026#34; c.Until = \u0026#34;2021-05-31\u0026#34; twint.run.Search(c) Perform analysis import pandas as pd df = pd.read_csv(\u0026#34;../datasets/merged.csv\u0026#34;, lineterminator=\u0026#39;\\n\u0026#39;, parse_dates=[\u0026#39;date\u0026#39;]) list(df) ['id', 'emoji', 'date', 'username', 'tweet', 'likes_count', 'place'] df.username.value_counts().head(20) ms_madhur 1433 sristee44 1121 beingsamikshya 871 anuskashresthax 618 nepalplanettrek 594 scousergirl 572 dreamingdr 569 nepaligentleman 476 itsme_shivangi 433 iamnabinraj75 410 sim_shrestha 402 milan_pu1 353 raunakbasnet1 332 thulokanxo 325 chetan_karki 302 saampokhrel 301 mongolianheartk 300 tenzintsetenbhu 290 rana1997rohit 275 fatyangri 256 Name: username, dtype: int64 df.emoji.value_counts().head(20) 😂 7576 😊 4016 😍 3781 😁 3482 🇳🇵 2785 😎 2270 ❤️ 2233 🙏 2122 😉 2000 😜 1463 😘 1432 😋 1241 💕 1173 😄 1095 👍 1001 👌 730 😭 637 😏 554 ✌️ 505 😒 483 Name: emoji, dtype: int64 df.date.min() Timestamp('2012-01-28 00:00:00') df.set_index(\u0026#39;date\u0026#39;).resample(\u0026#39;M\u0026#39;)[\u0026#39;id\u0026#39;].count() date 2012-01-31 2 2012-02-29 16 2012-03-31 13 2012-04-30 3 2012-05-31 11 ... 2021-01-31 212 2021-02-28 134 2021-03-31 183 2021-04-30 160 2021-05-31 119 Freq: M, Name: id, Length: 113, dtype: int64 df.set_index(\u0026#39;date\u0026#39;).resample(\u0026#39;M\u0026#39;)[\u0026#39;id\u0026#39;].count().plot() \u0026lt;AxesSubplot:xlabel='date'\u0026gt; df[df.emoji==\u0026#34;❤️\u0026#34;].set_index(\u0026#39;date\u0026#39;).resample(\u0026#39;M\u0026#39;)[\u0026#39;id\u0026#39;].count().plot() \u0026lt;AxesSubplot:xlabel='date'\u0026gt; df[df.emoji==\u0026#34;😷\u0026#34;].set_index(\u0026#39;date\u0026#39;).resample(\u0026#39;M\u0026#39;)[\u0026#39;id\u0026#39;].count().plot() \u0026lt;AxesSubplot:xlabel='date'\u0026gt; df[df.emoji==\u0026#34;🇳🇵\u0026#34;].set_index(\u0026#39;date\u0026#39;).resample(\u0026#39;M\u0026#39;)[\u0026#39;id\u0026#39;].count().plot() \u0026lt;AxesSubplot:xlabel='date'\u0026gt; df[df.emoji==\u0026#34;😂\u0026#34;].set_index(\u0026#39;date\u0026#39;).resample(\u0026#39;M\u0026#39;)[\u0026#39;id\u0026#39;].count().plot() \u0026lt;AxesSubplot:xlabel='date'\u0026gt; location_tweet = df.dropna(subset=[\u0026#39;place\u0026#39;]) import json import math def getlatlon(row): place = row.place place = place.replace(\u0026#34;\\\u0026#39;\u0026#34;, \u0026#34;\\\u0026#34;\u0026#34;) place = json.loads(place) lat = place[\u0026#39;coordinates\u0026#39;][0] lon = place[\u0026#39;coordinates\u0026#39;][1] return pd.Series([lat, lon],index=[\u0026#39;lat\u0026#39;,\u0026#39;lon\u0026#39;]) df = df.join(location_tweet.apply(getlatlon, axis=1, result_type=\u0026#34;expand\u0026#34;)) df.to_csv(\u0026#39;location.csv\u0026#39;, index=False) df id emoji date username tweet likes_count place lat lon 0 1398437418768818176 👌 2021-05-29 dendikapan शुभ बिहानी ☕️☕️ शुभदिनकाे कामना🙏😷👌🇳🇵💞 Hopefull... 3 NaN NaN NaN 1 1397792077149216769 👌 2021-05-27 dendikapan @damanbro66 Yes I like it 👌so much your post ... 0 NaN NaN NaN 2 1397341721566990336 👌 2021-05-26 dendikapan @KiranCh77 Really nice 👌thanks for sharing 👌 ... 0 NaN NaN NaN 3 1394863068035813379 👌 2021-05-19 iammuhnaj It's still Bull season👌 @ 𝙃𝙊𝙈𝙀 https://t.co/O... 1 {'type': 'Point', 'coordinates': [27.713776, 8... 27.713776 85.310244 4 1393821979032133635 👌 2021-05-16 chetan_karki #just #fun with #baby #myrahsofkarki 😊❤️ looki... 0 {'type': 'Point', 'coordinates': [27.67733347,... 27.677333 85.307636 ... ... ... ... ... ... ... ... ... ... 45362 389751359874289664 ❤️ 2013-10-14 ashmoo_moo #littleone #cuteness #dashain #tika #sister #c... 0 {'type': 'Point', 'coordinates': [27.73048071,... 27.730481 85.330964 45363 389704517815902208 ❤️ 2013-10-14 ashmoo_moo Baba. Prajjwal Dai. ❤️ #dashain #tika #family ... 0 {'type': 'Point', 'coordinates': [27.73855019,... 27.738550 85.338760 45364 387917234490048512 ❤️ 2013-10-09 ashmoo_moo #brother #cousins #mamaghar #nagpokhari #morni... 0 {'type': 'Point', 'coordinates': [27.71356987,... 27.713570 85.324463 45365 387542385145954304 ❤️ 2013-10-08 ashmoo_moo #cuteness #babysister #cousins #smile ❤️😘💋 @ M... 0 {'type': 'Point', 'coordinates': [27.7090305, ... 27.709031 85.326469 45366 386357639460179968 ❤️ 2013-10-05 ashmoo_moo on our way back homeeeeee.... ❤️😘 #lastnight #... 0 {'type': 'Point', 'coordinates': [27.73868383,... 27.738684 85.338705 45367 rows × 9 columns\ndf.emoji.value_counts() 😂 7576 😊 4016 😍 3781 😁 3482 🇳🇵 2785 😎 2270 ❤️ 2233 🙏 2122 😉 2000 😜 1463 😘 1432 😋 1241 💕 1173 😄 1095 👍 1001 👌 730 😭 637 😏 554 ✌️ 505 😒 483 💞 461 😇 432 🙌 423 ☺️ 375 😔 373 😌 332 😱 314 😑 296 😈 292 🙈 291 ☕ 277 🌞 213 😳 185 😩 181 😴 163 😷 135 👶 45 Name: emoji, dtype: int64 ","date":"2021-07-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/emojis_of_kathmandu/","section":"posts","tags":["tableau","viz"],"title":"The Emojis of Kathmandu"},{"categories":["cheers hospital","data viz","analytics"],"contents":"Cheers AI Diabetic Retinopathy and Glaucoma Detection Features Efficient Prediction Models Efficient models trained on Inception-v3, with weightage on recall.\nPatient Tracking Powerful MIS to create and track patient, and patient\u0026rsquo;s historical predictions.\nContinuous Learning Inputs reviewed by opthalmologists and added to training.\nWhy? The rationale for developing countries. Diabetic Retinopathy Diabetic Retinopathy is an eye illness caused by diabetes that may lead to vision impairment and even to blindness if it isn\u0026rsquo;t identified and treated early. Of the estimated 422 million diabetics globally, more than 148 million have DR and 48 million have Vision Threating DR (VTDR).\nHowever, because of insufficient specialists and eye care health workers globally as well as locally to screen everyone at risk, the situation seems acute especially in developing countries like Nepal. Besides, Nepal has difficult geographical terrain and people living in remorse remote areas with limited or no access to clinics and screening facilities making the condition even worse.\nGlaucoma Glaucoma is a diverse group of disorders representing the second prominent cause of blindness. It has already affected 91 million individuals all over the world. It has multiple risk factors such as older age, elevated intraocular pressure (IOP), and thinner central corneal thickness etc. However, one or more of these risk factors may or may not develop glaucoma making it difficult for accurate prediction of the disease. Additionally, since glaucoma can be asymptomatic, its detection before significant vision loss is critical. Hence, automated methods for predicting glaucoma could have a significant impact.\nAn intuitive app Easy to use, access managed platform, with the primary focus on providing assistance to our opthalmologists.\nProcess Glaucoma Prediction What worked? (90% accuracy) densenet sequential with ben on himanchu dataset, using NLLLoss criterion, Adam optimizer Limitation very much dependent on dataset disk extraction is good but is very subjective to the dataset trained on very small dataset Preliminary create a gmail account (glaucomadetection@gmail.com) understand the difference between possibility of glaucoma by classification (vs measurements) Preprocessing ben transformation extract disk from fundus images improve extraction algorithms perform EDA on disk image to find troubling images (cases where crop does not work) convert python function to extract disk to torch transform class (failed) transformation to disk during training failed. create a disk dataset before training the model. train on new dataset with and without ben transformation handle imbalanced class with class weighting convert Kaggle dataset to the format that we have templated our notebooks with for kaggle dataset get disks using new algorithm Obseverations in regards to disk generation extraction of disk does not help (too many vague areas left unfilled) however, cropping shows very good promise but, cropping requires somewhat similar of fundus images Datasets find datasets https://deepblue.lib.umich.edu/data/concern/data_sets/3b591905z, https://www.kaggle.com/andrewmvd/ocular-disease-recognition-odir5k create a dataset from Magrabia create a dataset from Messidor create a dataset from Ocular Disease Recognition create EDA on non measurement dataset (Ocular Disease Recognition) create a dataset from ocular disease recognition to include normal and glaucoma images (Kaggle dataset, custom generated, filtered)https://www.kaggle.com/sshikamaru/glaucoma-detection?select=glaucoma.csv train on Kaggle dataset (without changing anything) Training inception v3 with and without ben on ocular, kaggle, and himanchu dataset inception v3 with ben on ocular, kaggle, and himanchu dataset (disk extracted, normal, and cropped dataset) densenet linear with ben on ocular, kaggle, and himanchu dataset densenet linear with ben on ocular, kaggle, and himanchu dataset (disk extracted, normal, and cropped dataset) densenet sequential with ben on ocular, kaggle, and himanchu dataset densenet sequential with ben on ocular, kaggle, and himanchu dataset (disk extracted, normal, and cropped dataset) add datasets from cheers for testing add datasets from cheers for training Diabetic Retinopathy Prediction What worked? (90% accuracy) Large dataset from EyePACS (Kaggle competition used training (30%) and testing data (70%) from Kaggle. After the competition, the labels were published). Flipped the ratios for our use case. Remove out of focus images Remove too bright, and too dark images. Link to clean dataset https://www.kaggle.com/ayushsubedi/drunstratified To handle class imbalanced issue, used weighted random samplers. Undersampling to match no of images in the least class (4) did not work. Pickled weights for future use. Ben Graham transformation and augmentations Inception v3 fine tuning, with aux logits trained (better results compared to other architecture) Perform EDA on inference to observe what images were causing issues Removed the images and created another dataset (Link to the new dataset https://www.kaggle.com/ayushsubedi/cleannonstratifieddiabeticretinopathy See 5, 6, and 7 TODOS Datasets Binary Stratified (cleaned): https://drive.google.com/drive/folders/12-60Gm7c_TMu1rhnMhSZjrkSqqAuSsQf?usp=sharing Categorical Stratified (cleaned): https://drive.google.com/drive/folders/1-A_Mx9GdeUwCd03TUxUS3vwcutQHFFSM?usp=sharing Non Stratified (cleaned): https://www.kaggle.com/ayushsubedi/drunstratified Recleaned Non Stratified: https://www.kaggle.com/ayushsubedi/cleannonstratifieddiabeticretinopathy\nPriliminary create a new gmail account to store datasets (diabeticretinopathyglaucoma@gmail.com) https://www.youtube.com/watch?v=VIrkurR446s\u0026amp;ab_channel=khanacademymedicine What is diabetic retinopathy? collect all previous analysis notebooks conduct preliminary EDA (for balanced dataset, missing images etc) create balanced test train split for DR (stratify) store the dataset in drive for colab identify a few research papers, create a file to store subsequently found research papers identify right technology stack to use (for ML, training, PM, model versioning, stage deployment, actual deployment) perform basic augmentation create a version 0 base model apply a random transfer learning model create a metric for evaluation store the model in zenodo, or find something for version control create a model that takes image as an input create a streamlit app that reads model streamlit app to upload and test prediction test deployment to free tier heroku identify gaps create priliminary test set create folder structures for saved model in the drive figure out a way to move files from kaggle to drive (without download/upload) research saving model (the frugal way) research saving model to google drive after each epoch so that during unforseen interuptions, the training of the model can be continued Resource upgrade to 25GB RAM in Google Colab possibly w/ Tesla P100 GPU upgrade to Colab Pro Baseline medicmind grading (accuracy: 0.8) medicmind classification (0.47) Transfer Learning resnet alexnet vgg squeezenet densenet inception efficient net Dataset clean images create a backup of primary dataset (zip so that kaggle kernels can consume them too) find algorithms to detect black/out of focus images identify correct threshold for dark and out of focus images remove black images remove out of focus images create a stratified dataset with 2015 data only (convert train and test both to train and use), remove black images and out of focus images (also create test set) create non-stratified dataset with 2015 clean data only (train, test, valid) (upload in kaggle if google drive full) create a binary dataset (train, test, valid) create confusion matrices (train, test, valid) after clean up (dark and blurry) the model is confusing labels 0 and 1 as 2, is this due to disturbance in image in 0. concluded that the result is due to the model not capturing class 0 enough (due to undersampling) Inference create a csv with preds probability and real label calculate recall, precision, accuracy, confusion matrix identify different prediction issues relationship between difference in preds and accuracy inference issue: labels 0 being predicted as 4 inference issue: Check images from Grade 2, 3 being predicted as Grade 0 inference issue: Check images from Grade 4 being predicted as Grade 0 inference issue: Check images from Grade 0 being predicted as Grade 4 inference issue: A significant Grade 2 is being predicted as Grade 0 inference issue: More than 50% of Grade 1 is being predicted as Grade 0 create a new dataset Model Improvement research kaggle winning augmentation for DR research appropriate augmentation: optical distortion, grid distortion, piecewise affine transform, horizontal flip, vertical flip, random rotation, random shift, random scale, a shift of RGB values, random brightness and contrast, additive Gaussian noise, blur, sharpening, embossing, random gamma, and cutout train on various pretrained models or research which is supposed to be ideal for this case https://pytorch.org/vision/stable/models.html create several neural nets (test different layers) experiment with batch size Reducing lighting-condition effects Cropping uninformative area Create custom dataloader based on ben graham kaggle winning strategy finetune vs feature extract oversample undersample add specificity and sensitivity to indicators create train loss and valid loss charts test regression models (treat this as a grading problem) pickle weights Additional Models check if left/right eye classification model is required Additional datasets make datasets more extensive (add test dataset with recoverd labels to train dataset 2015) add APTOS dataset request labelled datasets from cheers add datasets from cheers for testing add datasets from cheers for training Test datasets find datasets for testing (dataset apart from APTOS and EyePACS) update folder structures to match our use case find dataset for testing after making sure old test datasets are not in vaid/train (4 will be empty) Concepts/Research Papers read reports from kaggle competition winning authors Deep Learning Approach to Diabetic Retinopathy Detection https://arxiv.org/pdf/2003.02261.pdf Google research https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45732.pdf Nature article https://www.nature.com/articles/s41746-019-0172-3 read ravi\u0026rsquo;s article https://deim.urv.cat/~itaka/itaka2/PDF/acabats/PhD_Thesis/TESI_doctoral_Jordi_De_la_Torre.pdf what can go wrong https://yerevann.github.io/2015/08/17/diabetic-retinopathy-detection-contest-what-we-did-wrong/ https://arxiv.org/pdf/1902.07208.pdf identify more papers ","date":"2021-04-04T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/cheers_ai/","section":"posts","tags":["python","ml","ai","nepal","flask"],"title":"Diabetic Retinopathy and Glaucoma Detection"},{"categories":["world bank","data viz","analytics"],"contents":"Data in News Revisited Ayush Subedi developed this dashboard with the guidance from Ravi Kumar. The dashboard is based on the methodology proposed by Klein, Galdin, and Mohamedou in 2016. This dashboard was initiated to assess the use of data in media in Nepal to provide an input to a World Bank report titled \u0026ldquo;Use of Data in the Private Sector of Nepal : The Current State and Opportunities in Finance, Education, and the Media\u0026rdquo;. This publication was funded by the Trust Fund for Statistical Capacity Building, a global grant facility administered by the Development Data Group of the World Bank on behalf of the contributing donors, and the Partnership for Knowledge-Based Poverty Reduction and Shared Prosperity, a World Bank project with support from the UK\u0026rsquo;s Foreign, Commonwealth and Development Office (FCDO) to increase the production and usage of data and statistics in Nepal.\nIntroduction How journalists use data in their reporting or in their writing can be viewed as a reflection of the country\u0026rsquo;s demand for statistics and data. The World Bank team developed an indicator model to assess the use of data by Nepali news portals based on the methodology proposed by Klein, Galdin, and Mohamedou 2016. As of 07 Jun 2021, the results show that very few news articles indicate the source of data or mention development indicators. However, articles that discuss data, reports, research, statistics, and related topics do critically engage with these ideas.\n101060 articles were analyzed to assess the use of data in media. In the previous implementation of this project, the source of data was RSS feeds from Nepali news portal. Unfortunately, due to most of the Nepali news portals deprecating their RSS feature, for this implementation, the news articles are scraped from article URLs posted by the official news portal\u0026rsquo;s account on twitter. The news portals that use twitter to post articles regularly are The Himalayan Times, Onlinekhabar, The Nepali Times, myRepublica, and The Kathmandu Post.\nImplementation Data in NEWS portal The results from the project was used in the research article published at http://documents1.worldbank.org/curated/en/805261601023506163/pdf/Use-of-Data-in-the-Private-Sector-of-Nepal-The-Current-State-and-Opportunities-in-Finance-Education-and-the-Media.pdf\nAs of 9th February 2020, the results showed that very few news articles indicate the source of data or mention development indicators. However, articles that discuss data, reports, research, statistics, and related topics do critically engage with these ideas.\nUnfortunately, the data collection process was halted because we ran out of our AWS Activate for Startups credits.\nTODOS create a simple architecture diagram curate a list of twitter handles of Nepali newspapers that put URL to their newspapers use twint to create datasets (historic) check if url is present, url does not 404, and url belongs to the publisher use Newspaper to collect important information (author, etc) and complete the dataset research dash, swifter and pandarallel optimize regular expression functions (if possible this should be performed during scraping) create a sqlite database to hold access information (and design schema around other meta) or find and alternative add checkpoints (using twint resume or build it manually) POC create a full working version of the product research downloading twint as pandas to merge with previous data Twitter handles @kathmandupost @thehimalayan @NepaliTimes @OnlineKhabar_En @RepublicaNepal The Frugal way This project leverages on Github Actions for scheduling and scraping purposes. This project is hosted on Heroku free tier (free ssl) Uses amazing open source projects, primariliy (Twint, Newspaper) etc Previous iteration Find the previous iteration here: https://ayushsubedi.github.io/posts/data_in_news/\n","date":"2021-03-03T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/data_in_news_revisited/","section":"posts","tags":["analysis","proxy"],"title":"Data in News Revisited"},{"categories":["open source","pet projects","proof of concept"],"contents":"kohokoho (को हो को हो !) Are you an aspiring data analyst? Do clients hesitate to give you their data because of all the sensitive information in it?\nOr, are you an entrepreneur hesitant to hire a data consultant due to the nature of your data?\nMask your data using kohokoho (को हो को हो !)\nKohokoho (को हो को हो) is a python package that anonymizes a dataset. Currently, it is still in its infancy and only supports a select few data types.\nFeel free to contribute with code (it\u0026rsquo;s all open source), or by giving it a better name :D\nInstallation pip install kohokoho\nInstallation (from source) Clone the repository git clone https://github.com/ayushsubedi/kohokoho\nCD into the cloned directory and create a virtualenv python -m venv env\nEnable virtualenv Windows\n.\\env\\Scripts\\activate\nMac/Linux\nsource env/bin/activate\nInstall dependency packages from requirements.txt pip install -r requirements.txt\nRun the app python kohokoho.py --csv={location of csv file}\nPackage link: https://pypi.org/project/kohokoho/\nTest it here https://kohokoho.herokuapp.com/\nVersion-O Demo ","date":"2021-02-03T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/kohokoho/","section":"posts","tags":["cli","python","web app","flask"],"title":"Kohokoho"},{"categories":["open source","pet projects","proof of concept"],"contents":"Choto pip install choto\nIf you spend most of your time on the terminal, and you need to update yourself on the news, use \u0026ldquo;choto\u0026rdquo;. Choto will summarize the news for you.\nPython package: https://pypi.org/project/choto/\nInstallation (from source) Clone the repository git clone https://github.com/ayushsubedi/choto\nCD into the cloned directory and create a virtualenv python -m venv env\nEnable virtualenv .\\env\\Scripts\\activate\nInstall dependency packages from requirements.txt pip install -r requirements.txt\nRun the app python choto.py --url=https://abc.xyz --ratio=0.5 --algorithm=bert\nUsage: choto.py [OPTIONS]\nOptions: --url TEXT Enter a valid URL --ratio FLOAT Ratio to summarize to. --algorithm [gensim|spacy|bert] Algorithm to use --help Show this message and exit. ","date":"2021-02-02T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/choto/","section":"posts","tags":["cli","python"],"title":"Choto"},{"categories":["moonlit","data viz","analytics"],"contents":"Developers in Nepal vs the world Link to the dashboard: https://devsinnepal.github.io/\nThis dashboard facilitates a comparative study of the coding landscape in Nepal with the rest of the world. This tool will help visualize and understand developers, their demographic and geographic profile, motivation, education, coding preference and several other factors.\nThe survey data has been utilized from StackOverflow. For almost a decade, StackOverflow\u0026rsquo;s annual Developer Survey has been the largest survey of people who code around the world. The survey is taken by nearly 65,000 people in 2020. 242 people participated from Nepal.\nWhile Stackoverflow also publishes its analysis on the survey annually, it primarily has a wide focus on aggregated global trends.\nInsights Nepal is one of the top destination to outsource technology. As we can see in the chart below, the annual median developer remuneration is the lowest in Nepal ($6300). It is still more than 6 times the median per capita income in Nepal. Even with fairly low remuneration, productivity is notably high. In the last 20 yrs, the quality of technology services these pool of talents can/have built is very close to the international standards.\nThe annual median remuneration in the US is USD. 115,000. Broken into hourly pay (40 hours in a week, 52 weeks in a year), a Nepali developer makes USD. 3.03 an hour whereas a developer in the US makes USD 55.30 an hour. There is a stark difference in the pay between the developers in US vs. Nepal which suggests that there is a great value for employers in outsourcing technology in Nepal. It\u0026rsquo;s a win-win for both, promotes high-paying growth-oriented jobs for talented Nepali developers and quality service at a lower cost for employers.\nSimilar to other countries as well.\nThe trend of developers participating in StackOverflow Survey from Nepal has increased over the years. This implies growing popularity of Stackoverflow among the developers and a possibility of more inclusive and accurate results.\nIn the trend below, a certain spike is seen during 2018. However, the number is not largely deviated. Considering, 2018 as an outlier, we can see a growing trend.\nJavaScript and Python are the two most desired language among developers of Nepal. CURRENTLY USED LANGUAGE DESIRED LANGUAGE NUMBER OF DEVELOPERS HTML/CSS JavaScript 86 JavaScript Python 83 HTML/CSS Python 82 SQL Python 62 This analysis also shows that most of the developers are currently familiar with HTML/CSS . However, most developers desire to learn JavaScript in future.\nFurthermore, most developers who have worked with JavaScript and HTML/CSS desire to learn Python indicating its popularity amongst the community too.\n","date":"2020-12-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/devs_in_nepal/","section":"posts","tags":["tableau","stackoverflow","analysis"],"title":"Devs in Nepal"},{"categories":["open source","pet projects","proof of concept"],"contents":"Affirmations with flutter It had been a while since I (subconsciously or not so subconsciously) pivoted my career from app development to data science. Man, I did not miss Android Studio clogging my base model laptop’s memory. However, my little cousin (who just ventured into app development) wanted help with one of the issues he was having. But, as someone who had not used semicolons and braces for almost couple of years, Dart was almost nauseating upon first glance. I promised him I would get around answering his queries, but after I built something simple first.\nAffirmations.dev provides quotes that are motivating to the devs. So, as a newbie in Flutter, it good to get all the positive reinforcement especially when Android Studio and the virtual device doesn’t leave much space for many Stackoverflow tabs.\nFew examples of what affirmations.dev returns: // 20200927192236 // https://www.affirmations.dev/ { \u0026quot;affirmation\u0026quot;: \u0026quot;Your life is already a miracle of chance waiting for you to shape its destiny\u0026quot; } // 20200927194030 // https://www.affirmations.dev/ { \u0026quot;affirmation\u0026quot;: \u0026quot;It is not a sprint, it is a marathon. One step at a time\u0026quot; } // 20200927194048 // https://www.affirmations.dev/ { \u0026quot;affirmation\u0026quot;: \u0026quot;You are learning valuable lessons from yourself every day\u0026quot; } Complete App import 'dart:convert'; import 'package:flutter/material.dart'; import 'package:http/http.dart' as http; class AffirmationsModel{ final String affirmation; AffirmationsModel({this.affirmation}); factory AffirmationsModel.fromJson(final json){ return AffirmationsModel( affirmation : json[\u0026quot;affirmation\u0026quot;] ); } } Future\u0026lt;AffirmationsModel\u0026gt; fetchAffirmations() async { final response = await http.get('https://www.affirmations.dev/'); if (response.statusCode == 200) { final jsonAffirmations = jsonDecode(response.body); return AffirmationsModel.fromJson(jsonAffirmations); } else { throw Exception('Failed'); } } void main() { runApp(AffirmationsApp()); } class AffirmationsApp extends StatefulWidget { AffirmationsApp({Key key}) : super(key: key); @override _AffirmationsAppState createState() { return _AffirmationsAppState(); } } class _AffirmationsAppState extends State\u0026lt;AffirmationsApp\u0026gt; { Future\u0026lt;AffirmationsModel\u0026gt; _futureAffirmationsModel; @override void initState() { super.initState(); _futureAffirmationsModel = fetchAffirmations(); } @override Widget build(BuildContext context) { return MaterialApp( title: 'Affirmations', theme: ThemeData( primarySwatch: Colors.pink, visualDensity: VisualDensity.adaptivePlatformDensity, ), debugShowCheckedModeBanner: false, home: Scaffold( backgroundColor: Colors.deepPurpleAccent, appBar: AppBar( title: Text('Affirmations'), centerTitle: true, ), body: Column(children: \u0026lt;Widget\u0026gt;[ Container( alignment: Alignment.center, decoration: BoxDecoration( shape: BoxShape.circle, color: Colors.lightBlueAccent, ), // width: 300, height: 300, padding: EdgeInsets.all(20.0), margin: EdgeInsets.all(80.0), child: FutureBuilder\u0026lt;AffirmationsModel\u0026gt;( future: _futureAffirmationsModel, builder: (context, snapshot) { if (snapshot.connectionState == ConnectionState.done \u0026amp;\u0026amp; snapshot.hasData) { return Text(snapshot.data.affirmation, textAlign: TextAlign.center, textScaleFactor: 1.4, style: TextStyle(color: Colors.white)); } else if (snapshot.hasError) { return Text(\u0026quot;${snapshot.error}\u0026quot;); } return CircularProgressIndicator(); }, ), ), Divider(), Container( child: RaisedButton( onPressed: () { setState(() { _futureAffirmationsModel = fetchAffirmations(); }); }, textColor: Colors.white, padding: const EdgeInsets.all(0.0), child: Container( decoration: const BoxDecoration( gradient: LinearGradient( colors: \u0026lt;Color\u0026gt;[ Color(0xEEFF1461), Color(0xDDFF1483), Color(0xFFFF1493), ], ), ), padding: const EdgeInsets.all(30.0), child: const Text('INSPIRE ME', style: TextStyle(fontSize: 15)), ), ), ) ])), ); } } ","date":"2020-07-27T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/affirmations_dev/","section":"posts","tags":["flutter","affirmations","android"],"title":"Affirmations with Flutter"},{"categories":["pet projects","data viz"],"contents":"K-Shaped recovery A K-shaped recovery encapsulates the idea that some industries in the economy will recover (or actually prevail in the context of Nepal), while others will decline or fail to recover nearly as quickly.\nOne example of the former includes the software service sector (outsourcing, e-commerce, delivery, etc.). On the unfortunate side, an example would be the hospitality/tourism sector.\nThe challenge for time series analysts is to figure out ways to fit this into their model depending on the COVID elasticity for their sectors. COVID, a \u0026ldquo;black swan event\u0026rdquo; has screwed up the Business As Usual.\n","date":"2020-07-15T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/k_shaped_recovery/","section":"posts","tags":["data viz"],"title":"K-Shaped recovery"},{"categories":["moonlit","analytics"],"contents":"Synthetic data based on pareto principle Nowadays, the phrase \u0026ldquo;data is the new oil\u0026rdquo; seems a little cliche. However, it is true. The demand for data has grown significantly, as people and businesses have realized the possibilities of leveraging data for informed decision making. Often, it is not possible to get data to our specific needs. As a ML teacher, I have heard my students claim that it is really challenging for them to find data to their specifications for several of the projects that they want to pursue (in this case for learning data science or for school projects). In other cases, where confidentiality is crucial, the possibility of open data is minimal (anonamized data is there but that is another difficult avenue). Synthetic data can help solve these issues. Synthetic data is \u0026ldquo;any production data applicable to a given situation that are not obtained by direct measurement\u0026rdquo; according to the McGraw-Hill Dictionary of Scientific and Technical Terms. However, synthetic does not imply random. The patterns in synthetic data should mimic real world patterns. This is a priliminary exercise.\nCreating a synthetic dataset for users and users transactions that demonstrate the Pareto principle. Compulsory fields include basic demographic information, email address, phone number, item number and price. Imports import pandas as pd import numpy as np, numpy.random import random import matplotlib.pyplot as plt Brute Force Solution Brute Force Assumption: All ticket size the same, Customers only purchase once.\nCreate a naive list for users This will be modified to look real soon. In real world case, this can be thought of user/client table with incremental id.\n# variables final_dataframe_size = 1000 # in this case, these are the 1000 customers who have bought something. # Generate users users = [x for x in range (1000, 10000)] users[:10] [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009] # shuffle the original list random.shuffle(users) users[:10] [8815, 2924, 6563, 3568, 3604, 6821, 1466, 9813, 3153, 6637] Sampling random 1000 users who have transacted. This is because of the assumption that not every user will have transacted. tran_users = random.sample(users, final_dataframe_size) print (len(tran_users)) print (tran_users[:10]) 1000 [6113, 5352, 4711, 2600, 5643, 3009, 7765, 7301, 1399, 2336] pareto_tran_users = random.sample(tran_users, int(.2 * final_dataframe_size)) len(pareto_tran_users) 200 df = pd.DataFrame({\u0026#39;user\u0026#39;: tran_users}) df.head() user 0 6113 1 5352 2 4711 3 2600 4 5643 If the sum of all transactions is 100000,\npareto users = 0.8*100000 = 80000 to be divided into 200 users, each user gets = 400\nnon pareto users = .2*100000 = 20000 to be divided into 800 users, each user gets = 25\ndef assign_amount(val): if val in pareto_tran_users: return 400 return 25 df[\u0026#39;transaction\u0026#39;] = df.user.apply(assign_amount) # validate pareto df.sort_values(\u0026#39;transaction\u0026#39;, ascending=False).head(int(.2 * final_dataframe_size)).sum()/df.sum() user 0.20033 transaction 0.80000 dtype: float64 Here we see 20 percen generating 80 percent of the revenue.\nImprovement 1 Make the sales figure more dynamic (instead of 25 and 400) def sum_to_x(n, x): values = [0.0, x] + list(np.random.uniform(low=0.0,high=x,size=n-1)) values.sort() return [values[i+1] - values[i] for i in range(n)] df2 = df.copy() df2_p = df2[df2.user.isin(pareto_tran_users)] df2_p_n = df2[~df2.user.isin(pareto_tran_users)] # .apply(sum_to_x(20000, 80000)) df2_p[\u0026#39;transaction\u0026#39;] = sum_to_x(200, 80000) df2_p_n[\u0026#39;transaction\u0026#39;] = sum_to_x(800, 20000) df2 = df2_p.append(df2_p_n) df2 user transaction 4 5643 271.252382 16 1403 64.145236 20 2671 150.894671 27 1881 3.295457 29 4574 129.600777 ... ... ... 991 7792 58.155219 992 4682 18.239724 994 5986 4.711870 996 8145 0.914484 998 4102 20.064127 1000 rows × 2 columns\ndf2.sum() user 5553121.0 transaction 100000.0 dtype: float64 # validate pareto df2.sort_values(\u0026#39;transaction\u0026#39;, ascending=False).head(200).sum().apply(lambda x: \u0026#39;%.3f\u0026#39; % x) user 1136745.000 transaction 82287.864 dtype: object Upload some data To make the data more realistic, we can add more features. In this case, mockaroo was used to create the features. df_users = pd.read_csv(\u0026#39;synthetic_users.csv\u0026#39;) df_users.head() id first_name last_name email gender 0 1 Pierette Deners pdeners0@1und1.de Female 1 2 Gage Opdenorth gopdenorth1@hugedomains.com Male 2 3 Raquel Gabriel rgabriel2@studiopress.com Female 3 4 Matti Sullly msullly3@soundcloud.com Female 4 5 Zita Doggart zdoggart4@ftc.gov Female The client table seems complete. Let us create an item table which follows normal distribution. df_items = pd.read_csv(\u0026#39;synthetic_items_catalog.csv\u0026#39;) df_items.drop_duplicates(subset=[\u0026#39;product\u0026#39;], keep=\u0026#39;first\u0026#39;, inplace=True) df_items[\u0026#39;id\u0026#39;] = df_items[\u0026#39;id\u0026#39;] + 1000 df_items id product 0 1001 Bread Ww Cluster 1 1002 Glove - Cutting 2 1003 Ice Cream - Super Sandwich 3 1004 Wine - Chateau Bonnet 4 1005 Ecolab - Ster Bac ... ... ... 992 1993 Table Cloth 53x53 White 993 1994 Durian Fruit 995 1996 Chinese Foods - Plain Fried Rice 996 1997 Wine - White, Ej Gallo 998 1999 Sugar - Fine 836 rows × 2 columns\nprices = np.random.normal(50, 20, 836) df_items[\u0026#39;prices\u0026#39;] = prices df_items[\u0026#39;prices\u0026#39;] = np.where(df_items[\u0026#39;prices\u0026#39;] \u0026lt; 0 , 2.0, df_items[\u0026#39;prices\u0026#39;]) df_items[\u0026#39;prices\u0026#39;] = round(df_items[\u0026#39;prices\u0026#39;]) df_items.describe() id prices count 836.000000 836.000000 mean 1469.366029 50.340909 std 285.036373 20.400653 min 1001.000000 2.000000 25% 1220.500000 36.000000 50% 1460.500000 50.000000 75% 1710.250000 64.000000 max 1999.000000 120.000000 Now we have users and product table. The product table consists of items that are uniformly distributed.\nnp.random.normal(0, 1, 10) array([ 0.7365823 , 0.18626006, -0.10850225, -0.8307533 , 0.18490917, 0.51846682, -1.20012347, 0.31953487, -0.12546108, 0.75146225]) ## Could not figure out how to generate normal distribution frame, ## so opting out for uniform distribution # weights_users = np.random.normal(1000, 1000, len(df_users.id)) weights_items = np.random.normal(1000, 1000, len(df_items.id)) tran_user = random.choices(random.sample(list(df_users.id), 268), k=10000) tran_items = random.choices(list(df_items.id), weights = weights_items, k=10000) df = pd.DataFrame.from_dict({\u0026#39;user_id\u0026#39;: tran_user, \u0026#39;item_id\u0026#39;: tran_items})\\ .set_index(\u0026#39;item_id\u0026#39;)\\ .join(df_items.set_index(\u0026#39;id\u0026#39;)) df_ = df.groupby(\u0026#39;user_id\u0026#39;)[[\u0026#39;prices\u0026#39;]].sum() df_.describe() prices count 268.000000 mean 1889.985075 std 343.707196 min 1161.000000 25% 1649.750000 50% 1896.500000 75% 2114.500000 max 2860.000000 # validate pareto # regardless of transaction table, hamro users 200 jana df_.reset_index(inplace=True) (df_.sort_values(\u0026#39;prices\u0026#39;, ascending=False).head(200).sum())/df_.sum() user_id 0.732714 prices 0.803493 dtype: float64 merged = df.reset_index()\\ .set_index(\u0026#39;user_id\u0026#39;)\\ .join(df_users.set_index(\u0026#39;id\u0026#39;), how=\u0026#39;outer\u0026#39;)\\ .reset_index()\\ .rename(columns={\u0026#39;index\u0026#39;:\u0026#39;product_id\u0026#39;, \u0026#39;level_0\u0026#39;: \u0026#39;user_id\u0026#39;}) merged user_id product_id product prices first_name last_name email gender 0 1 1013.0 Shrimp - Black Tiger 13/15 78.0 Pierette Deners pdeners0@1und1.de Female 1 1 1025.0 Longos - Lasagna Veg 48.0 Pierette Deners pdeners0@1und1.de Female 2 1 1032.0 Wine - Pinot Grigio Collavini 54.0 Pierette Deners pdeners0@1und1.de Female 3 1 1067.0 Shiro Miso 85.0 Pierette Deners pdeners0@1und1.de Female 4 1 1073.0 Pea - Snow 69.0 Pierette Deners pdeners0@1und1.de Female ... ... ... ... ... ... ... ... ... 10727 1000 1866.0 Figs 56.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10728 1000 1902.0 Wine - Harrow Estates, Vidal 62.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10729 1000 1924.0 Pancetta 25.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10730 1000 1933.0 Vodka - Smirnoff 23.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10731 1000 1994.0 Durian Fruit 51.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10732 rows × 8 columns\nmerged.groupby(\u0026#39;user_id\u0026#39;)[[\u0026#39;prices\u0026#39;]].sum().sort_values(\u0026#39;prices\u0026#39;, ascending=False).head(200).sum()/merged[\u0026#39;prices\u0026#39;].sum() prices 0.803493 dtype: float64 merged.to_csv(\u0026#39;merged.csv\u0026#39;, index=False) df_users.to_csv(\u0026#39;users.csv\u0026#39;, index=False) df_items.to_csv(\u0026#39;items.csv\u0026#39;, index=False) merged user_id product_id product prices first_name last_name email gender 0 1 1013.0 Shrimp - Black Tiger 13/15 78.0 Pierette Deners pdeners0@1und1.de Female 1 1 1025.0 Longos - Lasagna Veg 48.0 Pierette Deners pdeners0@1und1.de Female 2 1 1032.0 Wine - Pinot Grigio Collavini 54.0 Pierette Deners pdeners0@1und1.de Female 3 1 1067.0 Shiro Miso 85.0 Pierette Deners pdeners0@1und1.de Female 4 1 1073.0 Pea - Snow 69.0 Pierette Deners pdeners0@1und1.de Female ... ... ... ... ... ... ... ... ... 10727 1000 1866.0 Figs 56.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10728 1000 1902.0 Wine - Harrow Estates, Vidal 62.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10729 1000 1924.0 Pancetta 25.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10730 1000 1933.0 Vodka - Smirnoff 23.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10731 1000 1994.0 Durian Fruit 51.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10732 rows × 8 columns\ndf_users id first_name last_name email gender 0 1 Pierette Deners pdeners0@1und1.de Female 1 2 Gage Opdenorth gopdenorth1@hugedomains.com Male 2 3 Raquel Gabriel rgabriel2@studiopress.com Female 3 4 Matti Sullly msullly3@soundcloud.com Female 4 5 Zita Doggart zdoggart4@ftc.gov Female ... ... ... ... ... ... 995 996 Web Beauly wbeaulyrn@drupal.org Male 996 997 Joachim Silber jsilberro@cam.ac.uk Male 997 998 Kearney Huntly khuntlyrp@hud.gov Male 998 999 Robb Eads readsrq@sfgate.com Male 999 1000 Hillery Dickings hdickingsrr@gizmodo.com Male 1000 rows × 5 columns\ndf_items id product prices 0 1001 Bread Ww Cluster 55.0 1 1002 Glove - Cutting 34.0 2 1003 Ice Cream - Super Sandwich 26.0 3 1004 Wine - Chateau Bonnet 52.0 4 1005 Ecolab - Ster Bac 46.0 ... ... ... ... 992 1993 Table Cloth 53x53 White 3.0 993 1994 Durian Fruit 51.0 995 1996 Chinese Foods - Plain Fried Rice 23.0 996 1997 Wine - White, Ej Gallo 57.0 998 1999 Sugar - Fine 84.0 836 rows × 3 columns\n","date":"2020-07-03T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/synthetic_data_pareto/","section":"posts","tags":["python","pareto","synthetic"],"title":"Synthetic data based on pareto principle"},{"categories":["moonlit","data viz","analytics"],"contents":"1 million seconds equal 11.5 days, whereas 1 billion seconds equal 31.75 years. Very large numbers are baffling to us, humans. From an evolutionary point of view, we never had to deal with anything colossal. With the recent news of Amazon boss Jeff Bezos now being worth about as much as New Zealand’s economy, it is quite interesting to see how he compares to rest of the celebrities we consider rich. It is absolutely shocking that the wealth difference between Elon Musk and YOU is smaller than between Elon Musk and Jeff Bezos.\n","date":"2020-07-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/bezos/","section":"posts","tags":["tableau","data viz"],"title":"You vs Bezos - Weber Fencher law"},{"categories":["cheers hospital","data viz","analytics"],"contents":"Data analysis of Cheers hospital Since it’s inception, Hospital for Children Eye ENT and Rehabilitation Service (CHEERS) has amassed data from various sources such as all the software used in daily operations, research department and the finance department. With priority in budgeting which is always a significant factor, CHEERS wanted Moonlit Solutions to analyze the data from different sectors within the hospital and create forecasting models to help with their operational efficiency.\nScope of work Data curation from different sectors of the hospital (but only limited to their Hospital Management Information System) Data wrangling and principal component analysis Exploratory data analysis Creation and validation of different forecasting models Assist in the creation of the forecasting section in the hospital master plan that is currently being developed. Methodology After the acquisition stage, all analysis presented in notebooks followed the following methodology:\nGaining domain expertise Data wrangling and cleaning (a lot of imputing using the domain knowledge and meeting stakeholders) Exploratory Data Analysis Forecasting with multiple models (AR, MA, ARIMA, SARIMA, Bayesian, Deep Learning)and different KPIs (Daily sales, Daily patient visits) Optimization (Parameter tuning) Outputs Robust forecasting models predicting future sales A report document Dashboard showcasing interactive charts and tables (and all analysis notebooks for research reproducibility) The exploratory data analysis and forecasts generated went through various levels of quality assurance to validate the correctness. Unfortunately, the COVID-19 epidemic and subsequent lockdowns was going to affect the business. The model created wasBAU (Business As Usual). It did not forecast out of the world scenarios like COVID-19 (a Black Swan event). However, it will create a benchmark for measuring the effects of COVID-19 in the future. The forecasting can still be used to understand the impact of COVID-19 on the business. The exploratory data on the other hand, provides significant insights for the hospital\u0026rsquo;s strategy development.\n","date":"2020-03-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/cheers_analysis/","section":"posts","tags":["python","analysis","ML","ARIMA"],"title":"Cheers Hospital Analysis"},{"categories":["world bank","data viz","analytics"],"contents":"Data in News Link to the website\nHow journalists use data in their reporting or in their writing can be viewed as a reflection of the country’s demand for statistics and data. The report team, in partnership with the Society of Economic Journalists-Nepal (SEJON), organized a roundtable with 38 economic journalists, editors, and bureau chiefs in Nepal to discuss data use in news organizations. The team also administered an online questionnaire to 28 media professionals (see appendix A for the questionnaire and appendix B for the results). To complement these findings, the World Bank team developed an indicator model to assess the use of data by Nepali news portals based on the methodology proposed by Klein, Galdin, and Mohamedou 2016. The results show that very few news articles indicate the source of data or mention development indicators. However, articles that discuss data, reports, research, statistics, and related topics do critically engage with these ideas.\nMore than 4,100 articles were analyzed to assess the use of data in media. The data was collected between 9th December 2019 to 9th February 2020. The data source for the model was Nepali news portals that support RSS feeds and are published in English. The news portals that meet these criteria are the Himalayan Times, Onlinekhabar, the Nepali Times, the Telegraph Nepal, the Kathmandu Tribune, and Lokaantar.\nThe results of the analysis are as follows:\nLevel 1 analysis: The team analyzed whether articles indicated the presence of a data source, a statistical/development indicator, or keywords from papers on statistical capacity–building projects. Examples of keywords included household survey, population census, geospatial data, GDP, GNP, pay gap, trade balance, and unemployment rate. The keywords for data sources are comprised of word sequences of length two and scraped from https://klein.uk/literacy.html. The share of news articles that included at least one keyword that indicated “consistent non-critical” use of data averaged 7.6 percent (scores ranged from 2 percent to 12 percent).\nLevel 2 analysis: This level of analysis assessed whether an article critically engaged with topics of data or statistics or made arguments by using mathematical concepts. Examples of keywords included ambiguous, error, bias, fake, impartial, precise, scientific, and unreliable Seventeen percent of the more than 4,100 articles studied directly referenced data, records, research, statistics, or studies. Two-thirds of those articles included keywords.\nLevel 3 analysis: “Critical mathematical” use of data occurs when an article indicates questioning or interpreting of data and statistics. Examples of keywords included data manipulation, lead question, report bias, sample select, and sample size. No article met this criterion.\n","date":"2020-01-03T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/data_in_news/","section":"posts","tags":["analysis","proxy"],"title":"Data in News"},{"categories":["world bank"],"contents":"Nepal Data Literacy Program Nepal Data Literacy Program is designed to catalyze stronger data-driven decision-making by government and non-government actors (mass media, civil society, and academia) through targeted Data Literacy Workshop to help Nepal achieve its development goals.\nThe program comprises a 100 hour modular, customizable pedagogy to support both technical skills-building and efforts to enhance a ‘culture of data use’ among Nepalis.\nFor the event, my responsibilities were to develop course materials for Python and Machine Learning. Additionally, I was also tasked with reviewing course materials for other modules (Tableau, Statistics, Survey Methodologies etc.).\nSimilarly, I also developed the Nepal Data Literacy website that hosts the entire curriculum, instructor\u0026rsquo;s note, student\u0026rsquo;s workbook, analysis notebooks, datasets, participants projects, etc.\nDuring the event, I instructed Python and Machine Learning, including topics of Regression and Classification to industry experts on different fields (Academics, CSO, Media, Entrepreneurs, Managers, NGOs). This was exciting because two of my teachers who taught me in grade 8 were my students in this event. They were proud.\nThe content created for Unit 7 Supplement Materials Student Workbook Instructor Note Downloadable Slide Google Colab Files Python Basics Python Basics (Solutions) Probabilities in Python Probabilities in Python (Solutions) Introduction to Seaborn for visualization Introduction to Pandas Linear Regression Logistic Regression ","date":"2019-12-31T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/nepal_data_literacy/","section":"posts","tags":["data","literacy","teaching materials"],"title":"Nepal Data Literacy Program"},{"categories":["community","world bank"],"contents":"Journey to Spam Detection Spiralogics is hosting a first of it\u0026rsquo;s kind, AI Conference in Nepal this February - a platform for AI veterans and students to come together and network. This year, Spiralogics is focused on exploring and explaining the ways how AI can create positive impact on different aspects of our life. It is a platform which will act as a driving force in encouraging the implementation of AI (especially in the Nepalese environment). Their purpose is to bring together professionals and students under one roof to rediscover the possibilities that AI can create in our day to day lives.\nHow was I involved? I was asked to teach a 5 hour session to introduce the students to a concept of AI. I decided to conduct a workshop based session on Naive Bayes Classifier to detect spam.\nTopics covered Probability Primer: We look into some few probability problems and Monte Carlo Simulations to get a basic primer on probability and Python. Conditional Probability: We look into few examples on conditional probability. Bayes Theorem: We connect Conditional Probability with Bayes Theorem. We move from what we know to what we can infer and connect the dots. Sensitivity, Specificity and Confusion matrix: We look into concepts of Sensitivity, Specificity, TP, TN, FP and FN. We also look into Type I and Type II errors. Accuracy, Precision and Recall: We look into different ways to evaluate our models and realize when one is more important than the other. We also study F1 score which combines Precision and Recall. Naive Bayes: We move from Bayes Theorem to Naive Bayes and define what Naive means. NB Spam detection classifier: We build a binary classifier using Scikit learn and publicly available SMS datasets. We also evaluate our model’s performance. NB vs Bagging, Random Forest and Adaboost: We will not spend much time here. This notebook basically compares Naive bayes with other algorithmns for the same dataset. Link to the github repo.\n","date":"2019-12-20T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/spirathon/","section":"posts","tags":["bayes","python"],"title":"Spirathon AI Conference 2019"},{"categories":["world bank","community"],"contents":"What is a Solve-a-thon? A two-day Solveathon, the first of its kind, was organized by the World Bank and The Asia Foundation with support from UKAID at the Kathmandu University School of Management on 14-15 December. The event brought together data scientists, programmers, developers, researchers, and professionals with diverse backgrounds and provided them a platform to work collaboratively on data-driven projects to tackle developmental challenges.\nDuring the event, the participating teams worked with mentors and respective domain experts to come up with solutions, refine them and devise various prototypes. The participants worked on nine different projects – three culminated from the 100-hour Nepal Data Literacy Program organized by the World Bank, three proposed by the participants, and three selected by the team based on the pertinent developmental issues and availability of data. The Solveathon will award a total amount of 10,000 USD, divided among two to five projects, to further develop their prototypes. In this phase, the awardees will assess and validate the feasibility of proposed prototype– assumptions in the theory of change, analyze user needs, develop sustainability and action plan–, identify potential partners, and analyze whether similar projects are existent or being planned in the market. Implementation and Review Phase: Under this phase, the awardees will implement their idea or prototype, evaluate its success and demonstrate plans for sustainability of impact.\nI was part of the planning and execution committee, and a technical mentor to the participating team. Additionally, once the projects were submitted, I also was part of the team that judged the team based on various criteria such as complexity, sustainability, cost and team members. Three of the projects from the event have started taking good shape at the present.\nI was also responsible for creating the event website which you can find here.\n","date":"2019-12-05T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/solveathon_nepal/","section":"posts","tags":["community","mentorship","organizer"],"title":"Solveathon Nepal 2019"},{"categories":["community","british college"],"contents":"Nepal, students and AI The conference titled “Artificial Intelligence for Transforming Business and Society (AITB2019)”, brought together more than 100 international and national academics, industry experts, researchers, policy makers and students under one roof to share their research findings, experience, and expertise in the field of AI.\nAccording to the AITB2019 Organizing Committee, the topic of Artificial Intelligence(AI) was chosen to discuss the relevance of AI revolution happening all over the world and in the context of Nepal. It was also intended to bring the national and international intellect to share their ideas through their research and experience in the field of AI. The initiation of discussion about the importance for Nepal to keep up with the AI revolution and to make sure that the human resources are prepared for the future skills was the main aim of the conference.\nDiscussion Topics I was one of the panelist and we were asked the following questions:\nIn terms of the impact of AI, how big do you think the impact will be in our lives in the future? You are free to choose your timelines here. How important is it to be updated about AI today? Why does everyone need to know about AI? How can we inspire the young generation to be part of the AI revolution? or is it the other way around? (ie. do we need to inspire the older generation, policymakers to be part of the revolution to make the progress faster?) From the academic perspective, do you see a need to do something radically different on how we impart knowledge now vs when AI will be prevalent? There is a debate about jobs when AI will really come in effect? What is your stand in this debate? will there be more jobs or fewer jobs? Final question, do we need to fear AI? will we be taken over by machines? or will we all be cyborgs that will be completely in sync with the machines? The press release for the program concluded the following regarding our panel discussion.\n“The second panel moderated by Mr. Amod Niroula, Co-founder and Project Manager at ACT360, brought industry practitioners, entrepreneurs and academics together to discuss the need for developing AI strategies for tomorrow. Panelist Dr Bal Krishna Bal, Head of Department, Computer Science at Kathmandu University emphasized the crucial role of academia in providing the needed talent as well as the need for adaptation and updating of curriculum. Mr. Ayush Subedi, Co-founder/CTO of Moonlit Solutions stressed the need to first increase access to disaggregated data to be able to adopt AI. Mr. Ravi Bajracharya, Co- founder of Wiseyak shared his own experiences about adopting AI in the field of healthcare for increased access to rural Nepal. Chief Technology Architect of Makura Creations, Mr. Chandan Gupta shared his views about not taking the AI revolution as a threat to jobs but instead a need to be aware of the changing nature of jobs.”\n","date":"2019-11-05T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/british_ai_conference/","section":"posts","tags":["panels"],"title":"British College AI Conference"},{"categories":["trove"],"contents":"The extension to Trove Trove Summary is a web app (React) that generates summary of a web article (from its URL) and allows users to securely store them. Technically speaking, it follows the same philosophy of Trove (build in Blockstack and Gaia), and therefore inherits, decentralization, anonymity and encryption.\nHowever, the other aspect of Trove Summary is its backend that uses abstractive and extractive summarization models to generate reliable summary.\nExamples\nRequest\n/api/get_content?url=abc.xyz Result\n{ \u0026quot;authors\u0026quot;: [ ], \u0026quot;code_content\u0026quot;: 200, \u0026quot;description\u0026quot;: \u0026quot;Alphabet Inc. is a holding company that gives ambitious projects the resources, freedom, and focus to make their ideas happen — and will be the parent company of Google, Nest, and other ventures. Alphabet supports and develops companies applying technology to the world’s biggest challenges.\u0026quot;, \u0026quot;keywords\u0026quot;: [ ], \u0026quot;movies\u0026quot;: [ ], \u0026quot;publish_date\u0026quot;: null, \u0026quot;summary\u0026quot;: \u0026quot;We’ve long believed that over time companies tend to get comfortable doing the same thing, just making incremental changes.\\nI am really excited to be running Alphabet as CEO with help from my capable partner, Sergey, as President.\\nThis newer Google is a bit slimmed down, with the companies that are pretty far afield of our main internet products contained in Alphabet instead.\\nIn addition, with this new structure we plan to implement segment reporting for our Q4 results, where Google financials will be provided separately than those for the rest of Alphabet businesses as a whole.\\nSundar has been saying the things I would have said (and sometimes better!) for quite some time now, and I’ve been tremendously enjoying our work together.\\nSergey and I are seriously in the business of starting new things.\\nFor Sergey and me this is a very exciting new chapter in the life of Google—the birth of Alphabet.\u0026quot;, \u0026quot;summary_flag\u0026quot;: 1, \u0026quot;text\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Alphabet\u0026quot;, \u0026quot;top_image\u0026quot;: \u0026quot;https://abc.xyz/favicon-194x194.png\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Website\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://abc.xyz/\u0026quot; } Request\n/api/get_content?url=bbc.com/news/world-europe-43841194 Result\n{ \u0026quot;authors\u0026quot;: [ ], \u0026quot;code_content\u0026quot;: 200, \u0026quot;description\u0026quot;: \u0026quot;One of the world's biggest dance music stars dies in Oman, with no cause of death announced.\u0026quot;, \u0026quot;keywords\u0026quot;: [ ], \u0026quot;movies\u0026quot;: [ ], \u0026quot;publish_date\u0026quot;: null, \u0026quot;summary\u0026quot;: \u0026quot;Avicii, top electronic dance music artist, found dead at 28 Published duration 21 April 2018\\nSwedish DJ Avicii, one of the world's biggest dance music stars, has died in Oman at the age of 28.\\nThe electronic dance music (EDM) star, who reportedly made $250,000 (£180,000) a night on tour, had struggled with some health issues in the past, having his gall bladder and appendix removed in 2014\\n\\\u0026quot;I know I am blessed to be able to travel all around the world and perform, but I have too little left for the life of a real person behind the artist,\\\u0026quot; he said at the time.\\nAs well as working with the likes of Aloe Blacc and Rita Ora, Avicii collaborated with artists including Madonna and Coldplay.\\nFormer Radio 1 DJ Judge Jules, who often performed alongside him, said his biggest achievement was being the first electronic dance star to break America.\u0026quot;, \u0026quot;summary_flag\u0026quot;: 1, \u0026quot;text\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Avicii, top electronic dance music artist, found dead at 28\u0026quot;, \u0026quot;top_image\u0026quot;: \u0026quot;https://ichef.bbci.co.uk/news/1024/branded_news/54F2/production/_100964712_mediaitem100964710.jpg\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Article\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://www.bbc.com/news/world-europe-43841194\u0026quot; } Request\n/api/get_content?url=medium.com/@trovenow/the-underlying-architecture-of-trove-cbfa4ba18581 Result\n{ \u0026quot;authors\u0026quot;: [ ], \u0026quot;code_content\u0026quot;: 200, \u0026quot;description\u0026quot;: \u0026quot;After releasing Trove, our inboxes and DMs flooded with questions pertaining to the underlying architecture of Trove, and most importantly, about DAaps (Decentralized Apps) and the Blockstack…\u0026quot;, \u0026quot;keywords\u0026quot;: [ ], \u0026quot;movies\u0026quot;: [ ], \u0026quot;publish_date\u0026quot;: \u0026quot;Sun, 30 Jun 2019 16:21:44 GMT\u0026quot;, \u0026quot;summary\u0026quot;: \u0026quot;After releasing Trove, our inboxes and DMs flooded with questions pertaining to the underlying architecture of Trove, and most importantly, about DAaps (Decentralized Apps) and the Blockstack platform.\\nWe quickly put together a decentralized app that allowed users to save their favorite words.\\nThe primary purpose of Gaia is to store any relevant data for the apps that the user uses in the Blockstack ecosystem.\\nHowever, Blockstack also allows users to choose their own Gaia hub and to configure the back-end provider to store data with.\\nThe data for an app is stored in one or many text format files (JSON) within the app’s hub in Gaia.\\nThere is no possible way for the makers of an app to have all of the data of all of the users in a centralized repository.\\nEvery Trove user has three files associated with their Gaia hub to store bookmark, collections and archived objects.\u0026quot;, \u0026quot;summary_flag\u0026quot;: 1, \u0026quot;text\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;The Underlying Architecture of Trove\u0026quot;, \u0026quot;top_image\u0026quot;: \u0026quot;https://miro.medium.com/max/1200/0*CUrk-zRTYflhc_Ja.jpeg\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Article\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://medium.com/@trovenow/the-underlying-architecture-of-trove-cbfa4ba18581\u0026quot; } ","date":"2019-09-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/trove_summary/","section":"posts","tags":["Blockchain","Blockstack","Gaia"],"title":"Trove Summary"},{"categories":["trove"],"contents":"After releasing Trove, our inboxes and DMs flooded with questions pertaining to the underlying architecture of Trove, and most importantly, about DAaps (Decentralized Apps) and the Blockstack platform. The purpose of this post is to answer those burning questions and to take you on a journey of figuring all of this out ourselves. The engineers @Trove have strong understanding and experience of centralized applications. We followed the decentralized space, and also built small prototypes in Blockstack and Ethereum platforms. But, Trove is our first endeavour to create a full end-to-end Decentralized Application from scratch. We had questions. Actually, a lot of them.\nProof of Concept A proof of concept entails investigating to be convinced of an idea. In our case, the idea was a bookmark manager. However, we also had to deep dive into the Blockstack platform to get a gist of the second part of our idea, i.e., decentralization. We started with a very very simple decentralized app.\nWe quickly put together a decentralized app that allowed users to save their favorite words. Although the app does not make sense from a usability perspective, and might be hilarious to even think about deploying it to production, it helped answer a lot of our questions:\nHow will the users login/signup to use the app? The users will create an ID with Blockstack. The ID will be used to sign in all apps in the Blockstack ecosystem.\nWhere is the data stored? By default, when an ID is created in Blockstack, each user ID is also issued some storage space. This storage system is called Gaia. The primary purpose of Gaia is to store any relevant data for the apps that the user uses in the Blockstack ecosystem. However, Blockstack also allows users to choose their own Gaia hub and to configure the back-end provider to store data with. Learn more here.\nHow is the data stored in Gaia? Gaia is not a DBMS. The data for an app is stored in one or many text format files (JSON) within the app’s hub in Gaia.\nIs it encrypted? This depends on the developers. The developers can choose whether or not a file should be encrypted. For our case, the words in the proof of concept app were encrypted and the bookmarks for Trove are encrypted as well.\nHow is it decentralized? One of the most common questions we have been receiving is regarding decentralization. Several users new to the Blockstack ecosystem have asked, “If all of my bookmarks are in the same place, how is it decentralized?”. Well, there are a few ways of thinking about this. From the perspective of the developers, all of the data pertaining to the app are scattered in several Gaia storage all over. There is no possible way for the makers of an app to have all of the data of all of the users in a centralized repository. This makes it decentralized. Also, no central repository implies no machine learning algorithm tinkering to garner patterns and trends on collective data.\nNow, can we now build a MVP for our purposes? Yes. At this point, we had a better understanding of the approach to make Trove possible and optimized to leverage on Blockstack and Gaia. Moreover, we realized that a bookmark manager would be an ideal exploration for this platform because bookmarks are sacred to a specific user, and they are not normally shared. We might eventually work on the functionality of publicly shared bookmarks in the future if our users request it.\nProduct Specification and Minimum Viable Product The team collectively decided to pursue these functionalities for the first version of the app:\nCRUD bookmarks CRUD buckets/categories for bookmarks CRUD tags Favorite bookmarks Archive bookmarks Archive buckets/categories Extract meta tags of bookmarks Extract HTML body of bookmarks for Read Mode (Parked for future version of the app) Extract keywords from bookmarks (Parked for future version of the app) Filter using buckets/categories, tags, favourites Search Browser extensions Architecture How are the files stored in Gaia for Trove?\nEvery Trove user has three files associated with their Gaia hub to store bookmark, collections and archived objects. This schema is also backward compatible. Backward compatibility is a major issue with Gaia because schema changes in the future to incorporate any other features where we do not have access to user’s Gaia (by definition), is a major constraint and therefore needs precautions. **If we were to add Read Mode in the future (which will have massive content and makes no sense to be stored inside bookmark object anyway), it can be stored on a separate file {bookmark_id}.json and referenced with the id. This implies, in the future, a user will have 3+ files, while everything remains maintainable, compatible and scalable. A randomly generated id in base 36 (0–9, a-z) with a length of 9 is used for each category and each bookmark. The probability of collision for a user limits to 0 (1/36⁹). This decision was made so that ids can be assigned on the go without having to keep track of array sizes for incremental ids. When a bookmark is created, and assigned to a category, the bookmark object stores the id of the category and not the name. This referential association allows for seamless category name edits. For the sake of blog post brevity, we are leaving out a lot of the trivial implementation that is common with centralized architecture design pattern (archive, tags, filter, search etc). Extension\nWe initially started with one click bookmark save on our extension. This implied a seamless experience for the users. However, server-less comes with its limitation. In a centralized system, once the user clicked on the extension, we would have sent the URL to our back-end, scraped meta-tags and stored it in a database record referencing the user. We do not have the liberty here. The URL is sent to our open-sourced back-end for meta tag extraction, but that is all it does. It returns the response back to the extension, which does the rest. This implies a user would have to wait for a few seconds while:\nWe collect meta tags from back-end Collect all bookmarks from Gaia Change 2 to an array Append response from back-end to the array Save array to Gaia as a JSON Therefore, we decided to make it a two-step process. We are still researching on ways to make this part seamless to our users. URL Meta Extractor A simple Flask app using Newspaper package has been used for the purpose of URL extraction. The back-end extractor is open sourced at https://gitlab.com/trovenow/trovenow_url_parser. Use the app and tweet us ****for more clarification. We would love to hear your feedback. Also, please show your support by up-voting us here.\nFinal Version (v1) of Trove\n","date":"2019-06-30T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/trove_architecture/","section":"posts","tags":["Blockchain","Blockstack","Gaia"],"title":"Underlying Architecture of Trove"},{"categories":["moonlit","analytics"],"contents":"Fraud Detection Research Items Data Sets Relevant Papers Available Solutions Machine learning Pre-processing Features analysis Modelling Evaluation Suggested Solution 1. Data Sets The first step is to find fraud data sets for modeling purposes. Unfortunately, fraud data sets are really difficult to find publicly because of the confidential information that they contain. Listed below are some of the data sets found and notes on them.\nReal world data set from Kaggle (ULB)\nhttps://www.kaggle.com/mlg-ulb/creditcardfraud\nThe data set contains labelled credit card transactions labeled as fraudulent or genuine. Unfortunately, the column labels do not make sense because PCA has been applied for dimensional reduction. Therefore, it is very difficult to understand what each of the columns represent. Nonetheless, it is real world data. The data sets contains transactions made by credit cards in September 2013 by European cardholders. This data set presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The data set is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\nThis data set has been analysed and models have been created below in the document, with F1 score of 94%.\nSynthetic data set from Kaggle (NTNU)\nhttps://www.kaggle.com/ntnu-testimon/paysim1\nThe data set contains synthetic (created) transaction data. The advantage of using this data set is that PCA has not been pre-performed, thus allowing extraction of all useful information. However, the data set is scaled down to 1/4th of the original data set.\n2. Papers: Link Title Summary https://www.aaai.org/Papers/KDD/1998/KDD98-026.pdf Toward Scalable Learning with Non-uniform Class and Cost Distributions: A Case Study in Credit Card Fraud Detection Handling skewed datasets, compares credit card fraud detection models, and evaluate how the different sets of features have an impact on the results with the help of a real credit card fraud dataset provided by a large European card processing company (the dataset above). The results show an average increase in savings of 13% by including the proposed periodic features into the methods. Using 50-50 split in fraud, non-fraud leads to better models.\nvon Mises distribution: https://en.wikipedia.org/wiki/Von_Mises_distribution https://www.aaai.org/Papers/Workshops/1997/WS-97-07/WS97-07-015.pdf Credit Card Fraud Detection using Meta-Learning: Issues and Initials Results Apart from the finding like above (using balanced training), the paper talks about using metrics other than accuracy for model evaluation. http://journal.utem.edu.my/index.php/jtec/article/view/3571/2466 Credit Card Fraud Detection Using Machine Learning As Data Mining Technique 95% accuracy based on Naive based derivatives. https://www.jair.org/index.php/jair/article/view/10302/24590 SMOTE: Synthetic Minority Over-sampling Technique Using SMOTE method as described in the paper is another alternative of getting around the skewness problem. 3. Research on available solutions: Airbnb https://medium.com/airbnb-engineering/architecting-a-machine-learning-system-for-risk-941abbba5a60\nWays to mitigate potential bad actors to carry out different types of attacks:\nProduct changes: 2FA, email verification, etc etc Anomaly detection: Scripted attacks that can cause anomaly heuristics/machine learning model based on different factors Framework\nFast and robust Agile (catch up game) PMML: Predictive model markup language Openscoring: encodes several common types of machine learning models\nThey do not provide fraud detection as a service.\nPaypal\nhttps://venturebeat.com/2018/06/21/paypal-to-acquire-machine-learning-powered-fraud-detection-startup-simility/\nPaypal recently acquired Simility for fraud detection.\nSimility looks at various session, device, and behavioral bio-metrics and builds a profile for what constitutes “normal” user login behavior; if an anomaly is spotted, it can act to prevent the action.\nhttps://www.dropbox.com/s/ft3wu5ix15xukhc/Mobile%20Fintech%20Fraud.pdf?dl=0\nStripe\nhttps://stripe.com/us/radar\nEven if a card is new to your business, there’s an 89% chance it’s been seen before on the Stripe network.\n4. Machine Learning Supervised learning was applied to the PCA data set discussed in the data sets section. Different ensemble machine learning algorithms were tested, rather than using one particular algorithm for modelling. Metrics like Precision, Recall, F1 score were used to evaluate the model and get a better understanding of True Positives, True Negatives, False Positive and False Negatives. Link to complete notebook\nResults:\nRandom Forest\nAccuracy score for Random Forest : 0.9538461538461539 Precision score Random Forest : 0.98 Recall score Random Forest : 0.9245283018867925 F1 score Random Forest : 0.9514563106796116 Bagging\nAccuracy score for Bagging : 0.963076923076923 Precision score Bagging : 0.9867549668874173 Recall score Bagging : 0.9371069182389937 F1 score Bagging : 0.9612903225806452 AdaBoost\nAccuracy score for Ada Boost Classifier : 0.9446153846153846 Precision score Ada Boost Classifier : 0.9795918367346939 Recall score Ada Boost Classifier : 0.9056603773584906 F1 score Ada Boost Classifier : 0.9411764705882353 False Positives vs False Negatives\n","date":"2019-01-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/fraud_detection/","section":"posts","tags":["ml","python"],"title":"Fraud Detection"},{"categories":["proof of concept"],"contents":"Torpe Blockchain (तोर्पे ब्लोक्कचैन) A minimal blockchain data structure to understand the blockchain basics like hashing, nonce, states, genesis blocks etc. Hash Function A hash function is any function that can be used to map data of arbitrary size to data of a fixed size. The values returned by a hash function are called hash values, hash codes, digests, or simply hashes.\nimport hashlib def hash_sha256(raw): raw = str(raw).encode(\u0026#39;utf-8\u0026#39;) return hashlib.sha256(raw).hexdigest() hash_sha256(\u0026#39;torpe_blockchain\u0026#39;) 'e5367197a1f12480ec761306f2fa9d15b494d5a80e5a806713df9e60943f4faf' hash_sha256(\u0026#39;torpe_blockchain\u0026#39;) 'e5367197a1f12480ec761306f2fa9d15b494d5a80e5a806713df9e60943f4faf' hash_sha256(\u0026#39;torpe_blockchaiN\u0026#39;) '71fe90af906a9afa507ee54287595df6d7267df05428b3f91429371ebd27cb3e' Hashes for the same text are the same as seen above. Even when one character of the string is changed, the hash generated as a result seems to be completely random.\nSHA-256 collisions have not been found yet.\nNonce Number that can only be used once\nAn arbitrary numer used in cryptography to ensure uniqueness and prevent the rerunning of transactions (known as replay attack).\ndef hash_sha256_nonce(raw): raw_bytes = str(raw).encode(\u0026#39;utf-8\u0026#39;) hashed = hashlib.sha256(raw_bytes).hexdigest() nonce = 0 while (hashed[:5] != \u0026#39;00000\u0026#39;): nonce = nonce+1 raw.update({\u0026#39;nonce\u0026#39;: nonce}) raw_bytes = str(raw).encode(\u0026#39;utf-8\u0026#39;) hashed = hashlib.sha256(raw_bytes).hexdigest() return raw, hashed print (hash_sha256_nonce({\u0026#39;hello\u0026#39;: \u0026#39;proof_of_work\u0026#39;, \u0026#39;nonce\u0026#39;: 0})) ({'hello': 'proof_of_work', 'nonce': 623228}, '00000a9d45728c6f4d1eff383dab4d96b753495c8b312ecb5d1858116885ee55') Proof of work The proof of work for this case will be to generate hashes with five leading zeros (by incrementing the nonce). This is the \u0026ldquo;mining\u0026rdquo; part.\nBlock Blocks hold batches of valid transactions that are hashed and encoded into a Merkle tree Each block includes the cryptographic hash of the prior block in the blockchain, linking the two. The linked blocks form a chain. This iterative process confirms the integrity of the previous block, all the way back to the original genesis block.\nGenesis Block A genesis block or block0 is the first block of a block chain. The genesis block is almost always hardcoded into the software of the applications that utilize its block chain. It is a special case in that it does not reference a previous block\nimport datetime # Lets assume 5 person were given 100 coins each state = { \u0026#39;Person_1\u0026#39;: 100, \u0026#39;Person_2\u0026#39;: 100, \u0026#39;Person_3\u0026#39;: 100, \u0026#39;Person_4\u0026#39;: 100, \u0026#39;Person_5\u0026#39;: 100 } block0_data = { \u0026#39;timestamp\u0026#39;: datetime.datetime.now(), \u0026#39;index\u0026#39;: 0, \u0026#39;previous\u0026#39;: None, \u0026#39;transactions\u0026#39;: [state], \u0026#39;nonce\u0026#39;: 0 } raw, hashed = hash_sha256_nonce(block0_data) block0 = { \u0026#39;hash\u0026#39;: hashed, \u0026#39;data\u0026#39;: raw, } block0 {'hash': '0000044b11859efa71c555a87a68090f1f602cf8bcd35bb5446c3c5532f5ad5e', 'data': {'timestamp': datetime.datetime(2020, 9, 9, 7, 37, 44, 877080), 'index': 0, 'previous': None, 'transactions': [{'Person_1': 100, 'Person_2': 100, 'Person_3': 100, 'Person_4': 100, 'Person_5': 100}], 'nonce': 2700821}} This is the genesis block or block 0 here.\nTransactions Lets create some random transactions. The transactions for the demo purpose follow +x, -x semantic. See the examples below.\nimport random def random_transaction(state): temp_list = list(state.keys()) random.shuffle(temp_list) # randomly select two persons first_person = temp_list.pop() second_person = temp_list.pop() receive = random.randint(1, 10) give = -receive return { first_person:receive, second_person:give } test_transactions = [random_transaction(state) for x in range(5)] test_transactions [{'Person_3': 5, 'Person_5': -5}, {'Person_3': 7, 'Person_5': -7}, {'Person_4': 1, 'Person_1': -1}, {'Person_2': 4, 'Person_1': -4}, {'Person_4': 4, 'Person_5': -4}] Updating State def update_state(transaction, state): state = state.copy() for key in transaction: state[key] = state.get(key, 0) + transaction[key] return state for transaction in test_transactions: state = update_state(transaction, state) state {'Person_1': 95, 'Person_2': 104, 'Person_3': 112, 'Person_4': 105, 'Person_5': 84} Valid Transactions def check_transaction_validity(transaction, state): # check neg vs pos if sum(transaction.values()) is not 0: return False # check if amount in wallet to give for key in transaction.keys(): if state.get(key, 0) + transaction[key] \u0026lt; 0: return False return True for transaction in test_transactions: print (check_transaction_validity(transaction, state)) True True True True True # No balance print (check_transaction_validity({\u0026#39;A\u0026#39;: 5, \u0026#39;B\u0026#39;: -5}, {\u0026#39;A\u0026#39;: 0, \u0026#39;B\u0026#39;: 0})) False # Bad transaction print (check_transaction_validity({\u0026#39;A\u0026#39;: 5, \u0026#39;B\u0026#39;: 5}, {\u0026#39;A\u0026#39;: 50, \u0026#39;B\u0026#39;: 50})) False Initial State # Let us reset # Lets assume 5 person were given 100 coins each state = { \u0026#39;Person_1\u0026#39;: 100, \u0026#39;Person_2\u0026#39;: 100, \u0026#39;Person_3\u0026#39;: 100, \u0026#39;Person_4\u0026#39;: 100, \u0026#39;Person_5\u0026#39;: 100 } blockchain = [] # Adding the genesis block blockchain.append(block0) blockchain [{'hash': '0000044b11859efa71c555a87a68090f1f602cf8bcd35bb5446c3c5532f5ad5e', 'data': {'timestamp': datetime.datetime(2020, 9, 9, 7, 37, 44, 877080), 'index': 0, 'previous': None, 'transactions': [{'Person_1': 100, 'Person_2': 100, 'Person_3': 100, 'Person_4': 100, 'Person_5': 100}], 'nonce': 2700821}}] Non-genesis block / New block def new_block(transactions, blockchain): previous_block = blockchain[-1] data = { \u0026#39;timestamp\u0026#39;: datetime.datetime.now(), \u0026#39;index\u0026#39;: previous_block[\u0026#39;data\u0026#39;][\u0026#39;index\u0026#39;] + 1, \u0026#39;previous\u0026#39;: previous_block[\u0026#39;hash\u0026#39;], \u0026#39;transactions\u0026#39;: transactions, \u0026#39;nonce\u0026#39;: 0 } raw, hashed = hash_sha256_nonce(data) block = {\u0026#39;hash\u0026#39;: hashed, \u0026#39;data\u0026#39;: raw} return block sample_transactions = [random_transaction(state) for x in range(50)] sample_transactions [{'Person_5': 5, 'Person_1': -5}, {'Person_3': 10, 'Person_1': -10}, {'Person_2': 5, 'Person_4': -5}, {'Person_5': 9, 'Person_3': -9}, {'Person_3': 1, 'Person_2': -1}, {'Person_1': 9, 'Person_3': -9}, {'Person_1': 7, 'Person_3': -7}, {'Person_5': 4, 'Person_3': -4}, {'Person_5': 2, 'Person_4': -2}, {'Person_2': 4, 'Person_3': -4}, {'Person_3': 5, 'Person_5': -5}, {'Person_5': 1, 'Person_1': -1}, {'Person_1': 1, 'Person_2': -1}, {'Person_2': 7, 'Person_1': -7}, {'Person_2': 7, 'Person_5': -7}, {'Person_3': 2, 'Person_1': -2}, {'Person_3': 3, 'Person_1': -3}, {'Person_3': 3, 'Person_2': -3}, {'Person_3': 6, 'Person_1': -6}, {'Person_1': 5, 'Person_3': -5}, {'Person_2': 4, 'Person_3': -4}, {'Person_2': 1, 'Person_5': -1}, {'Person_1': 3, 'Person_2': -3}, {'Person_1': 10, 'Person_2': -10}, {'Person_3': 9, 'Person_5': -9}, {'Person_1': 3, 'Person_4': -3}, {'Person_4': 2, 'Person_3': -2}, {'Person_5': 6, 'Person_3': -6}, {'Person_2': 9, 'Person_1': -9}, {'Person_3': 3, 'Person_4': -3}, {'Person_3': 10, 'Person_4': -10}, {'Person_1': 9, 'Person_4': -9}, {'Person_2': 3, 'Person_1': -3}, {'Person_2': 6, 'Person_3': -6}, {'Person_4': 4, 'Person_1': -4}, {'Person_3': 7, 'Person_1': -7}, {'Person_3': 7, 'Person_1': -7}, {'Person_3': 5, 'Person_2': -5}, {'Person_3': 10, 'Person_2': -10}, {'Person_2': 1, 'Person_1': -1}, {'Person_1': 3, 'Person_5': -3}, {'Person_4': 4, 'Person_5': -4}, {'Person_1': 3, 'Person_2': -3}, {'Person_4': 1, 'Person_1': -1}, {'Person_5': 1, 'Person_4': -1}, {'Person_3': 5, 'Person_2': -5}, {'Person_1': 8, 'Person_4': -8}, {'Person_3': 8, 'Person_4': -8}, {'Person_3': 7, 'Person_4': -7}, {'Person_2': 1, 'Person_1': -1}] Transactions per block Bitcoin blocks used to contain fewer than 200 transactions and the largest number of transactions in a block was 1,976 at the time this answer was originally written (May 2013). In meanwhile (November 2017) the average number of transaction per block is well above 1500 with peaks above 2200.\n# Assume block size is 5 transactions_per_block = 5 transaction_block = [] for transaction in sample_transactions: if check_transaction_validity(transaction, state): state = update_state(transaction, state) transaction_block.append(transaction) if len(transaction_block) \u0026gt;= transactions_per_block: blockchain.append(new_block(transaction_block, blockchain)) transaction_block = [] import pprint pp = pprint.PrettyPrinter() for block in blockchain: pp.pprint(block) print(\u0026#39;\\n************************************************************************************\\n\u0026#39;) {'data': {'index': 0, 'nonce': 2700821, 'previous': None, 'timestamp': datetime.datetime(2020, 9, 9, 7, 37, 44, 877080), 'transactions': [{'Person_1': 100, 'Person_2': 100, 'Person_3': 100, 'Person_4': 100, 'Person_5': 100}]}, 'hash': '0000044b11859efa71c555a87a68090f1f602cf8bcd35bb5446c3c5532f5ad5e'} ************************************************************************************ {'data': {'index': 1, 'nonce': 2395688, 'previous': '0000044b11859efa71c555a87a68090f1f602cf8bcd35bb5446c3c5532f5ad5e', 'timestamp': datetime.datetime(2020, 9, 9, 7, 37, 58, 781195), 'transactions': [{'Person_1': -5, 'Person_5': 5}, {'Person_1': -10, 'Person_3': 10}, {'Person_2': 5, 'Person_4': -5}, {'Person_3': -9, 'Person_5': 9}, {'Person_2': -1, 'Person_3': 1}]}, 'hash': '00000303c468fe76fe73dfc089b856af02eda615c296ddde95c0c60999561048'} ************************************************************************************ {'data': {'index': 2, 'nonce': 2475862, 'previous': '00000303c468fe76fe73dfc089b856af02eda615c296ddde95c0c60999561048', 'timestamp': datetime.datetime(2020, 9, 9, 7, 38, 16, 454296), 'transactions': [{'Person_1': 9, 'Person_3': -9}, {'Person_1': 7, 'Person_3': -7}, {'Person_3': -4, 'Person_5': 4}, {'Person_4': -2, 'Person_5': 2}, {'Person_2': 4, 'Person_3': -4}]}, 'hash': '00000804a078686673c26bd3d391649da822c27a9eb87c2a86f07be5be7667c0'} ************************************************************************************ {'data': {'index': 3, 'nonce': 843595, 'previous': '00000804a078686673c26bd3d391649da822c27a9eb87c2a86f07be5be7667c0', 'timestamp': datetime.datetime(2020, 9, 9, 7, 38, 34, 360082), 'transactions': [{'Person_3': 5, 'Person_5': -5}, {'Person_1': -1, 'Person_5': 1}, {'Person_1': 1, 'Person_2': -1}, {'Person_1': -7, 'Person_2': 7}, {'Person_2': 7, 'Person_5': -7}]}, 'hash': '0000004fdf3bc704c17b3f38c8bcb0306443e24db0c5971c7494cea67e618fcd'} ************************************************************************************ {'data': {'index': 4, 'nonce': 456491, 'previous': '0000004fdf3bc704c17b3f38c8bcb0306443e24db0c5971c7494cea67e618fcd', 'timestamp': datetime.datetime(2020, 9, 9, 7, 38, 40, 443823), 'transactions': [{'Person_1': -2, 'Person_3': 2}, {'Person_1': -3, 'Person_3': 3}, {'Person_2': -3, 'Person_3': 3}, {'Person_1': -6, 'Person_3': 6}, {'Person_1': 5, 'Person_3': -5}]}, 'hash': '000001ea0d5ee3360f087e8fe25e643a8e749af6b42bf8c3045303a3e4dc80a5'} ************************************************************************************ {'data': {'index': 5, 'nonce': 1793595, 'previous': '000001ea0d5ee3360f087e8fe25e643a8e749af6b42bf8c3045303a3e4dc80a5', 'timestamp': datetime.datetime(2020, 9, 9, 7, 38, 43, 617133), 'transactions': [{'Person_2': 4, 'Person_3': -4}, {'Person_2': 1, 'Person_5': -1}, {'Person_1': 3, 'Person_2': -3}, {'Person_1': 10, 'Person_2': -10}, {'Person_3': 9, 'Person_5': -9}]}, 'hash': '00000101ed3a68b2340f553e1a373e7a590823fb34ee1e2f3c2e4ed44b647c2a'} ************************************************************************************ {'data': {'index': 6, 'nonce': 618433, 'previous': '00000101ed3a68b2340f553e1a373e7a590823fb34ee1e2f3c2e4ed44b647c2a', 'timestamp': datetime.datetime(2020, 9, 9, 7, 38, 56, 256050), 'transactions': [{'Person_1': 3, 'Person_4': -3}, {'Person_3': -2, 'Person_4': 2}, {'Person_3': -6, 'Person_5': 6}, {'Person_1': -9, 'Person_2': 9}, {'Person_3': 3, 'Person_4': -3}]}, 'hash': '00000b2395e1efb034610e89196aafe9407417bbac1cd60f300adccea3cd880d'} ************************************************************************************ {'data': {'index': 7, 'nonce': 4087255, 'previous': '00000b2395e1efb034610e89196aafe9407417bbac1cd60f300adccea3cd880d', 'timestamp': datetime.datetime(2020, 9, 9, 7, 39, 0, 705135), 'transactions': [{'Person_3': 10, 'Person_4': -10}, {'Person_1': 9, 'Person_4': -9}, {'Person_1': -3, 'Person_2': 3}, {'Person_2': 6, 'Person_3': -6}, {'Person_1': -4, 'Person_4': 4}]}, 'hash': '00000ef1b9a090a4b6636f86fa3d264dbcab13f5cf87d994637e656b6ec4abad'} ************************************************************************************ {'data': {'index': 8, 'nonce': 991443, 'previous': '00000ef1b9a090a4b6636f86fa3d264dbcab13f5cf87d994637e656b6ec4abad', 'timestamp': datetime.datetime(2020, 9, 9, 7, 39, 29, 983211), 'transactions': [{'Person_1': -7, 'Person_3': 7}, {'Person_1': -7, 'Person_3': 7}, {'Person_2': -5, 'Person_3': 5}, {'Person_2': -10, 'Person_3': 10}, {'Person_1': -1, 'Person_2': 1}]}, 'hash': '0000021d8da5dab8b274a0146db6ddd6b1c12f34d751f84e9755fae1ed6a42b3'} ************************************************************************************ {'data': {'index': 9, 'nonce': 24885, 'previous': '0000021d8da5dab8b274a0146db6ddd6b1c12f34d751f84e9755fae1ed6a42b3', 'timestamp': datetime.datetime(2020, 9, 9, 7, 39, 37, 55377), 'transactions': [{'Person_1': 3, 'Person_5': -3}, {'Person_4': 4, 'Person_5': -4}, {'Person_1': 3, 'Person_2': -3}, {'Person_1': -1, 'Person_4': 1}, {'Person_4': -1, 'Person_5': 1}]}, 'hash': '000002090602b338740f28ac66dfdc2f9949c8c40303574e0be08ad7033550bb'} ************************************************************************************ {'data': {'index': 10, 'nonce': 545615, 'previous': '000002090602b338740f28ac66dfdc2f9949c8c40303574e0be08ad7033550bb', 'timestamp': datetime.datetime(2020, 9, 9, 7, 39, 37, 232552), 'transactions': [{'Person_2': -5, 'Person_3': 5}, {'Person_1': 8, 'Person_4': -8}, {'Person_3': 8, 'Person_4': -8}, {'Person_3': 7, 'Person_4': -7}, {'Person_1': -1, 'Person_2': 1}]}, 'hash': '00000d59159d699830d454d68d2077ed40da8ae060ec67c19be0814d35a61e6f'} ************************************************************************************ The current state Syncing for the first time def validate_block(block, parent, state): error_msg = \u0026#39;Error in %d\u0026#39; % block[\u0026#39;data\u0026#39;][\u0026#39;index\u0026#39;] # check block hash assert block[\u0026#39;hash\u0026#39;] == hash_sha256(block[\u0026#39;data\u0026#39;]), error_msg # check block indices assert block[\u0026#39;data\u0026#39;][\u0026#39;index\u0026#39;] == parent[\u0026#39;data\u0026#39;][\u0026#39;index\u0026#39;] + 1, error_msg # check previous hash assert block[\u0026#39;data\u0026#39;][\u0026#39;previous\u0026#39;] == parent[\u0026#39;hash\u0026#39;], error_msg # validate all transactions for transaction in block[\u0026#39;data\u0026#39;][\u0026#39;transactions\u0026#39;]: assert check_transaction_validity(transaction, state), error_msg state = update_state(transaction, state) return state def check_chain(blockchain): state = {} for transaction in blockchain[0][\u0026#39;data\u0026#39;][\u0026#39;transactions\u0026#39;]: state = update_state(transaction, state) parent = blockchain[0] for block in blockchain[1:]: state = validate_block(block, parent, state) parent = block return state check_chain(blockchain) {'Person_1': 94, 'Person_2': 107, 'Person_3': 145, 'Person_4': 55, 'Person_5': 99} ","date":"2018-10-28T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/torpe/","section":"posts","tags":["blockchain","python"],"title":"Torpe Blockchain"},{"categories":["moonlit"],"contents":"Voronoi analysis and Sentiment Analysis in Business Feasibility study Apart from the conventional research methodology (convenience sampling, likert questionnaire, interview questions, swot analysis for competitions etc.) we tested two unconventional paradigms when a client approached us for feasibility of a business in Kathmandu.\nFirst of all, we looked into finding an ideal place for the business using voronoi diagrams. This was useful to avoid areas where competitions were prominent, and also to tap into neighbourhoods where demands were not met. Of course, factors such as population density, and similar business tending to sprout in close proximity needed to be considered, but voronoi diagrams was only used as a small piece in a very large puzzle.\nSecondly, we tried to understand people\u0026rsquo;s sentiment on the value the business was trying to sell, and also on current value providers. For this we used Twitter API to collect business relevant tweets from Kathmandu.\nApart from an aggregated view of the sentiments, we were able to gain insights on what was and was not working with services provided by current market players. This would allow our client to position their services by converting the weakness of their competition as their strength.\n","date":"2018-10-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/feasibility_study_vs/","section":"posts","tags":["data viz","voronoi","sentiment-analysis"],"title":"Feasibility study with sentiment analysis and voronoi"},{"categories":["pet projects","data viz","analytics"],"contents":"Concept Note on Inclusive Nepal (WIP) and Analysis of usage of Alt text in Nepali newspapers This is a work in progress\nThis notebook compiles statistics, resources and insights on:\n1. Status of Disability in Nepal 2. Nepali Mobile Apps and Web App penetration 3. Most used Nepali Mobile Apps 3. Most used Nepali Mobile Apps accessibility status 4. Comparitive analysis with other countries 5. Potential Solutions for few apps 6. Prototype and Tools suggestions for Accessibility 7. Published work regarding similar research Imports # Pandas import pandas as pd #Mapbox from mapboxgl.utils import * from mapboxgl.viz import * Status of Disability in Nepal df = pd.read_csv(\u0026#34;data_sources/disability_2011.csv\u0026#34;) df.head() District Male Female Population with Disability Percentage of PWDs 0 Kathmandu 9144 7978 17122 0.98 1 Morang 9494 7559 17053 1.77 2 Kailali 8143 7235 15378 1.98 3 Jhapa 8466 6910 15376 1.89 4 Chitwan 6973 5964 12937 2.23 df.head() District Male Female Population with Disability Percentage of PWDs 0 Kathmandu 9144 7978 17122 0.98 1 Morang 9494 7559 17053 1.77 2 Kailali 8143 7235 15378 1.98 3 Jhapa 8466 6910 15376 1.89 4 Chitwan 6973 5964 12937 2.23 df_geojson = pd.read_json(\u0026#39;data_sources/map.geojson\u0026#39;) df_geojson.head() type features 0 FeatureCollection {'properties': {'name': 'Humla', 'death': 1}, ... 1 FeatureCollection {'properties': {'name': 'Darchula', 'death': 2... 2 FeatureCollection {'properties': {'name': 'Bajhang', 'death': 3}... 3 FeatureCollection {'properties': {'name': 'Mugu'}, 'geometry': {... 4 FeatureCollection {'properties': {'name': 'Bajura'}, 'geometry':... df.set_index(\u0026#34;District\u0026#34;, inplace=True) def add_data(features): name = features.get(\u0026#39;properties\u0026#39;).get(\u0026#39;name\u0026#39;) if (name in df.index): percentage = df.loc[name][\u0026#39;Percentage of PWDs\u0026#39;] population = df.loc[name][\u0026#39;Population with Disability\u0026#39;] features[\u0026#39;properties\u0026#39;] = {\u0026#39;name\u0026#39;: name, \u0026#39;percentage\u0026#39;: percentage, \u0026#39;population\u0026#39;:population} return features df_geojson.features.apply(add_data) 0 {'properties': {'name': 'Humla', 'percentage':... 1 {'properties': {'name': 'Darchula', 'percentag... 2 {'properties': {'name': 'Bajhang', 'percentage... 3 {'properties': {'name': 'Mugu', 'percentage': ... 4 {'properties': {'name': 'Bajura', 'percentage'... ... 70 {'properties': {'name': 'Siraha', 'percentage'... 71 {'properties': {'name': 'Saptari', 'percentage... 72 {'properties': {'name': 'Morang', 'percentage'... 73 {'properties': {'name': 'Sunsari', 'percentage... 74 {'properties': {'name': 'Jhapa', 'percentage':... Name: features, Length: 75, dtype: object features = df_geojson[\u0026#39;features\u0026#39;].values.tolist() my_dict = {\u0026#34;type\u0026#34;:\u0026#34;FeatureCollection\u0026#34;, \u0026#34;features\u0026#34;:features} token = \u0026#34;pk................................................................\u0026#34; viz = ChoroplethViz(my_dict, access_token=token, color_property=\u0026#39;population\u0026#39;, color_stops=create_color_stops([0, 2500, 5000, 7500, 10000, 12500], colors=\u0026#39;YlOrRd\u0026#39;), color_function_type=\u0026#39;interpolate\u0026#39;, line_stroke=\u0026#39;--\u0026#39;, line_color=\u0026#39;rgb(128,0,38)\u0026#39;, line_width=1, opacity=0.8, center=(84, 28.5), zoom=6 ) viz.show() Percentage and Population of disabled in each district of Nepal viz = ChoroplethViz(my_dict, access_token=token, color_property=\u0026#39;percentage\u0026#39;, color_stops=create_color_stops([0, 1, 2, 3, 4], colors=\u0026#39;YlOrRd\u0026#39;), color_function_type=\u0026#39;interpolate\u0026#39;, line_stroke=\u0026#39;--\u0026#39;, line_color=\u0026#39;rgb(128,0,38)\u0026#39;, line_width=1, opacity=0.8, center=(84, 28.5), zoom=6 ) viz.show() Districts with most number of Disability (Sorted 5) df.sort_values([\u0026#39;Population with Disability\u0026#39;], ascending=False).head() Male Female Population with Disability Percentage of PWDs District Kathmandu 9144 7978 17122 0.98 Morang 9494 7559 17053 1.77 Kailali 8143 7235 15378 1.98 Jhapa 8466 6910 15376 1.89 Chitwan 6973 5964 12937 2.23 import pandas as pd from bs4 import BeautifulSoup from urllib.request import Request, urlopen import seaborn as sns site = \u0026#34;https://www.onlinekhabar.com/\u0026#34; hdr = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0\u0026#39;} req = Request(site,headers=hdr) page = urlopen(req) soup = BeautifulSoup(page, \u0026#34;lxml\u0026#34;) image_count = 0 alt_count = 0 collection = soup.findAll(\u0026#34;img\u0026#34;) for img in collection: image_count = image_count + 1 if \u0026#39;alt\u0026#39; in img.attrs: alt_count = alt_count + 1 print (\u0026#34;image_count\u0026#34;, image_count) print (\u0026#34;alt_count\u0026#34;, alt_count) print (\u0026#34;alt_count_percent\u0026#34;, alt_count/image_count*100) image_count 154 alt_count 1 alt_count_percent 0.6493506493506493 df = pd.read_csv(\u0026#39;data_sources/news_portals.csv\u0026#39;) df Portal Link 0 Online Khabar https://www.onlinekhabar.com/ 1 eKantipur http://www.ekantipur.com/ 2 Setopati https://www.setopati.com/ 3 The Himalayan Times https://thehimalayantimes.com/ 4 My Republica https://myrepublica.nagariknetwork.com/ 5 Nepal News https://www.nepalnews.com/ 6 Gorkhapatra http://www.gorkhapatraonline.com/ 7 Nepali Times https://www.nepalitimes.com/ def alt_counter(site): try: hdr = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0\u0026#39;} req = Request(site, headers=hdr) page = urlopen(req) soup = BeautifulSoup(page, \u0026#34;lxml\u0026#34;) image_count = 0 alt_count = 0 collection = soup.findAll(\u0026#34;img\u0026#34;) for img in collection: image_count = image_count + 1 if \u0026#39;alt\u0026#39; in img.attrs: alt_count = alt_count + 1 return (alt_count, image_count) except: return (None, None) df[\u0026#39;alt_count\u0026#39;],df[\u0026#39;image_count\u0026#39;]=zip(*df.Link.apply(alt_counter)) df Portal Link alt_count image_count 0 Online Khabar https://www.onlinekhabar.com/ 1 154 1 eKantipur http://www.ekantipur.com/ 129 147 2 Setopati https://www.setopati.com/ 121 122 3 The Himalayan Times https://thehimalayantimes.com/ 137 138 4 My Republica https://myrepublica.nagariknetwork.com/ 94 95 5 Nepal News https://www.nepalnews.com/ 198 212 6 Gorkhapatra http://www.gorkhapatraonline.com/ None None 7 Nepali Times https://www.nepalitimes.com/ 71 74 df[\u0026#39;percent\u0026#39;] = 100*df[\u0026#39;alt_count\u0026#39;]/df[\u0026#39;image_count\u0026#39;] df Portal Link alt_count image_count percent 0 Online Khabar https://www.onlinekhabar.com/ 1 154 0.649351 1 eKantipur http://www.ekantipur.com/ 129 147 87.7551 2 Setopati https://www.setopati.com/ 121 122 99.1803 3 The Himalayan Times https://thehimalayantimes.com/ 137 138 99.2754 4 My Republica https://myrepublica.nagariknetwork.com/ 94 95 98.9474 5 Nepal News https://www.nepalnews.com/ 198 212 93.3962 6 Gorkhapatra http://www.gorkhapatraonline.com/ None None NaN 7 Nepali Times https://www.nepalitimes.com/ 71 74 95.9459 sns.barplot(y=\u0026#39;Portal\u0026#39;, x=\u0026#39;percent\u0026#39;, data=df) ","date":"2018-08-08T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/inclusive_newspaper/","section":"posts","tags":["python","viz","maps"],"title":"Inclusive Nepal"},{"categories":["pet projects"],"contents":" Finding answers after Tootle In 2017, I went to Thailand to participate in the Seedstars summit. The platform provides funding to startups in emerging markets. The diversity of project ideas made me curious enough to scrape data from the Seedstars website. After leaving Tootle, I was interested in researching the NEXT product to invest my time in. The data scraped would allow me to create a one stop place for every startup that had applied to Seedstars (there were plenty).\nA small chunk of fintech startups that applied to Seedstars name problem solution stage Pymlo Pte. Ltd. Traditional accounting in SMEs are expensive, inefficient, and error-prone due to a lot of manual processes. Pymlo accounting platform helps non-accountant users in SMEs to manage accounting themselves easily at lower cost. Revenue Stage QuickCash Long \u0026amp; costly processing time, collateral requirement for working capital, no predictive data analytics platform for small business loans. QuickCash Platform analyzes business activity through innovative online scoring platform in minutes, providing working capital in one click Revenue Stage Afrikwity SME?s are playing a key role in africa positive trend (they represent 33% of GDP \u0026amp; 90% of private firms). 70% of SME are facing financing pb Afrikwity offers investors in the north opportunity to invest part of their savings in the capital of african innovative startups \u0026amp; SMEs. Beta Testing Stage S-Cash Payment 4 out of 5 people in developing countries are excluded from traditional financial services Offer the first digital financial service to savings, credit and payment that you can dedicate to your smartphone. Pre-Revenue Stage DIRECT SOLUTION SARL Trop de temps pour connaitre le prix de son assurance et souscrire. D\u0026rsquo;où un taux trop faible en Afrique. Mettre en place un chatbot qui permettre au client d\u0026rsquo;avoir son assurance en moins de 2mn. Beta Testing Stage Kibubu Limited Unplanned expenditures habits\\r\\r\\r\\r\\n - Withdraw money as one wishes not as needs.\\r\\r\\r\\r\\n - Involved in unplanned purchases. An application/A Platform to transform the traditional Kibubu (moneybox/piggy bank) into electronic one. Pre-Revenue Stage A-Trader Brokerage \u0026amp; Securities 82% of adults with savings but as of 2013, only 13.9% with access to formal banking, financial products, and savings and investment options. Web and app based platform providing investment options to any of 40 million mobile phone users in Tanzania, and 23 million internet users. Beta Testing Stage MaxiCash Financial Support in Africa through remittance agencies is inconvenient and costly; while Payment collection is complex and rudimentary. MaxiCash is a revolutionary way to enable Financial Support and Payment Collection in Africa with Cash Vouchers System using a Mobile App. Revenue Stage Riovic High capital costs in insurance are borne by consumers who end up paying high premiums and receive less value from their insurers. An online marketplace that provides companies in insurance with cost-efficient access to capital for various needs. Revenue Stage MamboPay Limited Most financially excluded people do not have access to a mobile phone and this puts them at a disadvantage and can not use mobile money. Our technology allows card holders (National ID card or MamboPay Card) to be able to make mobile money transactions using Card Numbers. Revenue Stage ytibcapital problem of growing cottage industry to a quality producing plant solution is to enable easy access to quality machinary andexpart knowladge through payment inkind to suppliers Beta Testing Stage DusuPay Ltd Lack of a proper payment gateway to Africa Creating an ubiquitous platform that works effectively with local modes of payment in various parts of the world to make payments possible Expansion Stage Fintech 60% of Nigerians still unbanked, \u0026lsquo;change\u0026rsquo; problem plaguing the street, low accountability and transparency on revenue Revenue assurance, getting the unbanked into the system,solving balance (\u0026lsquo;change\u0026rsquo;) problem, Revenue Stage Rafode ltd Rafode Addreses the problem of access to credit by rural poor to purchase solar lamps and energy savings cook stove and repay while using. Rafode provide solar lamps and energy savings cookstoves to rural poor on credit and repay the loan on instalments for a period of time Revenue Stage Flexitech Group Limited Tradition layaway method is tedious to manage and inconvenient for both the customer and the merchant. An automated layby system for both Merchants and their customers for the purchase of goods and services.\\r\\r\\r\\r\\n\\r\\r\\r\\r\\n Revenue Stage Bdrates Holdings Limited There are too many banks offering too many products. Offline comparison is tedious and inefficient. An online financial comparison and application can make the retail lending market more efficient for both borrowers and lenders. Revenue Stage FYPTO Leftover currencies: non exchangeable for small amount. Foreign travelers normally dump it at home or unwillingly spend it at airports. Deposit your leftover currencies easily to many FYPTO agents around you and use via FYPTO App in many ways that brings the value of it. Development Stage Imaginary Pay 1) Lack of Interaction between Financial Institutions\u0026rsquo; System. \\r\\r\\r\\r\\n2) There is no 24 by 7 model. \\r\\r\\r\\r\\n3) Data Access Restriction. Money Transfer and Payment Platform, which interact seamlessly within an ecosystem via API. Development Stage MyCash Online Currently, there is around 3.3 million* migrants, here in Malaysia. Most of them do not have access to any banks or credit cards. We are an online market place specially designed for the migrants, where they can purchase services online \u0026amp; pay using MyCash Coupons. Expansion Stage WHO\u0026rsquo;s GOOD Acquiring reliable environmental, social and governance (ESG) data to measure sustainability performance is time consuming and expensive WHO\u0026rsquo;S GOOD is an easy to use AI based platform that analyzes, compares and evaluates companies\u0026rsquo; ESG performance time and cost-effectively. Development Stage Smartly Getting started with investing in 2018 is still too complicated. Fully digital platform that teaches and allows everyday people to start investing and saving. Pre-Revenue Stage ZigWay Irregular incomes force poor families to take out expensive informal loans to meet daily needs like food, keeping them in a debt trap. We help people access quick automated Nano Loans ($5-50) to meet daily needs and smooth expenses in a cheaper, more flexible and safer way. Idea Stage Oncoinsurance Imperfect system of treating cancer in the country Getting better treatment in the case of insurance Revenue Stage Simple Invest Broker services are for traders and professionals. Retail investors has no suitable product to use for investing their money. Simple mobile app with brokerage account with AI roboadvisor Revenue Stage Mesfix The hard time that MSME´s face to find financial products with the traditional banking system Online platform that connects MSME´s in financial need with a community of people interested on investing from small amounts and high yield Revenue Stage Innovafunding lack of financing for small size companies An online platform that connects investors and small companies and help both to find finnancing and investment Beta Testing Stage Operadora SuSu SAPI de CV Companies offer employee\u0026rsquo;s savings fund, however these are a pain to manage and often lead to fraud when operated internally. After registering in the app employees can see their employee savings fund in real time and request a loan directly in the App. Revenue Stage Vest Wealth Management In Emerging Markets, the middle class doesn?t have access to the financial services like Investing because they are seen as \u0026ldquo;elite\u0026rdquo;. Vest is a digital investment advisor that target the middle class in emerging markets. Revenue Stage Payit Cash is expensive in time and money, and risky and that\u0026rsquo;s why we want to get rid of it! Mobile platform that makes it easy for users to pay everything digitally starting by:\\r\\r\\r\\r\\n- Our Friends\\r\\r\\r\\r\\n- Informal Market \\r\\r\\r\\r\\n- Services Pre-Revenue Stage Lefort More than 72% of SMEs in Mexico don\u0026rsquo;t have access to financing products. Current processes are manual and consume a lot time and resources. Replace a manual, complex and resource consuming process with a fully automatic service, 10 times cheaper 200 times faster. Revenue Stage Mutual People can\u0026rsquo;t do loans to another person or company legally in Brazil, just Banks. Mutual is the first Lending P2P Market Place enabling individuals to make and control their Loans each other with 100% autonomy 100% legal. Development Stage Facturedo Expensive and bad user experience (lack of transparency, pricing subject to bias) from existing traditional players. Platform with transparent and cheap pricing, plus great user experience. Revenue Stage RedCapital Small businesses have a little credit line on banks. So, they have higher rates in other financial institutions. On the RedCapital Website, SMEs request money and investors can choose the loans or the invoices that they would like to invest in. Expansion Stage Goodticket Tecnologia de Pagamentos Today the benefits market offers a bureaucratic and non malleable model, which ultimately makes the experience at the time of hiring a slow We offer a practical WEB platform for the company to manage simply by adding credits. For the beneficiaries, we are a quick app platform. Pre-Revenue Stage EASYCRÉDITO Credit is always the last eatapa buying process happens after the emotional experience and causes frustration when denied. Our solution enables you to transfer all the credit hire experience to a multi-channel online platform through applications or websites. Expansion Stage Neotic Large amount of info a stock trader has to deal with every day Providing a personnalisable robot that can crunch tons of data and learn from historical data. Revenue Stage 2nate Lack of money for fighting against poverty, cultural problems, social anomalies. Crowdfunding and micro-donations, What our fathers used to do. Pre-Revenue Stage Drupz Saving money is necessary for most of us, but we cannot do it effectively. It\u0026rsquo;s a complex, multi-faceted problem that is very hard to solve. Drupz\u0026rsquo;s novel machine learning algorithms analyze your financial behavior and automatically and seamlessly saves money for your goals. Beta Testing Stage AbantuCard ( Isogong pty ltd ) Its difficult to make better choice to make debit and credit card payments and to get cheaper and better interest on borrowings and savings Debit Card, Credit Card and Investment Account in ONE, automated, based on person to person network. Pre-Revenue Stage Democrance More than 350 million people in MENA do not have access to insurance- but are the ones that need insurance and protection the most. A FinTech platform that makes insurance affordable and accessible to the lower-income population and adds value to insurers and distributors Beta Testing Stage Finkee People usually know their income, but don\u0026rsquo;t know how to make their money work for them and to control where it is actually going. The best solution is to analyse your expenses and get personalized recommendations to achieve your goals with the help of our system. Development Stage Bloomzed (Rocket-A Lab) To make simple daily transactions a consumer needs to think about lots of stuff beforehand (withdraw money, change money, send money, etc.) Simplify, fasten and make more secure daily consumption and payment transactions via smartphone. Development Stage Expediente Azul Whenever a business customer asks for a loan, he has to send many documents, 5 to 20 on average, by email or on paper. Followup is a mess. A tool to easily capture, receive, analyze automatically and store the many documents sent by a particular customer that requests a loan. Revenue Stage Credytag The only credit card payment system in the market are card reader or a dongle. Both have allot of expensive and complicated requirements. Making the payment system work with just a QR Code and a phone that recibes SMS for the business and a mobile app for the consumer. Development Stage Kintos The limitation of income and credit cards access as well as financial education for young people. Loans and investments tuned just right for the necessities of young people, complemented with engaging financial education. Beta Testing Stage Companion Business Solutions There are more than 800k taxpayers in Azerbaijan and they have to use 3 web sites and softwares for sending reports and to the government. Using only one app we can simpler do all works in these portals, even in our phones and pads. Idea Stage FlutterPay Leverages a distributed ledger system to reduce costs associated with payments and increase speed. Leverages a distributed ledger system to reduce costs associated with payments and increase speed. Development Stage Fondify In Mexico there is a lack of option to get resources, traditional channels such as banks or government entities aren`t an option for creators We develop a Crowdfunding Platform inspired in the LATAM culture. With global funds for the creators from people who love their project Pre-Revenue Stage Dinerio People don´t save money due to a lack of knowledge and financial planning. There are no tools for people to track their expenses easily. Dinerio extracts transactions\u0026rsquo; information from online banking, categorizes everything and keeps track of budgets, automatically. Beta Testing Stage Distribution and Count by sectors Stages of startups Startup countries ","date":"2018-03-03T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/seedstars/","section":"posts","tags":["viz","python"],"title":"Startup analysis using Seedstars data"},{"categories":["open source","pet projects","proof of concept"],"contents":" Introduction\nI have been experimenting with Deep Learning models in PyTorch for a couple of weeks now. PyTorch is an open source python package that provides Tensor computation (similar to numpy) with GPU support. The dataset used for this particular blog post does no justice to the real-life usage of PyTorch for image classification. However, it serves as a general idea of how Transfer Learning can be used for more complicated image classification. Transfer learning, in a nutshell, is reusing a model developed for some other classification task, for your classification purposes. The dataset was created by scraping images from google image search.\nCreating the dataset\nFor our dataset, we need images of birds, planes, and Superman. We will be using the icrawler package to download the images from google image search.\nWe repeat the same for birds and Superman. Once all the files have been downloaded, we will restructure the folders to contain our training, testing and validating samples. I am allocating 70% for training, 20% for validating and 10% for testing.\nLoading the data\nPyTorch uses generators to read the data. Since datasets are usually large, it makes sense to not load everything in memory. Let\u0026rsquo;s import useful libraries that we will be using for classification.\nNow that we have imported useful libraries, we need to augment and normalize the images. Torchvision transforms is used to augment the training data with random scaling, rotations, mirroring and cropping. We do not need to rotate or flip our testing and validating sets. The data for each set will also be loaded with Torchivision\u0026rsquo;s DataLoader and ImageFolder.\nLet us visualize a few training images to understand the data augmentation.\nLoading a pre-trained model We will be using Densenet for our purposes.\nThe pre-trained model\u0026rsquo;s classifier takes 1920 features as input. We need to be consistent with that. However, the output feature for our case is 3 (bird, plane, and Superman).\nNow, let\u0026rsquo;s create our classifier and replace the model\u0026rsquo;s classifier.\nWe are using ReLU activation function with random dropouts with a probability of 20% in the hidden layers. For the output layer, we are using LogSoftmax.\nTraining Criterion, Optimizer, and Decay\nModel Training and Testing\nLet us calculate the accuracy of the model without training it first.\nThe accuracy is pretty low at this time, which is expected. The cuda parameter here is the boolean object passed for the availability of GPU hardware in the machine.\nLet us train the model.\nSince GPU is supported, the training took around 10 mins. The validation accuracy is almost 99%. Let us check the accuracy over training data again.\nImage Preprocessing We declare a few functions to preprocess images and pass on the trained model.\nPredicting by passing an image\nSince our model is ready and we have built functions that allows us to visualize, let us try it out on one of the sample images.\nSo, that is it.\n","date":"2018-02-02T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/bird_plane_superman/","section":"posts","tags":["ml","python","pytorch","classification"],"title":"Birds, Plane, Superman"},{"categories":["conceptual"],"contents":"Diffie-Hellman key exchange (The introductory content is from wikipedia)\nDiffie–Hellman key exchange is a method of securely exchanging cryptographic keys over a public channel and was one of the first public-key protocols as originally conceptualized by Ralph Merkle and named after Whitfield Diffie and Martin Hellman. DH is one of the earliest practical examples of public key exchange implemented within the field of cryptography.\nDiffie-Hellman is a way of generating a shared secret between two people in such a way that the secret can\u0026rsquo;t be seen by observing the communication. That\u0026rsquo;s an important distinction: You\u0026rsquo;re not sharing information during the key exchange, you\u0026rsquo;re creating a key together.\nThis is particularly useful because you can use this technique to create an encryption key with someone, and then start encrypting your traffic with that key. And even if the traffic is recorded and later analyzed, there\u0026rsquo;s absolutely no way to figure out what the key was, even though the exchanges that created it may have been visible. This is where perfect forward secrecy comes from. Nobody analyzing the traffic at a later date can break in because the key was never saved, never transmitted, and never made visible anywhere.\nThe way it works is reasonably simple. A lot of the math is the same as you see in public key crypto in that a trapdoor function is used. And while the discrete logarithm problem is traditionally used (the xy mod p business), the general process can be modified to use elliptic curve cryptography as well.\nBut even though it uses the same underlying principles as public key cryptography, this is not asymmetric cryptography because nothing is ever encrypted or decrypted during the exchange. It is, however, an essential building-block, and was in fact the base upon which asymmetric crypto was later built.\nCryptographic explanation The simplest and the original implementation of the protocol uses the multiplicative group of integers modulo p, where p is prime, and g is a primitive root modulo p. These two values are chosen in this way to ensure that the resulting shared secret can take on any value from 1 to p–1. Here is an example of the protocol, with non-secret values in blue, and secret values in red.\nImports import base64 from primesieve import nth_prime from random import randint from Crypto.Cipher import AES Key Sharing Public numbers # small prime number g = nth_prime(50) g 229 # large prime number p = nth_prime(1000) p 7919 Alice and Bob Private number a = nth_prime(randint(1, p-1)) b = nth_prime(randint(1, p-1)) a 49019 b 70639 Public Message Transfer Alice sends Bob publicly alice_sends = g**a % p Bob sends Alice publicly bob_sends = g**b % p Shared Secret key Alice shared_secret_key_alice = bob_sends**a % p shared_secret_key_alice 7065 Bob shared_secret_key_bob = alice_sends**b % p shared_secret_key_bob 7065 assert shared_secret_key_alice==shared_secret_key_bob shared_secret_key = shared_secret_key_alice Both Alica and Bob now have the secret key, without compromising their private keys.\nDH is public key/asymmetric crypto but not encryption. For the demo, AES 256 (takes 32 bytes)\nEncryption and Decryption Changing the key to 32 byte (a bad hacky way)\nkey = str(shared_secret_key) key_bytes = str.encode(key.zfill(32)) # The message should be a multiple of the byte size alice_to_bob_original = str.encode(\u0026#39;hello world how are you\u0026#39;.zfill(256)) cipher = AES.new(key_bytes, AES.MODE_ECB) cipher_msg = cipher.encrypt(alice_to_bob_original) cipher_msg is sent to the server, bob reads the cipher_msg and uses the secret key to decipher\ndecipher = AES.new(key_bytes, AES.MODE_ECB) print(decipher.decrypt(cipher_msg)) b'00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000hello world how are you' DH is prone to man-in-the-middle attacks The Diffie-Hellman Scheme does not provide authentication of any kind. It only allow 2 anonymous parties to share a common secret. But for all Alice knows, she could be shaking hands with the devil (instead of Bob). This is why we need at least one party to be authenticated.\nFor example: SSL (https), the webserver is authenticated using PKI (Public Key Infrastructure), and then a secure connection is established (D-H) between the website and the client. Since the website has been authenticated, the client can trust the website, but the website cannot trust the client. It is now safe for the client to provide his own authentication details on the webpage.\nFor a practical answer if you are configuring your SSL/TLS server: you should use a modulus of at least 2048-bit, and a generator g such that the order of g is a prime q of at least 256 bits; alternatively, you may use a modulus p which is a \u0026ldquo;safe prime\u0026rdquo; (the order of g will then be either a very big prime, or twice a very big prime, which is almost as good). Some people feel safer when they generate their DH parameters \u0026ldquo;themselves\u0026rdquo;(*) instead of reusing existing values; if that\u0026rsquo;s what it takes to allow you to sleep at night, then do it.\nElliptic Curve replaces primes with elliptic curve. The benefit is efficiency.\n","date":"2018-01-12T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/diffie_hellman/","section":"posts","tags":["encryption","python"],"title":"Diffie-Hellman key exchange (and AES-256)"},{"categories":["proof of concept"],"contents":"RSA RSA (Rivest–Shamir–Adleman) is one of the first public-key cryptosystems and is widely used for secure data transmission. In such a cryptosystem, the encryption key is public and it is different from the decryption key which is kept secret (private). In RSA, this asymmetry is based on the practical difficulty of the factorization of the product of two large prime numbers, the \u0026ldquo;factoring problem\u0026rdquo;. The acronym RSA is made of the initial letters of the surnames of Ron Rivest, Adi Shamir, and Leonard Adleman, who first publicly described the algorithm in 1978. Clifford Cocks, an English mathematician working for the British intelligence agency Government Communications Headquarters (GCHQ), had developed an equivalent system in 1973, but this was not declassified until 1997\nA user of RSA creates and then publishes a public key based on two large prime numbers, along with an auxiliary value. The prime numbers must be kept secret. Anyone can use the public key to encrypt a message, but with currently published methods, and if the public key is large enough, only someone with knowledge of the prime numbers can decode the message feasibly. Breaking RSA encryption is known as the RSA problem. Whether it is as difficult as the factoring problem remains an open question.\nRSA is a relatively slow algorithm, and because of this, it is less commonly used to directly encrypt user data. More often, RSA passes encrypted shared keys for symmetric key cryptography which in turn can perform bulk encryption-decryption operations at much higher speed.\nImports from primesieve import nth_prime RSA Algorithm Take two distinct, large primes p and q (Ideally these have a similar byte-length) Multiply p and q and store the result in n Find the totient for n using the formula φ(n)=(p−1)(q−1) Take an e coprime that is greater, than 1 and less than n Find d using the formula d⋅e≡1modφ(n) At this point, the pair (e, n) is the public key and the private key (d, n) is the private key.\np = nth_prime(10) q = nth_prime(15) n = p*q print (p,q,n) 29 47 1363 totient = (p-1)*(q-1) print (totient) 1288 Totient In number theory, Euler\u0026rsquo;s totient function counts the positive integers up to a given integer n that are relatively prime to n. It is written using the Greek letter phi as φ(n) or ϕ(n), and may also be called Euler\u0026rsquo;s phi function. It can be defined more formally as the number of integers k in the range 1 ≤ k ≤ n for which the greatest common divisor gcd(n, k) is equal to 1. The integers k of this form are sometimes referred to as totatives of n.\nThe line on the top represents distribution of prime numbers. The phi of a prime number is simply the (n-1)\nPhi function is multiplicative (for relatively prime numbers). Therefore, phi of A times B where A and B are prime is (A-1) times (B-1) Coprime In number theory, two integers a and b are said to be relatively prime, mutually prime, or coprime (also written co-prime) if the only positive integer (factor) that divides both of them is 1. Consequently, any prime number that divides one does not divide the other. This is equivalent to their greatest common divisor (gcd) being 1\nfrom math import gcd import random def modinv(a, m): for x in range(1, m): if (a * x) % m == 1: return x return None def coprimes(a): l = [] for x in range(2, a): if gcd(a, x) == 1 and modinv(x,a) != None: l.append(x) for x in l: if x == modinv(x,a): l.remove(x) return l coprime_list = coprimes(totient) secure_random = random.SystemRandom() e = secure_random.choice(coprime_list) d = modinv(e, totient) d 685 Private and Public key pairs print (\u0026#39;Public key pair:\u0026#39;, e, n) Public key pair: 1021 1363 print (\u0026#39;Private key pair:\u0026#39;, d, n) Private key pair: 685 1363 Test test = 2**e % n test 1029 test**d % n 2 Encryption and Decryption def encrypt(msg, pub, pri, mod): chars = [ord(x) for x in list(msg)] cipher = [] for char in chars: cipher.append(chr(char**pub%mod)) return \u0026#39;\u0026#39;.join(cipher) def decrypt(msg, pub, pri, mod): chars = [ord(x) for x in list(msg)] cipher = [] for char in chars: cipher.append(chr(char**pri%mod)) return \u0026#39;\u0026#39;.join(cipher) cipher = encrypt(msg = \u0026#39;hello\u0026#39;, pub=e, pri=d, mod=n) decrypt(cipher, pub=e, pri=d, mod=n) 'hello' Python library import rsa Bob generates a keypair, and gives the public key to Alice. This is done such that Alice knows for sure that the key is really Bob’s (for example by handing over a USB stick that contains the key).\n(bob_pub, bob_priv) = rsa.newkeys(512) (alice_pub, alice_priv) = rsa.newkeys(512) Alice writes a message, and encodes it in UTF-8. The RSA module only operates on bytes, and not on strings, so this step is necessary.\nmessage = \u0026#39;hello Bob!\u0026#39;.encode(\u0026#39;utf8\u0026#39;) Alice encrypts the message using Bob’s public key, and sends the encrypted message\ncipher = rsa.encrypt(message, bob_pub) Bob receives the message, and decrypts it with his private key.\ndecrypt_cipher = rsa.decrypt(cipher, bob_priv) print (decrypt_cipher) b'hello Bob!' Since Bob kept his private key private, Alice can be sure that he is the only one who can read the message. Bob does not know for sure that it was Alice that sent the message, since she didn’t sign it.\nSignature Suppose Alice uses Bob\u0026rsquo;s public key to send him an encrypted message. In the message, she can claim to be Alice but Bob has no way of verifying that the message was actually from Alice since anyone can use Bob\u0026rsquo;s public key to send him encrypted messages. In order to verify the origin of a message, RSA can also be used to sign a message.\nSuppose Alice wishes to send a signed message to Bob. She can use her own private key to do so. She produces a hash value of the message, raises it to the power of d (modulo n) (as she does when decrypting a message), and attaches it as a \u0026ldquo;signature\u0026rdquo; to the message. When Bob receives the signed message, he uses the same hash algorithm in conjunction with Alice\u0026rsquo;s public key. He raises the signature to the power of e (modulo n) (as he does when encrypting a message), and compares the resulting hash value with the message\u0026rsquo;s actual hash value. If the two agree, he knows that the author of the message was in possession of Alice\u0026rsquo;s private key, and that the message has not been tampered with since.\nThis works because multiplication is commutative so {\\displaystyle h=hash(m);(h^{e})^{d}=h^{ed}=h^{de}=(h^{d})^{e}\\equiv h{\\pmod {n}}} {\\displaystyle h=hash(m);(h^{e})^{d}=h^{ed}=h^{de}=(h^{d})^{e}\\equiv h{\\pmod {n}}} Thus, the keys may be swapped without loss of generality, that is a private key of a key pair may be used either to:\nsignature = rsa.sign(message, alice_priv, \u0026#39;SHA-1\u0026#39;) rsa.verify(message, signature, alice_pub) 'SHA-1' Complete Bob and Alice generate a keypair and share public keys.\n(bob_pub, bob_priv) = rsa.newkeys(512) (alice_pub, alice_priv) = rsa.newkeys(512) Alice writes a message.\nmessage = \u0026#39;hey Bob!\u0026#39;.encode(\u0026#39;utf8\u0026#39;) Alice signs the message with private key\nsignature = rsa.sign(message, alice_priv, \u0026#39;SHA-1\u0026#39;) Alice encrypts the message using Bob’s public key, and sends the encrypted message\ncipher = rsa.encrypt(message, bob_pub) Bob receives the message and checks for authenticity using alice\u0026rsquo;s public key\nrsa.verify(message, signature, alice_pub) 'SHA-1' Bob decrypts the message using his private key\ndecrypt_cipher = rsa.decrypt(cipher, bob_priv) print (decrypt_cipher) b'hey Bob!' signature b'\\x07Of\\xabW^\\x0b\\xaeFh\\x01L\\xf3\\x11\\xe0\\xe9\\\\\\x99r\\xc9\\x1c\\x044\\x11\\xfc{5\\xa3_ \\xeb\\xba\\xf8\\x84b\\x1e\\xb7\\xadK)\\xdf\\x9b\\x8f|\\xc4\u0026gt;\\x89\\x9d\\xdf\\x98OI\\xc1\\x87\\xc2\\xdd\\xf3\\xf6\\x16\\xae2`my' ","date":"2018-01-12T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/rsa_encryption/","section":"posts","tags":["encryption","python"],"title":"RSA encryption"},{"categories":["tootle"],"contents":"A brief history of Tootle Hello World, When Tootle started, there were four of us. At the time, we were working on EdCrayon (Three60’s Education and Classroom Management System). The development of EdCrayon was pretty much complete and one of the better schools in town had been implementing it for a year. We were busy with building fewer last minute requirements such as Nepalese school standard digital report cards and student ranking systems. After those were done and the academic session ended, we were preparing for the next academic session and we were also in talks with several schools for implementation of EdCrayon.\nhttps://princihere.files.wordpress.com/2016/10/androiddevices.png\nHowever, because of earthquake of 2015, the school that were lined up decided to back out and use their EdCrayon allocated resources on repairing the infrastructure damages. We would come to work and not feel productive at all. Due to the sheer boredom at work, we started researching on several ideas including location based services. One of the very first ideas was to create an app that would allow users to track location of Sajha buses on Google Maps so that they could plan on leaving offices/homes by referencing estimated time of arrival provided by the app. Even though Sajha buses were comparatively convenient, the problem was that people had to wait for the buses, generally up to half an hour, since bus stops arrival timings were more or less random. We developed the prototype for a Sajha bus route but since Sajha bus did not show any interest we decided to move on to other ideas.\nMeanwhile, I was researching on asynchronous API calls for a pet project of mine. I had been trying to figure out the best way to sync client’s on-device offline database and off-device master database. In this process, one of the first things that I did was to look into creating my own implementation by syncing Android SQLite database with MySQL database. Up until this point, when ever client sync was required, I would simply clear out the content adapter’s list, remove all elements, get all elements from master server and refill the adapter. This allowed for easy implementation. However, even if nothing was changed in the client database, whenever the client hit the sync button or pulled to refresh, the process would get repeated. Basically the question I was trying to answer was, “Is it faster and less expensive to individually assess for updates by comparing updated_date and id or to simply truncate and refill?” During the research for this, I stumbled upon newer technologies that would allow setting up changes listeners on client’s devices to refresh the changed list without having to trigger an action to sync the data. Using this finding and our research from location based services, we felt comfortable tackling the idea for a ride sharing application from a technological perspective, and Tootle was born. Tootle was not Tootle from the very beginning. We continued our research on appropriate Business Models, Brand Positioning, Business Strategy, and Marketing and Delivery. Based on the changes in how we were going to position our brand in the market, we changed the product name from CabIO (digital cabs), KAR.ma (share your ride for karma), Bzuli (environmentally conscious ride sharing) and finally to Tootle (a fun way to travel without restricting the service to electric vehicles and only four wheeled vehicles). In the meantime, based on the Business Models and Brand Positioning, several elements of the app were also changed.\nAlthough the concept of ride sharing is not new, and several companies such as Uber, Lyft, Ola and Go-Jek have implemented it tremendously well, Tootle is different mainly due to three core elements. First of all, we let our partners decide if they want to take a ride, i.e, we introduced the idea of casual Tootle partners. For example, a partner A can decide to give rides throughout the day and make this his/her full time job or simply give rides that matches his/her travel itinerary. The technology is adjusted in such a way that a tootle ride requests are sent to several partners within a vicinity rather than just one partner. Unlike aforementioned companies, there are no penalties involved for not taking a ride. Secondly, albeit not completely by choice, we have realized that frugality invites creativity. At every step of technology development, we have had to strategize inexpensive yet effective ways to solve problems. Although we are still purely in development phase as of today, I believe we can really take pride in what we have accomplished given the resources. This is also reflective in the product. We were forced to think about minimizing data consumption (given the high data cost in Nepal) and poor internet infrastructure. Currently, on average, a particular ride for a partner in terms of data exhausts 1 MB while him/her being able to log in, select appropriate ride, complete the ride and get paid, while the backend collects ride information such as timestamps for actions and exact route followed. This costs him/her 50 Paisa which is approximately $0.005. Finally, last but not the least, the major difference comes in the form of technology adaptation, contextualization and more importantly, communication. Kathmandu is traditionally more or less a close-knit community where people prefer talking to people to garner a sense of confirmation and safely. Therefore, it was crucial to build a simple and clean UI that gets the job done and focus massively on developing technologies that provided real time ride statistics to our team at call center so that they could assist people giving and taking Tootle rides. Similarly, since digitization of payment was essential but Nepal is not ready for secure and realtime credit card transaction, apart from local third party digital wallet integration we also have QR top-ups. To summarize, although from the surface, Tootle is a ride sharing application like Uber, it has established its own identity via contextualization of the requirements of Nepali needs. This is apparent throughout the technology.\nTootle Today From our experience, we have realized that market drives technology and not the other way around. We have also realized that although technology is merely a facilitator, it can do wonders to solve problems and invoke habit changes if done correctly. Now, we have a multidisciplinary team of 15 striving to make Tootle technologies and services better each passing day with the goal of doing it correctly.\n","date":"2017-10-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/tootle_history/","section":"posts","tags":["product"],"title":"A brief history of Tootle"},{"categories":["tootle"],"contents":"\nLink to Tootle Website\nTootle is a ride sharing service that matches bike riders and commuters. While the concept is not new, there are several contextualization factors that makes it unique. As research and development engineer and lead software developer, this was my biggest challenge to date and I learnt a lot of things. However, since we worked in a very small team, I occasionally came across problems related to branding, client relationship, marketing and overall B2C business model. This blog post will dive into those learnings rather than focus on technological implementations. To summarize the entire post, it is about my journey from a single dimensional software developer to a software developer who also understands project management, human resources, market, competition, branding and plethora of other factors that a software developer whose long term goal is to become a tech entrepreneur needs in their arsenal.\nStart small, www.rome.com was not built in a day This is trivial for most people, but not for me. There were times I was so focused on completing a task that I lost the bigger picture. Breaking down a task into smaller subtasks and solving them as soundly as possible is the key. Also, when you push yourself too hard, you miss small details that might have huge consequences in the future. Think scalability while solving little problems. Like Will Smith said, \u0026ldquo;You don\u0026rsquo;t set out to build a wall. You don\u0026rsquo;t say \u0026lsquo;I\u0026rsquo;m going to build the biggest, baddest, greatest wall that\u0026rsquo;s ever been built.\u0026rsquo; You don\u0026rsquo;t start there. You say, \u0026lsquo;I\u0026rsquo;m going to lay this brick as perfectly as a brick can be laid. You do that every single day. And soon you have a wall.\u0026rdquo; Also, Hofstadter\u0026rsquo;s law will come along to shatter all your to-do plans for the day. It\u0026rsquo;s just the way it is. Deal with it.\nDevelopment and design is an iterative, never ending process. It is not a sprint but a marathon. Seeking perfectionism hurts. First of all, there is no perfect code or design. There might be something that looks, feels and works great today, but it is never going to be perfect. You will always have to come back to that line of code, redesign that slider that looked great once upon a time and continuously update based on user feedback. Coding is a little of writing code and a lot of maintaining it. You are never really done. Project preplanning and risk management might help with not having to go back to drawing board again and again, but development and design is very much an agile process.\nHire people who love what they do and love learning Coworkers become your second family. During the course of Tootle development, I actually spent more time with my coworkers than my family. Fortunately, everyone involved with Tootle are very passionate, hard-working and motivated people who never shy from learning. Also, certain characteristics such as being ardent, an embodiment and advocate of the product, and hardworking are more important than being skillful. I was also on the hiring team and I made sure to look for these characteristics. After all, Leicester City won 2015-2016 Barclays Premier League primarily because of their team spirit.\nPrioritize what is important There will be a lot of bugs. Code related stuff aside, there will also be several things that need attention. However, time is limited. At times you could be in front of your computer debugging for hours and hours, but still the issue tracker list and crash reporting list will be full. Therefore it is necessary to prioritize based on impact level. Assessing impact of a particular task can be difficult. However, if you have an awesome team, it is just a matter of discussing from a business and technical point of view. Also distinguishing between bugs, enhancement, improvement proposal and task is very important.\nCommunication solves 95% of all problems Communication is the key. Often during the day, talk to coworkers about the problems that you are having. First of all, when you explain the problem to someone, you understand the problem better yourself. Secondly, you are also in sync with what problems everyone else is facing. Similarly, simply talking about work related problems, expressing how you feel about deadlines, doing risk analysis together, discussing business strategy together, communicating dissatisfactions etc. will help solve a lot of problems earlier. So, constantly take a break, look outside the window, get a glass of water and simply talk.\nStress can be a catalyst if applied properly This project was very much stressful. Retrospectively, it was supposed to be too because of its complexity. One of the things I really struggled with is handling stress. I have gotten better over the course of this project. However, I am still working on learning to change stress to a catalyst. I hope to become Arsene Wenger someday (giggles). However, I have learnt that if things don\u0026rsquo;t work tonight, you simply have to sleep over it. The freshness of morning solves a lot of problems.\nYou could also just watch SRK dance in Hosh na khud kahi josh\u0026hellip;.\nGo running in the morning, be healthy This one is self-explanatory. Run to remain stress free and healthy. I stopped running for a while because I was lazy. But, I was also lazy because I was not running.\nCompetition is good While reading The Personal MBA by Josh Kaufman, I realized that having market competition is actually advantageous. This concept was really counterintuitive to me previously. Competition is good because it implies higher probability of fulfillment of something known as the Iron Law of Market. It basically means that like you, other people have also seen the availability of the market you are targeting. Similarly, there is a lot to learn from your competition. Learning and applying the principles of game theory helps a lot too. Awesome tech implementation is second to client\u0026rsquo;s requirement You could spend weeks and months on awesome, flawless and amazing features that your clients don\u0026rsquo;t really care about or half an hour on simple features that makes all the difference to them. For example, adding a bitcoin as payment system would be cool, but allowing a female client to select a female driver makes the product secure and also adds marketing and advertising weight.\nBeta tests are really really really important You could sit hours upon hours, days upon days trying to find and fix all the problems. But, finding problems is more difficult once you know your product inside out. You are in a controlled office environment with fast internet, limited real test devices and a clear understanding of product workings. There is no way for you to find all the problems. The solution is to find real users to test, use and provide feedbacks. Canary tests and beta tests are therefore very important.\nJust because you like it, does not mean users are going to like it also You might absolutely love your product. However, it does not mean every user will like it also. Be prepared to get bad ratings. Also, they are not wrong to dislike something you adore so much.\nPrevious projects and keeping up to date with latest technologies help a lot I really believe this one is a biggie. Regardless of whether or not a product succeeds, it is always a stepping stone for future products. This product would not have been possible or would have been terribly difficult without Edcrayon, Edquake, and dozens of other location related prototype. When prototyping, research on newer stacks and practices.\nIn the end, revenue matters the most In the words of Josh Kaufman, Do not be a mercenary since dedication in craft, patience to find right market to be dedicated towards and consistency is required to eventually make money.\nAlso, do not be a crusader since you need money to pay the bills.\nWish us good luck with our tootle journey.\n","date":"2017-01-28T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/tootle_learnings/","section":"posts","tags":["product"],"title":"Learnings from Tootle"},{"categories":["three60"],"contents":" The objective of the game was to use texts, audios, videos, badges, quizzes, points, stories and characters to educate users on preparation for earthquake and decision making during and after earthquake.\nApplication Description\nBefore the game starts, a teaser video is played where an event of earthquake is shown inside a home scenario. The video is shown in order to reflect the importance of earthquake preparedness. The video can be skipped the next time as per the choice of the player.\nApplication Home Screen\nThe application home screen allows users to start a new game, resume from where they left off previously, change game settings such as sound and language settings and submit their score to leaderboards, and quit the application. It also displays current user name and character selection along with all the badges that the user has won.\nStarting a new game\nUpon starting a new game, a dialog appears where the user enter his/her name and selects a character. The user can choose two characters that of a boy or a girl in order to play the game. These characters will also act as protagonists in animated stories.\nBadges\nBadges are provided to users upon completion of a milestone or a level. The badges provides an incentive for the players and makes the game more engaging and interesting.\nPSA and Video Activities\nEngines have been setup to support audio and video activities.\nThe radio can be used as an informative element in the game. For example:\nA PSA for earthquake preparation informing about vital items one needs in their emergency backpack can be played in the radio.\nThe video platform can support a series of storytelling addressing to different type of circumstances and safety tips regarding the effects of earthquake. Nepali subtitles also appear on the bottom of each videos.\nDrag and Drop Activity\nEngines have been setup to support drag and drop activities.\nBased on this drag and drop engine we have developed relevant games. For example:\nPreparing a GoBag\nFollowing the PSA, the users are required to drag and drop items from a set of vital items (such as first aid kid) and useless items (such as toys) to their backpack. Since the backpack can only hold a certain number of items, the users will have to optimize for best preparation. Upon completion, the app lets the users know if their selection is correct.\nSimilarly, the app will let the users know if their selection was wise. In case of wrong selection, the app will also let the users know why their selection was incorrect. Unlike other quiz activities, the difference here is that the users will still be allowed to go back to change their answer. The rationale was to follow a formative teaching approach.\nQuiz Engines\nEngines have been setup to support quiz platform.\nQuizzes produces better organization of knowledge by helping the brain organize material in clusters to allow better retrieval. It also identifies gap in knowledge and lets people know what is learned and what is not.\nThe quiz engines supports quiz games which can be very informative. The answers chosen can also lead to some consequent information or videos later.\nThere are four different types of quiz questions.\nGrid Select True and False Multiple Choice Questions (3 Options) Multiple Choice Questions (4 Options) The first two types of questions carry a marking rubric of 200 points. The user receives complete points if he/she answers it correctly in the first go. The point a user can collect from each quiz is reduced depending on the number of mistakes.The third question carry a marking rubric of 300 points and the fourth question carry a marking rubric of 400 points. Similarly, depending on the mistakes, the point is reduced. The result is displayed with additional tip information and graphics.\nExample 1: Grid Quiz Activity\nExample 2: Multiple Choice Quiz Activity (3 options)\nLeaderboards and Facebook sharing\nThe application will allow users to submit their points to a global leaderboards system. This will allow users to compete with other players.\n","date":"2016-10-22T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/edquake/","section":"posts","tags":["android","product"],"title":"Edquake"},{"categories":["three60"],"contents":"Edcrayon’s Universal Deployment Model, converts tablets or phones to a smart, self-paced, predictive learning environment. It packages the following features:\nThousands of qualitative educational content that students can browse from and download for offline use. Gamified and engaging UI to make sure students love learning from the application. Serves as supplement to topics being taught in school or simply for self-study. Available in different languages to facilitate mother tongue learning. Learning materials is localized and contextualized too. Provides practice lessons and quizzes auto-graded by the application so that the students can work their way through the assessments as in self-paced learning. Features quiz engines that adapts to students previous knowledge on a particular subject material. If a student struggles with a specific type of problem the application will generate more questions of similar type. Provides performance charts and allows students to track their own performance. Facilitates discussion forums to allow students from different places to discuss and share ideas. The performance data will also be sent to regional or central data repository for analysis purposes. The data will help decision makers to facilitate better or different features and contents to ensure more productivity and performance. Current challenges in education\nDelivery of published books to every part of the country has been a significant problem due to the expenses involved in publishing and also due to the lack of road infrastructure. Even in cases where books are delivered on a timely basis, it only facilitates traditional teacher-centered teaching pedagogy. Twenty first century education calls for individualized learning which cannot be achieved since published books are not tailored to learners specific needs. Not all students get the optimum learning value from text based materials alone. Several schools have a large teacher to student ratio. Teachers also invest a huge portion of their time on things like keeping comprehensive performance records of every student, preparing and keeping track of lesson plans, and various other administrative tasks. If a system aided the teachers with these mundane tasks, the time saved can instead be utilized for student teaching and classroom engagement. Individualized attention can be provided to the students even in classes where the ratio is high. Decision makers in schools do not have all the performance data of all students and/or teachers at all time. A quantitative analytical report of performance with historical data assists in decision making processes. Schools still follow standardized and summative assessment system. This is partially because the system is inherent in us. However, we believe that the main reason is simply logistical. Large student size, lack of formative assessment tools, lack of engines to create individualized assessment etc. are some contributors. Use of technology in education can facilitate myriad of tools that help in solving the challenges. Edcrayon facilitates easy delivery of qualitative media contents, smart algorithms and/or modules that allow for individualized learning via tailored content pushing and group creation, self-paced learning, smart performance charts, and formative and summative assessment models.\nCurrently, private schools have benefitted from Edcrayon’s Classroom Deployment Model. This deployment model converts a traditional classroom environment to technology based, student-centered model. It packages the following features:\nConstitute all the elements of Universal Deployment Model with an addition to a guided learning and teaching environment. Replaces the traditional learning and teaching classroom environment to a student centered, formatively assessed, skill based, interactive learning and teaching environment. Rather than learning at their own pace, which has its own merits, this approach allows students to communicate and learn from teachers and other students in the classroom. Facilitates teachers with student performance charts. Allows teachers to assign tailored materials for more individualized learning approach. Facilitates formative and Inquiry based assessment through continuous feedbacks from teacher and other students. Performance data of students and teacher are available to decision makers. Government proposed Continuous Assessment System is implemented. Makes it very easy to perform day to day tasks such as attendance, register keeping, etc.\nSome Screenshots of Android Application:\nSome Screenshots of Web Application:\nMy role of R\u0026amp;D engineer and Project Lead incorporated architecting overall logic and process flow of Administrative and TeacherUI consisting User, Content, Classroom, Health and Pedagogy Management System. I also designed and co-developed EdCrayon Android application incorporating offline services, interactive charts, formative pedagogy, quiz engines among other features. Additionally, I developed several database administrative tools, wrote several crons and selenium test scripts, and managed multiple school servers. Similarly, I also trained and mentored junior programmers in programming methodologies and best practices. Additionally, I was also involved in training teachers, school-coordinators and school principal to use EdCrayon. Apart from my primary focus on technology, I also served as a core group member in defining and prioritizing technology investments and business objectives.\nPrototype samples: Sample 1 Sample 2 Sample 3\n","date":"2016-02-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/edcrayon/","section":"posts","tags":["android","product"],"title":"Edcrayon"},{"categories":["pet projects"],"contents":"\nIt uses MPAndroidChart which allows users to scroll within the graphs too.\n","date":"2016-01-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/etymology/","section":"posts","tags":["android","etymology"],"title":"Etymology and Ngram"},{"categories":["three60"],"contents":"\nImproving Literacy has been one of the biggest challenges faced by the developing world.\nUnited Nations underscored the importance of combating inequalities in education in its Sustainable Development Goals (Post 2015 Agenda) as the proposed SDG 4 suggests the international community to “Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all by 2030”.\nIn Nepal, the literacy rate for the total population is 57.4 % which shows that the inequalities still persist (CIA World Factbook). Significant portion of the population have been deprived of learning opportunities due to various reasons including early marriage, gender inequalities, social and family pressure to engage into economic activities at an early age etc. Research also suggests that in developing countries, there is very low self esteem among people who are illiterates or semi literates.\nIn this digital world, they are even at greater disadvantage. Those with the least amount of schooling will find it increasingly more difficult to participate in the evolving knowledge-based societies, deepening the social divide (Reimers, 2000). There needs to be an innovative approach to match both the education and technological gap. Recent development in the mobile technology provides us with great opportunities to fill this gap.\nThe mobile devices offer both affordability and storage capacity which makes it possible to equip with different types of educational content. There are over 5 billion mobile subscribers worldwide today –an astounding number considering the world’s current population which is roughly 6.8 billion (ITU, 2010). The rapid proliferation of mobile technologies throughout the world has brought substantial attention to the potential to leverage the power of these new technologies to address decades old problems, including educational inequalities (see Keen and Mackintosh, 2001; Ling, 2004).\nProduct / Technology Concept\nThe ICT tool (Android application) for the training will have different components for learning, assessments, data-collection and data-analysis. The idea is to make the application interactive and to reflect a game-environment where users will be motivated by game elements such as unlocking levels (lessons) as they complete certain tasks. The lessons will be based on the UNESCO’s newly-literate book (Mathani) for Awadhi language (Part-2). However, certain levels from Part-1 will also be added as a means to bridge their previous knowledge.\nSome of the lessons that will be included for the pilot are as follows:\nWord formation Basic math (addition and subtraction) Paragraph reading Paragraph listening Word math Time/Calendar Filling forms, writing letters etc. Some forms of assessments/practice lessons that will be included in the pilot are as follows:\nAndroid drawing canvas Multiple choice questions True/False questions Filling in the blanks Writing lessons The following user data will be collected within the application with the assumption that every learner will have a personal device to work on:\nUser information Level progression data Demographic data Application usage data Performance data Location data The data can be synced in real-time if Internet connectivity is not an issue. However, in places where connectivity can be a problem, or it is expensive, an alternative approach where data is synced periodically can be utilized.\n","date":"2015-10-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/neoliteracy/","section":"posts","tags":["Android"],"title":"Neo-literacy app concept"},{"categories":["pet projects"],"contents":"\nI recently stumbled upon a customer loyalty measurement scale termed Net Promoter Scale (NPS in short) via a conversation with a businessy friend of mine who was researching on it. I was truly amazed by the simplicity yet brilliance of NPS. Moreover, this is ideal for someone like me who wants to get honest customer feedback without harassing them with poorly designed questionnaires regarding a product.\nThe pith of NPS lies in the question:\n“How likely is it that you would recommend our company/product/service to a friend or colleague?”\nThose who respond with a score of 9-10 are called Promoters, and are considered likely to show positive behaviors like repeat purchase and positive referrals. Those who respond with a score of 0-6 are labeled Detractors, and they are believed to exhibit the negative behaviors like driving away from the brand, negative referrals. Responses of 7 and 8 are labeled Passives, and their behavior falls in the middle of Promoters and Detractors.\nThe NPS Calculation formula\n(Number of Promoters — Number of Detractors) / (Number of Respondents) x 100\nNPS ranges from -100 to 100 (inclusive).\nWith me doing the charting part and my friend doing the result implication and significance part we developed a simple NPS calculator app that takes total detractors, passives and promotors to output NPS. Refer to the screenshots below to identify what different NPS score signify.\n","date":"2015-04-04T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/nps/","section":"posts","tags":["Android"],"title":"Net Promoter Scale (NPS)"},{"categories":["three60"],"contents":" I had contributed as a debugger on Seeds of Empowerment\u0026rsquo;s Swahili Phonics application a while back. This blog post lists various test cases for Swahili Phonics Learning Application for identification of completed requirements and current issues. Thereafter, it also lists the problems that were solved.\n1. User interface and interaction\nThe application has great user interface. Since the majority of targeted users are young children, it is important to make learning fun. This application successfully does that. The only issue with the user interface is that it does not adapt to the full screen width.\nExpected screen:\nCurrent screen:\n2. Data collection\nThe application collects all sorts of data from the players and stores it locally in the user\u0026rsquo;s device. The players can re-login using their user-name and password and the application allows continuation from where they left before. The application also stores every hits and misses of a user. Although not currently implemented, the data can be used for analysis purposes to identify things like what set of phonetic sound players mostly struggle with.\n3. Algorithm\nAlthough the application provides a great framework for scalability, localization in different languages, and analytics there are few issues that we feel need some attention. We have been able to identify the following problems. We were not really sure if these problems were known since the realization came from analysis of the code rather than from the use of application. Therefore, we decided to list these problems with screen-shots so that it becomes more easier to understand.\nI) random number generation problem\nIn the current implementation, random numbers help generation of correct and incorrect answer. However, correct answer is always selected from the current level the player is at but incorrect answer set is generated from level 1 through current level. Because of this, although the options will have certain phoneme that are from previous level, they will never be the correct answer.\nSome examples:\nIf a user is at level 2, the correct option will never be from level 1. To generalize, if the user is at level n, the possible set of answers will only be from the phonemes allocated for that particular level n and not from any levels less than n.\nIn this case, the answer only be “bo”, “be” or “bi”. With progression of levels, it becomes more noticeable:\nHere the possible answer is only one (ie. 4). Also, with the progression of levels, the answers will most likely be the option that the user has not seen before. The game will not chose any phonemes as correct that were part of the option set in previous levels.\nII) the phonemes are always chosen from a statically defined list\nSince the game always refers to a single static list of phoneme for selecting correct and incorrect options, it does not adapt to re-test previous misses of a user.\nIII) chances of missing some phonetics because of dynamic array creation\nThe application generates a list during application runtime to check if all phonemes for a particular level have been tested. If a phoneme is tested twice in a level and the player chose the correct answer again, the stars are not increased because the generated list already has the phoneme. It was added to the list when the player answered correctly the first time around. This is a great implementation. However, there is also a small issue with this. Since the list is generated during runtime but the number of stars a player has in a particular level is stored and retrieved when the player re-logins, chances are, some phonemes are never tested.\nAn example, If Player A starts the game, and in Level 1 he/she successfully recognizes phonemes for {a, e, o, u}, the screen-shot would look something like this:\nWhen the player is playing the game, the game initializes an array and stores all the successful hits exactly once. The list has “a”, “e”, “o”, and “u” at this point. The player is promoted to another level when he/she has five stars.\nThe stars are increased if a phoneme can be added to a list (the condition being it does not already exist in the list). In this case, the stars will only be increased if the phonetic for “i” is tested and the player successfully answers that.\nBut, if the player decides to leave the game at this point to continue at another time, the array that stored all previous successful hits will be reset. Although he/she will still have 4 stars, now all it takes to move to another level is to identify the first phoneme put by the game. Because of this issue, the might progress to next level without having to identify phoneme for “i”.\n4. Adaptability\nThis feature is currently missing because it depends on algorithm design. Once the issues are resolved in that part, the next step would be to work on adapting the levels to test users on previously missed phonetics.\n5. Report Generation\nAlthough the application successfully collects useful data, the data is currently not being utilized for analysis.\nIdentified solutions for aforementioned bugs:\nThe solutions have already been implemented in the application. Currently, only algorithm specific problems have been solved. The application still needs additional design and game elements to make it more interactive.\n","date":"2015-02-02T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/swahili/","section":"posts","tags":["android","stanford"],"title":"Swahili Phonics App with Stanford University"},{"categories":["pet projects"],"contents":" The Zorganian Republic has some very strange customs. Couples only wish to have female children as only females can inherit the family\u0026rsquo;s wealth, so if they have a male child they keep having more children until they have a girl. If they have a girl, they stop having children. What is the ratio of girls to boys in Zorgania?\nThe ratio of girls to boys in Zorgania is 1:1. This might be a little counter-intuitive at first. Here are some ways of tackling this problem. 1. Monte Carlo Simulation: Although, Monte Carlo simulation does not necessarily show why the result is 1:1, it is appropriate because of the very counter-intuitive nature of the problem. At the very least, it helps us see that the result is indeed 1:1. Therefore, this is a good start.\nThe following R code estimates the probability of a child being a boy in Zorgania.\ncouples \u0026lt;- 100000 boycount \u0026lt;- 0 for (i in 1:couples){ # 0: boy while (sample(c(0,1),1) == 0) { boycount=boycount+1 } } probability \u0026lt;- boycount/(couples+boycount) Result:\n2. Understanding the question better: Here is another question: What is the probability of getting a tail in a fair coin toss, if all seven previous tosses resulted in heads? Since coin flips are independent events, the probability is still going to be 0.5. Similarly in this case, the child births are independent. It does not matter if the couples stop giving birth after they have a baby-girl. The expected value is unchanged.\nFor example, consider five couples: C1, C2, C3, C4 and C5. If B-\u0026gt; Boy and G-\u0026gt; Girl. Using R\u0026rsquo;s sample(). For C1: {B, G} For C2: {G} For C3: {B, B, G} For C4: {B, G} For C5: {G}\nNow, ignore the couples and only consider the children. The children are {B, G, G, B, B, G, B, G, G}. The only thing happening here is simply the generation of either a B or a G with equal probability for each generation. At this point, it is quite obvious that the part that has to do with \u0026ldquo;couple\u0026hellip;.\u0026rdquo; in the question is to mislead and confuse similar to the \u0026ldquo;previous seven tosses..\u0026rdquo; example that I mentioned in the beginning of 2.\n3. Counting:\nIf we start with 512 couples (hence 512 first borns), half of them are going to have a girl as their first. Those couples will stop having children. Among, the other half couples who had a son as their first child, half of them are going to have a girl as their second child and so on. At every step there is an equal numbers of boys and girls. Therefore, the expected ratio is 1:1.\n","date":"2014-04-18T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/zorgania/","section":"posts","tags":["R","monte carlo","probability"],"title":"Republic of Zorgania"},{"categories":["pet projects"],"contents":"The Du Compiler: This is the naive/brute-force implementation of the Mandelbrot Set plotting. I just followed the algorithm.\n# Plotting the Mandelbrot Set # length of sequence for real and imaginary parts of complex numbers length \u0026lt;- 1000 # sequences for real and imaginary parts real = seq(-1.8,0.6, len=length) imaginary = seq(-1.2,1.2, len=length) result \u0026lt;- matrix(nrow = length, ncol = length) for (i in 1:length) { for (j in 1:length) { result[i,j]=inmandelbrotset(complex(real = real[i], imaginary = imaginary[j])) } } image(result, axes=FALSE) # function that checks if a point E mandelbrot set inmandelbrotset \u0026lt;- function(c) { dwell.limit \u0026lt;- 2048 z \u0026lt;- 0 for (i in 1:dwell.limit) { z \u0026lt;- z ** 2 + c if (Mod(z) \u0026gt; 2) { return(FALSE) } } return(TRUE) } Adding colors: We now have a Boolean matrix that records if a point is in the Mandelbrot Set. Since the matrix can only have two values : true or false, thus far, we have only been able to plot read and white images. The next step is to add colors such that we get more information on when a particular point escapes the radius of 2. Again, this is the naive/brute force way of doing it.\n# Mandelbrot Plotting with colors length \u0026lt;- 1000 real = seq(-2.0,2.0, len=length) imaginary = seq(-2.0,2.0, len=length) result \u0026lt;- matrix(nrow = length, ncol = length) dwell.limit \u0026lt;- 512 for (i in 1:length) { for (j in 1:length) { z \u0026lt;- 0 c \u0026lt;-complex(real = real[i], imaginary = imaginary[j]) for (k in 1:dwell.limit) { z \u0026lt;- z ** 2 + c if (Mod(z) \u0026gt; 2) { result[i,j]=k break } } } } set.seed(2) image(result,breaks=0:dwell.limit ,col=c(1,sample(terrain.colors (dwell.limit-1,alpha = .8))),asp=1,ax=F) and, just for the heck of it..\nASCII Mandelbrot Set using R (naive)\ns \u0026lt;- seq(-1.7,1.2, by =.1) a \u0026lt;- \u0026quot;\u0026quot; for (i in 1:length(s)) { for (j in 1:length(s)) { a\u0026lt;-cat(a,inmandelbrotset(complex(r = s[j], i = s[i]))) } a \u0026lt;- cat(a,\u0026quot;\\n\u0026quot;) } Achieved by returning a \u0026quot; \u0026quot; or \u0026ldquo;#\u0026rdquo; instead of FALSE or TRUE from function \u0026ldquo;inmandelbrotset\u0026rdquo;. A better algorithm Utilizing R\u0026rsquo;s easy to use lists in implementation:\n# more efficient algorithm to plot the Mandelbrot set sequence \u0026lt;- seq(-2,2,len=1000) dwell.limit \u0026lt;- 200 # matrix of points to be iterated complex.matrix \u0026lt;- t((sapply(sequence,function(x)x+1i*sequence))) in.mandelbrot.index \u0026lt;- 1:length(complex.matrix) iter=z=array(0,dim(complex.matrix)) for(i in 1:dwell.limit){ # complex quadratic polynomial function for all points z[in.mandelbrot.index]=complex.matrix[in.mandelbrot.index]+z[in.mandelbrot.index]^2 # boolean matrix result=Mod(z[in.mandelbrot.index])\u0026lt;=2 # if result is false, store the iteration iter[in.mandelbrot.index[!result]]=i # save all the index where points are still in the mandelbrot in.mandelbrot.index=in.mandelbrot.index[result] } set.seed(19) image(iter,main=paste(\u0026quot;Iterations: \u0026quot;, i, sep=\u0026quot; \u0026quot;), breaks=0:dwell.limit ,col=c(1,sample(rainbow (dwell.limit-1,alpha = .8))),ax=F, asp=1) Plotting the Julia set A little modification to the code above (red and white Mandelbrot) produces Julia Sets. The idea here is to set a constant C and send Z to the function instead of C.\nc \u0026lt;- complex(real=-0.1,imaginary=0.651) label \u0026lt;- toString(c) injulia \u0026lt;- function(z) { dwell.limit \u0026lt;- 128 for (i in 1:dwell.limit) { z \u0026lt;- z ** 2 + c if (Mod(z) \u0026gt; 2) { return(FALSE) } } return(TRUE) } Adding colors: This is achieved by following the same process as above.\nSierpinski Gasket using Chaos game\n#### Chaos game for generation of Sierpinski Gasket # 1. Take 3 points in a plane to form a triangle, you need not draw it. # 2. Randomly select any point inside the triangle and consider that your current position. # 3. Randomly select any one of the 3 vertex points. # 4. Move half the distance from your current position to the selected vertex. # 5. Plot the current position. # 6. Repeat from step 3 plot.new() iterations \u0026lt;- 2000 vertices \u0026lt;- matrix(c(0,0,0.5,1,1,0),3,2, byrow=T) current.point \u0026lt;- c(0.5,0.5) random.vertex \u0026lt;- sample(1:3,iterations,replace=T) plot.result = matrix(nrow=iterations,ncol=2) for (i in 1:iterations){ current.point \u0026lt;- (current.point+vertices[random.vertex[i],])/2 plot.result[i,] \u0026lt;- current.point } points(plot.result,pch = 46) Adding colors:\npoints(plot.result,pch = 46,col=c(13,3,41)[random.vertex]) ","date":"2014-04-15T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/fractal_plots/","section":"posts","tags":["mandelbrot","R","data viz"],"title":"Fractal Plots"},{"categories":["pet projects"],"contents":"\nFor quite some time now, my father and my uncle have been debating over the coordinates of their houses with respect to each others. Albeit barely 750 meters from each other, there is not much visible reference points or landmarks to figure it out accurately. The blame is on the molasses thick concrete jungle of Kathmandu valley. (Side-note: Kathmandu will soon be synonymous to the word asphyxiation). Although, I sincerely appreciate their curiosity, I think it is time to end this for once and for all.\nI have used photos of my sister (Ashma) and my cousin (Samip) as labels to the directions.\nThe app implements Canvas to \u0026ldquo;draw\u0026rdquo; the direction. The two GPS coordinates (obtained from Google Earth) were hard-coded into the program and Azimuth from orientation sensor was used to calculate the direction. Basically, it is a compass that points the direction from one house to the other instead of pointing North. Apart from this rather trivial implementation, the code can be modified to achieve some fun/interesting/useful developments. For instance, the direction of Mecca for Muslim prayers is one that comes to mind. Or, it could be modified into a bearing pointer app by using GPS data and some input EditTexts.\nCode Snippet onCreate\nprotected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); mCustomDrawableView = new CustomDrawableView(this); setContentView(mCustomDrawableView); // Register the sensor listeners Resources res = getResources(); samip = BitmapFactory.decodeResource(res, R.drawable.samip); ashma = BitmapFactory.decodeResource(res, R.drawable.ashma); // Fill in correct latitude and longitude currentLoc.setLatitude(0.000000); currentLoc.setLongitude(0.00000); currentLoc.setAltitude(00); destinationLoc.setLatitude(0.00000); destinationLoc.setLongitude(0.0000); mSensorManager = (SensorManager) getSystemService(SENSOR_SERVICE); accelerometer = mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER); magnetometer = mSensorManager.getDefaultSensor(Sensor.TYPE_MAGNETIC_FIELD); } Canvas\nFloat azimuth; public Bitmap samip; public Bitmap ashma; public class CustomDrawableView extends View { Paint compassAxis = new Paint(); Paint compassCircle = new Paint(); Paint compasAarrow = new Paint(); public CustomDrawableView(Context context) { super(context); compassAxis.setColor(0xff00ff00); compassAxis.setStyle(Style.STROKE); compassAxis.setStrokeWidth(2); compassAxis.setAntiAlias(true); compassCircle.setColor(0xff000000); compassCircle.setStyle(Style.STROKE); compassCircle.setStrokeWidth(10); compassCircle.setAntiAlias(true); compasAarrow.setColor(0xff0000ff); compasAarrow.setStyle(Style.STROKE); compasAarrow.setStrokeWidth(3); compasAarrow.setAntiAlias(true); }; protected void onDraw(Canvas canvas) { int width = getWidth(), height = getHeight(); int centerx = width / 2, centery = height / 2; /* * Drawing the axis and circle Being symmetrical, these don't need * to be rotated */ // Axis canvas.drawLine(centerx, 0, centerx, height, compassAxis); canvas.drawLine(0, centery, width, centery, compassAxis); // Circle canvas.drawCircle(centerx, centery, 200, compassCircle); /* * since this was a pretty small scope app, magnetic north was not * changed to real north. See: \u0026quot;GeomagneticField\u0026quot; */ // Used Float instead of float for this check if (azimuth != null) { // Converting radians to degrees float temp = (float) Math.toDegrees(azimuth); float bearing = currentLoc.bearingTo(destinationLoc); float direction = temp - bearing; canvas.rotate(-direction, centerx, centery); } canvas.drawLine(centerx, centery - 200, centerx, centery, compasAarrow); canvas.drawBitmap(samip, centerx + 5, centery - 200, compassAxis); canvas.drawBitmap(ashma, centerx + 5, centery - 15, compassAxis); } } Screenshots:\nAnd after some time wasting:\n","date":"2014-03-31T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/thelostbrothers/","section":"posts","tags":["android"],"title":"The Lost Brothers"},{"categories":["pet projects"],"contents":"Android+Arduino RC Car The previous attempt was in shambles due to the lack of proper products. This time, I got myself a Sainsmart L293D motor driver (actually a cloned version of the ever so popular Adafruit L293D driver) and a handy four wheel drive chassis. Hereupon, the only adjustment required was the use of analog pins as digital pins. This is the consequence of the motor driver using up all the digital pins and leaving no pins for the Bluetooth shield. I also hooked up the Arduino to a USB power bank.\nTools used: ~ Arduino Uno ~ 4WD Chassis ~ SainSmart motor driver (L293D) ~ Sunkee 30ft Bluetooth Module ~ Anker 15000 mAh power bank\nArduino\n//Project: Android RC Car //Author: Ayush Subedi #include \u0026lt;AFMotor.h\u0026gt; //import Adafruit Motor library #include \u0026lt;SoftwareSerial.h\u0026gt;// import the serial library SoftwareSerial newPorts(15, 17); // RX =15= A1, TX=17=A3 AF_DCMotor motor1(1, MOTOR12_1KHZ); // create motor #1, 1KHz pwm AF_DCMotor motor2(2, MOTOR12_1KHZ); // create motor #2, 1KHz pwm AF_DCMotor motor3(3, MOTOR34_1KHZ); // create motor #3, 1KHz pwm AF_DCMotor motor4(4, MOTOR34_1KHZ); // create motor #4, 1KHz pwm void setup() { newPorts.begin(9600); motor1.setSpeed(255); // set the speed to 200/255 motor2.setSpeed(255); // set the speed to 200/255 motor3.setSpeed(255); // set the speed to 200/255 motor4.setSpeed(255); // set the speed to 200/255 } void loop() { while (newPorts.available() \u0026gt; 0) { char ch = newPorts.read(); newPorts.println(newPorts.read()); executeReceivedCommand(ch); } } void executeReceivedCommand(char command) { switch (command) { //Forward case '0': motor1.run(FORWARD); motor2.run(FORWARD); motor3.run(FORWARD); motor4.run(FORWARD); break; //Reverse case '1': motor1.run(BACKWARD); motor2.run(BACKWARD); motor3.run(BACKWARD); motor4.run(BACKWARD); break; //Left : skid steering case '3': motor1.run(FORWARD); motor4.run(FORWARD); motor2.run(RELEASE); motor3.run(RELEASE); break; //Right : skid steering case '4': motor2.run(FORWARD); motor3.run(FORWARD); motor1.run(RELEASE); motor4.run(RELEASE); break; //Stall case '2': motor1.run(RELEASE); motor2.run(RELEASE); motor3.run(RELEASE); motor4.run(RELEASE); break; } } In a nutshell, the Android device sends Char type to Arduino which is used to rotate the motors to maneuver towards a desired direction.\nForward: 0 Reverse: 1 Stop: 2 Left: 3 Right: 4\nThe car turns left and right by implementing skid steering.\nPictures and Video:\n","date":"2014-01-31T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/rc_car/","section":"posts","tags":["android","arduino"],"title":"Android+Arduino RC Car"},{"categories":["pet projects"],"contents":"Snakes and Ladders This game needs no introduction. Although there are several variations of this game (mainly on position of the snakes and ladders), the rules are generally the same. Some of the rules, (which are part of my algorithm) are mentioned below. Also, this game requires no skill as it solely depends on luck/probability. This allows several mathematical questions relating to this game be answered using Monte Carlo simulation. However, for this post, I will only be investigating from an analytic/subjective stand point. That is, I will be using Markov Chain to answer those questions. Markov Chain Wikipedia link\nThe game\u0026rsquo;s Markov property or memorylessness allows us to use Markov Chain. That is, the probability of occurrence of next event only depends on current event and not on any other events that occurred before. An example from our game: It does not matter if the player reached square 31 using the ladder from square 9, or by traversing the hard way around. Once the player is in 31, the probability of getting to square 32 does not depend on the \u0026ldquo;past\u0026rdquo;. Transition matrix Wikipedia link Transition matrix is a 2 dimensional array that encapsulates state transitional probabilities. For example, a transition matrix P, given the following information (Mathematical model of one dimensional random walk),\nwould be,\nTransition matrix for Snakes and Ladders Trivial Transition Matrix\nFirst of all, consider a case where there are no snakes or ladders. Let us call this our Trivial (for the lack of words) scenario.\nThis definitely makes for a boring game. However, it helps with the math. First of all, the square 0 is the position the player is before the game starts. Now, since we are using a cubic die, on the first roll, the probability of going from 0 to 1, 0 to 2, 0 to 3, 0 to 4, 0 to 5, and 0 to six is 1/6. Since we are not accounting for snakes or ladders, the probability of going from box i to boxes i+1, i+2, i+3, i+4, i+5, and i+6 are all going to be 1/6, unless we run out of space. That is, if we are at 97, the person can only move to next step if the die rolls 1, 2 or 3. For anything greater, the person will not go to the next step. Therefore, in this case, the probabilities are: 97 to 98 = 1/6, 97 to 99 = 1/6, 97 to 100 = 1/6 and 97 to 97 = 3/6.\nBuilding the Trivial transition matrix based on the aforementioned rule.\nimport Jama.Matrix; /** * @return 101X101 Transition Matrix for case: Trivial */ public Matrix trivialMatrix() { int difference, playerPosition = 0, matrixSize=101; double transitionM[][] = new double\\[matrixSize\\][matrixSize]; double probability = 1.0 / 6; for (playerPosition = 0; playerPosition \u0026lt; transitionM.length; playerPosition++) { for (int i = 1; i \u0026lt;= 6; i++) { if ((difference = matrixSize - playerPosition) \u0026lt;= 6) { for (int k = 1; k \u0026lt; difference; k++) { transitionM\\[playerPosition\\][playerPosition + k] = probability; } transitionM\\[playerPosition\\][playerPosition] = (6 - difference + 1) * probability; } else { transitionM\\[playerPosition\\][playerPosition + i] = probability; } } } return new Matrix(transitionM); } Top-Left\nBottom-Right\nNon-Trivial Transition Matrix\nSnakes:\n98 ~ 78, 95 ~ 75, 93 ~ 73, 87 ~ 24, 64 ~ 60, 62 ~ 19, 56 ~ 53, 49 ~ 11, 48 ~ 26, 16 ~ 6\nLadders:\n1 ~ 38, 4 ~ 14, 9 ~ 31, 21 ~ 42, 28 ~ 84, 36 ~ 44, 51 ~ 67, 71 ~ 91, 80 ~ 100\nI decided to use a simple List implementation for this. This might most likely be the Brute-Force implementation (I know several ways to make it better but none to make it worse). One way to make it more efficient would be to use 82 by 82 matrix instead of 101 by 101.\nThe advantage of using this implementation over the 82 by 82 matrix (apart from easy implementation) is that this method can be used for any snakes and ladders board variation. It also allows us to check for some hypothetical cases or answer more important questions. Eg: What is the best way to position snakes and ladders for maximum thrill to a player?\nBuilding the non-Trivial transition matrix\n/** * @return 101X101 Transition Matrix for case: non-Trivial */ public Matrix nonTrivialMatrix() { int playerPosition,matrixSize=101,difference; List\u0026lt;Integer\u0026gt; from = Arrays.asList(1, 4, 9, 21, 28, 36, 51, 71, 80, 98, 95, 93, 87, 64, 62, 56, 49, 48, 16); List\u0026lt;Integer\u0026gt; to = Arrays.asList(38, 14, 31, 42, 84, 44, 67, 91, 100, 78, 75, 73, 24, 60, 19, 53, 11, 26, 6); double probability = 1.0 / 6; double transitionM[][] = new double\\[matrixSize\\][matrixSize]; for (playerPosition = 0; playerPosition \u0026lt; transitionM.length; playerPosition++) { if (!from.contains(playerPosition)) { for (int i = 1; i \u0026lt;= 6; i++) { if ((difference = 6 - playerPosition) \u0026lt;= 6) { for (int k = 1; k \u0026lt; difference; k++) { if (from.contains(playerPosition + k)) { transitionM\\[playerPosition\\][to.get(from.indexOf(playerPosition + k))] = probability; } else { transitionM\\[playerPosition\\][playerPosition + k] = probability; } } if (from.contains(playerPosition)) { transitionM\\[playerPosition\\][to.get(from.indexOf(playerPosition))] = (6 - difference + 1) * probability; } else { transitionM\\[playerPosition\\][playerPosition] = (6 - difference + 1) * probability; } } else { if (from.contains(playerPosition + i)) { transitionM\\[playerPosition\\][to.get(from.indexOf(playerPosition + i))] = transitionM\\[playerPosition\\][to.get(from.indexOf(playerPosition + i))] + probability; } else { transitionM\\[playerPosition\\][playerPosition + i] = transitionM\\[playerPosition\\][playerPosition + i] + probability; } } } } } return new Matrix(transitionM); } Top-Left\nBottom-Right\nProbability Vector Wikipedia link\nProbability vectors\nrepresents the probability of being on a certain square after n dice rolls. It is a vector with non-negative entries that add up to one.\nimplies that the probability of being on square 0 is 1. This is our input.\n\u0026hellip;\u0026hellip;\nIf P is the Trivial transition matrix,\nIf P is the non-Trivial transition matrix.\nBuilding the Probability Vector\n/** * @return Probability Vector with 1 being the first element */ public Matrix probabilityVector() { double probabilityV[] = new double[101]; probabilityV[0] = 1; return new Matrix(probabilityV, 1); } Question 1: Probability of being on square s after n dice rolls: Using Vn-1 * P = Vn\n/** * @param transitionMatrix * @param probabilityVector * @param diceRolls */ public static void squareProbability(Matrix transitionMatrix, Matrix probabilityVector, int diceRolls) { NumberFormat nf = NumberFormat.getInstance(); nf.setMinimumFractionDigits(20); System.out.println(\u0026quot;Dice rolls: \u0026quot;+diceRolls); for (int i = 1; i \u0026lt;= diceRolls; i++) { probabilityVector = probabilityVector.times(transitionMatrix); } probabilityVector.print(nf, 3); } Some outputs for the Trivial matrix: I used html tables to simulate the board (basically printed the html tags within java code).\nSome outputs for the non-Trivial matrix:\nQuestion 2: Minimum length of a game and occurrence probability That is, after how many n, is the probability at square 100 greater than 0 for the first time?\nFor the trivial case, the answer is, ceiling of 100/6 = 17.\nFor the non-trivial case,\n/** * @param transitionMatrix * @param probabilityVector */ public static void gameCompletion(Matrix transitionMatrix, Matrix probabilityVector) { NumberFormat nf = NumberFormat.getInstance(); nf.setMinimumFractionDigits(30); int box = 100; int diceRolls = 0; while (probabilityVector.get(0, box) == 0) { diceRolls++; probabilityVector = probabilityVector.times(transitionMatrix); } System.out.println(\u0026quot;The game can be completed in min of \u0026quot; + diceRolls + \u0026quot; dice rolls.\u0026quot;); System.out.println(\u0026quot;Probability of it happening: \u0026quot; + nf.format(probabilityVector.get(0, box))); } Results:\nNon-Trivial Matrix\nThe game can be completed in min of 7 dice rolls. Probability of it happening: 0.001564643347050754000000000000 Rolls of {4,6,6,2,6,4,6} is one shortest solution. However, in theory, the game could last forever. Therefore, there is no longest game.\nTrivial Matrix\nThe game can be completed in min of 17 dice rolls. Probability of it happening: 0.000000000009038995585604526000 Rolls of {6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6} is the only shortest solution. Longest game in this case is 100 dice rolls. The player will have to roll 1, 100 consecutive times from start. Question 3: Expected length of a game: Let subStochasticMatrix be the 100 by 100 matrix obtained by deleting the last row and column of the transition matrix. Also, let I be 100 by 100 identity matrix. Let inverse be the inverse of the difference of I and subStochasticMatrix. The expected number of rolls is given by the sum of entries in top row of the matrix inverse.\n/** * @param transitionMatrix * @param probabilityVector */ public static void expectedLength(Matrix transitionMatrix, Matrix probabilityVector){ Matrix subStochasticMatrix=transitionMatrix.getMatrix(0, 99, 0 ,99); Matrix I = Matrix.identity(100,100); Matrix inverse = (I.minus(subStochasticMatrix)).inverse(); double sum=0; for (int i =0;i\u0026lt;=99;i++){ sum=sum+inverse.get(0, i); } System.out.println(\u0026quot;Expected game length: \u0026quot;+sum); } Results:\nNon-Trivial Matrix\nExpected game length: 39.59836564020812 Trivial Matrix\nExpected game length: 33.33333333333334 ","date":"2013-12-02T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/snakesandladders/","section":"posts","tags":["markov chains","probability","java"],"title":"Snakes and Ladders"},{"categories":["capstone project"],"contents":"The Du Compiler: For our Computer Science capstone project, we take on the beast - building a compiler from scratch. We had about three months to do it but we really had no clue of what we were doing for the first month and a half. Although we knew plenty of what there was to know about compilers in theory, we soon realized that building one from scratch was not going to be easy. We started with baby steps, and by the time the project was due, we were taking giant leaps (we had no other option). In the end, it turned out to be a reasonably fine compiler and an excellent experience. During these three months, I had the best and the worst experiences of my academic life. Therefore, this project is really special.\nSince this project belongs to two other people also, I won\u0026rsquo;t be posting any source code. This post will just have some example code for Du-Compiler.\nFinally, for anyone looking for a reasonably challenging senior projects, I would strongly encourage building a compiler. I say this for couple of reasons. First of all, it changes the way we look at code. It gives a better understanding of what exactly happens when the compile button is hit. In another words, it makes you aware of what is going on internally. Secondly, it involves learning/relearning several Computer Science topics such as regular expressions, hash table, data-structures, tree traversals, assembly level programming etc. It also involves A LOT of coding. When it all pans out, you are going to love what you have in your skill set.\nExamples //Hello World duhawk helloworld{ duPrint(%Hello World%); } //Simple Addition duhawk simpleAdd{ int a; a=5; int b; b = a + 5; duPrint b; } //Result: 10 //Simple Pattern duhawk test{ int i; i=1; int j; j=1; while (i\u0026lt;=10){ j=1; while (j\u0026lt;=i){ duPrint(%*%); j=j + 1; } duPrintln(%%); i= i + 1; } } /* Result * ** *** **** ***** ****** ******* ******** ********* ********** */ duhawk test{ int c; int d; duPrint (%Multiplication table of: %); duInput a; duPrint (%upto: %); duInput b; for (c=1;c\u0026lt;=b;c=c + 1){ duPrint a; duPrint (% X %); duPrint c; duPrint (% = %); d = a*c; duPrintln d; } } /* Input for a = 19 Input for b = 15 Result: Multiplication table of: 19 upto: 15 19 X 1 = 19 19 X 2 = 38 19 X 3 = 57 19 X 4 = 76 19 X 5 = 95 19 X 6 = 114 19 X 7 = 133 19 X 8 = 152 19 X 9 = 171 19 X 10 = 190 19 X 11 = 209 19 X 12 = 228 19 X 13 = 247 19 X 14 = 266 19 X 15 = 285 */ ","date":"2013-09-05T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/ducompiler/","section":"posts","tags":["compiler","java","capstone"],"title":"Compiler from Scratch"},{"categories":["capstone project"],"contents":"Area of the Mandelbrot Set: The area of the Mandelbrot Set The Mandelbrot set is a fractal (illustrates self-similarity). The set is obtained from the quadratic recurrence equation,\nwith\n, where points\nin the complex plane for which the orbit of\ndoes not tend to infinity are in the set. The Mandelbrot set is a compact set, contained in the closed disk of radius 2 around the origin. Since it is contained in a disk of known area, it is possible to approximate the area of the Mandelbrot Set\u0026rsquo;s using Monte Carlo method.\nJava: Since Java does not inherently understand complex numbers, a \u0026ldquo;real\u0026rdquo; approach will be applied to perform the quadratic recurrence equation,\nFirst, as shown in the figure above, inscribe the disk in a square of length 4 units. Let\nrepresent the coordinate along x-axis (real) and\nrepresent the coordinate along y axis. Now set\nand\n, where\nand\nare randomly generated real numbers from [-2, 2]. Basically, the\nand\ncoordinates are being duplicated at this step to preserve the point\n. Next, iteratively compute the following from\n(for programming purposes, choose a large Dwell Limit). Simultaneously, check if\n. If yes, increase count (not in the set) by 1 and get out of the loop (since all points should be contained in the disk).\nCompute the ratio of number of points that are in the set to total number of points used. Then multiply the area of square (16 units square) to get the approximate are of the Mandelbrot set.\nJava Code:\nimport java.util.Date; import java.util.Random; public class MandelbrotArea { public static int mcRep = 5000; public static int dwellLimit = 2048; /** * @return random double in [-2,2] */ public static double random() { return (new Random().nextDouble() * 4) - 2; } /** * @param r: real part of the complex number * @param s: imaginary part of the complex number * @return */ public static boolean isMandelbrotSet(double r, double s) { double a = r, b = s, temp; // Iterative function for (int j = 1; j \u0026lt;= dwellLimit; j++) { temp = a; a = Math.pow(a, 2) - Math.pow(b, 2) + r; b = (2 * temp * b) + s; if (Math.pow(a, 2) + Math.pow(b, 2) \u0026gt; 4) { return false; } } return true; } public static void main(String[] args) { long startTime = new Date().getTime(); long count = 0; for (int i = 0; i \u0026lt;= mcRep; i++) { if (isMandelbrotSet(random(), random())) { count++; } } System.out.println(\u0026quot;Input -\u0026gt; DwellLimit: \u0026quot; + dwellLimit + \u0026quot;, McRep: \u0026quot; + mcRep); System.out.println(\u0026quot;Area: \u0026quot; + ((double) (count * 16)) / mcRep); System.out.println(\u0026quot;Execution time: \u0026quot; + (new Date().getTime() - startTime) + \u0026quot; ms\u0026quot;); } } Result:\nInput -\u0026gt; DwellLimit: 2048, McRep: 5000 Area: 1.5136 Execution time: 389 ms R\nmonte.Carlo \u0026lt;- 5000 x \u0026lt;- runif(monte.Carlo, -2, 2) y \u0026lt;- runif(monte.Carlo, -2, 2) list \u0026lt;- numeric(monte.Carlo) for (j in 1:monte.Carlo){ list[j] \u0026lt;- if (inmandelbrotset(complex(real = x[j], imaginary = y[j]))) 1 else 0 } area\u0026lt;-mean(list)*16 # function that checks if a point E mandelbrot set inmandelbrotset \u0026lt;- function(c) { dwell.limit \u0026lt;- 2048 z \u0026lt;- 0 for (i in 1:dwell.limit) { z \u0026lt;- z ** 2 + c if (Mod(z) \u0026gt; 2) { return(FALSE) } } return(TRUE) } ","date":"2013-01-02T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/mandelbrot_area/","section":"posts","tags":["mandelbrot","java","monte carlo"],"title":"Area of the Mandelbrot Set"},{"categories":["capstone project"],"contents":"The Faro Shuffle: A Faro shuffle is probably one of the most intricate skills in an elite magician\u0026rsquo;s arsenal. Although it\u0026rsquo;s simple in concept, it is extremely difficult to perform and it typically needs years and years of practice. The idea of a shuffle is to randomize the cards, and since a typical shuffle is sloppy, that is usually true. However, a Faro shuffle is not random at all. In a perfect shuffle or a Faro shuffle the cards are divided exactly in half (top and bottom) and then interspersed alternately. Twenty one consecutive (out) Faro shuffles for a 50-card deck. However, it only takes eight consecutive Faro shuffles to bring a 52-card deck back to its original order. Interesting. Java: If the number of cards in the deck is 12,\npublic class outFaro { public static void main(String[] args) { // the number of card in the deck int numberofcardindeck = 12; // creating an array of cards int[] deck = new int[numberofcardindeck]; // printing the order of cards in original deck System.out.println(\u0026quot;Original deck\u0026quot;); for (int i = 0; i \u0026lt; numberofcardindeck; i++) { deck[i] = i + 1; System.out.print(deck[i] + \u0026quot; \u0026quot;); } System.out.println(); // dividing the cards in half int[] firsthalfdeck = new int[numberofcardindeck / 2]; int[] secondhalfdeck = new int[numberofcardindeck / 2]; // performing a (out) Faro shuffle for (int Faro = 1; Faro \u0026lt;= 50; Faro++) { for (int i = 0; i \u0026lt; numberofcardindeck / 2; i++) { firsthalfdeck[i] = deck[i]; secondhalfdeck[i] = deck[i + (numberofcardindeck / 2)]; } for (int i = 0; i \u0026lt; numberofcardindeck / 2; i++) { deck[2 * i] = firsthalfdeck[i]; deck[2 * i + 1] = secondhalfdeck[i]; } System.out.println(); System.out.println(\u0026quot;Order under Faro Shuffle: \u0026quot; + Faro); for (int i = 0; i \u0026lt; numberofcardindeck; i++) { System.out.print(deck[i] + \u0026quot; \u0026quot;); } // done when the second card comes back to its original position if (deck[1] == 2) { break; } } } } Result:\nOriginal deck 1 2 3 4 5 6 7 8 9 10 11 12 Order under Faro Shuffle: 1 1 7 2 8 3 9 4 10 5 11 6 12 Order under Faro Shuffle: 2 1 4 7 10 2 5 8 11 3 6 9 12 Order under Faro Shuffle: 3 1 8 4 11 7 3 10 6 2 9 5 12 Order under Faro Shuffle: 4 1 10 8 6 4 2 11 9 7 5 3 12 Order under Faro Shuffle: 5 1 11 10 9 8 7 6 5 4 3 2 12 Order under Faro Shuffle: 6 1 6 11 5 10 4 9 3 8 2 7 12 Order under Faro Shuffle: 7 1 9 6 3 11 8 5 2 10 7 4 12 Order under Faro Shuffle: 8 1 5 9 2 6 10 3 7 11 4 8 12 Order under Faro Shuffle: 9 1 3 5 7 9 11 2 4 6 8 10 12 Order under Faro Shuffle: 10 1 2 3 4 5 6 7 8 9 10 11 12 After 10 out-Faro shuffles, the card returns to its original permutation. Therefore, 10 is the order of the permutation.\nThe table below shows number of cards in a deck and the number of out-Faro shuffles required to bring it back to its original permutation.\nThe definition of permutation and permutation group:\n“A permutation of a set A is a function from A to A that is both one-to-one and onto. A permutation group of a set A is a set of permutations of A that forms a group under function composition.”\nA Faro Shuffle is one-to-one and onto. At any frequency of the shuffle, every index (1-n) will have a unique card. Also, for every card, there is a unique index regardless of the frequency of the shuffle. The permutation for every “Number of cards” can be thought as a group under the operation - FaroShuffle. Also, since “Number of Faro Shuffles” returns us back to the original permutation, we can establish it as the order of the permutation group.\nTheorem: The order of a permutation of a finite set written in disjoint cycle form is the least common multiple of the lengths of the cycles.The order of a permutation of a finite set written in disjoint cycle form is the least common multiple of the lengths of the cycles.\nThis is one of the most important Group Theory Theorems. Let us consider a 52-card deck to demonstrate this. From the Java code, the order is 8 when n is 52. Instead of looking at disjoint cycle form of the permutation to figure out the order of each element, we can also look at every permutation the original permutation has been to before returning to the original permutation. For n = 52, the table below shows all the permutations.\nFrom the table above, order of 1 and 52 = 1, order of 18 and 35 = 2 and order of rest of the elements = 8 The L.C.M (1,2,8) = 8 is the order of the group according to the Theorem. This is also what we got from the Java code.\nConjectures / Proofs:\nIn-Faro and Out-Faro In an out-Faro shuffle, the top card from first half of the deck always remains on top. In an in-Faro shuffle, the top card from second half will be the new top card of the new shuffled deck.\nSince I did not have a deck of card with me, and I was bored, I wrote a very simple Android app to demonstrate in and out shuffle.\nGenerating a random deck with 4 cards. Notice that the Eight of Hearts is on top.\nOut-Faro 1: Eight of Hearts is on top.\nOut-Faro 2: Eight of Hearts is still on top and the deck is back in its original permutation. Therefore, order = 2.\nIn-Faro 1: Using the same deck, Eight of Hearts is no longer on top.\nIn-Faro 2: Again\u0026hellip;\nIn-Faro 3:\nIn-Faro 4: Back to its original order. Order = 4.\nMore screenshots:\nAnother reason for writing the app was to see the connection between Binary Number System and Faro Shuffle. I read somewhere that magicians and gamblers use this for their advantage.\nConsider a random eight card deck.\nNow, the trick is, to send the Queen of Clubs to say for example, 7th position in the deck (6 cards on top of it), the gambler would perform two in-Faros and one out-Faro. 6 in binary is 110; so, for digit 1, the magician would perform an in-Faro and for 0, an out-Faro.\n1st in-Faro:\n2nd in-Faro:\nout-Faro:\nQueen of Clubs is now in the 7th position. Also, its really interesting that this works irrespective of the number of cards in the deck.\nI also found that the order of (2n+2) out-Faro shuffle is equal to the order of (2n) in-Faro shuffles.\nI find that pretty interesting and I do not really see that to be obvious. It also implies that for no 2n, In-Faro=Out-Faro (I think). I will be investigating on these more later.\n","date":"2012-12-05T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/faro_shuffle/","section":"posts","tags":["faro","java","group theory"],"title":"The Faro Shuffle"},{"categories":null,"contents":"Actively seeking leadership roles in Analytics, Data Science or Machine Learning. ","date":"0001-01-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/resume/","section":"","tags":null,"title":"Resume"}]