[{"categories":["wip"],"contents":"Special Topics on High-Dimensional Data Analytics Functional Data Analysis Image Analysis Tensor Data Analysis Optimization Application Regularization Functional Data Analysis Regression - Least square Estimates Polynomial Regression Splines Order-M Splines B-Splines Smoothing Splines Natural Cubic Splines K-Nearest Neighbor Kernel Smoother Regression RBF Kernel Functional PCA Image Analysis Transformation Convolution Convolution with a Mask Segmentation K-means clustering Edge detection using derivatives Sobel Operator Kirsch Operator Prewitt Mask and Gaussian Mask. Tensor Data Analysis Tensor data analysis Outer product Inner product Kronecker product Khatri-Rao product Hadamard product Defining tensor ranks Rank one tensor Candecomp/parafac (CP) decomposition Tucker decomposition Optimization Regularization Ridge Lasso Non-negative Adaptive Lasso Group Lasso ","date":"2023-07-27T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/topics_on_hdda/","section":"posts","tags":["wip"],"title":"Special Topics on High-Dimensional Data Analytics"},{"categories":["georgia-tech","capstone-project"],"contents":"As a part of Georgia Tech OMSA, MGT 6203: Data Analytics For Business Presentation Recording Tableau Dashboard Presentation Slides Final Report ","date":"2023-07-26T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/school_satisfaction_and_school_ranking/","section":"posts","tags":["correlation","Random Forest","XGBoost","analytics","group-project","machine-learning","random-forest","data-viz"],"title":"Enhancing Decision-Making for Parents and Authorities, A Comprehensive Analysis and Mapping of School Performance in New York City"},{"categories":["gatech","chatgpt"],"contents":"Machine Learning for Trading Machine learning plays a vital role in trading by enabling the analysis of vast amounts of financial data and the development of predictive models. It leverages algorithms and statistical techniques to identify patterns, make predictions, and generate insights for informed trading decisions. Machine learning algorithms can be applied to various aspects of trading, including price prediction, risk management, portfolio optimization, market analysis, and automated trading. By leveraging machine learning, traders can uncover hidden patterns in data, adapt to changing market conditions, and improve decision-making processes, ultimately aiming to achieve better trading performance and profitability.\nSections Manipulating Financial Data Computational Investing Learning algorithms for Trading Manipulating Financial Data Pandas Pandas is a popular Python library that provides powerful data manipulation and analysis tools. It\u0026rsquo;s widely used for working with various types of data, including stock data analysis.\nImporting Pandas:\nimport pandas as pd Loading Data: Load the stock data into a Pandas DataFrame. There are various ways to load data, such as reading from a CSV file or querying an API. Here\u0026rsquo;s an example of loading data from a CSV file:\ndf = pd.read_csv(\u0026#39;stock_data.csv\u0026#39;) Exploring Data: Use various Pandas functions to explore and understand the data. Some commonly used functions include head(), tail(), info(), describe(), and shape.\ndf.head() # Display the first few rows of the DataFrame df.info() # Get information about the DataFrame df.describe() # Statistical summary of the data df.shape # Get the number of rows and columns in the DataFrame Data Cleaning: Perform any necessary data cleaning steps, such as handling missing values, removing duplicates, and converting data types.\ndf.dropna() # Drop rows with missing values df.drop_duplicates() # Remove duplicate rows df[\u0026#39;date\u0026#39;] = pd.to_datetime(df[\u0026#39;date\u0026#39;]) # Convert the \u0026#39;date\u0026#39; column to datetime Data Manipulation: Pandas functions can be used to manipulate the data according to any analysis requirements. It can be used to filter rows, select specific columns, create new columns, apply mathematical operations, and more.\n# Selecting specific columns df[[\u0026#39;date\u0026#39;, \u0026#39;close_price\u0026#39;]] # Filtering rows df[df[\u0026#39;volume\u0026#39;] \u0026gt; 1000000] # Creating new columns df[\u0026#39;returns\u0026#39;] = df[\u0026#39;close_price\u0026#39;].pct_change() # Applying mathematical operations df[\u0026#39;moving_average\u0026#39;] = df[\u0026#39;close_price\u0026#39;].rolling(window=20).mean() Data Visualization: Pandas can work well with other libraries like Matplotlib or Seaborn to create visualizations of the stock data.\nimport matplotlib.pyplot as plt df.plot(x=\u0026#39;date\u0026#39;, y=\u0026#39;close_price\u0026#39;, title=\u0026#39;Stock Price\u0026#39;) plt.show() These are just a few examples of how Pandas can be used for stock data analysis. Pandas provides a wide range of functions and methods that can be used to manipulate, analyze, and visualize stock data effectively.\nSlicing and indexing Pandas provides several methods for slicing and indexing data in a DataFrame. Here are some commonly used techniques for slicing data with Pandas:\nColumn Selection: To select a single column, the square bracket notation with the column name as a string can be used: df[\u0026#39;column_name\u0026#39;] To select multiple columns, provide a list of column names within the square brackets: df[[\u0026#39;column_name1\u0026#39;, \u0026#39;column_name2\u0026#39;, ...]] Row Selection: To select rows based on a specific condition, use boolean indexing: df[condition] For example, to select rows where the \u0026lsquo;price\u0026rsquo; column is greater than 100: df[df[\u0026#39;price\u0026#39;] \u0026gt; 100] Slicing Rows: To slice rows based on their position, use the loc or iloc accessor: loc is label-based and inclusive of the endpoints. iloc is index-based and exclusive of the endpoints. For example, to slice the first five rows: df.iloc[0:5] # Exclusive of the endpoint To slice rows by labels, use: df.loc[start_label:end_label] # Inclusive of the endpoints Slicing Rows and Columns: To slice both rows and columns simultaneously, use the loc or iloc accessor with row and column selections separated by a comma: df.loc[start_label:end_label, [\u0026#39;column_name1\u0026#39;, \u0026#39;column_name2\u0026#39;, ...]] df.iloc[start_index:end_index, [column_index1, column_index2, ...]] For example, to slice the first five rows and select columns \u0026lsquo;price\u0026rsquo; and \u0026lsquo;volume\u0026rsquo;:\ndf.iloc[0:5, [\u0026#39;price\u0026#39;, \u0026#39;volume\u0026#39;]] Numpy Numpy is a fundamental Python library that provides efficient numerical computing capabilities. It offers a powerful array data structure and a wide range of mathematical functions, making it useful for financial research and analysis. Here are some key points about Numpy focused on its application in financial research:\nNumerical Data Handling: Numpy provides the ndarray (N-dimensional array) data structure, which is highly efficient for handling large volumes of numerical data. It allows for fast element-wise operations and supports various numerical data types, including integers, floating-point numbers, and complex numbers. Array Creation and Manipulation: Numpy offers functions to create and manipulate arrays, such as np.array(), np.zeros(), np.ones(), np.arange(), and np.linspace(). These functions are beneficial for creating arrays representing financial data, such as price series, returns, or volume data. Mathematical Operations: Numpy provides a comprehensive set of mathematical functions and operators that can be applied to arrays. These include basic arithmetic operations, statistical functions (mean(), std(), min(), max(), etc.), linear algebra functions (dot(), inv(), eig(), etc.), and more advanced functions for trigonometry, exponentials, logarithms, and random number generation. These operations can be leveraged to perform calculations on financial data efficiently. Data Aggregation and Summary Statistics: Numpy functions are helpful for calculating summary statistics on financial data. Functions like np.sum(), np.mean(), np.std(), np.median(), and np.percentile() allow you to calculate aggregate measures, central tendency, dispersion, and percentiles on arrays or subsets of data. Time Series Analysis: Numpy provides tools for working with time series data, including date and time handling. The np.datetime64 data type enables storing and manipulating date and time values, allowing for easy handling of temporal aspects in financial research. Broadcasting and Vectorization: Numpy\u0026rsquo;s broadcasting feature allows for performing element-wise operations between arrays of different shapes and sizes, making it efficient for vectorized calculations. This feature is particularly useful when working with arrays representing financial data, as it enables applying operations across entire arrays without explicit looping. Integration with Other Libraries: Numpy plays a vital role in the scientific Python ecosystem and integrates well with other libraries commonly used in financial research. For example, Numpy arrays can be seamlessly used with Pandas DataFrames, providing efficient data processing and analysis capabilities. By leveraging Numpy\u0026rsquo;s capabilities, financial researchers can efficiently handle and analyze large datasets, perform mathematical computations, calculate summary statistics, and conduct time series analysis. Its fast execution and integration with other libraries make it a valuable tool for financial research and analysis.\nGlobal Statistics To calculate global statistics of stock prices in Python, you can use the Pandas library to load and manipulate stock price data. Here\u0026rsquo;s an example of how you can calculate common statistics such as mean, standard deviation, minimum, maximum, and percentiles for stock prices: -Import the necessary libraries: import pandas as pd import numpy as np Load the stock price data into a Pandas DataFrame. Assuming you have a CSV file named \u0026lsquo;stock_prices.csv\u0026rsquo; with a \u0026lsquo;price\u0026rsquo; column containing the stock prices, you can use the following code: df = pd.read_csv(\u0026#39;stock_prices.csv\u0026#39;) Calculate the desired statistics using Numpy functions on the \u0026lsquo;price\u0026rsquo; column: mean_price = np.mean(df[\u0026#39;price\u0026#39;]) std_price = np.std(df[\u0026#39;price\u0026#39;]) min_price = np.min(df[\u0026#39;price\u0026#39;]) max_price = np.max(df[\u0026#39;price\u0026#39;]) percentiles = np.percentile(df[\u0026#39;price\u0026#39;], [25, 50, 75]) Print or use the calculated statistics as needed: print(\u0026#34;Mean price:\u0026#34;, mean_price) print(\u0026#34;Standard deviation:\u0026#34;, std_price) print(\u0026#34;Minimum price:\u0026#34;, min_price) print(\u0026#34;Maximum price:\u0026#34;, max_price) print(\u0026#34;25th, 50th, and 75th percentiles:\u0026#34;, percentiles) Rolling Statistics To calculate rolling statistics for stock prices in Python, you can use the rolling window functionality provided by Pandas. Here\u0026rsquo;s an example of how you can calculate rolling mean and standard deviation for stock prices:\nImport the necessary libraries: import pandas as pd import numpy as np Load the stock price data into a Pandas DataFrame. Assuming you have a CSV file named \u0026lsquo;stock_prices.csv\u0026rsquo; with a \u0026lsquo;price\u0026rsquo; column containing the stock prices, you can use the following code: df = pd.read_csv(\u0026#39;stock_prices.csv\u0026#39;) Convert the date column to a datetime type if it is not already in that format: df[\u0026#39;date\u0026#39;] = pd.to_datetime(df[\u0026#39;date\u0026#39;]) Sort the DataFrame by the date column in ascending order: df = df.sort_values(\u0026#39;date\u0026#39;) Calculate the rolling mean and standard deviation using the rolling() function on the \u0026lsquo;price\u0026rsquo; column: window_size = 20 # Define the rolling window size df[\u0026#39;rolling_mean\u0026#39;] = df[\u0026#39;price\u0026#39;].rolling(window=window_size).mean() df[\u0026#39;rolling_std\u0026#39;] = df[\u0026#39;price\u0026#39;].rolling(window=window_size).std() In the code above, window_size represents the number of observations to include in each rolling window. You can adjust it based on your specific requirements.\nPrint or use the rolling statistics as needed: print(df[[\u0026#39;date\u0026#39;, \u0026#39;price\u0026#39;, \u0026#39;rolling_mean\u0026#39;, \u0026#39;rolling_std\u0026#39;]]) This code will display the \u0026lsquo;date\u0026rsquo;, \u0026lsquo;price\u0026rsquo;, \u0026lsquo;rolling_mean\u0026rsquo;, and \u0026lsquo;rolling_std\u0026rsquo; columns of the DataFrame, showing the calculated rolling statistics.\nBy applying these steps, you can calculate rolling statistics, such as the rolling mean and standard deviation, for stock prices using Python and Pandas. Feel free to modify the code to incorporate additional rolling statistics or customize the output to suit your needs.\nBollinger bands Bollinger Bands is a popular technical analysis tool used to identify potential price trends and volatility in financial markets. It consists of three lines plotted on a price chart: the middle band (usually a simple moving average), an upper band (typically two standard deviations above the middle band), and a lower band (usually two standard deviations below the middle band). Here\u0026rsquo;s an example of how you can calculate and plot Bollinger Bands using Python and Pandas:\nImport the necessary libraries: import pandas as pd import numpy as np import matplotlib.pyplot as plt Load the stock price data into a Pandas DataFrame. Assuming you have a CSV file named \u0026lsquo;stock_prices.csv\u0026rsquo; with a \u0026lsquo;price\u0026rsquo; column containing the stock prices, you can use the following code: df = pd.read_csv(\u0026#39;stock_prices.csv\u0026#39;) Calculate the middle band, upper band, and lower band using rolling mean and standard deviation: window_size = 20 # Define the rolling window size df[\u0026#39;middle_band\u0026#39;] = df[\u0026#39;price\u0026#39;].rolling(window=window_size).mean() df[\u0026#39;std\u0026#39;] = df[\u0026#39;price\u0026#39;].rolling(window=window_size).std() df[\u0026#39;upper_band\u0026#39;] = df[\u0026#39;middle_band\u0026#39;] + 2 * df[\u0026#39;std\u0026#39;] df[\u0026#39;lower_band\u0026#39;] = df[\u0026#39;middle_band\u0026#39;] - 2 * df[\u0026#39;std\u0026#39;] In the code above, the \u0026lsquo;middle_band\u0026rsquo; is calculated as the rolling mean of the \u0026lsquo;price\u0026rsquo; column, while the \u0026lsquo;std\u0026rsquo; represents the rolling standard deviation.\nPlot the Bollinger Bands: plt.figure(figsize=(10, 6)) plt.plot(df[\u0026#39;price\u0026#39;], label=\u0026#39;Price\u0026#39;) plt.plot(df[\u0026#39;middle_band\u0026#39;], label=\u0026#39;Middle Band\u0026#39;) plt.plot(df[\u0026#39;upper_band\u0026#39;], label=\u0026#39;Upper Band\u0026#39;) plt.plot(df[\u0026#39;lower_band\u0026#39;], label=\u0026#39;Lower Band\u0026#39;) plt.title(\u0026#39;Bollinger Bands\u0026#39;) plt.xlabel(\u0026#39;Date\u0026#39;) plt.ylabel(\u0026#39;Price\u0026#39;) plt.legend() plt.show() The code above will create a line plot with the stock price (\u0026lsquo;price\u0026rsquo;) and the Bollinger Bands: the middle band (\u0026lsquo;middle_band\u0026rsquo;), upper band (\u0026lsquo;upper_band\u0026rsquo;), and lower band (\u0026rsquo;lower_band\u0026rsquo;). Daily returns Daily returns refer to the percentage change in the value of an asset from one trading day to the next. It is a commonly used metric to measure the performance and volatility of an asset over time. Daily returns can be calculated using the following mathematical equation:\nDaily Return = (Price_today - Price_yesterday) / Price_yesterday where Price_today is the closing price of the asset on the current day, and Price_yesterday is the closing price of the asset on the previous day.\nTo calculate daily returns in Python, you can use the Pandas library. Here\u0026rsquo;s an example of Python code that calculates daily returns from a DataFrame containing historical price data:\nimport pandas as pd # Assuming you have a DataFrame named \u0026#39;df\u0026#39; with a \u0026#39;closing_price\u0026#39; column df[\u0026#39;daily_return\u0026#39;] = df[\u0026#39;closing_price\u0026#39;].pct_change() # Print the DataFrame with daily returns print(df[[\u0026#39;date\u0026#39;, \u0026#39;closing_price\u0026#39;, \u0026#39;daily_return\u0026#39;]]) In the code above, the pct_change() function is used to calculate the percentage change between consecutive values in the \u0026lsquo;closing_price\u0026rsquo; column. The result is stored in a new column named \u0026lsquo;daily_return\u0026rsquo; in the DataFrame.\nThe printed DataFrame will display the \u0026lsquo;date\u0026rsquo;, \u0026lsquo;closing_price\u0026rsquo;, and \u0026lsquo;daily_return\u0026rsquo; columns, showing the historical prices and corresponding daily returns.\nCumulative returns Cumulative returns, in finance and trading, represent the total percentage change in the value of an asset over a given period. It provides an understanding of the overall performance and growth of an investment over time. Cumulative returns can be calculated by multiplying the daily returns together and then subtracting 1. The mathematical equation for calculating cumulative returns is as follows:\nCumulative Return = (1 + Daily Return_1) * (1 + Daily Return_2) * ... * (1 + Daily Return_n) - 1 where Daily Return_1, Daily Return_2, \u0026hellip;, Daily Return_n are the daily returns for each respective trading day.\nTo calculate cumulative returns in Python, you can use the Pandas library. Here\u0026rsquo;s an example of Python code that calculates cumulative returns from a DataFrame containing daily return data:\nimport pandas as pd # Assuming you have a DataFrame named \u0026#39;df\u0026#39; with a \u0026#39;daily_return\u0026#39; column df[\u0026#39;cumulative_return\u0026#39;] = (1 + df[\u0026#39;daily_return\u0026#39;]).cumprod() - 1 # Print the DataFrame with cumulative returns print(df[[\u0026#39;date\u0026#39;, \u0026#39;daily_return\u0026#39;, \u0026#39;cumulative_return\u0026#39;]]) In the code above, the cumprod() function is used to calculate the cumulative product of the (1 + daily_return) values. The result is then subtracted by 1 to obtain the cumulative return. The cumulative returns are stored in a new column named \u0026lsquo;cumulative_return\u0026rsquo; in the DataFrame.\nThe printed DataFrame will display the \u0026lsquo;date\u0026rsquo;, \u0026lsquo;daily_return\u0026rsquo;, and \u0026lsquo;cumulative_return\u0026rsquo; columns, showing the historical daily returns and corresponding cumulative returns.\nHistograms and Scatter Plots Histograms provide a graphical representation of the distribution of a dataset. In the context of market analysis, histograms are often used to visualize the frequency distribution of stock prices, trading volumes, or other relevant financial variables. They display the number of occurrences or the probability of data falling within different intervals, allowing analysts to identify patterns, outliers, and the shape of the distribution. Histograms help in understanding the central tendency, dispersion, and skewness of the data, providing valuable insights into market dynamics.\nScatter plots, on the other hand, visualize the relationship between two variables. In market analysis, scatter plots are commonly used to explore the correlation or association between two financial variables, such as the relationship between stock prices and trading volumes. Each data point represents a pair of values for the two variables, and their positions on the plot indicate the values of the variables. Scatter plots provide a visual indication of the strength, direction, and pattern of the relationship between the variables. They can help identify trends, patterns, outliers, or potential trading opportunities based on the observed relationships between variables.\nBoth histograms and scatter plots facilitate the exploration and analysis of financial data, enabling market analysts to uncover patterns, relationships, and potential insights that can inform trading strategies and decision-making processes.\nKurtosis Kurtosis is a statistical measure that quantifies the shape of a probability distribution. In market analysis, kurtosis helps evaluate the distribution of returns or other financial variables. It measures the tail-heaviness or tail-thinness of the distribution compared to a normal distribution. High kurtosis indicates heavy tails, implying a higher likelihood of extreme values, while low kurtosis suggests lighter tails and a more peaked distribution. Kurtosis analysis aids in understanding the level of risk and potential outliers in the data, which are crucial considerations for assessing investment strategies and managing portfolio risk.\nBeta vs correlation Beta and correlation are both metrics used in finance to measure the relationship between two variables, but they serve different purposes and provide distinct insights.\nCorrelation measures the strength and direction of the linear relationship between two variables. It ranges between -1 and +1, where -1 represents a perfect negative correlation, +1 represents a perfect positive correlation, and 0 indicates no correlation. Correlation helps in understanding the degree to which changes in one variable are associated with changes in another variable. In finance, correlation is commonly used to assess the relationship between the returns of different assets or the relationship between an asset\u0026rsquo;s returns and a benchmark index. It helps to identify diversification opportunities and understand how assets move in relation to each other.\nBeta, on the other hand, is a measure of systematic risk or volatility of an asset relative to a benchmark, usually the overall market represented by an index such as the S\u0026amp;P 500. It quantifies the sensitivity of an asset\u0026rsquo;s returns to the movements of the market. A beta of 1 indicates that the asset tends to move in sync with the market, while a beta greater than 1 indicates higher volatility than the market, and a beta less than 1 indicates lower volatility. Beta is used to evaluate the risk-reward tradeoff of an asset and to assess its potential impact on a portfolio\u0026rsquo;s overall risk. Investors often consider beta when constructing portfolios to balance risk exposure and diversify holdings.\nIn summary, correlation measures the degree of linear relationship between two variables, while beta measures the relative volatility or risk of an asset compared to a benchmark. Correlation helps identify associations between variables, while beta aids in assessing the systematic risk of an asset and its impact on portfolio performance. Both metrics provide valuable insights in different aspects of financial analysis and decision-making.\nDaily Portfolio values The daily portfolio value can be calculated by normalizing it with the values of the first day, allocating the portfolio based on the desired weights, and then calculating the position values by multiplying the allocated weights with the starting values of each asset. Finally, the portfolio value is obtained by summing the position values.\nNormalize the daily portfolio value by dividing it by the value of the portfolio on the first day. This normalization allows for comparison and analysis of the portfolio\u0026rsquo;s performance over time. Calculate the allocation of the portfolio by determining the desired weights for each asset. The allocation specifies the proportion of the portfolio\u0026rsquo;s total value that will be invested in each asset. These weights can be based on factors like risk tolerance, investment strategy, or market conditions. Compute the position values by multiplying the allocated weights with the starting values of each asset. This step determines the initial value of each asset position in the portfolio. Calculate the portfolio value by summing the position values. The portfolio value represents the total worth of the portfolio on a given trading day, taking into account the values of all the assets held in the portfolio. Portfolio statistics Daily Returns: Daily Return = (Portfolio Value_today - Portfolio Value_yesterday) / Portfolio Value_yesterday\nCumulative Returns: Cumulative Return = (Portfolio Value_today - Portfolio Value_start) / Portfolio Value_start\nAverage Daily Returns: Average Daily Return = mean(Daily Returns)\nStandard Deviation of Daily Returns: Standard Deviation = std(Daily Returns)\nSharpe Ratio: Sharpe Ratio = (Average Daily Return - Risk-Free Rate) / Standard Deviation of Daily Returns\nSharpe ratio Risk adjusted return All else being equal lower risk is better higher return is better SR also considers risk free rate of return (which is 0% for practical purposes) Parameterized model A parameterized model, in the context of finance and trading, refers to a mathematical or statistical model that includes parameters as variables that can be adjusted or optimized based on specific criteria or data. These models provide a flexible framework for analyzing financial data, making predictions, and generating insights.\nIn a parameterized model, the parameters represent various characteristics or assumptions that govern the behavior of the model. These parameters can be estimated, calibrated, or optimized using historical data, statistical techniques, or other methods. By adjusting the values of the parameters, analysts can test different scenarios, evaluate the model\u0026rsquo;s performance, and make informed decisions based on the desired objectives.\nThe advantage of parameterized models lies in their ability to adapt to different market conditions, asset classes, or investment strategies. By incorporating parameters, the models can capture specific features or dynamics of the financial markets and provide more accurate predictions or analysis.\nExamples of parameterized models in finance include regression models, time series models like ARIMA or GARCH, option pricing models such as Black-Scholes, and machine learning models like neural networks or random forests. Each of these models contains parameters that can be adjusted or optimized to enhance their performance and align them with the characteristics of the data or the specific requirements of the analysis.\nBy utilizing parameterized models, market analysts and researchers can gain deeper insights into financial data, forecast future market trends, manage risk, and optimize investment strategies. The flexibility and adaptability of these models make them valuable tools for decision-making and analysis in the dynamic and complex world of finance.\nOptimizer An optimizer, in the context of finance and mathematical modeling, refers to a computational algorithm or method used to find the optimal solution for a given problem. It is designed to search through a space of possible solutions and identify the values or configurations that optimize a specific objective or satisfy certain constraints.\nAn optimizer typically works by iteratively adjusting the input variables or parameters of a model, evaluating the corresponding output or objective function, and updating the variables based on a defined optimization criterion. The process continues until a satisfactory solution is found, often the one that minimizes or maximizes the objective function within the given constraints.\nIn finance, optimizers are extensively used in areas such as portfolio optimization, asset allocation, risk management, and trading strategy development. They enable investors and analysts to find the optimal allocation of assets, determine the optimal weights or positions for a portfolio, or identify the optimal parameters for a trading strategy.\nVarious optimization algorithms exist, ranging from simple techniques like grid search and random search to more advanced methods such as gradient-based optimization (e.g., gradient descent), evolutionary algorithms, or convex optimization algorithms. The choice of optimizer depends on the nature of the problem, the complexity of the model, and the desired solution accuracy.\nComputational Investing Liquidity is a measurement of how easy it is to buy or sell shares in a fund. ETFs, or exchange-traded funds are the most liquid of funds. They can be bought and sold easily and near-instantly during the trading day just like individual stocks; ETFs, though, represent some distribution of stocks. The volume of an ETF is just as important to its liquidity: because there are often millions of people trading it, it’s easy to get your buy / sell order filled. A large-cap stock like Apple refers to a stock with a large market capitalization. Market capitalization is a metric of a stock’s total shares times its price. It’s worth noting that the price of a stock has no relation to the value of a company; it only describes the cost of owning a single share in that company. If you can afford the market capitalization of a company, you can afford to buy the company in its entirety and take over its ownership. A bull market or a bullish position on a stock is an optimistic viewpoint that implies that things will continue to grow. On the other hand, a bear market or a bearish position is pessimistic (or cautionary, or realistic, depending on how you see the glass) about the future of an asset. Types of Managed Funds ETFs (Exchange Traded Funds) ETFs, or exchange-traded funds, are investment funds that are traded on stock exchanges, similar to individual stocks. They are designed to track the performance of a specific index, sector, commodity, or asset class. ETFs offer investors a way to gain exposure to a diversified portfolio of assets without directly owning the underlying securities.\nStructure: ETFs are structured as open-end investment companies or unit investment trusts. They issue shares to investors, and these shares represent an ownership interest in the ETF\u0026rsquo;s underlying assets.\nUnderlying Assets: ETFs can track a wide range of underlying assets, including stock indexes (such as the S\u0026amp;P 500), bond indexes, commodity prices, currencies, or a combination of assets. The ETF\u0026rsquo;s performance is designed to closely mirror that of its underlying index or asset class.\nCreation and Redemption: Authorized Participants (APs) play a crucial role in the creation and redemption of ETF shares. They are typically large institutional investors, such as market makers or authorized broker-dealers. APs create new shares of an ETF by delivering a basket of the underlying assets to the ETF issuer, and in return, they receive ETF shares. Conversely, they can redeem ETF shares by returning them to the issuer in exchange for the underlying assets.\nListing and Trading: ETFs are listed on stock exchanges, making them easily tradable throughout the trading day. Investors can buy and sell ETF shares through brokerage accounts, just like they would trade individual stocks. The price of an ETF share is determined by market demand and supply and can sometimes deviate slightly from the net asset value (NAV) of the underlying assets.\nBenefits of ETFs:\nDiversification: ETFs offer investors exposure to a broad range of securities within a single investment. This diversification can help reduce risk compared to investing in individual stocks or bonds. Liquidity: ETFs are traded on stock exchanges, providing investors with liquidity. They can be bought or sold throughout the trading day at market prices. Transparency: ETFs disclose their holdings on a daily basis, allowing investors to see exactly which securities they own. This transparency helps investors make informed decisions. Lower Costs: ETFs generally have lower expense ratios compared to mutual funds. They often passively track an index rather than actively managed funds, resulting in lower management fees. Flexibility: ETFs can be used for various investment strategies, including long-term investing, short-term trading, or tactical asset allocation. It\u0026rsquo;s important to note that while ETFs offer many benefits, they also carry risks. The value of an ETF can fluctuate based on the performance of its underlying assets, and there are potential risks associated with market volatility, liquidity, and tracking error.\nMutual Funds Mutual funds are investment vehicles that pool money from multiple investors to invest in a diversified portfolio of securities, such as stocks, bonds, or a combination of both. They are managed by professional investment firms or asset management companies.\nStructure: Mutual funds are set up as open-end investment companies. This means that the fund continuously issues and redeems shares based on investor demand. Investors purchase shares of the mutual fund at the net asset value (NAV), which is calculated by dividing the total value of the fund\u0026rsquo;s assets by the number of shares outstanding.\nProfessional Management: Mutual funds are managed by professional fund managers or investment teams who make investment decisions on behalf of the fund. The fund manager conducts research, performs security analysis, and selects investments based on the fund\u0026rsquo;s investment objective and strategy.\nInvestment Objectives and Strategies: Mutual funds can have various investment objectives and strategies. For example, a mutual fund may aim to achieve long-term capital appreciation, income generation, or a blend of both. The investment strategy could be actively managed, where the fund manager actively selects and manages the fund\u0026rsquo;s portfolio, or passively managed, where the fund aims to replicate the performance of a specific index.\nDiversification: Mutual funds provide diversification by investing in a wide range of securities. By pooling money from multiple investors, the fund can hold a diversified portfolio of stocks, bonds, or other assets. This diversification helps spread the investment risk and reduces the impact of any single security\u0026rsquo;s performance on the overall portfolio.\nNet Asset Value (NAV): The NAV of a mutual fund represents the per-share value of the fund\u0026rsquo;s assets. It is calculated by subtracting the fund\u0026rsquo;s liabilities from its total assets and dividing the result by the number of shares outstanding. The NAV is typically calculated at the end of each trading day.\nFees and Expenses: Mutual funds charge fees and expenses to cover the costs of managing the fund. These fees may include an expense ratio, which covers management fees, administrative expenses, and other operational costs. Additionally, some funds may charge sales loads, which are fees paid when purchasing or selling shares of the fund.\nLiquidity: Mutual funds are priced and traded at the NAV at the end of each trading day. Investors can buy or sell shares directly with the fund company or through brokerage accounts. Mutual funds are generally considered to be liquid investments, as they provide investors with the ability to buy or sell shares on any business day.\nBenefits of Mutual Funds:\nProfessional Management: Mutual funds are managed by experienced professionals who make investment decisions based on their expertise and research. Diversification: Mutual funds offer instant diversification by investing in a broad range of securities, reducing the risk associated with investing in individual stocks or bonds. Accessibility: Mutual funds are accessible to a wide range of investors, as they have relatively low minimum investment requirements. Liquidity: Investors can typically buy or sell mutual fund shares on any business day at the NAV, providing liquidity. Flexibility: Mutual funds offer various investment strategies and asset classes to cater to different investor preferences and goals. Risks of Mutual Funds:\nMarket Risk: The value of mutual fund shares can fluctuate based on the performance of the underlying securities, and investors may experience losses if the market declines. Fees and Expenses: Mutual funds charge fees and expenses, which can affect the overall returns earned by investors. Management Risk: The performance of a mutual fund depends on the investment decisions made by the fund manager. Poor investment choices or ineffective management can negatively impact returns. No Guarantees: Mutual funds do not provide guaranteed returns, and investors may not receive back the full amount of their initial investment. Hedge Funds Hedge funds are alternative investment vehicles that are designed for wealthy individuals or institutional investors. Unlike mutual funds, hedge funds are typically only available to accredited investors due to their complex nature and higher risk profile. Hedge funds employ a range of investment strategies and techniques to seek higher returns, often through active management and the use of leverage.\nStructure: Hedge funds are structured as private investment partnerships or limited liability companies. They are managed by professional investment managers or investment firms who act as general partners or managers of the fund.\nInvestment Strategies: Hedge funds employ various investment strategies with the goal of generating higher returns than traditional investments. These strategies can include long and short positions in stocks, bonds, commodities, currencies, derivatives, and other financial instruments. Hedge funds can also utilize leverage (borrowed money) to amplify potential returns.\nLimited Regulation: Hedge funds often operate with fewer regulatory restrictions compared to mutual funds. This allows them to have more flexibility in their investment strategies, including the ability to engage in short selling, derivative trading, and alternative investments.\nPerformance Fees: Hedge funds typically charge performance fees in addition to management fees. The performance fee is a percentage of the fund\u0026rsquo;s profits, usually around 20%. This fee structure aligns the interests of the fund managers with those of the investors, as the managers earn higher fees when they generate positive returns.\nRisk Management: Hedge funds often employ risk management techniques to mitigate potential losses. This can involve diversifying investments, hedging against market downturns, and implementing risk controls. However, it\u0026rsquo;s important to note that hedge funds can still be subject to substantial risk, and their strategies may not always be successful.\nAccess and Investor Requirements: Hedge funds generally have higher minimum investment requirements compared to mutual funds, often ranging from hundreds of thousands to millions of dollars. They are typically open only to accredited investors, who have higher income or net worth thresholds set by regulatory authorities.\nLiquidity and Lock-up Periods: Hedge funds often have restrictions on liquidity. Investors may face limited redemption options and longer lock-up periods, where their investment is tied up for a specific period, typically one year or more. This illiquidity is intended to provide fund managers with more flexibility in managing investments and executing strategies.\nBenefits of Hedge Funds:\nPotential Higher Returns: Hedge funds aim to generate higher returns by using sophisticated investment strategies, including short selling, leverage, and alternative investments. Diversification: Hedge funds often employ a wide range of investment strategies and can invest across multiple asset classes, offering potential diversification benefits to investors. Active Management: Hedge fund managers actively monitor and adjust their investment portfolios, seeking opportunities to capitalize on market inefficiencies and generate alpha (excess returns). Risks of Hedge Funds:\nHigher Risk: Hedge funds typically carry higher risk compared to traditional investments. The use of leverage, complex strategies, and alternative investments can amplify potential losses. Limited Transparency: Hedge funds are less regulated than mutual funds, and they often have limited disclosure requirements. Investors may have less visibility into the fund\u0026rsquo;s holdings and investment decisions. Limited Liquidity: Hedge funds may have restrictions on withdrawals and longer lock-up periods, limiting investors\u0026rsquo; access to their capital. Potential for High Fees: Hedge funds generally charge higher management and performance fees compared to traditional investment options, which can erode overall returns. Compensation Assets under management (AUM): The amount of other people\u0026rsquo;s money the fund manager is responsible for. Managers of ETFs are paid in expense ratio (0.01% to 1.00% of AUM) Mutual Funds are pain in expense ratio (0.5% to 3.00% of AUM) Hedge Funds Two and Twenty structure. 2% of AUM and 20% of profits Who are the investors in Hedge Funds? Hedge fund investors can be a diverse group of individuals, institutions, and organizations. Here are some common types of hedge fund investors:\nHigh-Net-Worth Individuals (HNWIs): These are wealthy individuals who have a substantial amount of investable assets. HNWIs often invest in hedge funds to diversify their portfolios and seek higher returns. Family Offices: Family offices manage the financial affairs and investments of wealthy families. They may allocate a portion of their assets to hedge funds to achieve specific investment goals. Pension Funds: Pension funds manage retirement assets on behalf of employees. Some pension funds, especially those with larger assets, invest in hedge funds to diversify their portfolios and potentially enhance returns. Endowments and Foundations: Educational institutions, charitable foundations, and other similar organizations may invest in hedge funds to generate income for their operations or to support their philanthropic activities. Insurance Companies: Some insurance companies allocate a portion of their investment portfolios to hedge funds in order to enhance overall returns and manage risk. Sovereign Wealth Funds: These funds are created by governments to manage and invest surplus funds, often derived from commodity exports or foreign exchange reserves. Sovereign wealth funds may invest in hedge funds as part of their overall investment strategy. Funds of Funds: These are investment vehicles that pool capital from multiple investors to invest in a portfolio of hedge funds. Funds of funds provide diversification and professional management for investors who may not have direct access to hedge funds. Institutional Investors: This category includes various institutions such as banks, asset management firms, and corporations. Institutional investors often have dedicated teams or departments that manage their investments, which may include hedge fund allocations. Goals of hedge funds The goals of hedge funds can vary depending on their investment strategies and the preferences of their managers. However, there are several common goals that hedge funds typically aim to achieve:\nCapital Appreciation: Hedge funds often seek to generate positive returns on their investments, aiming for capital appreciation and growth of the fund\u0026rsquo;s assets over time. The primary goal is to outperform traditional investment vehicles, such as stock market indices or mutual funds. Risk Management and Preservation of Capital: While hedge funds are known for their potential to generate high returns, they also prioritize risk management. Hedge fund managers employ various strategies to mitigate downside risks and preserve capital, aiming to protect investors\u0026rsquo; assets during market downturns. Absolute Returns: Hedge funds typically pursue absolute returns, aiming to generate positive performance regardless of market conditions. Unlike traditional investment funds that often benchmark their performance against a specific market index, hedge funds aim to generate returns that are not reliant on overall market performance. Diversification: Hedge funds often use diverse investment strategies across different asset classes, including stocks, bonds, commodities, currencies, and derivatives. By diversifying their investments, hedge funds aim to reduce risk and potentially enhance returns through exposure to various market opportunities. Active Management and Flexibility: Hedge funds have the advantage of flexibility and the ability to implement active investment strategies. They can take both long and short positions, engage in leverage, use derivatives, and employ other sophisticated techniques to exploit market inefficiencies and generate returns. Capital Preservation in Down Markets: Some hedge funds aim to provide downside protection during market downturns. They may use strategies such as hedging, short-selling, or employing market-neutral approaches to reduce correlation with broader market movements and potentially deliver positive returns even in challenging market conditions. Alpha Generation: Hedge funds often strive to generate alpha, which represents the excess return earned beyond what would be expected based on the risk exposure of their investments. By identifying and exploiting market inefficiencies or mispriced assets, hedge funds aim to generate alpha and deliver superior risk-adjusted returns. Hedge funds metrics Hedge funds employ a wide range of metrics and indicators to evaluate investment opportunities, monitor portfolio performance, and make informed decisions. The specific metrics they chase can vary depending on the fund\u0026rsquo;s investment strategy and objectives. Here are some commonly used metrics in the hedge fund industry:\nReturn on Investment (ROI): ROI is a fundamental metric that measures the profitability of an investment. Hedge funds closely track the returns generated by their investments to assess the success of their strategies and compare them against their targets or benchmarks. Alpha: Alpha represents the excess return generated by a hedge fund compared to its expected return based on its risk exposure. Hedge funds aim to achieve positive alpha, as it indicates that they have outperformed the market or their benchmark, taking into account the level of risk undertaken. Sharpe Ratio: The Sharpe ratio measures the risk-adjusted return of an investment by considering the excess return earned relative to its volatility or risk. Hedge funds often strive for higher Sharpe ratios, indicating that they are generating superior returns for the level of risk taken. Volatility: Volatility measures the degree of price fluctuations in an investment or a portfolio. Hedge funds may target specific levels of volatility based on their risk appetite and investment strategies. Some funds may seek to reduce volatility by employing hedging or risk management techniques. Maximum Drawdown: Maximum drawdown refers to the largest peak-to-trough decline in the value of a hedge fund or investment portfolio over a specific period. Hedge funds aim to minimize drawdowns as they can significantly impact investor capital. Lower maximum drawdowns indicate better risk management. Information Ratio: The information ratio measures the excess return generated by a hedge fund relative to a benchmark, considering the level of active risk taken. It assesses the fund manager\u0026rsquo;s ability to generate returns through active management decisions and market insights. Risk Metrics: Hedge funds closely monitor various risk metrics such as Value-at-Risk (VaR), which estimates the potential loss under adverse market conditions, and tracking error, which measures the deviation of a fund\u0026rsquo;s returns from its benchmark. These metrics help hedge funds assess and manage the risks associated with their investment strategies. Liquidity Metrics: Hedge funds may track liquidity metrics to assess the ease of buying or selling assets in their portfolios. Measures such as bid-ask spreads, trading volumes, and market depth can help hedge funds gauge the liquidity of their investments and ensure they can exit positions when necessary. Computing in a Hedge Fund Computing plays a crucial role in the operations of hedge funds, enabling efficient data analysis, trading strategies, risk management, and overall portfolio management. Here are some key aspects of computing within a hedge fund:\nData Management: Hedge funds handle vast amounts of data from various sources, including market data, economic indicators, company financials, news feeds, and more. Computing systems are used to collect, store, and organize this data for analysis and decision-making. This may involve the use of databases, data warehouses, and data lakes. Quantitative Analysis: Hedge funds often employ quantitative analysts (quants) who develop mathematical models and algorithms to analyze data, identify patterns, and generate trading signals. These models can range from statistical models and machine learning algorithms to more complex quantitative finance models. High-performance computing systems are often used to perform computationally intensive tasks and backtest strategies. Algorithmic Trading: Hedge funds commonly utilize algorithmic trading, where computer algorithms execute trades based on predefined rules and strategies. These algorithms take into account various factors such as market conditions, pricing data, and order book information. Low-latency computing systems are often employed to execute trades quickly and efficiently. Risk Management: Hedge funds have sophisticated risk management systems to monitor and assess potential risks associated with their portfolios. These systems use computing power to calculate risk metrics, such as Value-at-Risk (VaR), stress tests, and scenario analyses. Risk models are often run on computing clusters to analyze the potential impact of different market conditions on the fund\u0026rsquo;s holdings. Portfolio Management and Optimization: Computing systems are used for portfolio management tasks, including portfolio construction, rebalancing, and optimization. Advanced optimization algorithms help hedge funds determine optimal asset allocations based on desired risk-return trade-offs, constraints, and market conditions. Market Data Analysis: Hedge funds analyze market data in real-time to identify trading opportunities, monitor market trends, and make informed investment decisions. This involves processing and analyzing vast amounts of streaming market data using computing systems, often with the help of complex event processing (CEP) techniques. Infrastructure and Connectivity: Hedge funds require robust computing infrastructure to support their operations. This includes servers, data storage systems, network infrastructure, and connectivity to exchanges, brokers, and other trading platforms. Redundancy and high availability are critical to ensure uninterrupted operations and minimize downtime. Data Security: Hedge funds handle sensitive financial data and must maintain strict data security measures. This includes encryption, access controls, secure networks, and data backup systems to protect against unauthorized access, data breaches, and system failures. The Order Book An order book is a key component of financial markets, particularly in the context of exchanges or trading platforms. It is a record of buy and sell orders for a particular security, such as stocks, bonds, or cryptocurrencies, organized by price and time. The order book provides market participants with transparency regarding the supply and demand dynamics of the security.\nHere\u0026rsquo;s how an order book typically works:\nBuy and Sell Orders: Market participants can submit buy or sell orders for a specific security. Buy orders represent the demand for the security at a certain price, while sell orders represent the supply of the security at a given price. Price Levels: The order book organizes these buy and sell orders into different price levels. Each price level represents a specific price at which orders are placed. The highest bid price (buy orders) and the lowest ask price (sell orders) are often displayed prominently. Quantity: Along with the price, the order book also shows the quantity or volume of shares or contracts being bid or offered at each price level. This provides information about the liquidity available at different price points. Best Bid and Ask: The order book highlights the best bid price and the best ask price, which represent the highest bid and lowest ask prices available in the market at a given moment. The difference between the best bid and ask prices is known as the bid-ask spread. Market Depth: Market depth refers to the cumulative quantity of buy and sell orders available at different price levels. It shows the potential buying and selling pressure in the market and helps market participants assess the level of liquidity. Market Order Execution: When a market participant submits a market order to buy or sell a security, it is typically executed against the best available prices in the order book. The market order consumes the available liquidity in the order book until the entire order is filled. Limit Order Execution: Limit orders specify the desired price at which a participant wants to buy or sell a security. These orders are placed in the order book and remain there until they are matched with a counterparty. If a buy limit order matches a sell limit order at the specified price, a trade occurs. Order Book Updates: The order book is continuously updated as new orders are submitted or existing orders are modified or canceled. The order book reflects real-time changes in supply and demand dynamics, allowing participants to observe shifts in market sentiment. The order book is an essential tool for traders, providing them with visibility into market liquidity, price levels, and potential trading opportunities. By analyzing the order book, traders can make informed decisions about when to place orders, at what price, and how much liquidity is available to support their trades.\nHow orders get to the exchange? Orders can reach exchanges through various channels, including direct connections, brokers, and alternative trading venues. Here\u0026rsquo;s a general overview of how orders reach exchanges and the role of dark pools:\nDirect Market Access (DMA): Institutional investors and some high-frequency trading firms have direct market access to exchanges. They establish direct connections to the exchange\u0026rsquo;s trading system, enabling them to send orders directly without intermediaries. DMA allows for faster order execution and greater control over the order routing process. Brokers and Trading Platforms: Most individual investors and some institutional investors route their orders through brokers or trading platforms. These intermediaries receive orders from clients and act as an interface between the client and the exchange. Brokers typically offer access to multiple exchanges, allowing clients to choose the desired trading venue. Smart Order Routing (SOR): When an order is received by a broker or a trading platform, they may use smart order routing technology. SOR algorithms analyze various factors such as price, liquidity, execution speed, and regulatory requirements to determine the optimal destination for the order. SOR aims to maximize the chances of obtaining the best execution possible by routing the order to the most suitable market or venue. Primary Exchanges: The primary exchanges, such as the New York Stock Exchange (NYSE) or NASDAQ, are the most widely known trading venues. Orders sent directly to these exchanges or routed through brokers are executed on their centralized order books. These exchanges provide transparent markets where orders are visible to all participants, allowing for price discovery and liquidity. Dark Pools: Dark pools are alternative trading venues that offer a level of anonymity and reduced market impact for large institutional orders. Dark pools operate differently from primary exchanges as they do not display order details in the public order book. Instead, they match buy and sell orders internally, away from public view. Dark pools are designed to facilitate large block trades with reduced information leakage and minimize market impact. Crossing Networks: Some brokers operate crossing networks, which are internal matching engines that facilitate the execution of orders from their own clients. These orders are not routed to external exchanges. Crossing networks aim to match buy and sell orders within the broker\u0026rsquo;s client base, providing potential price improvement and confidentiality. Electronic Communication Networks (ECNs): ECNs are electronic platforms that connect buyers and sellers directly. They provide a venue for trading securities and can be accessed by market participants, including institutional investors and retail traders. ECNs often offer fast order matching, access to multiple markets, and display order information for transparency. Geographic arbitrage Geographic arbitrage refers to the practice of taking advantage of price or valuation discrepancies between different geographic regions or markets. It involves exploiting the differences in prices, costs, or economic conditions across countries or regions to generate profits.\nStop Loss Stop Loss is an order placed by an investor to automatically sell a security if it reaches a specified price, limiting potential losses.\nStop Gain Stop Gain is an order placed by an investor to automatically sell a security if it reaches a specified price, securing profits and preventing potential losses.\nTrailing Stop A trailing stop is a type of stop loss order that adjusts dynamically with the market price, moving in lockstep to protect profits by automatically selling a security if its price drops a certain percentage or amount from its highest point.\nShort selling Short selling is a trading strategy where an investor borrows a security from a broker and sells it in the market, anticipating that the price of the security will decline. The investor aims to buy back the security at a lower price in the future to return it to the broker, thereby profiting from the price difference. Short selling allows investors to potentially profit from falling prices and is commonly used for speculative purposes, hedging, or market-making activities. However, it carries inherent risks, as there is unlimited potential for loss if the price of the security being shorted rises significantly.\nEvaluating the \u0026ldquo;true\u0026rdquo; value of a company Intrinsic value of a company The intrinsic value of a company refers to the estimated underlying worth or fair value of the company\u0026rsquo;s business, assets, and cash flows. It is an assessment of what the company is truly worth based on its fundamental characteristics, financial performance, growth prospects, and other relevant factors.\nCalculating the intrinsic value involves analyzing various aspects of the company, such as its earnings, revenue, cash flow, assets, liabilities, industry trends, competitive position, management quality, and overall economic conditions. Different valuation methods, such as discounted cash flow (DCF) analysis, comparable company analysis, or asset-based valuation, can be used to estimate the intrinsic value.\nThe intrinsic value is often compared to the market price of the company\u0026rsquo;s stock to determine if the stock is overvalued or undervalued. If the intrinsic value is higher than the market price, the stock may be considered undervalued and potentially a good investment opportunity. Conversely, if the intrinsic value is lower than the market price, the stock may be considered overvalued, signaling a potential selling opportunity.\nBook value of the company The book value of a company, also known as the net book value or shareholder\u0026rsquo;s equity, represents the value of a company\u0026rsquo;s assets minus its liabilities as reported on the balance sheet. It provides an accounting-based measure of the company\u0026rsquo;s net worth or equity position.\nMarket cap The market capitalization (market cap) of a company is a measure of its total market value, representing the worth of the company as perceived by the market. It is calculated by multiplying the company\u0026rsquo;s current stock price by the total number of outstanding shares.\nThe formula for market cap is as follows:\nMarket Cap = Stock Price x Number of Outstanding Shares Rule of 72 The Rule of 72 is a simplified mathematical rule used to estimate the time it takes for an investment or a sum of money to double, given a fixed interest rate. It provides a quick approximation of the doubling time based on the concept of compound interest.\nThe Rule of 72 is applied as follows:\nDoubling Time ≈ 72 / Interest Rate\nor\nInterest Rate ≈ 72 / Doubling Time\nWhere:\nDoubling Time represents the estimated time it takes for an investment or sum of money to double. Interest Rate represents the fixed annual interest rate or rate of return. For example, if you have an investment with an annual interest rate of 6%, you can estimate that it will take approximately 12 years (72 / 6) for your investment to double.\nThe Rule of 72 is a simple approximation and assumes a constant interest rate and compound interest. It is most accurate for interest rates in the range of 6% to 10%. However, for higher or lower interest rates, the approximation becomes less precise. Additionally, it does not take into account factors such as inflation, taxes, or other variables that may affect investment returns.\nThe future value of money The present value (PV) and future value (FV) of money are related through a mathematical formula that takes into account the time period and the interest rate. The formula to calculate the present value (PV) based on a future value (FV) is as follows:\nPV = FV / (1 + r)^n Where: PV = Present Value FV = Future Value r = Interest rate (expressed as a decimal) n = Number of periods or time period The Capital Asset Pricing Model The Capital Asset Pricing Model (CAPM) is a financial model used to estimate the expected return on an investment by considering the relationship between its systematic risk and expected return. It provides a framework for pricing risky securities and determining an appropriate required rate of return.\nThe CAPM is based on the following formula:\nExpected Return = Risk-Free Rate + Beta x (Market Return - Risk-Free Rate) Where:\nExpected Return is the anticipated return on the investment. Risk-Free Rate is the return on a risk-free investment, typically represented by the yield on government bonds. Beta is a measure of the investment\u0026rsquo;s systematic risk or sensitivity to market movements. Market Return is the expected return on the overall market. The CAPM assumes that investors are risk-averse and require compensation for bearing systematic risk beyond the risk-free rate. It suggests that an investment\u0026rsquo;s expected return should increase in proportion to its systematic risk (as measured by beta). The formula calculates the expected return by adding a risk premium (Beta x (Market Return - Risk-Free Rate)) to the risk-free rate.\nKey assumptions of the CAPM include efficient markets, where all relevant information is reflected in asset prices, and a single-period investment horizon. The model also assumes that investors have homogeneous expectations and hold well-diversified portfolios.\nThe CAPM is widely used in finance for determining the appropriate discount rate for investment valuation, evaluating the performance of investment portfolios, and estimating the cost of equity capital for companies. However, it has its limitations and critics, as it relies on simplifying assumptions and may not fully capture the complexities of real-world market dynamics.\nPassive vs Active Investing Passive investing and active investing are two contrasting investment approaches that differ in terms of strategy, management style, and investment philosophy. Here\u0026rsquo;s an overview of each:\nPassive Investing: Passive investing, also known as index investing or passive management, involves constructing a portfolio that aims to replicate the performance of a specific market index, such as the S\u0026amp;P 500. The primary goal is to match the returns of the chosen index rather than trying to outperform it. Passive investors believe that markets are efficient and that it is challenging to consistently beat the market over the long term. Key characteristics of passive investing include:\nIndex-based approach: Passive investors invest in index funds or exchange-traded funds (ETFs) that hold a diversified portfolio of securities to mimic the performance of a specific index. Lower costs: Passive investing generally incurs lower fees and expenses compared to active investing, as it requires minimal research and portfolio management. Buy and hold strategy: Passive investors typically maintain a long-term investment approach, avoiding frequent trading or market timing. Broad market exposure: Passive strategies offer exposure to an entire market or a specific segment, providing diversification and representing the overall market performance. Active Investing: Active investing involves actively managing a portfolio with the goal of outperforming the market or a specific benchmark. Active investors believe that it is possible to identify undervalued securities or exploit market inefficiencies through research, analysis, and active decision-making. Key characteristics of active investing include:\nIndividual security selection: Active investors analyze and select specific stocks, bonds, or other securities based on their research and evaluation of company fundamentals, market trends, and other factors. Higher costs: Active investing typically involves higher costs compared to passive investing, as it requires more research, analysis, and trading activity. Portfolio turnover: Active managers frequently buy and sell securities in an attempt to take advantage of market opportunities or manage risk. Flexibility and customization: Active investing allows for a more tailored approach, with the ability to deviate from market indices and adjust the portfolio based on the manager\u0026rsquo;s outlook and investment strategy. Efficient market hypothesis: The Efficient Market Hypothesis (EMH) is a theory in finance that suggests financial markets are efficient in reflecting all available information into security prices. According to the EMH, it is not possible to consistently achieve above-average returns through stock picking or market timing, as stock prices already incorporate all relevant information.\nKey principles of the Efficient Market Hypothesis include:\nInformation Efficiency: The EMH assumes that financial markets efficiently incorporate all publicly available information, including historical data, financial statements, news, and other market-relevant information. In an efficient market, prices adjust quickly and accurately to new information, making it difficult for investors to gain an advantage by acting upon it. Three Forms of Market Efficiency: The EMH categorizes market efficiency into three forms: Weak Form Efficiency: Prices reflect past trading information, such as historical prices and trading volume. Technical analysis techniques based on past price patterns would not consistently generate abnormal returns. Semi-Strong Form Efficiency: Prices reflect all publicly available information, including not only past trading data but also fundamental and non-public information, such as earnings reports, news announcements, and analyst recommendations. Neither technical nor fundamental analysis would consistently yield superior returns. Strong Form Efficiency: Prices reflect all information, including public and non-public information. This implies that even insider information would not provide an advantage, as it is already factored into prices. Implications for Investors: The EMH suggests that investors cannot systematically beat the market or consistently identify mispriced securities, as any available information is already incorporated into prices. Therefore, passive investing through strategies like index funds or exchange-traded funds (ETFs) that track broad market indices is considered a rational approach. While the Efficient Market Hypothesis provides a framework for understanding market efficiency, it has been subject to criticism. Critics argue that markets may not always be fully efficient due to behavioral biases, information asymmetry, or temporary market inefficiencies that can be exploited by skilled investors. As a result, various investment strategies, such as active management or value investing, continue to be pursued by those who believe in the potential to outperform the market.\nArbitrage Pricing Theory The Arbitrage Pricing Theory (APT) is a financial theory that attempts to explain the relationship between the expected returns of an asset and its risk factors. It is an alternative to the Capital Asset Pricing Model (CAPM) and provides a multi-factor model for asset pricing.\nKey features of the Arbitrage Pricing Theory include:\nMulti-Factor Model: APT posits that the expected return of an asset is influenced by multiple risk factors, which are systematic influences that affect the asset\u0026rsquo;s returns. These risk factors can be economic variables such as interest rates, inflation, market indices, or industry-specific factors. No Arbitrage: APT assumes the absence of arbitrage opportunities, meaning that it is not possible to make riskless profits by exploiting mispriced securities. The theory suggests that market prices adjust quickly to eliminate any potential arbitrage opportunities. Linear Relationship: APT assumes a linear relationship between the risk factors and the expected returns of an asset. It suggests that the sensitivity of an asset\u0026rsquo;s returns to each risk factor can be quantified through factor loadings or coefficients. Risk Premiums: APT predicts that investors require a risk premium for exposure to each risk factor. The size of the risk premium depends on the perceived riskiness of the factor and its impact on the asset\u0026rsquo;s returns. Arbitrage Pricing: APT allows for the identification of mispriced assets by comparing their expected returns, as estimated using the multi-factor model, with their actual market prices. If an asset\u0026rsquo;s expected return does not match the return implied by the APT model, an arbitrage opportunity may exist. APT is a more flexible model compared to the CAPM, as it considers multiple risk factors and does not rely on the assumptions of market efficiency or a single market portfolio. However, APT requires identifying and estimating the relevant risk factors specific to a particular asset or market, which can be challenging.\nWhile APT provides a framework for understanding asset pricing, it is not as widely used as the CAPM in practical applications. Nevertheless, it has contributed to the development of factor-based investing and the understanding of the relationship between risk factors and asset returns.\nTechnical Analysis Technical analysis is a methodology used in financial markets to evaluate and forecast future price movements of securities, such as stocks, currencies, commodities, and indices. It relies on the analysis of historical price and volume data, along with various technical indicators and chart patterns, to make investment decisions.\nKey aspects of technical analysis include:\nPrice Patterns: Technical analysts study various patterns formed by historical price data, such as trends (uptrends, downtrends, or sideways movements), support and resistance levels, chart patterns (e.g., head and shoulders, double tops/bottoms), and trend lines. These patterns are believed to provide insights into future price movements. Technical Indicators: Technical analysts use a wide range of indicators that mathematically analyze price and volume data to generate trading signals. Examples of popular indicators include moving averages, oscillators (e.g., Relative Strength Index - RSI, Stochastic Oscillator), and momentum indicators (e.g., Moving Average Convergence Divergence - MACD). These indicators help identify overbought or oversold conditions, trend strength, and potential reversals. Volume Analysis: Volume, the number of shares or contracts traded, is considered a significant factor in technical analysis. Changes in trading volume can indicate the strength or weakness of price movements, confirmation or divergence of trends, or the presence of buying or selling pressure. Market Sentiment: Technical analysis takes into account market sentiment, which reflects the collective psychological and emotional outlook of market participants. It is believed that market sentiment can influence price movements and can be inferred from indicators like the put/call ratio, investor surveys, or sentiment indicators. Timeframes: Technical analysis can be applied to various timeframes, ranging from intraday charts to long-term charts. Different timeframes may reveal different patterns and trends, catering to traders with different investment horizons. Technical analysis assumes that historical price patterns, along with associated indicators and patterns, can provide insights into future price movements. Critics argue that technical analysis is based on subjective interpretations and lacks a solid foundation in fundamental analysis or economic factors.\nTraders and investors who use technical analysis aim to identify trading opportunities, determine entry and exit points, manage risk, and assess the probability of price movements. It is often used alongside other forms of analysis, such as fundamental analysis, to make more informed investment decisions.\nTechnical Indicator: Momentum Momentum, in the context of financial markets, refers to the tendency of an asset\u0026rsquo;s price to continue moving in the same direction over a certain period of time. It is a key concept in technical analysis and is based on the belief that assets that have performed well or poorly in the recent past will continue to do so in the near future.\nPrice Trend: Momentum focuses on identifying and capitalizing on existing price trends. It assumes that assets that have been rising in price will continue to rise, while those that have been falling will continue to decline.\nRelative Strength: Momentum analysis often involves comparing the performance of one asset relative to others in the same market or sector. Assets that have demonstrated relatively stronger performance compared to their peers are considered to have positive momentum.\nTime Frame: Momentum analysis can be applied to various timeframes, ranging from short-term intraday movements to longer-term trends. Different traders and investors may use different timeframes to capture momentum opportunities based on their trading strategies and investment goals.\nMomentum Indicators: Technical analysts use various momentum indicators to identify and quantify the strength of price trends. Examples of momentum indicators include the Relative Strength Index (RSI), Moving Average Convergence Divergence (MACD), and Stochastic Oscillator. These indicators help assess whether an asset is overbought or oversold and whether the momentum is likely to continue or reverse.\nMomentum trading strategies typically involve buying assets that have exhibited positive momentum and selling or short-selling assets that have shown negative momentum. Traders aim to profit from the continuation of trends by entering positions in the direction of the established momentum. Risk management techniques, such as stop-loss orders, are often employed to limit potential losses if the momentum reverses.\nDealing with Data Tick A \u0026ldquo;tick\u0026rdquo; refers to the smallest possible price movement for a financial instrument, such as a stock, futures contract, or currency pair. The tick size is the minimum price increment that the price can move up or down. It represents the precision with which prices are quoted in the market.\nThe tick size varies depending on the specific financial instrument and the exchange where it is traded. For example, in the stock market, the tick size is typically a penny (or a fraction of a penny), while in the futures market, it may be a different amount.\nStock Split A stock split is a corporate action taken by a publicly traded company to increase the number of its outstanding shares while simultaneously reducing the share price in order to make the shares more affordable to investors. The overall value of the company remains the same after a stock split.\nStock splits are usually expressed as a ratio, such as 2-for-1, 3-for-1, or any other combination. Here\u0026rsquo;s how it works:\n2-for-1 Stock Split: In a 2-for-1 stock split, for every one share an investor owns before the split, they receive two shares after the split. For example, if an investor holds 100 shares of a company\u0026rsquo;s stock trading at 100 dollars per share, after the 2-for-1 split, they will have 200 shares priced at 50 dollars per share (100 shares x 2). 3-for-1 Stock Split: In a 3-for-1 stock split, for every one share an investor owns before the split, they receive three shares after the split. If they held 50 shares priced at 150 dollars per share before the split, they would have 150 shares priced at 50 dollars per share after the 3-for-1 split. The primary purpose of a stock split is to make the company\u0026rsquo;s stock more accessible to a broader range of investors, especially those with smaller amounts of capital. When the share price is lower, investors with limited funds can participate in the market more easily. Stock splits do not change the total market capitalization of the company or the proportional ownership of shareholders.\nIt\u0026rsquo;s important to note that a stock split is different from a stock dividend. In a stock dividend, the company issues additional shares to its existing shareholders as a way of distributing its profits or retained earnings.\nStock splits are typically a sign of a company\u0026rsquo;s confidence in its future growth prospects. They are not uncommon for companies that experience significant share price appreciation and want to maintain a reasonable share price for retail investors.\nDividends Dividends are payments made by a corporation to its shareholders as a distribution of the company\u0026rsquo;s profits or retained earnings. When a company earns a profit, it has several options for using that money, such as reinvesting it back into the business for expansion or paying off debts. Another common option is to return some of the profits to the shareholders in the form of dividends.\nDividends are typically paid out in cash, but they can also be paid in the form of additional shares of stock or other property. The amount of dividends paid to each shareholder is usually proportional to the number of shares they own. For example, if a company declares a dividend of 0.50 dollars per share and a shareholder owns 100 shares, they would receive 50 dollars in dividends.\nDividends can be paid on a regular basis, such as quarterly or annually, or the company may decide to pay special or one-time dividends based on its financial performance or specific events. The decision to pay dividends is made by the company\u0026rsquo;s board of directors, and the amount and frequency of dividends can vary depending on the company\u0026rsquo;s profitability, financial health, and growth opportunities.\nInvestors often see dividends as a way to generate income from their investments, especially in stable and mature companies with a history of consistent dividend payments. Dividend-paying stocks are popular among income-seeking investors, retirees, and those looking for a steady income stream.\nEfficient Market Hypothesis The Efficient Market Hypothesis (EMH) is a theory in financial economics that suggests that financial markets are efficient and that asset prices always fully reflect all available information. In other words, according to the EMH, it is impossible to consistently \u0026ldquo;beat the market\u0026rdquo; by identifying undervalued or overvalued assets because all relevant information is already incorporated into the prices.\nThe concept of the Efficient Market Hypothesis was developed by economist Eugene Fama in the 1960s and has been a fundamental principle in modern finance theory ever since. The hypothesis is based on three key assumptions:\nPerfect Competition: The hypothesis assumes that financial markets are characterized by perfect competition, meaning there are many buyers and sellers, and no individual participant can significantly influence prices. Rational Investors: It assumes that all market participants are rational and always act in a way to maximize their expected utility, based on all available information. Immediate Information Processing: The EMH assumes that all relevant information is available to investors at the same time and that they immediately and accurately process that information to adjust prices accordingly. The Efficient Market Hypothesis is usually divided into three forms:\nWeak Form EMH: This form of the hypothesis asserts that stock prices already reflect all past trading information, including price and volume data. In other words, technical analysis, which relies on historical price patterns, should not be able to consistently predict future price movements. Semi-Strong Form EMH: This version of the hypothesis states that stock prices already reflect all publicly available information, including financial statements, news, and other non-confidential information. Thus, fundamental analysis, which involves examining a company\u0026rsquo;s financials and prospects, should not provide an advantage in predicting future prices. Strong Form EMH: The strong form asserts that stock prices already reflect all information, whether it is public or private. This includes insider information that is not available to the general public. If the strong form holds, then no individual or entity, not even insiders, can consistently earn above-average returns based on private information. The Fundamental Law of Active Portfolio Management The Fundamental Law of Active Portfolio Management, also known as the Fundamental Law of Active Management or simply the Fundamental Law, is a key concept in the field of portfolio management. It was developed by Richard Grinold, a finance professor, and Ronald Kahn, a quantitative analyst, and was first published in their 1999 book \u0026ldquo;Active Portfolio Management.\u0026rdquo;\nThe Fundamental Law relates a portfolio\u0026rsquo;s expected excess return to two fundamental components: skill and breadth. It provides a quantitative framework for evaluating the performance of active portfolio managers, helping to distinguish between luck and skill in their investment decisions.\nThe formula for the Fundamental Law of Active Portfolio Management is as follows:\nInformation Ratio (IR) = IC (Information Coefficient) * √(BR) (Breadth)\nInformation Ratio (IR): The Information Ratio measures the portfolio manager\u0026rsquo;s ability to generate excess returns relative to a benchmark, adjusted for the level of risk taken. It is calculated as the ratio of the expected excess return (active return) to the tracking error of the portfolio. The higher the Information Ratio, the better the manager\u0026rsquo;s skill in generating consistent excess returns. Information Coefficient (IC): The Information Coefficient represents the manager\u0026rsquo;s ability to generate forecasts that are accurate and valuable. It quantifies the correlation between the manager\u0026rsquo;s forecasted returns and the realized returns. A perfect forecast would have an IC of 1, while an IC of 0 indicates that the manager\u0026rsquo;s forecasts are no better than random guesses. Breadth (BR): The Breadth component captures the number of independent investment opportunities that the portfolio manager can exploit. It reflects the diversification of the active positions within the portfolio. A larger breadth implies more opportunities to generate excess returns. The Fundamental Law states that to achieve a higher Information Ratio, a portfolio manager can do one of the following:\nIncrease the Information Coefficient (IC): Improve the accuracy of their forecasts and the ability to identify mispriced assets or alpha-generating opportunities. Increase the Breadth (BR): Diversify the portfolio to include more independent alpha sources, which reduces the impact of idiosyncratic risk and improves the overall risk-adjusted performance. The Fundamental Law of Active Portfolio Management is a valuable tool for understanding the relationship between skill, diversification, and the ability to generate alpha in active portfolio management. It helps investors and portfolio managers assess the effectiveness of their investment strategies and identify potential areas for improvement.\nPortfolio Optimization and efficient frontier Mean-Variance Optimization (MVO) is a widely used quantitative approach in finance and portfolio management to construct an optimal portfolio that maximizes expected returns for a given level of risk or minimizes risk for a given level of expected returns. It was first introduced by Harry Markowitz in his seminal paper \u0026ldquo;Portfolio Selection\u0026rdquo; in 1952, which laid the foundation for modern portfolio theory.\nThe key idea behind Mean-Variance Optimization is to find the allocation of assets in a portfolio that strikes a balance between the desire for higher returns and the aversion to risk. The process involves the following steps:\nExpected Returns: Investors first estimate the expected returns of each asset in the portfolio based on historical data, forecasts, or other relevant information. These expected returns represent the mean or average return that investors expect to earn from each asset. Risk (Variance or Standard Deviation): The risk of an asset is typically measured by its variance or standard deviation. Variance quantifies the dispersion of an asset\u0026rsquo;s returns from its expected return. Standard deviation is simply the square root of variance. The higher the variance (or standard deviation), the higher the asset\u0026rsquo;s risk. Covariance and Correlation: Investors also need to calculate the covariance or correlation between each pair of assets in the portfolio. Covariance measures how two assets move together, while correlation standardizes the covariance to a value between -1 and +1, where -1 indicates a perfect negative relationship, +1 indicates a perfect positive relationship, and 0 indicates no relationship. Efficient Frontier: Mean-Variance Optimization seeks to find the combination of assets that generates the highest expected return for a given level of risk or the lowest risk for a given level of expected return. This set of optimal portfolios is referred to as the \u0026ldquo;efficient frontier.\u0026rdquo; It represents the set of portfolios that provides the best risk-reward trade-offs. Risk Tolerance: Finally, investors must define their risk tolerance level, which indicates how much risk they are willing to bear in pursuit of higher returns. The choice of portfolio from the efficient frontier will depend on an investor\u0026rsquo;s risk preferences. Mean-Variance Optimization has been a cornerstone of modern portfolio theory and has greatly influenced the practice of portfolio management. However, critics argue that it makes some simplifying assumptions, such as assuming that returns follow a normal distribution and that investors are solely focused on risk and return, neglecting other aspects like liquidity preferences or behavioral biases. As a result, alternative approaches, like Black-Litterman model and Conditional Value-at-Risk (CVaR) optimization, have been proposed to address some of these limitations.\nLearning Algorithms for Trading Parametric vs non parametric A parametric learner, in the context of machine learning, refers to a model that makes strong assumptions about the underlying data distribution. It assumes a specific functional form or structure for the relationship between the input variables and the output variable. In other words, the model is characterized by a fixed number of parameters that need to be estimated from the training data. Examples of parametric learners include linear regression, logistic regression, and neural networks. Once the parameters are estimated, the model can make predictions or classifications based on new input data. Parametric learners tend to be computationally efficient and require less training data, but their performance heavily depends on the accuracy of the assumed parametric form.\nOn the other hand, a non-parametric learner does not make explicit assumptions about the underlying data distribution or functional form. Instead, it seeks to directly learn the relationship between the input variables and the output variable from the training data. Non-parametric learners, such as k-nearest neighbors, decision trees, and support vector machines, can adapt to more complex and flexible relationships in the data. They typically have more parameters and their complexity grows with the size of the training set. Non-parametric learners may require more data for training and can be computationally more expensive, but they offer greater flexibility in capturing intricate patterns in the data.\nKNN K-Nearest Neighbors (KNN) is a popular algorithm used in machine learning for both classification and regression tasks. In the context of classification, KNN predicts the class of a new data point based on the classes of its K nearest neighbors in the feature space. The algorithm assumes that similar instances tend to have similar labels.\nOverfitting occurs when a model learns too much from the training data, including noise and irrelevant patterns, which leads to poor generalization on unseen data. KNN can be prone to overfitting when the value of K is too small. With a small K, the model can become overly sensitive to the local characteristics of the training data, potentially causing the model to memorize the training examples and perform poorly on new instances.\nimport numpy as np class KNNClassifier: def __init__(self, k): self.k = k def fit(self, X, y): self.X_train = X self.y_train = y def predict(self, X): y_pred = [] for sample in X: distances = np.sqrt(np.sum((self.X_train - sample)**2, axis=1)) nearest_indices = np.argsort(distances)[:self.k] nearest_labels = self.y_train[nearest_indices] unique, counts = np.unique(nearest_labels, return_counts=True) y_pred.append(unique[np.argmax(counts)]) return y_pred Kernel regression In kernel regression, the main idea is to assign weights to nearby data points based on their distance from the point being estimated. These weights, known as kernel weights, determine the influence of each data point on the estimation. The closer a data point is to the target point, the higher its weight and vice versa.\nRMSE Root Mean Square Error (RMSE) is a commonly used metric to evaluate the performance of regression models. It measures the average deviation between the predicted and actual values of the target variable. RMSE provides a quantitative measure of the model\u0026rsquo;s accuracy by calculating the square root of the mean of squared differences between the predicted and actual values. Pros of RMSE: RMSE takes into account both the magnitude and direction of errors, giving a comprehensive assessment of the model\u0026rsquo;s performance. It is widely used and easily interpretable, allowing for meaningful comparisons between different models or techniques. RMSE penalizes larger errors more heavily than mean absolute error, making it more sensitive to outliers. Cons of RMSE: Since RMSE is based on squared differences, it amplifies the impact of large errors, which can be problematic if outliers or extreme values are present in the data. RMSE does not have the same unit of measurement as the target variable, making it less interpretable in terms of the original scale. It assumes that errors follow a Gaussian distribution and that there is no heteroscedasticity (unequal variance) in the residuals. Here\u0026rsquo;s an example of Python code for calculating RMSE from scratch:\nimport numpy as np def rmse(y_true, y_pred): squared_errors = (y_true - y_pred) ** 2 mean_squared_error = np.mean(squared_errors) rmse = np.sqrt(mean_squared_error) return rmse In the code above, the rmse function takes the true values (y_true) and predicted values (y_pred) as input. It calculates the squared differences between the true and predicted values, computes the mean squared error, and returns the square root of the mean squared error as the RMSE.\nWhen using this implementation, it\u0026rsquo;s important to ensure that the true and predicted values are in the same format and shape. Additionally, data preprocessing, feature engineering, and model selection should be performed prior to calculating RMSE to ensure accurate evaluation of the model\u0026rsquo;s performance.\nMAE Mean Absolute Error (MAE) is a widely used metric for evaluating the performance of regression models. It measures the average absolute difference between the predicted and actual values of the target variable. MAE provides a straightforward measure of the model\u0026rsquo;s accuracy without considering the direction of errors. Pros of MAE: MAE is robust to outliers since it does not involve squaring the differences between predicted and actual values. It treats all errors equally regardless of their magnitude. It is easily interpretable as it has the same unit of measurement as the target variable, allowing for direct comparison and understanding of the model\u0026rsquo;s performance. MAE does not make any assumptions about the underlying distribution of errors and is less sensitive to heteroscedasticity. Cons of MAE: Since MAE does not square the errors, it may be less sensitive to large errors compared to metrics like RMSE, which can be a disadvantage when outliers need to be given more weight in the evaluation. MAE does not provide information on the variance or distribution of errors, making it less informative for certain types of analysis or decision-making. Here\u0026rsquo;s an example of Python code for calculating MAE from scratch:\nimport numpy as np def mae(y_true, y_pred): absolute_errors = np.abs(y_true - y_pred) mean_absolute_error = np.mean(absolute_errors) return mean_absolute_error In the code above, the mae function takes the true values (y_true) and predicted values (y_pred) as input. It calculates the absolute differences between the true and predicted values, computes the mean of these absolute differences, and returns it as the MAE.\nWhen using this implementation, ensure that the true and predicted values are in the same format and shape. Additionally, perform any necessary data preprocessing, feature engineering, and model selection before calculating MAE to ensure accurate evaluation of the model\u0026rsquo;s performance.\nCross validation Cross-validation is a resampling technique used in machine learning to assess the performance and generalization ability of a model. It involves partitioning the available data into multiple subsets or folds, where each fold is used as both a training set and a validation set in a series of iterations. Cross-validation provides a more reliable estimate of the model\u0026rsquo;s performance by evaluating its consistency across different data subsets. Pros of Cross-Validation: Cross-validation provides a more robust evaluation of the model\u0026rsquo;s performance compared to a single train-test split, as it utilizes multiple subsets of the data for training and testing. It helps to estimate how well the model generalizes to unseen data and provides insights into the model\u0026rsquo;s stability and consistency. Cross-validation allows for tuning hyperparameters and selecting the best model configuration by comparing the performance across different folds. Cons of Cross-Validation: Implementing cross-validation can be computationally expensive, especially for large datasets or complex models, as it requires fitting and evaluating the model multiple times. In some cases, the performance of a model can vary significantly across different folds, leading to a less reliable estimate of its generalization ability. Cross-validation may not account for certain types of data dependencies, such as time-series data, where the order of observations is important. Here\u0026rsquo;s an example of Python code for implementing k-fold cross-validation from scratch:\nimport numpy as np def cross_validation(X, y, model, k): n = len(X) fold_size = n // k scores = [] for i in range(k): start = i * fold_size end = start + fold_size X_train = np.concatenate((X[:start], X[end:]), axis=0) y_train = np.concatenate((y[:start], y[end:]), axis=0) X_val = X[start:end] y_val = y[start:end] model.fit(X_train, y_train) score = model.evaluate(X_val, y_val) # Evaluation metric specific to the model scores.append(score) return scores In the code above, the cross_validation function takes the input features (X), target variable (y), the model to evaluate, and the number of folds (k) as input. It iteratively partitions the data into training and validation sets, fits the model on the training data, and evaluates its performance using a specific evaluation metric. The function returns a list of scores obtained from each fold.\nIt\u0026rsquo;s important to note that the code provided is a basic implementation and may need to be modified or extended depending on the specific requirements of the model and evaluation metric. Additionally, the model.fit and model.evaluate methods represent placeholder functions and should be replaced with the appropriate methods for the chosen model.\nEnsemble learners Ensemble learning is a machine learning technique that combines multiple individual models, called base models or weak learners, to improve predictive performance and generalization ability. The idea behind ensemble learning is to leverage the diversity of the base models and aggregate their predictions to make a final prediction that is often more accurate and robust than that of any individual model. Ensemble learners can be categorized into two main types: bagging and boosting.\nBagging: Bagging stands for bootstrap aggregating. It involves training multiple base models independently on different subsets of the training data, created through bootstrap sampling (sampling with replacement). The predictions from these models are then combined, typically through majority voting (for classification) or averaging (for regression), to obtain the final prediction. The goal is to reduce variance and improve generalization by reducing the impact of individual noisy or overfitting models. Boosting: Boosting aims to sequentially train a series of base models, where each subsequent model focuses on correcting the mistakes made by the previous models. In boosting, the training data is reweighted, giving higher importance to the instances that were misclassified by previous models. The predictions of the base models are combined by weighted voting or weighted averaging to obtain the final prediction. Boosting methods, such as AdaBoost, Gradient Boosting, and XGBoost, often achieve high accuracy by iteratively building strong models from weak ones. Here\u0026rsquo;s an example of Python code for implementing ensemble learning using the Random Forest algorithm, which is a popular ensemble method based on bagging:\nfrom sklearn.ensemble import RandomForestClassifier # Create an ensemble of 100 decision tree classifiers ensemble = RandomForestClassifier(n_estimators=100) # Train the ensemble on the training data ensemble.fit(X_train, y_train) # Make predictions using the ensemble predictions = ensemble.predict(X_test) In the code above, the RandomForestClassifier class from the scikit-learn library is used to create an ensemble of 100 decision tree classifiers. The n_estimators parameter specifies the number of base models in the ensemble. The ensemble is then trained on the training data (X_train and y_train), and predictions are made on the test data (X_test) using the predict method.\nReinforcement Learning Reinforcement Learning (RL) is a type of machine learning paradigm where an agent learns to make decisions and take actions in an environment to achieve a specific goal. Unlike supervised learning, where the model is trained on labeled data, or unsupervised learning, where the model finds patterns and structures in unlabeled data, RL focuses on learning through interaction with an environment and receiving feedback in the form of rewards or penalties.\nThe basic components of a reinforcement learning system are as follows:\nAgent: The agent is the learner or decision-maker that interacts with the environment. It makes observations, takes actions, and learns from the rewards or penalties it receives. Environment: The environment is the context or setting in which the agent operates. It can be anything from a virtual environment in a computer simulation to a real-world scenario. Actions: At each time step, the agent chooses an action from a set of possible actions based on its current state and the information it has learned from previous interactions. State: The state represents the current situation or context of the agent within the environment. It captures the relevant information necessary for the agent to make decisions. Rewards: After taking an action, the agent receives feedback in the form of rewards or penalties from the environment. Positive rewards encourage the agent to take actions that lead to the desired goal, while negative rewards discourage undesired actions. The objective of the agent in reinforcement learning is to learn a policy, which is a mapping from states to actions, that maximizes the cumulative reward over time. The agent employs exploration and exploitation strategies to balance between trying out new actions (exploration) and exploiting the knowledge it has gained so far to make optimal decisions (exploitation).\nReinforcement learning has been successfully applied in various fields, including robotics, game playing (e.g., AlphaGo), autonomous vehicles, recommendation systems, finance, and more. Deep Reinforcement Learning (DRL), which combines reinforcement learning with deep neural networks, has shown remarkable achievements in complex tasks by utilizing deep learning\u0026rsquo;s ability to handle high-dimensional input data.\nOne of the key challenges in reinforcement learning is the trade-off between exploration and exploitation, and the potential for the agent to get stuck in suboptimal solutions (local optima). Researchers continue to develop new algorithms and techniques to address these challenges and further advance the capabilities of reinforcement learning in practical applications.\nQ Learning Q-learning is a popular model-free reinforcement learning algorithm used to find an optimal policy for an agent to make decisions in an environment. It was developed by Christopher Watkins in his PhD thesis in 1989. Q-learning is a type of Temporal Difference (TD) learning, which means it learns from the difference between its predictions and the observed rewards obtained from the environment.\nThe central idea behind Q-learning is to estimate the value of taking a particular action in a given state, called the action-value function or Q-function. The Q-value represents the expected cumulative reward the agent can achieve by starting in a particular state, taking a specific action, and following an optimal policy thereafter.\nThe Q-learning algorithm works as follows:\nInitialization: Initialize the Q-function arbitrarily for all state-action pairs. Typically, the Q-values are initialized to zero, or a small random value. Exploration vs. Exploitation: The agent interacts with the environment by taking actions based on its current policy. Initially, it often explores the environment by selecting random actions (exploration) to discover new strategies. As the learning progresses, the agent starts exploiting the Q-values it has learned to choose the actions with the highest Q-values. Update Q-values: After each action, the agent receives a reward from the environment and observes the new state. The Q-value for the (state, action) pair is updated using the Bellman equation: Q(s, a) = Q(s, a) + α * [r + γ * max Q(s\u0026rsquo;, a\u0026rsquo;) - Q(s, a)]\nwhere:\nQ(s, a): The Q-value for state s and action a. α: The learning rate, which determines how much the agent updates its Q-values based on new information. r: The reward received by taking action a in state s. γ: The discount factor, which balances immediate rewards versus future rewards. max Q(s\u0026rsquo;, a\u0026rsquo;): The maximum Q-value for the next state s\u0026rsquo; and all possible actions a\u0026rsquo;. Continue Exploration and Exploitation: The agent continues to interact with the environment, updating Q-values after each action, and refining its policy to improve performance over time. Q-learning is known to converge to the optimal Q-values and an optimal policy in the limit as the agent explores the environment indefinitely. It is especially effective in situations where the agent has no prior knowledge of the environment, and the transition model and reward function are unknown.\nQ-learning has been widely used in various applications, such as game playing, robotic control, and optimization problems, and has paved the way for more advanced deep reinforcement learning algorithms like Deep Q-Networks (DQNs) that leverage deep neural networks to approximate the Q-function in high-dimensional state spaces.\n","date":"2023-06-04T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/machine_learning_for_trading/","section":"posts","tags":["linear-regression","knn","trading","ml4t","technical-analysis","indicators","parametric","CAPM","algorithmic-trading","rolling-statistics","hedge-funds","beta","alpha"],"title":"Machine Learning for Trading"},{"categories":["gatech"],"contents":"General overview and key concepts In plain English, optimization is the action of making the best or most effective use of a situation or resource. Optimization problems are of great practical interest. For example, in manufacturing, how should one cut plates of a material so that the waste is minimized? In business, how should a company allocate the available resources that its profit is maximized? Some of the first optimization problems have been solved in ancient Greece and are regarded among the most significant discoveries of that time. In the first century A.D., the Alexandrian mathematician Heron solved the problem of finding the shortest path between two points by way of the mirror.\nThis result, also known as Heron’s theorem of the light ray, can be viewed as the origin of the theory of geometrical optics. The problem of finding extreme values gained special importance in the seventeenth century, when it served as one of the motivations in the invention of differential calculus, which is the foundation of the modern theory of mathematical optimization.\nGeneric form of optimization problem: $min$ $f(x)$ $s.t.$ $x \\in X $\nThe vector $x = (x_1, . . . , x_n)$ is the optimization variable (or decision variable) of the problem The function $f$ is the objective function A vector $x$ is called optimal, or a solution (not optimal solution) of the problem, if it has the smallest objective value among all vectors that satisfy the constraints $X$ is the set of inequality constraints Mathematical ingredients: Encode decisions/actions as decision variables whose values we are seeking Identify the relevant problem data Express constraints on the values of the decision variables as mathematical relationships (inequalities) between the variables and problem data Express the objective function as a function of the decision variables and the problem data. Minimize or Maximize an objective function of decision variable subject to constraints on the values of the decision variables.\nmin or max f(x1, x2, .... , xn) subject to gi(x1, x2, ...., ) \u0026lt;= bi i = 1,....,m xj is continuous or discrete j = 1,....,n The problem setting Finite number of decision variables A single objective function of decision variables and problem data Multiple objective functions are handled by either taking a weighted combination of them or by optimizing one of the objectives while ensuring the other objectives meet target requirements. The constraints are defined by a finite number of inequalities or equalities involving functions of the decision variables and problem data There may be domain restrictions (continuous or discrete) on some of the variables The functions defining the objective and constraints are algebraic (typically with rational coefficients) Minimization vs Maximization Without the loss of generality, it is sufficient to consider a minimization objective since maximization of objective function is minimization of the negation of the objective function Program vs Optimization A program or mathematical program is an optimization problem with a finite number of variables and constraints written out using explicit mathematical (algebraic) expressions The word program means plan/planning Early application of optimization arose in planning resource allocations and gave rise to programming to mean optimization (predates computer programming) Example: Designing a box: Given a $1$ feet by $1$ feet piece of cardboard, cut out corners and fold to make a box of maximum volume: Decision: $x$ = how much to cut from each of the corners? Alternatives: $0\u0026lt;=x\u0026lt;=1/2$ Best: Maximize volume: $V(x) = x(1-2x)^2$ ($x$ is the height and $(1-2x)^2$ is the base, and their product is the volume) Optimization formulation: $max$ $x(1-2x)^2$ subject to $0\u0026lt;=x\u0026lt;=1/2$ (which are the constraints in this case)\nThis is an unconstrained optimization problem since the constraint is a simple bound based.\nExample: Data Fitting: Given $N$ data points $(y_1, x_1)\u0026hellip;(y_N, x_N)$ where $y_i$ belongs to $\\mathbb{R}$ and $x_i$ belongs to $\\mathbb{R}^n$, for all $i = 1..N$, find a line $y = a^Tx+b$ that best fits the data. Decision: A vector $a$ that belongs to $\\mathbb{R}^n$ and a scalar $b$ that belongs to $\\mathbb{R}$ Alternatives: All $n$-dimensional vectors and scalars Best: Minimize the sum of squared errors Optimization formulation: $\\begin{array}{ll}\\min \u0026amp; \\sum_{i=1}^N\\left(y_i-a^{\\top} x_i-b\\right)^2 \\ \\text { s.t. } \u0026amp; a \\in \\mathbb{R}^n, b \\in \\mathbb{R}\\end{array}$\nThis is also an unconstrained optimization problem.\nExample: Product Mix: A firm make $n$ different products using $m$ types of resources. Each unit of product $i$ generates $p_i$ dollars of profit, and requires $r_{ij}$ units of resource $j$. The firm has $u_j$ units of resource $j$ available. How much of each product should the firm make to maximize profits? Decision: how much of each product to make Alternatives: defined by the resource limits Best: Maximize profits Optimization formulation: Sum notation: $\\begin{array}{lll}\\max \u0026amp; \\sum_{i=1}^n p_i x_i \\ \\text { s.t. } \u0026amp; \\sum_{i=1}^n r_{i j} x_i \\leq u_j \u0026amp; \\forall j=1, \\ldots, m \\ \u0026amp; x_i \\geq 0 \u0026amp; \\forall i=1, \\ldots, n\\end{array}$ Matrix notation: $\\begin{array}{cl}\\max \u0026amp; p^{\\top} x \\ \\text { s.t. } \u0026amp; R x \\leq u \\ \u0026amp; x \\geq 0\\end{array}$\nExample: Project investment A firm is considering investing in $n$ different R\u0026amp;D projects. Project $j$ requires an investment of $c_j$ dollars and promises a return of $r_j$ dollars. The firm has a budget of $B$ dollars. Which projects should the firm invest in? Decision: Whether or not to invest in project Alternatives: Defined by budget Best: Maximize return on investment Sum notation: $\\begin{aligned} \\max \u0026amp; \\sum_{j=1}^n r_j x_j \\ \\text { s.t. } \u0026amp; \\sum_{j=1}^n c_j x_j \\leq B \\ \u0026amp; x_j \\in{0,1} \\forall j=1, \\ldots, n\\end{aligned}$ Matrix notation: $\\begin{aligned} \\max \u0026amp; r^{\\top} x \\ \\text { s.t. } \u0026amp; c^{\\top} x \\leq B \\ \u0026amp; x \\in{0,1}^n\\end{aligned}$\nThis is not an unconstrained problem.\nIdentify basic portfolio optimization and associated issues Examine the Markowitz Portfolio Optimization approach Markowitz Principle: Select a portfolio that attempts to maximize the expected return and minimize the variance of returns (risk) For multi objective problem (like defined by the Markowitz Principle), two objectives can be combined: Maximize Expected Return - $\\lambda$*risk Maximize Expected Return subject to risk \u0026lt;= s_max (constraint on risk) Minimize Risk subject to return \u0026gt;= r_min (threshold on expected returns) Optimization Problem Statement Given $1000, how much should we invest in each of the three stocks MSFT, V and WMT so as to : - have a one month expected return of at least a given threshold - minimize the risk(variance) of the portfolio return Decision: investment in each stock alternatives: any investment that meets the budget and the minimum expected return requirement best: minimize variance Key trade-off: How much of the detail of the actual problem to consider while maintaining computational tractability of the mathematical model? Requires making simplifying assumptions, either because some of the problem characteristics are not well-defined mathematically, or because we wish to develop a model that can actually be solved Need to exercise great caution in these assumptions and not loose sight of the true underlying problem Assumptions: No transaction cost Stocks does not need to be bought in blocks (any amount \u0026gt;=0 is fine) Optimization Process: Decision Problem -\u0026gt; Model -\u0026gt; Data Collection -\u0026gt; Model Solution -\u0026gt; Analysis -\u0026gt; Problem solution No clear cut recipe Lots of feedbacks and iterations Approximations and assumptions involved in each stage Success requires good understanding of the actual problem (domain knowledge is important) Classification of optimization problems The tractability of a large scale optimization problem depends on the structure of the functions that make up the objective and constraints, and the domain restrictions on the variables. Functions Variable domains Problem Type Difficulty All linear Continuous variables Linear Program Easy Some nonlinear Continuous variables Nonlinear Program or Nonlinear Optimization Problem Easy/Difficult Linear/nonlinear Some discrete Integer Problem or Discrete Optimization Problem Difficult Optimization Problem Description Difficulty Linear Programming A linear programming problem involves maximizing or minimizing a linear objective function subject to a set of linear constraints Easy to moderate Nonlinear Programming A nonlinear programming problem involves optimizing a function that is not linear, subject to a set of nonlinear constraints Moderate to hard Quadratic Programming A quadratic programming problem involves optimizing a quadratic objective function subject to a set of linear constraints Moderate Convex Optimization A convex optimization problem involves optimizing a convex function subject to a set of linear or convex constraints Easy to moderate Integer Programming An integer programming problem involves optimizing a linear or nonlinear objective function subject to a set of linear or nonlinear constraints, where some or all of the variables are restricted to integer values Hard Mixed-integer Programming A mixed-integer programming problem is a generalization of integer programming where some or all of the variables can be restricted to integer values or continuous values Hard Global Optimization A global optimization problem involves finding the global optimum of a function subject to a set of constraints, which may be nonlinear or non-convex Hard Stochastic Optimization A stochastic optimization problem involves optimizing an objective function that depends on random variables, subject to a set of constraints Hard Subclasses of NLP (Non Linear Problem) Unconstrained optimization: No constraints or simple bound constraints on the variables (Box design example above) Quadratic programming: Objectives and constraints involve quadratic functions (Data fitting example above), subset of NLP Subclasses of IP (Integer Programming) Mixed Integer Linear Program All linear functions Some variables are continuous and some are discrete Mixed Integer Nonlinear Program (MINLP) Some nonlinear functions Some variables are continuous and some are discrete Mixed Integer Quadratic Program (MIQLP) Nonlinear functions are quadratic Some variables are continuous and some are discrete subset of MINLP Why and how to classify? Important to recognize the type of an optimization problem: to formulate problems to be amenable to certain solution methods to anticipate the difficulty of solving the problem to know which solution methods to use to design customized solution methods how to classify: check domain restriction on variables check the structure of the functions involved Linear Algebra Primer Vectors: Vectors are mathematical objects that have both magnitude and direction. They can be represented as ordered lists of numbers or as arrows in space. Vectors are often used to represent physical quantities such as velocity or force. In two-dimensional space, a vector is represented by an ordered pair of numbers (x, y), and in three-dimensional space, it is represented by an ordered triple (x, y, z). Vectors can be added and subtracted, and multiplied by a scalar (a single number). They also have properties such as the dot product and cross product. In computer science and programming, a vector is also a data structure that can store multiple values of the same type. Matrices: Matrices are rectangular arrays of numbers that can be used to represent linear transformations and systems of linear equations. They are also used to represent data in statistics and machine learning. Linear equations: Linear equations are equations that involve only linear terms, such as x and y, rather than more complex functions like sin(x) or e^x. They can be represented using matrices and solved using techniques like Gaussian elimination. Eigenvectors and eigenvalues: Eigenvectors are special vectors that are unchanged by a linear transformation, except for a scaling factor. Eigenvalues are the corresponding scaling factors. They are useful in many applications, such as analyzing networks and modeling physical systems. Vector spaces: Vector spaces are sets of vectors that satisfy certain properties, such as closure under addition and scalar multiplication. They are used to represent many mathematical objects, such as functions and polynomials. Inner products: An inner product is a function that takes two vectors as input and produces a scalar as output. It is used to measure the angle between vectors and the length of a vector. Orthogonality: Two vectors are orthogonal if they are perpendicular to each other. Orthogonal vectors have many important applications, such as in least squares regression and in the Gram-Schmidt process for orthonormalizing a set of vectors. The second derivative test is a method used in calculus to determine the nature of the critical points of a function, which can be either a maximum, minimum, or saddle point. A critical point is a point on the graph of a function where the derivative is either zero or undefined. To apply the second derivative test, we need to find the critical points of the function by setting its first derivative equal to zero and solving for the variables. Then, we can determine the nature of these critical points by examining the sign of the second derivative of the function evaluated at the critical points. Specifically: If the second derivative is positive at a critical point, then the function has a local minimum at that point. If the second derivative is negative at a critical point, then the function has a local maximum at that point. If the second derivative is zero at a critical point, then the second derivative test is inconclusive, and we need to use other methods to determine the nature of the critical point. The vectors $x$ and $y$ are orthogonal if $x^Ty=0$, they make an acute angle if $x^Ty\u0026gt;0$ and an obtuse angle if $x^Ty\u0026lt;0$ Also, $x^Ty=||x||.||y||cos\\theta$ A set of vectors are linearly independent if none of the vectors can be written as a linear combination of the others. That is the unique solution to the system of equations. There can be at most $n$ linearly independent vectors in $R^n$ Any collection of $n$ linearly independent vectors in $R$ defines a basis (or a coordinate system) of $R^n$, any vector in $R^n$ can be written as a linear combination of the basis vectors The unit vectors $e^1= [1, 0, \u0026hellip;0]^T$, $e^2= [0, 1, \u0026hellip;0]^T$,\u0026hellip;,$e^n= [0, 0, \u0026hellip;1]^T$, define the standard basis for $R^n$ The rank of a matrix is a measure of the \u0026ldquo;nondegeneracy\u0026rdquo; of the matrix and it is one of the most important concepts in linear algebra. It is defined as the dimension of the vector space spanned by its columns or rows. Intuitively, it represents the number of linearly independent columns or rows in the matrix. row rank = column rank = rank($A$). $A$ is full rank if rank($A$) = min($m$, $n$) A system of equations has a solution when the equations are consistent, meaning that there is at least one set of values for the variables that satisfies all of the equations. If the equations are inconsistent, meaning that there is no set of values that satisfies all of the equations, then the system of equations has no solution. An affine function is a function that is defined as a linear combination of variables, with the addition of a constant term. An affine function can be written as: f(x) = a_1x_1 + a_2x_2 + ... + a_nx_n + b Where x_1, x_2, \u0026hellip;, x_n are the input variables, a_1, a_2, \u0026hellip;, a_n are the coefficients, and b is a constant term. An affine function is a generalization of a linear function, which does not have the constant term.\nMultivariate Calculus Primer Hessian matrix The Hessian matrix is a square matrix of second-order partial derivatives of a scalar-valued function of multiple variables. The Hessian matrix of a scalar-valued function f(x) of n variables x = (x1, x2, \u0026hellip;, xn) is defined as the matrix of second-order partial derivatives of f with respect to x, with the i-th row and j-th column containing the second partial derivative of f with respect to xi and xj. The Hessian matrix is often used in optimization, for example, to find the local minima or maxima of a function. A point where the Hessian is positive definite is a local minimum, while a point where the Hessian is negative definite is a local maximum. If the Hessian is positive semi-definite, it\u0026rsquo;s a saddle point. It is important to notice that the Hessian Matrix is symmetric, therefore it has real eigenvalues and it is diagonalisable. $H(f)_{i,j}=\\frac{\\partial^2f}{\\partial x_i \\partial x_j}$ The symmetry of second derivatives (also called the equality of mixed partials) refers to the possibility of interchanging the order of taking partial derivatives of a function. The symmetry is the assertion that the second-order partial derivatives satisfy the identity. In the context of partial differential equations it is called the Schwarz integrability condition. $\\frac{\\partial^2f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2f}{\\partial x_j \\partial x_i}$ Taylor Approximation The Taylor series of a real or complex-valued function f (x) that is infinitely differentiable at a real or complex number a is the power series.\nLet $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ be a differentiable function and $\\mathbf{x}^0 \\in \\mathbb{R}^n$.\nFirst order Taylor\u0026rsquo;s approximation of $f$ at $\\mathbf{x}^0$ : $$ f(\\mathbf{x}) \\approx f\\left(\\mathbf{x}^0\\right)+\\nabla f\\left(\\mathbf{x}^0\\right)^{\\top}\\left(\\mathbf{x}-\\mathbf{x}^0\\right) $$ Second order Taylor\u0026rsquo;s approximation of $f$ at $\\mathbf{x}^0$ : $$ f(\\mathbf{x}) \\approx f\\left(\\mathbf{x}^0\\right)+\\nabla f\\left(\\mathbf{x}^0\\right)^{\\top}\\left(\\mathbf{x}-\\mathbf{x}^0\\right)+\\frac{1}{2}\\left(\\mathbf{x}-\\mathbf{x}^0\\right)^{\\top} \\nabla^2 f\\left(\\mathbf{x}^0\\right)\\left(\\mathbf{x}-\\mathbf{x}^0\\right) $$ ` Sets in Optimization Problems A set is closed if it includes its boundary points. Intersection of closed sets is closed. Typically, if none of inequalities are strict, then the set is closed. A set is convex if a line segment connecting two points in the set lies entirely in the set. A set is bounded if it can be enclosed in a large enough (hyper)-sphere or a box. A set that is both bounded and closed is called compact. $R^2$ is closed but not bounded $x^2+y^2\u0026lt;1$ is bounded but not closed $x+y\u0026gt;=1$ is closed but not bounded $x^2+y^2\u0026lt;=1$ is closed and bounded (compact) An optimal solution of maximizing a convex function over a compact set lies on the boundary of the set. Convex Function A function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is convex if $$ f(\\lambda \\mathbf{x}+(1-\\lambda) \\mathbf{y}) \\leq \\lambda f(\\mathbf{x})+(1-\\lambda) f(\\mathbf{y}) \\quad \\forall \\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n \\text { and } \\lambda \\in[0,1] $$ \u0026ldquo;Function value at the average is less than the average of the function values\u0026rdquo; This also implies that $a^Tx+b$ is convex (and concave) For a convex function the first order Taylor\u0026rsquo;s approximation is a global under estimator A convex optimization problem has a convex objective and convex set of solutions. Linear programs (LPs) can be seen as a special case of convex optimization problems. In an LP, the objective function and constraints are linear, which means that the feasible region defined by the constraints is a convex set. As a result, the optimal solution to an LP is guaranteed to be at a vertex (corner) of the feasible region, which makes it a convex optimization problem. A twice differentiable univariate function is convex if $f^{\u0026rsquo;\u0026rsquo;}(x)\u0026gt;=0$ for all $x \\in R$ To generalize, a twice differentiable function is convex if and only if the Hessian matrix is positive semi definite. A positive semi-definite (PSD) matrix is a matrix that is symmetric and has non-negative eigenvalues. In the context of a Hessian matrix, it represents the second-order partial derivatives of a multivariate function and reflects the curvature of the function. If the Hessian is PSD, it indicates that the function is locally convex, meaning that it has a minimum value in the vicinity of that point. On the other hand, if the Hessian is not PSD, the function may have a saddle point or be locally non-convex. The PSD property of a Hessian matrix is important in optimization, as it guarantees the existence of a minimum value for the function. Sylvester\u0026rsquo;s criterion is a method for determining if a matrix is positive definite or positive semi-definite. The criterion states that a real symmetric matrix is positive definite if and only if all of its leading principal minors (i.e. determinants of the submatrices formed by taking the first few rows and columns of the matrix) are positive. If all the leading principal minors are non-negative, then the matrix is positive semi-definite. Operations preserving convexity Nonnegative weighted sum of convex functions is convex, i.e. if $f_i$ is convex and $\\alpha_i \\geq 0$ for all $i=1, \\ldots, m$, then $g(\\mathbf{x})=\\sum_{i=1}^m \\alpha_i f_i(\\mathbf{x})$ is convex. Maximum of convex functions is convex. Composition: Let $f: \\mathbb{R}^m \\rightarrow \\mathbb{R}$ be a convex function, and $g_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ be convex for all $i=1, \\ldots, m$. Then the composite function $$ h(\\mathbf{x})=f\\left(g_1(\\mathbf{x}), g_2(\\mathbf{x}), \\ldots, g_m(\\mathbf{x})\\right) $$ is convex if either $f$ is nondecreasing or if each $q_i$ is a linear function. Convexity Preserving Set Operations Intersection of convex sets is a convex set Intersection of non convex sets might be a convex set Union of two convex set might not be a convex set Sum of convex set is a convex set Product of convex set is a convex set Convex Optimization Problem An optimization problem (in minimization) form is a convex optimization problem, if the objective function is a convex function and constraint set is a convex set. The problem $min$ ${f(x) : x \\in X}$ is a convex optimization problem if $f$ is a convex function and $X$ is a convex set. To check if a given problem is convex, we can check convexity of each constraint separately. (This is a sufficient test, not necessary). $\\begin{array}{cl}\\min \u0026amp; f(\\mathbf{x}) \\ \\text { s.t. } \\end{array}$ $\\begin{array}{cl} g_i(\\mathbf{x}) \\leq b_i \\quad i=1, \\ldots, m \\ \u0026amp; h_j(\\mathbf{x})=d_j \\quad j=1, \\ldots, \\ell \\ \u0026amp; \\mathbf{x} \\in \\mathbb{R}^n\\end{array}$ Sufficient and necessary In mathematical logic, the terms \u0026ldquo;sufficient\u0026rdquo; and \u0026ldquo;necessary\u0026rdquo; are used to describe the relationship between two conditions. A condition A is considered \u0026ldquo;sufficient\u0026rdquo; for a condition B if whenever condition A is true, condition B is also guaranteed to be true. In other words, if A is sufficient for B, then having A implies having B. A condition B is considered \u0026ldquo;necessary\u0026rdquo; for a condition A if whenever condition B is false, condition A is also guaranteed to be false. In other words, if B is necessary for A, then not having B implies not having A. Together, \u0026ldquo;necessary and sufficient\u0026rdquo; means that the two conditions are equivalent, in the sense that if one is true, then the other must also be true, and if one is false, then the other must also be false. In mathematical terms, A is necessary and sufficient for B if and only if (A if and only if B). \u0026ldquo;being a male is a necessary condition for being a brother, but it is not sufficient — while being a male sibling is a necessary and sufficient condition for being a brother\u0026rdquo; Epigraph of a function An epigraph of a function is a graphical representation of the function\u0026rsquo;s domain and range. It is formed by the region above the graph of the function and the line x = a for some value of a. The epigraph represents all possible values of the function for all values of x greater than or equal to a. It is used in optimization problems to visualize the feasible region for the optimization variable. A function (in black) is convex if and only if the region above its graph (in green) is a convex set. This region is the function\u0026rsquo;s epigraph. The epigraph and the $\\alpha$ level set, of a convex function are convex sets. Outcomes of Optimization Any $x \\in X$ is a feasible solution of the optimization problem (P) Feasible solution = A solution that satisfies all the constraints An unbounded problem must be feasible An optimization problem is unbounded, if there are feasible solutions with arbitrarily small objective values.(limits to negative infinity for minimization problem) If $X=\\emptyset$ then no feasible solutions exist, and the problem (P) is said to be infeasible. If $X$ is a bounded set, then P cannot be unbounded The problem $\\min {3x+ 2y: x+ y\u0026lt;=1,x\u0026gt;=2,y\u0026gt;=2}$ is infeasible An optimization problem can have 4 possible outcomes. The outcome can be infeasible, unbounded (but feasible), have no optimal solution, have one optimal solution, or have multiple optimal solutions Existence of Optimal Solutions The Weierstrass extreme value theorem asserts that if you minimize a continuous function over a closed and bounded set in $R_n$, then the minimum will be achieved at some point in the set. Sufficient conditions: if the constraint set is bounded and non empty (feasible), then continuity and closedness guarantees an optimal solution exist. Local and Global Optimal Solutions Local optimal solutions are also global optimal solutions for convex optimization problems Every global optimal solution is a local optimal solution, but not vice versa The objective function value at different local optimal solutions may be different The objective function value at all global solutions must be the same If the problem is convex, since any local solution is a global solution, we can be sure that if we find a local solution, that is also a global solution. Idea of Improving Search Most optimization algorithms are based on the paradigm of improving search: Start from a feasible solution Move to a new feasible solution with a better objective value, Stop if not possible Repeat step 2 In general, we are only able to look in the \u0026ldquo;neighborhood\u0026rdquo; of the current solution in search of a better feasible solution (solutions that are within a small positive distance from the current solution) The move direction and step size should ensure that the new point is feasible and has an improved objective function value The improving search is better for local solutions, but for convex, in principal it can be used to find global solutions (by definition) Optimality Certificates Optimality Certificates and Relaxations A certificate or a stopping condition is an easily checkable condition such that if the current solution satisfies this condition then it is guaranteed to be optimal or near optimal Lower bound (a Priori) that the objective value of any solution cannot be lower than. Suppose we have a feasible solution $x\u0026rsquo;$ to an optimization problem with an objective value of $f(x\u0026rsquo;)$. Suppose the optimal objective value of the problem is $v*$. Then the absolute optimality gap of the solution is $gap(x\u0026rsquo;)$ = $f(x\u0026rsquo;) - v*$. And, the relative gap is $(f(x\u0026rsquo;) - v*)$/$v*$. The gap and rgap are always non negative. We do not know $v*$ but we do know the lower bound $L$. From definition, $L\u0026lt;=v*\u0026lt;=f(x\u0026rsquo;)$ A lower bound allows us to get an upper bound on the solution. For two optimization problem (P) $min$ $f(x)$ $s.t.$ $x \\in X $ and (Q) $min$ $g(x)$ $s.t.$ $x \\in Y $, Problem (Q) is a relaxation of P if $X \\subseteq Y$ (problem Q admits more solution than P) and/or $f(x) \u0026gt;= g(x) \\forall x \\in X $ Obtained by enlarging the feasible region and underapproximating the objective function. We do not have to do both of those (see equals to sign) Relaxation should be easier to solve. Optimal value of the relaxation provides a lower bound on the original problem. (This provides the optimality certificate.) If the relaxation is infeasible then the original problem is also infeasible. Suppose only the constraints are relaxed, then if a solution to the relaxation is feasible to the original problem then it must be an optimal solution to the original problem. A lower bound on the optimal value provides a way to certify the quality of a given solution. Lagrangian Relaxation and Duality Very specific type of relaxation Lagrangian relaxation is a method used in optimization to solve a difficult problem by relaxing some of its constraints and instead optimizing a modified objective function known as the Lagrangian function. The Lagrangian function is constructed by adding a penalty term for each constraint to the original objective function. The penalty term is multiplied by a non-negative Lagrange multiplier that represents the slack in the constraint. By choosing appropriate values for the multipliers, the relaxed problem can be made to approximate the original problem. The dual problem attempts to find the relaxation with the tightest bound (or the largest lower bound) Weak duality: dual optimal value \u0026lt;= original optimal value Some times we get strong duality (for LP) Unconstrained Optimization: Derivative Based Optimality Conditions Unconstrained, that is the constraints are only $x \\in R^n$ and twice differentiable If a solution is a local optimal solution of an unconstrained problem, then the gradient vanishes at the point (First order optimality condition) Hessian is a positive semidefinite (Second order optimality condition) The conditions are necessary but not sufficient. Example: $f(x_$)$ For example for, $f(x)=x^3$, at point 0, both of the conditions are satisfied. However, it is neither a local min or max. A sufficient (but not necessary) condition would be the gradient vanishing at the point, and is the Hessian is positive definite. Gradient Descent The gradient descent method moves from one iteration to the next by moving along the negative of the gradient direction in order to minimize the function. Gradient descent is a optimization algorithm used to minimize the error of a machine learning model. It is an iterative method that updates the model parameters in the direction of the negative gradient of the cost function with respect to the parameters. The gradient indicates the direction of steepest increase in the cost function and the descent refers to moving in the direction of negative gradient to find the minimum of the cost function. The learning rate determines the size of the steps taken to reach the minimum and the algorithm stops when the change in cost is below a certain threshold or when a maximum number of iterations is reached. Let $x^k$ be the current iterate, and we want to chose a downhill direction $d^k$ and a step size $a$ such that $f(x^k+ad^k)\u0026lt;f(x^k)$ By Taylor\u0026rsquo;s expansion, $f(x^k+ad^k) \\approx f(x^k) + a \\nabla f(x^k)^Td_k$ So we want $\\nabla f(x^k)^Td_k \u0026lt; 0$. The steepest descent direction is $d^k = - \\nabla f(x^k) $ Step size can be identified using a line search. That is, define a function $g(a) := f(x^k + ad^k)$. Choose $a$ to minimize $g$. It can also be a small fixed step size. Newton\u0026rsquo;s Method Newton\u0026rsquo;s Method is a second-order optimization algorithm that is used to find the minimum of a function. It is an iterative method that updates the parameters by using the gradient of the function (first derivative) and the Hessian matrix (second derivative) to find the direction of the local minimum. The algorithm starts with an initial guess for the parameters and iteratively updates them using the Newton-Raphson formula until the change in the parameters is below a certain threshold or a maximum number of iterations is reached. Newton\u0026rsquo;s Method is faster and more precise than gradient descent for well-behaved functions, but it can be sensitive to poor initialization and can get stuck in local minima. $x^{k+1} $ = $x^k$ - $[\\nabla^2$ $f(x_k)]^{-1}$ $ \\nabla f(x^k)$ If started close enough to local minimum and the Hessian is positive definite, then the method has quadratic convergence Not guaranteed to converge. The Newton direction may not be improving at all. If the Hessian is singular (or close to singular) at some iteration, we cannot proceed. Computing gradient as well as the Hessian and its inverse is expensive. Quasi-Newton Methods Blend of gradient descent and Newton\u0026rsquo;s method. Avoids computation of Hessian and its inverse $x^{k+1} $ = $x^k$ - $a_k H_k$ $ \\nabla f(x^k)$, where $H_k$ is an approximation of $[\\nabla^2$ $f(x_k)]^{-1}$ and $a_k$ is determined by line search Unconstrained Optimization: Derivative Free Methods for Univariate Functions Golden Section Search: Start with an initial interval $[x_l, x_u]$ containing the minima, and successively narrow this interval Golden Section Search is an optimization algorithm used to find the minimum of a unimodal function, i.e., a function with a single minimum. The method is based on the idea of dividing an interval that contains the minimum into three sections, with the middle section being proportional to the golden ratio. The algorithm iteratively narrows down the interval by selecting the section that contains the minimum and discards the other sections. The process continues until the interval is sufficiently small and the minimum can be approximated with a desired accuracy. Golden Section Search is a bracketing method, which means it only requires the function to be unimodal and does not require the derivative or any other information about the function. It is a simple and efficient method for finding the minimum of unimodal functions, but it is slower than more sophisticated optimization methods for functions with multiple minima or more complex structures. Step 0: Set $x_1 = x_u - a(x_u-x_l)$ and $x_2=x_l+a(x_u-x_l)$ Step 1: If $(x_u-x_l) \u0026lt;= \\epsilon$ stop and return $x^* = 0.5(x_l+x_u)$ as the minima Example of how to use scipy.optimize.minimize to minimize a scalar function: import numpy as np from scipy.optimize import minimize def objective_function(x): return x**2 + 5*np.sin(x) x0 = np.array([1.0]) # Initial guess result = minimize(objective_function, x0, method=\u0026#39;BFGS\u0026#39;) print(\u0026#34;Minimum at:\u0026#34;, result.x) Methods for Multivariate Function The Nelder-Mead method is a optimization algorithm used to minimize a scalar function of several variables. It is a derivative-free method, meaning that it does not require the gradient of the objective function to be calculated. It works by constructing a simplex (a set of vertices) in the high-dimensional space defined by the input variables, and then iteratively modifying the vertices to find the minimum. Here\u0026rsquo;s an example of how to use scipy.optimize.minimize with the Nelder-Mead method: import numpy as np from scipy.optimize import minimize def objective_function(x): return x**2 + 5*np.sin(x) x0 = np.array([1.0]) # Initial guess result = minimize(objective_function, x0, method=\u0026#39;Nelder-Mead\u0026#39;) print(\u0026#34;Minimum at:\u0026#34;, result.x) Nelder-Mead method is a numerical algorithm for minimizing a multivariate function using only function evaluations It is not guaranteed to converge but often works well. Linear optimization Linear Optimization Modeling - Network Flow Problems Introduction to LP Modeling A linear program is composed of: Variables $x=(x_1,x_2,x_3\u0026hellip;,x_n)$ Linear objective function $f(x_1,x_2,x_3\u0026hellip;,x_n)=\\sum_{i=1}^n c_i x_i = c^Tx$ Linear constraints: $\u0026gt;=, \u0026lt;= or =$ All linear problems can be written as a inner product of two vectors. The objective function must be a linear function of the variables. The constraints must be linear inequality or equality constraints. Optimal Transportation Problem The transportation problem is a type of linear programming problem that deals with finding the optimal assignment of resources to meet a set of demands. The problem is typically framed as a network flow problem, where the goal is to find the maximum flow from a set of sources to a set of destinations. In a transportation problem, the goal is to find the least cost way to transport a given amount of goods from a set of sources (e.g. factories) to a set of destinations (e.g. warehouses) subject to certain constraints such as limited supply at the sources and limited demand at the destinations. The cost of transporting a unit of goods from a source to a destination is represented by a cost matrix, which is usually obtained through market research or historical data. There are various algorithms that can be used to solve transportation problems, including the North-West Corner Method, the Minimum Cost Method (also known as the Vogel\u0026rsquo;s Approximation Method), and the Modified Distribution Method. The most popular algorithm for solving transportation problems is the Iterative Proportional Fitting (IPF) algorithm, also known as the MODI (Modified Distribution) method. The transportation problem is an important optimization problem with numerous real-world applications, including supply chain management, distribution systems, and logistics planning. There are $m$ suppliers, $n$ customers. Supplier $i$ can supply up to $s_i$ units of supply, and customer $j$ has $d_j$ units of demand. It costs $c_{ij}$ to transport a unit of product from supplier $i$ to customer $j$. We want to find a transportation schedule to satisfy all the demand within minimum transportation cost. Formulation 1: $\\begin{array}{ll}\\min \u0026amp; \\sum_{i=1}^m \\sum_{j=1}^n c_{i j} x_{i j} \\ \\text { s.t. } \u0026amp; \\sum_{i=1}^m x_{i j}=d_j, \\quad \\forall j \\ \u0026amp; \\sum_{j=1}^n x_{i j} \\leq s_i, \\quad \\forall i \\ \u0026amp; x_{i j} \\geq 0, \\quad \\forall i, j .\\end{array}$ Formulation 2: $\\begin{array}{ll}\\min \u0026amp; \\sum_{i=1}^m \\sum_{j=1}^n c_{i j} x_{i j} \\ \\text { s.t. } \u0026amp; \\sum_{i=1}^m x_{i j}\u0026gt;=d_j, \\quad \\forall j \\ \u0026amp; \\sum_{j=1}^n x_{i j} \\leq s_i, \\quad \\forall i \\ \u0026amp; x_{i j} \\geq 0, \\quad \\forall i, j .\\end{array}$ But \u0026gt;= inequality in the second formulation will be satisfied as = at optimal solution, thus, the two formulations are equivalent The graphs here are bipartite. The total supply is greater than or equal to the total demand. Maximum Flow Problem The maximum flow problem is a classical problem in network flow theory that aims to find the maximum amount of flow that can be sent from a source node to a sink node in a network, subject to capacity constraints on the edges. The maximum flow problem is a special case of the more general minimum cut problem, which aims to find the minimum capacity of a cut that separates the source and the sink in the network. A network in this context is represented as a graph, where the nodes represent the vertices and the edges represent the capacities of the arcs. The source node is where the flow originates, and the sink node is where the flow terminates. The capacity constraints on the edges determine the maximum amount of flow that can be sent through a particular edge. There are several algorithms that can be used to solve the maximum flow problem, including the Ford-Fulkerson algorithm, the Edmonds-Karp algorithm, and the push-relabel algorithm. These algorithms work by finding augmenting paths in the residual network, which is a network derived from the original network that represents the remaining capacities of the edges after some flow has already been sent. The algorithms continue to find augmenting paths until no more can be found, at which point the maximum flow has been found. The maximum flow problem has many real-world applications, including traffic flow in transportation networks, the allocation of bandwidth in communication networks, and the distribution of resources in supply chain networks. The graphs here are directed $\\begin{array}{ll}\\max \u0026amp; b_s \\ \\end{array}$ $\\begin{array}{ll} \\text { s.t. } \u0026amp; \\sum_{k \\in O(i)} x_{i k}-\\sum_{j \\in I(i)} x_{j i}=b_i \\quad \\forall i \\ \u0026amp; b_t=-b_s \\ \u0026amp; b_i=0, \\quad \\forall i \\neq s, t \\ \u0026amp; 0 \\leq x_{i j} \\leq u_{i j}, \\quad \\forall(i, j) \\in \\mathcal{A} .\\end{array}$ Minimum Cut Problem The Maximum Cut Problem is a well-known optimization problem in computer science and mathematics. The goal of the problem is to divide a given graph into two sets of vertices such that the sum of the weights of the edges between the two sets is as large as possible. Formally, given a graph G = (V,E) with a weight function w : E → R, the maximum cut problem is to find a partition of the vertices into two sets S and T such that the sum of the weights of the edges between S and T is maximized. The problem is NP-hard, meaning that finding the optimal solution is computationally infeasible for large graphs. However, there are approximate algorithms that can find near-optimal solutions, such as semidefinite programming, spectral methods, and local search algorithms. The maximum cut problem has a wide range of applications, including network design, image and signal processing, and machine learning. Minimum cut = Maximum flow Shortest Path Problem The Maximum Cut Problem is a well-known optimization problem in computer science and mathematics. The goal of the problem is to divide a given graph into two sets of vertices such that the sum of the weights of the edges between the two sets is as large as possible. Formally, given a graph G = (V,E) with a weight function w : E → R, the maximum cut problem is to find a partition of the vertices into two sets S and T such that the sum of the weights of the edges between S and T is maximized. The problem is NP-hard, meaning that finding the optimal solution is computationally infeasible for large graphs. However, there are approximate algorithms that can find near-optimal solutions, such as semidefinite programming, spectral methods, and local search algorithms. The maximum cut problem has a wide range of applications, including network design, image and signal processing, and machine learning. Shortest Path Problem is a Flow problem if we are shipping 1 unit of flow from $s$ to all other nodes $\\begin{array}{ll}\\min \u0026amp; \\sum_{(i, j) \\in \\mathcal{A}} c_{i j} x_{i j} \\ \\end{array}$ $\\begin{array}{ll}{ s.t. } \u0026amp; \\sum_{k \\in O(i)} x_{i k}-\\sum_{j \\in I(i)} x_{j i}=-1 \\forall i \\neq s \\ \u0026amp; \\sum_{k \\in O(s)} x_{s k}-\\sum_{j \\in I(s)} x_{j s}=n-1 \\ \u0026amp; x_{i j} \\geq 0, \\quad \\forall(i, j) \\in \\mathcal{A} .\\end{array}$ LP model for market clearing: Rosenbrock function The Rosenbrock function is a widely used test function in optimization and is often used as a performance test for optimization algorithms. Here\u0026rsquo;s a simple code to plot the Rosenbrock function in Python using Matplotlib:\nimport numpy as np import matplotlib.pyplot as plt def rosenbrock(x, y): return (1-x)**2 + 100*(y-x**2)**2 x = np.linspace(-2, 2, 400) y = np.linspace(-1, 3, 400) X, Y = np.meshgrid(x, y) Z = rosenbrock(X, Y) fig = plt.figure(figsize=(10, 8)) ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) ax.plot_surface(X, Y, Z, cmap=\u0026#39;viridis\u0026#39;) ax.set_xlabel(\u0026#39;X axis\u0026#39;) ax.set_ylabel(\u0026#39;Y axis\u0026#39;) ax.set_zlabel(\u0026#39;Z axis\u0026#39;) plt.show() LP model for Electricity Markets Decision variables Generator output: $p_i$ for each generator $i \\in G$ Power flow: $f_{ij}$ on each edge $(i,j) \\in E$ Nodal potential $\\theta_i$ on each node $i \\in N$ Objective function: minimize the cost of production, $\\sum_{i=1}^{G} c_ip_i$ Constraints: Flow conservation (input=output) for source node $p$ we have: (\u0026ldquo;sum of everything going out\u0026rdquo;) - (\u0026ldquo;sum of everything going in\u0026rdquo;) = $p$ for demand node $d$ we have: (\u0026ldquo;sum of everything going out\u0026rdquo;) - (\u0026ldquo;sum of everything going in\u0026rdquo; ) = $-d$ for node which is neither source nor demand we have: (\u0026ldquo;sum of everything going out\u0026rdquo;) - (\u0026ldquo;sum of everything going in\u0026rdquo;) = $0$ Nodal potential Flow limit constraint Generator physical limit constraint Inventory Control Problem a company must commit to specific production quantity x before knowing the exact demand $d$ after seeing the demand, the company decides how many to sell and how many to sell at a discounted price of $v$ This is an example of Decision Making under Uncertainty Here and Now decision: production quantity $x$ Wait and See decision: sell quantity $y$, discount quantity $z$ Objective: minimize production cost and expected future cost Stochastic program: $min_{x} cx + E_d[Q(x,d)]$ s.t $0\u0026lt;=x\u0026lt;=\\hat{x}$ $Q(x,d) = min_{y,z} -r.y-s.z$ s.t $y\u0026lt;=d, y+z\u0026lt;=x, y\u0026gt;=0, z\u0026gt;=0$ Generation Capacity Expansion An electric utility company plans to build new generation stations to serve growing demand, called generation capacity expansion. New generation capacity has to be decided before demand and future fuel price are known Future demand and fuel prices are not known at the moment of making capacity decision, but can be estimated as random variables. After demand is realized, the utility company schedules existing and new generators based on capacity expansion decision. Financial Planning A family wishes to provide for a child\u0026rsquo;s college education 12 years later. The family currently has 100k and decides how to invest in any of 5 investments Investment can be adjusted every 4 years. So there are 3 periods The returns of investments are unknown and modeled as random variables The family wants to maximize the total expected return A problem of decision making under uncertainty Decision Types Here-and-Now: decision made before knowing uncertain parameters Wait-and-See: decision made after knowing uncertain parameters Basic Geometric Objects Points and vectors Point: geometric object in space Algebraically, a point in n-dimensional space is given by its coordinates: $x = (x_1, \u0026hellip;, x_n)^T \\in R^n$ We always write a vector as a column vector A point is also called a vector Rays, lines, and their parametric forms A ray consists of a starting point $a$ and all the points in a direction $d$ Algebraically it is a set: {$x$ $\\in R^n | x = a + \\theta d$, $\\forall$ $\\theta \u0026gt;=0 $} A line consists of two rays starting at a point pointing two opposite directions. Algebraically it is a set: {$x$ $\\in R^n | x = a + \\theta d$, $\\forall$ $\\theta \\isin R $} For ray and line, it is parametric because a and d are known, and $\\theta$ is the parameter Plane and solutions of linear equations A plane in $R^2$ is just a line. $a_1x_1+a_2x_2=c$ This plane is a line but it is not a parametric representation of a line. A plane in $R^3$ is $a_1x_1+a_2x_2+a_3x_3=c$ If c is 0, plane passes through the origin. Hyperplane and a Linear equation The concept of plane can be extended to any dimension R^n Algebraically, $a_1x_1+a_2x_2+\u0026hellip;+a_nx_n=c$ can be written as $a^Tx=c$ Halfspace and a linear inequality In $R^2$, a halfspace is half of the whole space A halfspace also consists of the line dividing the space There are two halfspace in $R^2$, but both include the dividing line Same definition can be extended to a halfspace $H_1$ = {$x \\in R^n: a^Tx\u0026gt;=c$} $H_2$ = {$x \\in R^n: a^Tx\u0026lt;=c$} Polyhedron and serveral hyperspaces A polyhedron is the intersection of a finite number of halfspaces Geometric Aspects of Linear Optimization Corner Points Instead of edges, look at Corner Points Corner points are responsible for generating the set Convex combination of two points in the action of generating it Convex Combination of Two Points Given two points, $a$, $b$ $\\in R^n$, a convex combination of $a, b$ is given by $x = \\lambda a + (1- \\lambda)b$ for some $\\lambda \\in [0, 1]$ Geometrically, x is on the line segment connecting a and b Given a point $x$ is a convex combination of $a_1, \u0026hellip; a_m$ if $x$ can be written as $x = \\sum_{i=1}^m \\lambda_ia_i$ And, $\\sum_{i=1}^m \\lambda_i = 1, \\lambda_i\u0026gt;=0$ for $i = 1, \u0026hellip; , m$ Corner points are special points, and therefore we give them a special name: Extreme Point A point x in a polyhedron P is an extreme point if and only if x is not a convex combination of other two different points in P. Convex Hull A convex hull of $m$ points $a_1, \u0026hellip;., a_m$ is the set of all convex combinations of $a_1, .., a_m$ denoted as $conv$ $x{a_1,.., a_n}$ Theorem: A nonempty and bounded polyhedron is the convex hull of its extreme points. A bounded polyhedron is a polyhedron that does not extend to infinity in any direction. Conic Hull A polyhedron is unbounded iff there are directions to move to infinity without leaving the polyhedron. Recession direction: a ray that we never leave in the direction of the polyhedron However, there are special rays on the edge which can be used to generate all other rays A ray $d$ is a conic combination of two rays, $e_1$, $e_2$ if d is a nonnegative weighted sum of $e_1$, $e_2$ The set of all conic combination of rays $r_1, \u0026hellip;, r_m$ is called the conic hull of $r_1, \u0026hellip;, r_m$ The sum of $\\lambda$ does not have to equal to 1 here. A ray $e$ in a cone C is called an extreme ray, if $e$ is a conic combination of other two different rays in the cone C Extreme Ray and Extreme Point If a polyhedron is bounded, there is no extreme ray If a polyhedron is bounded, there must be an extreme point If a polyhedron is unbounded, it must have an extreme point Polyhedron Representations Halfspace representation Extreme Point representation Weyl-Caratheodory Theorem Any point $x$ in a polyhedron can be written as a sum of two vectors $x = x^\u0026rsquo; + d$ where $x^\u0026rsquo;$ is in the convex hull of its extreme points and d is in the conic hull of its extreme rays. $P =$ $conv$ ${x_1, \u0026hellip;, x^m} + $ $conic$ ${e^1, \u0026hellip;, e^k}$ Algebraic Aspect of Linear Optimization Active constraints: A linear constraint that is satisfied as equality at a given point is said to be active or binding at that point. Otherwise, if an inequality constraint is satisfied as strict inequality at a point, it is called inactive. Linear independent constraints: If the normal directions of two or more linear constraints are linearly independent, then these constraints are called linearly independent Linearly independent active constraints: Active constraints that are linearly independent Basic solution: The unique solution of $n$ linearly independent active constraints in $R^n$ Basic feasible solution (BFS): Basic solution that is feasible. Basic Feasible Solution = Extreme Point Standard Form of writing an LP A standard form linear program is written as $min$ $ c^Tx$ $s.t.$ $Ax=b, x\u0026gt;=0 \\in X $ $x \\in R^n$ that is, there are $n$ variables $A \\in R^{m*n}$, ie there are m equality constraints We always assume all the $m$ equality constraints are linearly independent Equality constraints and Nonnegative constraints on all variables The first constraint is data dependent, whereas the second one is not Any linear program can be transformed into LP Advantage of standard form LP: Complicating constraints are all equality Only inequality constraints are simple, no negativity constraints, which do not depend on data Basic Solution to standard form LP A basic solution is the unique solution to $n$ linearly independent active constraints. For a standard form LP, we already have $m$ linearly independent active constraints. Need $n-m$ additional linearly independent active constraints Where to find them? Only from nonnegative constraints: $x_i \u0026gt;= 0$ But which to choose to make active? Choose $m$ such linearly independent columns, denote the corresponding $m*m$ matrix as B, called basis matrix. The corresponding $(n-m)$ $x_i$s are denoted as $x_N$, non basic variables Choose $x_i=0$ for all $i$ corresponds to the columns in $N$, $x_N$ = 0 Why do we care? Not every LP has a BFS, not every polyhedron has an extreme point (Think about a line or a halfspace) So which LP has a BFS? A polyhedron P has an extreme point iff it does not contain a line Corollary: A feasible standard form LP always has a BFS If an LP has a finite optimal solution, then an optimal solution is a BFS That does not mean all optimal solution must be BFS Because feasible standard form LP must have a BFS And because an optimal solution must be a BFS Then, an optimal solution of standard for LP must be a BFS So we only need to look at BFSs, and select the one BFS with the minimum obj cost This is why BFS is very important for linear programming. Local Search In a feasible region General idea (does not have to be a LP) Start from some solution, and move to certain direction to a new point, but stay in feasible region. Algorithm Generic algorithmic idea Gradient Descent and Newton Method uses local search Step size should be chosen properly, and the position should be feasible Local Search works well for convex optimization (A local minimum of a convex program is also a global minimum) Not in general for non convex optimization problems (Local search can get stuck) Local Search for LP We only need to look at basic feasible solution.\nThe key step is to find a direction $d$ and step size $\\theta$ so that:\n$d$ points from a BFS to one of its adjacent BFS That adjacent BFS should reduce objective value Move along the favorable direction as much as possible to maintain feasibility and to reduce objective Stop when optimal solution is fount (or cannot be found) Two BFS are adjacent if they share the same $n-1$ linearly independent active constraints.\nTwo adjacent BFSs must share the same set of $n-m-1$ nonbasic variables as n-m-1 active constraints, and differ in one nonbasic variable.\nExample, if $x=(x_1, \u0026hellip;, x_5)$ has nonbasic variables $x_3 = x_4 = x_5 = 0 $, then its adjacent BFS must share two of these three nonbasic variables, i.e. $x_3=x_4=x_2=0$ may be nonbasic variable in an adjacent BFS.\nSimplex Method The simplex method is a linear programming algorithm that is used to solve optimization problems with linear constraints and a linear objective function. It involves iteratively constructing a sequence of feasible solutions that converge to an optimal solution. At each iteration, the simplex method selects a non-basic variable to become basic and then computes a feasible solution by solving a set of linear equations. If the solution is not optimal, the method determines a new non-basic variable to become basic and repeats the process until an optimal solution is found. The method is based on the fact that a linear programming problem can be represented graphically as a polyhedron in high-dimensional space, and the optimal solution lies at one of the extreme points of the polyhedron. The simplex method works by traversing the edges of the polyhedron until the optimal extreme point is reached. The simplex method is a powerful tool for solving large-scale linear programming problems and is widely used in industry, finance, and other fields. However, it has some limitations, such as its inability to handle nonlinear constraints and its susceptibility to numerical instability when dealing with ill-conditioned matrices. Degeneracy Degeneracy in the simplex method refers to a situation where the simplex algorithm encounters multiple optimal solutions or cycles in the iteration process. In other words, a degenerate linear programming problem has more than one basic feasible solution with the same objective function value. Degeneracy can occur when one or more constraints in the linear programming problem are redundant or when there is a linear dependence among the constraints. This leads to a reduced dimensionality in the space of feasible solutions, resulting in more than one optimal solution or cycle in the iteration process. Degeneracy can pose challenges for the simplex method since it can lead to slow convergence, cycling, or termination of the algorithm before finding an optimal solution. This is because the simplex method relies on selecting non-basic variables to become basic and constructing a feasible solution by solving a set of linear equations. In a degenerate case, some of the variables may become redundant, leading to cycles in the iteration process. To address degeneracy, various modifications to the simplex method have been proposed, such as the use of anti-cycling rules, perturbation techniques, or alternative algorithms such as interior-point methods. These modifications aim to reduce or eliminate the effects of degeneracy on the convergence of the algorithm and ensure finding an optimal solution. Bland\u0026rsquo;s rule for degeneracy Bland\u0026rsquo;s rule ensures that the simplex method always chooses the variable with the smallest index as the entering variable and the variable with the smallest subscript as the leaving variable. In other words, Bland\u0026rsquo;s rule breaks ties in the selection of entering and leaving variables in favor of the variable with the smallest index or subscript. By always selecting the variable with the smallest index or subscript, Bland\u0026rsquo;s rule guarantees that the simplex method cycles through all basic feasible solutions before returning to a previous solution. This eliminates the possibility of the algorithm getting stuck in a cycle and ensures that it converges to an optimal solution eventually. Although Bland\u0026rsquo;s rule can increase the number of iterations required to solve a degenerate linear programming problem, it provides a provably optimal solution and eliminates the possibility of cycling or termination before finding an optimal solution. Bland\u0026rsquo;s rule is widely used in software implementations of the simplex method and has been shown to be effective in practice. Linear Program Duality LP duality is at the core of linear programming theory. Provides new perspective on understanding LP, is important for designing algorithms, and has many applications (pricing, game theory, robust optimization, and many more). Linear Program (LP) duality is a powerful concept in optimization theory that establishes a relationship between the primal and dual LP problems. The duality principle provides insights into the structure of optimization problems, helps in understanding the solutions and provides a tool for solving LP problems. In LP duality, there are two LP problems, known as the primal problem and the dual problem. The primal problem is the original LP problem that seeks to minimize or maximize a linear objective function subject to a set of linear constraints. The dual problem is constructed from the primal problem, and it seeks to maximize or minimize a function subject to a set of constraints. The duality principle states that the optimal value of the primal problem is equal to the optimal value of the dual problem. Furthermore, the optimal solutions of both problems are related in a specific way. This relationship is known as the duality gap, which is the difference between the optimal values of the primal and dual problems. There are two forms of LP duality: weak duality and strong duality. Weak duality states that the optimal value of the dual problem is always greater than or equal to the optimal value of the primal problem. In contrast, strong duality states that if the primal problem has an optimal solution, then the dual problem also has an optimal solution, and the duality gap is zero. The dual LP problem provides useful information about the primal LP problem. For example, the dual problem provides a lower bound on the optimal value of the primal problem, and it can be used to derive sensitivity analysis and shadow prices. Additionally, the dual problem can be used to reformulate the primal problem and generate alternative solutions. A fundamental motivation of LP duality is to find a systematic way to construct a lower bound to the original LP Original LP (Primal LP) $Z_p = min \\lbrace c^Tx:Ax=b,x \\ge 0 \\rbrace $ Any feasible solution $x$ provides an upper bound on $Z_p$, ie, $Z_p \\le c^Tx$ What about a lower bound, i.e $Z_D \\le Z_p$? This lower bound is useful because if the lower bound is very close to an upper bound, we have a good estimate of the true optimal. However, to get a lower bound, we need to modify the original LP In particular, we need to relax the problem. Principles of relaxation works for general optimization problems, far beyond LP Principle of Relaxation Relaxation: Find a new objective function that is always smaller or equal to the original objective function at any feasible point Find a feasible region that is larger than the feasible region of the original problem Minimize a function lower than the original objective over a region that is larger than the original one. The optimal objective of the new problem will be a lower bound to the original one. Among all possible relaxations and lower bounds, find the best lower bound. Systematic way to carry out relaxation Step 1: Relax the objective function Step 2: Relax the feasible region by ignoring constraints Separability refers to the property that the objective function of the Lagrangian dual problem can be expressed as the sum of separate functions, each of which depends only on a subset of the variables of the primal problem. Step 3 Find the best lower bound. Primal and Dual Pair Is Lagrangian relaxation a dual problem to the primal? Yes, Lagrangian relaxation is a way of obtaining a lower bound on the optimal value of the primal problem by constructing a dual problem. In Lagrangian relaxation, the primal problem is first converted into its Lagrangian dual problem by introducing Lagrange multipliers, which are used to form a penalty term that is added to the original objective function. The resulting Lagrangian function is then minimized subject to the constraints, resulting in a lower bound on the optimal value of the primal problem. The Lagrangian dual problem is formulated by taking the infimum (minimum) of the Lagrangian over all possible values of the Lagrange multipliers. The dual problem is a maximization problem that seeks to find the maximum value of the infimum, subject to certain constraints that are derived from the original primal problem. The duality theorem states that the optimal value of the primal problem is equal to the optimal value of the dual problem, and that any feasible solution to one problem gives a lower bound or upper bound for the other problem. Therefore, Lagrangian relaxation can be seen as a way of constructing the dual problem to the primal problem, and finding a lower bound on the optimal value of the primal problem. Important If a linear program has an unbounded feasible region, then it can either be unbounded or have a finite optimal solution. If it has finite optimal solution than its dual must also have finite optimal solution. If it is unbounded, then its dual must be infeasible. If a linear program has an unbounded feasible region, then its dual problem cannot have an unbounded optimal solution. If a linear program has an unbounded feasible region, then its dual may be infeasible or have a finite optimal solution. Linear Programming weak duality Given a primal LP in minimization, by the construction of the dual, the objective value of any feasible solution of the dual problem provides a lower bound to the primal objective cost Theorem 1 (Linear programming weak duality): If $x$ is any feasible solution to the primal minimization LP, and y is any feasible solution to the dual maximization LP, then $c^Tx \\ge b^Ty$ This implies: If the optimal cost of the primal minimization problem is $-\\inf$ then the dual maximization problem must be infeasible. If the optimal cost of the dual maximization problem is $+\\inf$ then the primal minimization problem must be infeasible. Let $x^*$ be feasible to the primal problem and $y*$ be feasible to the dual problem, and suppose $c^Tx^*=b^Ty^*$, then $x^*$ and $y^*$ are optimal solutions to the primal and dual problems, respectively. Weak duality does not hold if problem is infeasible. Strong Duality Theorem 1 (Linear programming strong duality): If a primal linear program has a finite optimal solution $x^*$, then its dual linear program must also have a finite optimal solution $y^*$, and the respective optimal objective values are equal, ie, $c^Tx = b^Ty$ Strong duality is the single most important theorem in LP. Its proof is very illuminating. Tables of possibilities SOB method for creating dual of a LP Complementarity Slackness Complementarity slackness is a fundamental concept in optimization theory that arises in the context of solving optimization problems with inequality constraints. It provides insights into the structure of the solutions and helps in understanding the behavior of the constraints in the optimization process. In an optimization problem with inequality constraints, the optimal solution typically satisfies the constraints with equality or with some slackness. Complementarity slackness is the condition that ensures that a constraint is either binding (i.e., satisfied with equality) or non-binding (i.e., satisfied with strict inequality) in the optimal solution. More formally, let x be a feasible solution to an optimization problem with inequality constraints, and let s be the slack variables associated with the constraints. The complementarity slackness condition requires that the product of the slack variables and the dual variables associated with the constraints is equal to zero. Robust Optimization Making decision during uncertainty Robust optimization is a mathematical optimization technique that seeks to find a solution that is optimal under a set of possible scenarios, often in the presence of uncertain or varying parameters. It is particularly useful when dealing with systems that are subject to variability, such as financial or transportation systems, where decisions need to be made under uncertain conditions. In robust optimization, instead of trying to find a single optimal solution, a set of feasible solutions is identified that can perform well across a range of possible scenarios. The objective function is typically defined as a worst-case scenario, which ensures that the selected solution is optimal under all possible scenarios. Robust optimization can be used in a variety of applications, including portfolio optimization, supply chain management, and resource allocation. It has become increasingly popular in recent years due to its ability to provide robust solutions that can withstand unpredictable changes in the environment. We need to formulate constraint in such a way that solution we obtain will allow production to survive all possible realizations of the coefficients. Large Scale Optimization Cutting Stock Problem The cutting stock problem is a combinatorial optimization problem that involves cutting large sheets of material, such as paper or metal, into smaller pieces of specific sizes in order to minimize waste. The objective is to determine the most efficient cutting pattern that can be used to produce a given number of smaller pieces of the desired sizes, while minimizing the amount of leftover material. The cutting stock problem is a common problem in the manufacturing industry, where it is used to optimize the use of raw materials and minimize production costs. It can be formulated as a linear programming problem, where the decision variables are the number of cuts made in each direction, and the objective function is to minimize the amount of leftover material. Gilmore-Gomory Formulation $min \\sum_{i=1}^N x_i$, s.t $Ax=b, x \\gt 0$ the coefficients are 1 where the columns of A are the patterns to cut one large roll $b$ is the amount of demand of each size of smaller rolls The number of ways to cut a large roll into smaller ones is usually astronomical very large number of variables Column Generation Pick a subset Solve the restricted master problem (RMP) A feasible solution of RMP can be made into a feasible solution of MP. This is because RMP has all the constraints in MP. A basic feasible solution of RMP can made into a basic feasible solution of MP For an optimal BFS of RMP we can compute reduced cost of all nonbasis variables, if any reduced cost is negative, then we know the optimal solution of RMP if not optimal for MP We can add the new variable with negative reduced cost to RMP solve the new RMP and repeat the process. The procedure of finding a variable with negative reduced cost is called the Pricing Problem. Pricing Problem: Compute all the reduced costs of $x$. If all reduced costs are nonnegative, then $x$ is optimal for MP. Otherwise, we find a new column to add to RMP. Important The features of the cutting stock problem that make column generation a feasible approach to solve: The cutting stock problem formulation has objective coefficients all equal to 1. The cutting stock problem has columns with special structures which can be generated by another optimization problem that is easy to solve. The cutting stock problem has a small number of rows The column generation algorithm will terminate if all columns have been added or if no column can further reduce the number of big rolls used to satisfy demand. Constraint generation can be used when problems have too many constraints, but not many variables, or with too many rows and not many columns. The constraint generation algorithm will terminate if all the constraints are satisfied by the current solution, or if all the contraints are added. Correctness and Convergence The algorithm is correct because of the key properties of RMP Does the algorithm converge? Yes, because the algorithm always adds new columns and never disregards any. The worst case is all the columns of MP are used. Dantzig-Wolfe The Dantzig-Wolfe decomposition (also known as the column generation method) is a technique for solving large-scale linear programming problems that have a special structure. It is named after George Dantzig and Philip Wolfe, who first proposed the method in the 1960s. The Dantzig-Wolfe decomposition method decomposes a large linear programming problem into smaller sub-problems, each of which can be solved independently. The method is particularly useful when the original problem has a large number of constraints, but only a small number of variables are involved in each constraint. The basic idea of the method is to introduce new variables (known as columns) into the problem gradually, one at a time, and to solve the resulting sub-problem using standard linear programming techniques. The optimal solution to the sub-problem is then used to generate a new column, which is added to the problem and the process is repeated until the optimal solution to the original problem is found. The Dantzig-Wolfe decomposition method can be used to solve a wide range of linear programming problems, including those with integer variables and those with non-linear objective functions. It is particularly useful for problems that involve complex constraints or require the solution of large-scale optimization problems. Important To use Dantzig-Wolfe decomposition algorithm, the problem must have a special structure where: all constraints are linear and have a block angular structure the objective funciton is a linear function We solve Dantzig-Wolfe decomposition by using column generation because: the number of extreme points can be huge there are not many constraints The pricing problem is relatively easy to solve because: We can use simplex method instead of enumeration over all the extreme points There are no complicating constraint so that we can solve those pricing problems with angular structures in a distributed manner. Moore Penrose Pseudoinverse The Moore-Penrose pseudoinverse, also known as the Moore-Penrose inverse or simply the pseudoinverse, is a generalization of the matrix inverse for non-square matrices. It is named after Elisha L. Moore and Roger Penrose, who independently introduced the concept in the mid-20th century. The pseudoinverse is defined for any m-by-n matrix A, where m and n need not be equal, and it is denoted by A+. The pseudoinverse has several important properties, including: A+AA+A=A+ AA+A=AA+ (AA+)\u0026rsquo;=AA+ (A+A)A=A where A\u0026rsquo; denotes the transpose of A. The pseudoinverse is useful in a variety of applications, including linear regression, least-squares approximation, and control theory. In particular, if A has linearly independent columns, then A+A is the unique solution to the linear system Ax=b that minimizes the Euclidean norm of the error vector Ax-b. The pseudoinverse can be computed using singular value decomposition (SVD) or the QR decomposition. In particular, if A has full column rank, then its pseudoinverse can be computed as A+(A\u0026rsquo;A)^(-1) where A\u0026rsquo; denotes the transpose of A. Convex Conic Optimization Nonnegative Orthant cone The nonnegative orthant cone is a special type of cone in linear algebra and convex analysis. It is defined as the set of all nonnegative vectors in n-dimensional Euclidean space, denoted as R^n_+, where R^+ denotes the set of nonnegative real numbers. Generalizations of linear programming to nonlinear programming through convex cones and generalized inequalities A set K is called convex cone if K is convex and $ax \\in K$ for all $a \\ge 0$ whenever $x \\in K$ What is the relation between order and cone? Order is a comparison relationship between two elements $a$ and $b$, usually written as $a \\gt b$ An order $\\succeq_K$ is defined by an underlying convex cone K as $a \\succeq_K b$ iff $a-b \\in K$ A standard form LP can be viewed as $min$ ${c^Tx: Ax=b, x \\gt_{R_+^n}0}$ An elegant way to generalize linear programming is to generalize $R_+^n$ to a general convex cone $K$ Linear Conic Programming: $min$ $c^T x: Ax=b, x \\ge_K 0$ Linear Conic Programming is a type of optimization problem that involves finding the best solution to a linear objective function subject to a set of linear constraints and the requirement that certain variables lie in a cone. A cone is a set of vectors that satisfies certain properties, such as being non-negative or having a fixed norm. In Linear Conic Programming, the constraints are expressed in the form of linear equations or inequalities, while the requirement that certain variables lie in a cone is expressed using conic constraints. Common types of cones include the non-negative orthant, the second-order cone, the semi-definite cone, and the exponential cone. The goal of Linear Conic Programming is to find a feasible solution that satisfies all the constraints and optimizes the objective function. This type of optimization problem arises in a variety of applications, such as portfolio optimization, transportation planning, and engineering design. Linear Conic Programming is a powerful tool that can be solved efficiently using specialized algorithms, such as interior-point methods. Second order cone $L^3 = \\lbrace (x,y,z) : \\sqrt{x^2-y^2} \\le z \\rbrace$ = $\\lbrace(x,y,z):||[x;y]||_2 \\le z \\rbrace$ Integer optimization Integer optimization is a type of optimization problem where the decision variables are required to take integer values. This is in contrast to continuous optimization, where the decision variables can take any real value. Integer optimization problems arise in a variety of fields, including operations research, computer science, engineering, and economics. Examples of integer optimization problems include finding the optimal assignment of workers to shifts, determining the best routes for vehicles to travel, and selecting the optimal set of investments to make. Integer optimization is often more difficult than continuous optimization, because the feasible set of integer solutions is typically discrete and non-convex. This means that traditional optimization techniques, such as gradient descent, cannot be used. Instead, specialized algorithms, such as branch and bound or cutting plane methods, are used to find optimal or near-optimal solutions. Integer optimization is also sometimes referred to as mixed-integer optimization, when the problem includes both integer and continuous decision variables. Binary Optimization Models A special and important class of discrete optimization models are those where the discrete variables and required to be binary, that is, they are required to take values of 0 and 1. $min$ $f(x)$ $s.t.$ $g_i(x) \\le b_i, x \\in R^{n-p} \\times \\lbrace 0,1 \\rbrace^p$ Set Packing, Covering and Partitioning Set Packing: A set packing is a collection of sets in which no two sets share a common element. In other words, a set packing is a collection of non-overlapping sets. The objective in set packing is to find the largest possible subset of the sets that do not overlap. Set Covering: A set covering is a collection of sets that together contain every element in a given universe. In other words, a set covering is a collection of sets that covers all the elements of a universe. The objective in set covering is to find the smallest possible subset of the sets that covers all the elements. Set Partitioning: Set partitioning is a way to divide a set into non-empty subsets such that each element belongs to exactly one subset. In other words, set partitioning is a way to divide a set into mutually exclusive and exhaustive subsets. The objective in set partitioning is to find a partition that satisfies some given criteria. The set packing problem arises when each set element must appear in at most one subset. In this case, the constraints are of the less-than-or-equal form. The set partitioning problem arises when each set element must appear in exactly one subset, and the constraints in this problem are equality constraints.\nStore Location Example Where should store be located so that it can maximize the number of customers? maximize total customers, constrained to among the locations same location cannot attract more than one city pupl is the python package that can be used to model this Set Packing: Given $m$ elements and a collection of subsets $S_1, \u0026hellip;. , S_n \\belongs {1,..,m} with associated nonnegative weights $w_1, \u0026hellip;, w_n$ pick sets from this collection such that they are disjoint and the sum of the weights is maximized. Linear Programming Relaxation Decision variable is only taking continuous value to generate the relaxation (drop the integer constraints). If LP relaxation is infeasible, so it the IP If LP relaxation is unbounded, then the IP can either be infeasible or unbounded. If LP relaxation has an optimal solution, then the IP could be infeasible or have an optimal solution It always holds that $v_{LP} \\le v_{IP}$ If an optimal solution to the LP is an integer, then it is an optimal solution to the IP. It\u0026rsquo;s solution can sometimes be rounded to get a good solution to the IP Ideal Formulations Stronger Formulation: Stronger formulation lead to stronger LP relaxations, and so better LP relaxation better bounds, and sometimes LP relaxations solutions that are feasible to the MLP The formulation of an MLP can be strengthened by adding constraints (valid inequalities) by tightening constraint coefficients by introducing new variables and constraints An ideal formulation of a MILP is one whose LP relaxation solves the MLP Ideal formulations are hard to obtain, so we strive to obtain strong formulation that approximate the ideal formulation Branch Bound Algorithm Branch and bound is a popular algorithm used for solving mixed-integer linear programming (MILP) problems. The basic idea behind branch and bound is to divide the problem into smaller subproblems, solve each subproblem separately, and then combine the solutions to obtain an overall solution to the original problem. Here are the steps involved in the branch and bound algorithm for MILP: Solve the relaxed linear programming (LP) problem, which is the MILP problem with the integer constraints removed. This provides an initial solution to the MILP problem. If the LP solution satisfies all the integer constraints, then we have found an optimal solution to the MILP problem. Otherwise, select one of the integer variables with a non-integer value in the LP solution. Create two new subproblems by branching on the selected variable: one subproblem where the variable is fixed to its integer floor, and another subproblem where the variable is fixed to its integer ceiling. Solve each of the subproblems using the LP solver. If a subproblem has an integer solution that is worse than the current best solution, we can prune that branch of the search tree. Otherwise, continue branching until we have found an optimal integer solution or all branches have been pruned. Once all branches have been pruned, the best integer solution found during the search is the optimal solution to the MILP problem. ","date":"2023-05-02T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/deterministic_optimization/","section":"posts","tags":["optimization","linear-algebra","objective-function","cost-function","integral","simplex","column-generation","linear-algebra","calculus","convex"],"title":"Deterministic Optimization"},{"categories":["gatech","hci"],"contents":"Human-Computer Interaction Introduction to HCI The goal is to create a scenario where user spends as much time as possible to accomplish the task, while the interface kind of vanishes during that interaction. A video game, as an example gives the user an experience of being in the game, not tinkering with the controller. We might be experts at interacting with computers, but that does not make us experts at designing interactions between other humans and computers. HCI is a subset of Human Factors Engineering (Industrial Design, HCI, Product Design). UI Design and UX Design and Interaction Design are a subset of HCI. Many principle of HCI come from Human Factors Engineering applied to computers. Tha gap between HCI and Interaction Design is shrinking (as computers become ubiquitous). HCI is about understanding the interaction between users and computers, and UX design is about dictating the interaction between users and computer. The relationship is however, deeply symbiotic. Human Factors Engineering is a merger of Engineering and Psychology. Psychology and HCI are also deeply symbiotic. HCI is about research (needfinding, prototyping, evaluation) and design (distributed cognition, mental models, universal design). HCI at its core: Research informs Design and Design informs Research. Exploring HCI Three main application areas:\nTechnologies: Virtual Reality, Augmented Reality, UbiComp and Wearables, Robotics, Mobile Ideas: Context-Sensitive Computing (equipping user interfaces with historical, geographical, or other forms of contextual knowledge), Gesture-Based Interaction (interacting with interfaces using hand or body gestures), Pen-and Touch-Based Interaction, Information Visualization (representing abstract data visually to help humans understand it), CSCW (Computer Supported Cooperative Work) Interaction (Distributed Team, Distributed Work, Temporal and Geographical dimension): Example: Slack, Online course, etc., Social Computing Domain: Special Needs (example: communicating data using sound for visually impaired people), Education (you might not always want to make something super easy), Healthcare (fitbit, virtual reality), Security (for security to be useful, it has to be usable). Captcha has become easy now with a checkbox, Games (topics of HCI are super salient here) Introduction to Principles User uses interfaces to accomplish a task Need to understand the user goals and the task Focusing on task and not the interface, allows us to come up with something amazing (nest instead of iteration on thermostat) How to identify a task? Watch real users - get people and ask them to work with you Talk to them - talk through the user Start small - find small task which user performs Abstract up - keep asking why till you reach beyond scope of design (small + small + big - series of why) You are not your user Goals of HCI: Useful Usable (the big concern) If we were focusing on building better maps, we would not add navigation (think about what the user wants) Ex situ - in a controlled or otherwise inauthentic environment In situ - within the authentic context View/Role of a human within a system Processor Nothing more than Sensory processor, take input and spit output out, like another computer in the system, interface must fit within known human limits: (what humans can sense, what they can store in memory, and what they can physically do in the world) usability: interface is physically usable (users can see colors, touch the buttons etc.) evaluated by quantitative methods/experiments. - - \u0026ldquo;Behaviorism\u0026rdquo; from psychology how quickly can the user complete the task?, how quickly can they react to incoming stimulus? not the one generally taken when talking about good design. The processor model is concerned with objective, measurable outcomes, so noting the efficiency with which you can accomplish different tasks is relevant here. Control study interfaces and get the timing Pros Use existing data. Objective comparison (text v/s voice). Cons Reason is not visible why text is not better than voice. cannot differentiate by expertise. Suitable for only evaluation and optimization, not redesign. Predictor Ask interviews and focus groups Get full details on textual description Why use different interfaces at different time why choice, why particular option \u0026ldquo;Cognitivism\u0026rdquo; from psychology here we care about the users knowledge, experience, expectation and thought process, we want users to predict what will happen, what are they thinking, interface must fit with knowledge, evaluated by qualitative studies. The predictor model is concerned with what you predict will be the outcome of your action, and whether you can interpret whether the outcome matched your prediction, so focus on how the user perceives and interprets what they should do and whether it was successful. The predictor model is not about interfaces predicting users intentions. The predictor model is about paying attention to what the user predicts the outcome of their action will be, and how they will interpret what they see after their action is complete. Pros More complete picture of interaction Caters for experts and novices Cons Analysis is costly Bias in analysis. Focus on specific set of data points. Ignore broader context. Example: Only person and interface. Example maps in driving vs map in labs. *5 sec disappearance of search bar Participant Consider broader activity not just inside their head but outside world, interface must fit with context, evaluated by in-situ studies (evaluated in the real world). \u0026ldquo;Functionalism/Systems Psychology\u0026rdquo; from psychology Pros Evaluates interaction in context Capture authentic user attentions Cons Expensive to perform and analyze both Requires real functional interfaces, no prototypes Hard to use when starting fresh Lots of uncontrollable variables Each design informs one another User experience, Sans Design There is always user experience UX design focuses on dictating users experience It comes due to humans and tasks Scope of user experience: Individual - Feels of design like vs dislike Group - FB keeps people in touch Societal - Twitter and Arab Spring Feedback Cycles Way in which people interact with the world and get feedback on the results of those interactions Feedback cycles are hallmark of AI All of HCI can be interpreted as an application of feedback cycles The Gulf of Execution is a term used in the field of human-computer interaction to describe the gap between a user\u0026rsquo;s goals or intentions and the actions they must take in order to achieve those goals using a computer system. The Gulf of Execution refers to the difficulty or complexity a user may experience in understanding and using the system to accomplish their task. Gulf of Execution The distance between user\u0026rsquo;s goals and the execution of the actions required to realize those goals How do the users know what they can do? How do the users figure out how to make those goals a reality? How do they figure out what actions to take to make the state of the system match their goal state? How hard is it to do in the interface what is necessary to accomplish the goals? What is the difference between what the user thinks they have to do vs what they actually have to do? Phases: Identify Intentions (what their goal is) in the context of the system Identify Actions necessary to make the goal a reality Execute in Interface Tips for Gulf of Execution Make functions discoverable Let the user mess around Be consistent with other tools - Ctrl+C for Copy, Ctrl+P for Paste Know your user - command line vs UI Feed-forward - Feedforward (give what will happen if you keep doing same thing, refresh icon on facebook timeline when users drag down) Gulf of evaluation The distance between the effects of those actions and the user\u0026rsquo;s understanding of the results The output of the action that the user took Phases: Interface output - What did it do upon action (audio/video). Interpretation - Can user interpret meaning of output? Evaluation - Evaluate the interpretation to check if goals was realized or not. Tips for Gulf of evaluation Give feedback constantly: Give feedback at every step of the process Give feedback immediately: Let users know they have been heard (greying out icons when you click) Match the feedback to the action (Subtle action, subtle feedback; signification action, significant feedback ) Vary your feedback (auditory, haptic etc.) Leverage direct manipulation (dragging in UI, pulling things around etc.) Norman\u0026rsquo;s Feedback Cycle Stage The goal: What do I want to accomplish and why? Plan: How can I do it? Specify: What options do I have? Perform: What can I do now? Perceive: What just happened? Interpret: What does it mean? Compare: Is this okay? Have I accomplished my goal? Direct Manipulation Direct manipulation in Human-Computer Interaction (HCI) refers to a style of user interface design where the user interacts with graphical objects on the screen directly and immediately, as opposed to indirectly or through a separate command language. The goal of direct manipulation is to make the interface more intuitive and easier to use by allowing users to manipulate objects and see the results of their actions in real-time. Examples of direct manipulation interfaces include desktop environments like Microsoft Windows and Apple\u0026rsquo;s MacOS, as well as mobile operating systems like Apple\u0026rsquo;s iOS and Google\u0026rsquo;s Android. Invisible interface Directly manipulating to perform a task (touch screen allowing for zooming with hand gesture) Direct manipulation is a powerful method for shortening gulfs. VR is also allowing for direct manipulation Don Normans paper on direct manipulation Some lessons from the Direct Manipulation Interfaces paper:\nDistance - the distance between users goals and the system itself. The greater the cognitive load required to use the system, the less direct to the interaction with the system actually feels. Distance can be broken into: Semantic distance - distance between users goals and their expression in the system, i.e., how hard it is to know what to do? Articulatory distance - distance between that expression and its execution, i.e., how hard it is to actually do what you know to do? Direct engagement - systems that best exemplify direct manipulation all give the qualitative feeling that one is engaged with the control of the objects - not with the programs, not with the computer, but with the semantic object of our goals and intentions Invisibility by learning Cars are an example Leveraging prior expectation and providing quick feedback Tips for invisible interfaces: Use affordances: Places where visual design suggest how its supposed to be used: buttons for pressing etc. Know your user: Invisible means different things to different users. Differentiate your user: Allow multiple ways of accomplishing something. Let your interface teach: Teach users more efficient ways of doing something. Talk to your user: Ask them what the users are thinking (if they are talking about the interface, do they talk about the task or the interface, if they talk about the interface, it is pretty visible). Human abilities Who the human is and what they are capable of doing? Input, processing and output Input, and how humans react Average person can sense and perceive (eyes: color and movement) Visual Auditory Haptic Feedback Use recognition over recall Sensation and Perception - Visual Visual system is important for cognition HCI is majorly connected to visual perception Center - Color Periphery - Tracking movement Old audiences - Fonts, visual acuity drop Sensation and Perception - Auditory Noises - pitch and loudness Ears - Localize sounds near vs far Cant process ears (move around) Sensation and Perception - Skin Pressure, vibration and temperature. Cant filter touch, like listening Its only used for personal feedback Memory Chunking - chunking is a grouping together several bits of information into one chunk Perceptual store - very short term memory lasting less than a second. Baddley - Hitch memory model Visuospatial Sketchpad - stores visual information Phonologic loop - (verbal/ auditory information) Episodic Buffer - (integrating information from other system) Central Executive Expertise delays forgetting in perceptual buffer Short-term memory - memory that is able to hold about four to five chunks at a time Chunking is grouping bits of information Focus on recognizing things rather than recall them Effect of HCI Recognition vs Recall - Minibar (recognize the icons) Long-term memory - seemingly unlimited store of memories; it is harder to put things into long-term memory than short-term memory Put short term memory system Reminder experiment - David\u0026rsquo;s card idea Cognition One of the elements of cognition is learning, there are two types: Interfaces should teach them how to learn. Procedural learning - how to do something. We do this HCI. Unconsciously competent (Cant translate them to declarative knowledge, as we don\u0026rsquo;t know). Easy for us to use hard for others. Declarative learning - knowledge about something Cognitive Load When designing user interfaces, cognitive load comes into play as follows:\nWe want to reduce the cognitive load posed by the interface so the user can focus on the task We want to understand the context of what else is going on while users are using our interface Example - IDE error checking in programming Reducing cognitive load Tips:\nUse multiple modalities - Visual and Verbal Let the modalities complement each other Give the user control of the pace Emphasize essential content and avoid the clutter Offload tasks (to the interface) Perception - main ways people perceive the world around them through sight, sound, and touch.\nCognition - memory and learning\nMotor system - how the person interacts with the world around them\nDesign Principles and Heuristics Discoverability Relevant function should be made visible so the user can discover them as opposed to having to read about them in some documentation or learn them through some tutorials When user doesn\u0026rsquo;t know what to do, they should be able to figure out what to do Simplicity There is often tension between discoverability and simplicity. One argues to be seen and on argues to keep the interface simple, a balance between these two is often required. Use of design is easy to understand, regardless of user\u0026rsquo;s experience, knowledge, language skills, or current concentration level User should be given only whats needed. Affordances The design of the thing affords or hints at the way it\u0026rsquo;s suppose to be used. Signifiers are meant to close the gap between the affordance of an object (how the object suggests it should be used) and the perceived affordance (what the user thinks on how the object should be used). Affordance - Inherent property of device. We cant introduce it. Perceived - What human perceives. But it can be wrong. Signifier - It helps perceived affordance = Affordance Mapping Relationship between two things. Mapping user - worlds (cut copy paste) Refers to creating interfaces where the design makes it clear what the effect will be when using them (this is different than affordances where affordances suggests how to use objects) (think about multiple monitors) Mapping vs Affordance Affordance - design suggests what to do Mapping - design shows what the effects will be Ex - Light switch Whats the effect? Dials on stove (icon for burners). More clear. Perceptibility Refers to the user\u0026rsquo;s ability to perceive the current state of the system. (On or off in switch). Problem in David\u0026rsquo;s cieling fan Consistency We should be consistent both within and across interfaces to minimize the amount of learn the user needs to do to learn our interface; in this way we create affordances on our own. URL in blue in wiki page. Common functions across interfaces. Ordering menus/ Shortcuts in PPT. Flexibility Wherever possible, we should support the different interactions in which people engage naturally, rather than forcing them into one against their expertise or against their preference Equity Complementary to flexibility, helping all users have the same user experience. Avoid segregation and stigmatization. In the image below, it might look it they are competing, but no. Design is useful and marketable to people with diverse abilities. Equity is all about making sure users have similar user experience, and flexibility allows for the means to get there. Think Password resets settings Ease and comfort Ease: the design can be used efficiently an comfortably and with a minimum of fatigue; Comfort: appropriate size and space is provided for approach, reach, manipulation, and use regardless of user\u0026rsquo;s body size, posture, or mobility Structure We should organize our user interfaces in ways that helps the user\u0026rsquo;s mental model match the actual content of the task Wall Street website vs print version Constraints Preventing the user from putting an input that wasn\u0026rsquo;t going to work anyway. UI design is transparent. Constraint is visible. Limitting the set of possible actions (to make sure users do not use wrong input, and help users with right input) Norman\u0026rsquo;s four types of constraints: Physical - Physically prevent wrong action. USB sticks. Cultural - Line in escalator in Japan. Semantic - Inherent to situation. Purpose of rare view mirror. Must reflect behind. Logical - Self evident based on situation at hand. One hole to one screw. How do we deal with constraints? There are two ways: Tolerance - users shouldn\u0026rsquo;t be at risk of causing too much trouble accidentally. Undo and Redo. The design should be flexible and tolerant. Feedback - user should be informed on why the error happened and how to avoid it in the future. Worst example: blue screen of death. Documentation even though it\u0026rsquo;s better if the system can be used without documentation, it may be necessary to provide help and documentation. Any such information should be easy to search, focused on the user\u0026rsquo;s task, list concrete steps to be carried out, and not be too large Avoid this all together, but good to have Mental Models and Representations Mental Models Understanding to the world around you, that you hold in your head Mental model - an internal, simulatable understanding of external reality. Good interface will give good mental models. Is a person understanding of real word working - processes relationship and connection in real systems Predict and check outcome of mental model. Basket ball example. Simulate the event to make prediction When reality does match? why is mental model wrong? discomfort, curious? frustration and never will get it. Match users models and interface Design systems that acts how its expected or teach people how they act. Mental models are not unique to HCI. Also in Edtech. Good representation show how system works. But very challenging Five Principals Predictability - User should be able to predict what will happen before perform it. Synthesizability - But you should know the process on how you reached current state. Log of commands in CLI. Familiarity - Leverage actions with user is familiar. Like affordance. Generalizability - Knowledge of one user interface should generalize to others. Consistency - Single action and consistent. Ctrl+X for cut only. Representations Internal symbols for an external reality. Helps users learners to use our interface quickly. Make the solution self evident. Here are some characteristics of good representations: Depicts explicit relationships - Laying thing out helps easy understanding Brings objects and relationships together - Objects and relationships are explicit. No extraneous information - Say only left to right, left out stupid information. Expose natural constraints - Brings environment into pictures. Analogies And Metaphors If you can ground your interface in something users already know, you can get a solid foundation in teaching them how to use your interface. Ex - Wall street website and paper are similar. However, when you use analogies to other interfaces, users may not know where the analogy ends. Therefore, we should pay special attention to misconceptions analogies introduce. Analogies make the interface more learnable, but they may restrict the interface to outdated constraints. Metaphors and analogies - used to create good representations. Design Principles Revisited How do mental models and representations tie into HCI design principles? Analogies and metaphors - Principle of consistency Interfaces should teach the user how the system works - Principle of affordances Representations map the interface to the task at hand - Principle of mapping Learning Curve Expertise vs Experience. Above line of proficiency. Rapid Learning curve with limited experience Difficult interface have slower learning curves Consistency with analogies and representations Slips The user has the right mental model, but does the wrong thing anyway Norman\u0026rsquo;s division: Action-based - Places where the user performs the wrong action, or performs the right action on the wrong object, even though they knew the correct action Memory lapse - Occurs when the user forgets something they knew to do Mistakes The user has the wrong mental model, and does the wrong thing as a result Norman\u0026rsquo;s division: Rule-based - occurs when the user correctly assesses the state of the world but makes the wrong decision based on it Knowledge-based - occur when the user incorrectly assesses the state of the world in the first place Memory lapse - similar to memory lapse slips but focuses on forgetting to fully execute a plan Learned Helplessness A user\u0026rsquo;s sense that they are helpless to accomplish their goals in an interface. Window hang while reading large dataset. It\u0026rsquo;s related to educational technology. Expert Blind Spot When you are an expert in something there are parts of the task that you do subconsciously without even thinking about them. I am not my user. Task Analysis Human information processor model Cognitive Task analysis (similar to the predictor model) Human information processor model A human information processor model; it builds off the processor model of the human\u0026rsquo;s role in a system. One of the human information processor model is teh GOMS model. There are four categories in the GOMS model:\nGoals - users goals in the system Operators - user operations to carry out method Methods - user can use to complete (Methods - Operator 1\u0026mdash;n) Selection rules - which to select among different methods This model proposes that a human has a set of goals and methods they can choose from to accomplish those goals. Each method is comprised of a series of operators which help carry out that method. Lastly, they use some set of selection rules to help decide what method to choose from.\nExample - Transfer information to coworkers - Email, chat, In person, etc.\nStrengths And Weaknesses Of GOMS Weaknesses of GOMS model: Does not address complexity - There are many methods and sub-methods. Standard GOMS rules this out. Assumes user is an expert - GOMS doesn\u0026rsquo;t account for novices and user errors. I don\u0026rsquo;t know highway in USA. Strengths of GOMS model: Formalize user interaction into steps - Interaction steps to measure for efficiency. Helps to narrow down the time for each steps. Finding areas of improvement. Count time for each operator, easy to get keychain while holding something in hand. Types of GOMS KLM-GOMS -\u0026gt; Keystroke level model here operator + execution time - efficiency determination original work had 6 types of operators, won\u0026rsquo;t work on modern ideas CMN GOMS -\u0026gt; Card, Moran and Newell GOMS Hierarchical Goals and choose multiple goals Very low level goals (moving text, delete phrases) Model how long each individual GOMS to take Find place which we can cut out NGOMSL - Natural language GOMS Working memory if exploited can be identified lends itself for human interpretation. 5 Tips: Developing GOMS Models Focus on small goals GOMS should be small, abstract up from there. Example - Navigating end of document Nest goals, not operators GOMS of Navigation GOMS for changing lanes and plotting routes Operators are smallest atoms of GOMS models. Don\u0026rsquo;t breakdown further Differentiate descriptive What people do? and prescriptive What they wanna do? GOMS of former doesn\u0026rsquo;t mean they will do later. They will not do that? Assign costs to operators Measurement of operators will take. Use GOMS to trim waste Use GOMS to cut cost by reducing operators. GOMS to Cognitive Task Analysis The GOMS model assumes the human is an input-output machine (processor model). However, human reasoning may be too nuanced and complex to be so simplified. Cognitive task analysis is another way of examining tasks but it puts a much higher emphasis on things like memory, attention, and cognitive load (predictor model). Behaviorism vs Cognitivism Observable of things Get into mind Cognitive Task Analysis Its collection of methods focus on what we cant see.\nCognitive task analysis are concerned with the underlying thought process associated with performing a task. Most methods follow a particular common sequence:\nCollecting preliminary knowledge No experts needed, but need some familiarity (observe people performing tasks) Identify knowledge representations What does user know what they need to complete a task. Ex: Ordering of tasks/ Memorization etc. For navigation, monitoring and sequence of actions. Apply focused knowledge elicitation methods Identify task, knowledge by think out loud about it. Get user to tell us what they have in mind. What changed their approach, what did they do in prior and what they do after change. Analyze and verify data acquired Confirming if understanding is correct Format results for intend application We take results and models user Result looks like a flow chart, with various tasks in each box.\nHierarchical Task Analysis Tasks could be broken and small tasks could be reused. In the image above, Checkout can be represented as a subtask. This form of task analysis helps us understand what tools might already be available to accomplish certain portions of our task, or how we might design certain things to transfer between different tasks and different contexts. Hierarchical task analysis process: Abstracting out unnecessary details for a certain level of abstraction. Modularizing designs or principles so that they can be transferred between different tasks or different contexts. Organizing the cognitive task analysis in a way that makes it easier to understand and reason over. Cognitive Task Analysis Strengths And Weaknesses Like the GOMS model, cognitive task analysis also have strengths and weaknesses. Strengths: Emphasizes mental processes: Unlike GOMS, emphasis on whats goes on users head Formal enough to for interface design: Easy to communicate and compare Weaknesses: Time-intensive - They involve talking and systematic analysis of data May deemphasize context - Role of artifacts and details in world Ill-suited for novices - Who is trying to use an interface. Other task analysis Human information\nKLM - Keystroke level model TLM - Touch level model MLP - Model human processor CPM-GOMS - Parallel tasks. NGOMSL - Natural language. Cognitive Models\nCDM - Critical decision model - Focus on critical decision TKS - Task knowledge structures - Focus on user knowledge. CFM - Cognitive function model - Focus on complexity Applied CTA Skilled CTA Distributed Cognition Distributed cognition suggests models of cognition should be extended outside the mind. An example is you can either add large numbers in your head (much more difficult) or add large numbers on a piece of paper. Each object in the the process can be said to extend the cognitive process. Don\u0026rsquo;t get smarter by pen and paper, cognition got distributed among the artifacts. How A Cockpit Remembers Its Speeds Given the dynamic nature of flight, how does a cockpit remember its speeds?\nThe paper references to a cockpit and not one individual component as the cockpit comprises of more than one cognitive component to remember speeds:\nLong-term memory: a library of configurations (booklet) Short-term memory: a specific configuration (one sheet) Working memory: use of speed bugs on a dial Deliberation: the two pilots in the cockpit Each one of these components helps in remembering the speed of a plane by serving as an individual cognitive component in the cognitive system. Distributed Cognition To Cognition Load Artifacts are extra memory to brain. Driving example is cognitive overload. GPS is a approach. Cruise control. Offload tasks to artifacts Distributed Cognition as Lens A way of approaching/looking at the problem/design. Separation of monitors. Distributed Cognition To Social Cognition Distributed cognition is concerned with how the mind can be extended by relations with other artifacts and other individuals. Social cognition is concerned with distributing cognition across individuals. Example : map reading during driving in old days. Social Cognition Social cognition is about how social connections create systems that can, together, accomplish tasks. For example, you and your friend navigating to a place. Social cognition is also concerned with the cognitive underpinning of social interactions themselves. it is interested in how perception, memory and learning relate to social phenomenon. (Think designing social media) Situated Action Situated action is strongly concerned with the context within which people interact. However, situation action is not interested in the long-term and enduring permanent interactions amongst these things. Situated action is interested in the kinds of novel situational problems that arise all the time. How do we find out more about these problems? We must examine the interfaces we design within the context in which they\u0026rsquo;re used We must understand the task the user performs grows out of interaction with the interface The task doesn\u0026rsquo;t exist until the user gets started, and once they start, they define the task Situated Action And Memory Memory is context dependent. People will often remember e.g., a list of personal tasks because it is part of a larger narrative versus remembering a list of tasks someone else gave them. Activity Theory A massive and well-developed set of theories regarding interaction between various pieces of an activity. This theory predates HCI and there are some contributions of activity theory to HCI we should be aware of: Task to Activity theory Why we see task and then design Up and down hierarchy - due to learning. It predates HCI Activity theory generalizes our unit of analysis from the task to the activity; we\u0026rsquo;re not just interested in what users doing but why users are doing it Activity theory puts an emphasis on the idea that we can create low level operations from higher level actions Activity theory points out that actions by the user can actually move up and down a hierarchy There are several similarities between Activity Theory and Distributed Cognition (both are focused on goals, where as Situated Action is focused on improvisation) Interfaces and Politics Designing for change, and anticipating the change from our design.\nChange: A Third Motivation There are three goals of HCI:\nHelp a user do a task Understand how a user does a task Change the way the user does a task Car - Seatbelt example - Not usability but safety.\nPaper - Do artifacts have politics? Nuclear Power Can be used in Totalitarian. Push for tech carries politics Solar power equalitarian society Two types Inherently political Nuclear power (top down)- Authoritarian Solar Power (distributed) - Egalitarian society Technical arrangements as forms of order Technology can change social order - Context and purpose - Busting unions. Change By Design Ability of interfaces to change behavior can be abused. Normal designs with underlying politics People can design things to intentionally create a negative (designing bridges too low so that it\u0026rsquo;s harder for poor people to go to a destination) or positive change (Facebook\u0026rsquo;s like button). Wealthy people can go to park, low bridges for poor people (social order), Net neutrality Positive interactions: Like button and emotions in Facebook. Societal trend. Change By Happenstance Need not always by design. Bicycle - Societal change Women using cycle instead relying on someone, wardrobe change. While people could intentionally create a positive or negative design, this could happen unintentionally as well for positive (the bicycle giving women freedom to travel independently) and negative (internet access) cases. Existing Infrastructure and Internet Value-sensitive Design Value-sensitive design seeks to provide theory and method to account for human values in a principled and systematic manner throughout the design process. Another dimension to consider while designing: Usable, Useful and Value sensitive Privacy by design - Privacy is the value. Preserve value. Value-sensitive Design and Information Systems Conceptual Investigations - Thought experiments, role value play in stakeholders Empirical Investigations - target users - exploring how they make sense of values Technical Investigation - target systems - Fundamental feature - Proactive, usability vs human values,\nValue-sensitive Design Across Cultures One of the challenges of value-sensitive design is that values are different across cultures (culture that values privacy vs cultures that value frees speech due to censorship).\nRight to be forgotten, value held by EU. Google was not developed based on that. Not universally shared. Privacy vs Free speech. 5 Tips: Value-sensitive Design Start early - Identify values early on design process and check throughout Know your users - Know user\u0026rsquo;s values. Challenges are to be identified. Consider both direct and indirect stakeholder - People who don\u0026rsquo;t user but affected by it. Bank UI. Brainstorm the interface\u0026rsquo;s possibilities - How it could be used. Tracking hours - unjust call for termination. Choose carefully between supporting values and prescribing values - We should not prescribe values for everyone. Be careful about support and change. Reversing The Relationship Technology changes society but society could also change technology too (e.g., demand for single platform to link to others for TV subscriptions). Bulbs - florescent bulbs vs Electricity Bill Satisfaction of politics, preserve power of organization. Conclusion To Principles Human As Processor: At the narrowest level, we might view HCI as the interaction between a person and an interface. This view is shared by models such as the GOMS model. Human As Predictor: Most of the time we are viewing HCI as the user interacting through some interface to accomplish some task. At this level we can look interactions in terms of the gulf of execution and evaluation. We can use tools like cognitive task analysis and hierarchical task analysis to understand things like user\u0026rsquo;s mental models, errors, and mappings between representations and the underlying tasks. Design principles are made. Human As Participant: At the highest level, we are interested in how interactions occurs beyond just the individual, interface, and task. At this level we can look at interactions in terms of activity theory where interactions include elements of the context surrounding the task. We could also look at how artifacts combine to accomplish a task through distributed cognition. Other times we can look at deeply understanding the situated context in which a person is acting through situated action. Additionally, we could also look at how users integrate through norms and relations with social cognition. There are times where we should keep in mind the intended and unintended positive and negative changes our designs might have on society during design. 5 Tips: On-Screen UI Design Use a grid Use a whitespace Know your Gestalt principles - how user perceive and group projects. Reduce clutter - #1-#3 helps in this. Design in grey-scale Gestalt principles The following are the key Gestalt principles:\nLaw of proximity: This principle states that objects that are near each other tend to be grouped together. Law of similarity: This principle states that objects that share similar characteristics, such as shape, color, or texture, tend to be grouped together. Law of closure: This principle states that people tend to perceive incomplete shapes or forms as complete objects by filling in the missing information. Law of symmetry: This principle states that people tend to perceive objects that are symmetrical as a single unit or figure. Law of continuity: This principle states that people tend to perceive objects as continuous and flowing, even when they are interrupted by other objects. Law of figure-ground: This principle states that people tend to perceive objects in a scene as either figures (the objects of focus) or the background against which they are seen. Introduction to Methods Research Methods Design better than existing design In order to design interactions that are better than existing designs, it is important to take into consideration the user needs at each stage of the design process New way to old task Novelty should have the purpose, and understand the users task User Centered Design Prioritizing user needs while recognizing we do not know their needs Design is often done to meet some functional specification, instead of meeting the user\u0026rsquo;s need You are not your user Principles of User-Centered Design There are six principles of user-centered design: The design is based upon an explicit understanding of users, tasks, and environments - Do needfinding Leverage this knowledge throughout the design process. Users are involved throughout design and development - Interviews, Surveys, working on design team etc. The design is driven and refined by user-centered evaluation. Real users evaluate the prototype. The process is iterative - No single shot results. Even after being realesed. The design addresses the whole user experience - Entire experience is to be considered. The design team includes multidisciplinary skills and perspectives - CS Scientists, Pyschologists, Designer, Domain experts and more. Stakeholders There are many types of stakeholders who user or are impacted by our interface: Primary - Our user who uses the interface directly. Grade book tool - Teachers send progress reports to parents Secondary - are people who don\u0026rsquo;t use our system directly but who might interact with the output of it in some way. - Grade book tool Parents receive the output. Tertiary - are people who never interact with the tool or output but who are nonetheless impacted by the existence of the tool. Grade book tool - Students The Design Life Cycle User-centered design is about integrating the user into every phase of the design life cycle. We need to know two things: What the design life cycle is? How to integrate the user into each phase? The design life cycle in four phase: Needfinding: gather comprehensive understanding of task the user is are trying to perform: who the user is, what the context of the task is, why they are doing the task, and any other information Design alternatives (early ideas on different ways to perform a task) Prototyping (low fidelity to improved, so that we can put it in front of our users) Evaluation (put them in front of our users) The cycle does not end at product launch. Quantitative data Descriptions, Measurements Observations described or summarized numerically Supports formal tests, comparisons, and conclusions Is strong for a small class of things Captures narrow view of what we might be interested in examining What Qualitative data Preferences, Performances Observations described or summarized non-numerically Supports any kind of response or observation Covers a broader picture of what we\u0026rsquo;re examining More flexible, but it is more prone to biases Description, observations, natural language Convert qualitative to quantitative data. How and Why Mixed method: A mixture of qualitative and quantitative data from same participants.\nTypes Of Nominal Data There are four main types of quantitative data:\nNominal - Categorical - Number of instances of different categories Single nominal - One category Multiple nominal - More than one category Binary (yes/no) and Non binary Ordinal - Similar to nominal, but there is explicit ordering. Scale of 1-5. Gap is unclear. It can be multinominal Binary (Fail/Pass) and Non binary Interval - We do know exact difference between value. Commuting between 4-6am. 64 degree celcius is not twice as warm as 32. Can be Discrete or Continious. Ratio - Ratio data. Absolute value and ratio could be established. Can be Discrete or Continious. Types Of Qualitative Data Depends on how its gathered There are many types of qualitative data, below are some examples: Transcripts - Interview/Focus group Field notes - Participant Observation Artifacts - Reviews for Existing interfaces Others - Many more/ Not mentioned Span is a lot larger Expensive to analyze, interpretation bias Qualitative data -to- Quantitative using coding We don\u0026rsquo;t loose, only transformation Documented methodology is obtained (for review purposes/reproducibility) Always mix qualitative and quantitative data in HCI Typically these transformed data are nominal Ethics and Human Research Origin Of Institutional Review Board Due to unethical experiments such as the Milgram Experiment, Tuskegee Syphilis Experiment, and Stanford Prison Experiment the National Research Act was enacted (1974). This led to the creation of institutional review boards to oversee research at universities. In general, the benefits to society must outweigh the risks to the subjects in the case of these experiments. Belmont Report - summarizes basic ethical principles that research must follow in order to receive government support. demanded rigorous consent procedures positive outweighed negatives and rights are always preserved benefits of study should outweighs risk to participants Fair selection of subjects Value of Research Ethics Benefits are worth the risk and are significant in nature. IRB is sensitive about coercions when participants feel coerced, it impacts our results. Inherent bias on participants, effect on results. Not just ethical, but doing good research. IRB also monitors the research is sound and useful. IRB Protocol Protocol is a description regarding a project Status: Approved, New Waiting for PI, Withdrawn, Waiting for Sign-Off Add research personnel Protocol title Select role Certification is needed Primary Investigator (PI) will be always first, and must be a faculty How to create a protocol in IRBWISE Protocol description covers study at high level. Research design and methodology (what users will experience and in what order) Experimental designs Duration of participation Data collection methods Benefits outweigh the risk Risks are to be added. Statistical Analysis (Qualitative research might not have this) Start and End dates Human Subject Interaction details Will directly involve direct interaction describe subjects and data we plan to collect (are we being fair to all genders) Vulnerable population - special accommodation is to be needed (they might not have the ability to give full consent) Scientific justification is needed for no of participants Effect size is needed Inclusion and Exclusion status Subjects age ranges Recruitment plan - (how are we going to find our subjects) Compensation provision IRB Consent Procedure What kind of consent is received? Written or Waiver Some narrow circumstances - Now direct effect Waiver of documentation of consent Written consent is only identity of participant Implied consent and can withdraw anytime Justification is needed for consent waiver At risk population needs more information Concealment, deception - Temporary prototype is needed Upload concent form Data Management Question Clinical research and Biological research How do we keep data safe DoD, Radiation and Nano tech - Involvement Interview script, recruitment script, survey etc. No conflict of Interest and submit to PI. Needfinding and Requirements Gathering Introduction to Needfinding Avoid preconceived notions (if all you have is a hammer, everything looks like a nail) Defining general questions about the user Generating answers about the user Formalizing models of the users Data Inventory Some understanding of the data we want to gather. These are the questions we want to answer:\nWho are the users? Age, gender, level of expertise etc Where are the users? What is their environment? What is the context of the task? What is competing for user attention? What are their goals? What are they trying to accomplish? What do they need? What are the physical objects/information/collaborators they need? What are their tasks? What are they doing physically, cognitively, socially? What are their subtasks? How do they accomplish those subtasks? The Problem Space Where is the task occurring? What else is going on, what are the user\u0026rsquo;s explicit and implicit needs? broader view of the problem space start with general types and observations and move through progressively more targeted types of need finding. User Types Full range of users for which we are designing Task is usually the same for both, but the audience, their motivation and their needs are different Identify different types of user and perform needfinding exercise on all of them Kid vs Adults Experts vs Novices Avoiding Bias in Needfinding Confirmation bias: we see what we want to see, preconceived ideas, test empirically, keep samples covered, involve multiple individuals in needfinding process Observer bias: Subconsciously bias the users. Avoid leading questions Social desirability bias: Make naturalistic observations, record objective bias. Voluntary response bias: Oversampling. Reduce survey shown. Recall bias: Misleading and incorrect recall, resulting in faulty data, people are not good what they did during activity done in past. Naturalistic Observation Observing people in their natural context Cannot interact with users directly Not sure what those users are thinking Tips on Naturalistic Observation Take notes Start specific, and then abstract (do not summarize too soon) Spread out your sessions Find a partner Look for questions Five tips to use during observation: Take notes - Gather targeted information and observation about what you see. Start specific, then abstract - Risk Tunnel vision. Write down individual action, do not summarize/analyze from the beginning Spread out your sessions Find a partner - Take notes and compare with partners Look for questions - Should inform questions what you need during targeted needfinding Participant Observation Although you are not your user, you can act as a participant Do not overrepresented your own experience Hacks and Workarounds How do users use hacks/workarounds to accomplish a task? But ask them why they use the hacks Errors Slips or mistakes that users frequently make while performing the task within the interface Errors happen because of weak mental models Apprenticeship and Ethnography Researching a community by becoming a participant in it (become an expert in it: pretty much) Interviews and Focus Groups Just talk to them Focus group can lead to convergent thinking 5 Tips for effective interviews: Focus on the 6 W\u0026rsquo;s (who, what, where, when, why and how), open ended semi structured questions, Avoid yes and no, use openended and semiphrases questions Be aware of bias Listen Organize the interview (introduction, lighter qn for trust gathering, summary at end for the user to understand the purpose) Practice Think-Aloud Asking user to think out loud while in the context of the task Useful because we get info that user might forget later, but thinking out loud might cause them to act differently Post-Event Protocol (is better option) Surveys Larger number of questions/surveyors Not as through, but powerful to get large number of data Also helpful in identifying what to ask during interviews Tips: Less is more Be aware of bias Tie them to inventory Test them out Iterate Questions in survey: Be clear: Code numbers with what they mean (likert), don\u0026rsquo;t ask overlapping range, time box frequency based questions Be concise: Might be a tradeoff with clear, ask in plain language Be specific: Avoid double barrel questions, avoid questions that allow internal conflict, avoid questions on super big ideas Be expressive: Allow users to be expressive, emphasize users opinions, user ranges instead of yes/no questions, give levels of frequency or agreement, allow users to add nominal categories Be unbiased: Leave open ended question open, avoid loaded questions (wasted vs spent) Be usable: Provide progress bar, page length consistency, order your questions logically with a flow, alert users about unanswered questions, preview survey yourself. Other Data Gathering Logs, Product Reviews, Already existing reviews (critique already existing interface) etc. Iterative Needfinding Needfinding on its own can be a cycle Revisiting the inventory Revisit these:\nWho are the users? Age, gender, level of expertise etc Where are the users? What is their environment? What is the context of the task? What is competing for user attention? What are their goals? What are they trying to accomplish? What do they need? What are the physical objects/information/collaborators they need? What are their tasks? What are they doing physically, cognitively, socially? What are their subtasks? How do they accomplish those subtasks? Representing the Need User needs can be formulated to something usable Task and subtasks, hierarchy, flowchart with decisions, diagram of structural relationships between the components in the system and how they interact. summarize to task analysis (data gathered can summarize task analysis) Defining the Requirements They should be specific and evaluative Functionality, usability, learnability, accessibility Compatibility, compliance, cost Used to evaluate the interface going forward Design Alternatives The biggest mistake a designer can make is jumping straight to designing an interface without understanding the users or understanding the task. The second biggest mistake is settling on a single design idea or a single genre of design ideas too early. After having a good understanding of needs of our user, brainstorm on task we have been investigating The settling on a single idea can take on multiple forms: Staying too allegiant to existing designs or products Focusing too strongly on one alternative from the very beginning Tunnel vision: focusing on one alternative from the very beginning Design space: the area in which we design our solution Our goal during the design alternative phase is to explore the possible design space. We do not want to narrow down the design space too early. Brainstorming Explore possible design space (brainstorm lots of potential spaces) Start with individual brainstorming Generate lots of ideas Groups tend to coalesce around conclusion earlier Tips on individual brainstorming Write down the core problem Constrain yourself Aim for 20 Take a break Divide and conquer Challenges in Group Brainstorming The challenges are: Social loafing - the tendency to exert less effort working in groups than working alone Conformity - the tendency to agree with or follow the group\u0026rsquo;s reasoning and ideas Production blocking - the tendency of some individuals in discussions to block other individual\u0026rsquo;s participation Performance matching - the tendency to match one\u0026rsquo;s level of performance to other collaborators Power dynamics - the tendency to defer to more senior individuals, or to overpower less senior individuals (gender, race, etc.) Rules for Group Brainstorming Expressiveness - Any idea that comes out share out loud Non-evaluation - No evaluation Quantity - More ideas better Building - Build on the other ideas There are four additional rules: Stay focused - keep goal in mind No explaining ideas - Say idea and move on Revisit the problem - Hit a road block and revisit encourage others - Encourage them to do so. 5 Tips for group brainstorming Go through every individual idea Find the optimal size Set clear rules for communication - Set timer and no one can block others Set clear expectations - How long session wil go? set an expectation End with ideas, not decisions - several ideas and come back to pursue Fleshing out ideas some ideas can be dismissed easily, which is fine\ncombine multiple ideas, dismiss with some analysis\nHere are methods to flesh out ideas:\nPersonas - create actual characters to represent our users. Emphatic reasoning. User profiles - defining a number of different variables about our users and possibilities within each - Expertise, Motivation, Usage Frequency, Literacy etc. Timelines/Journey Maps - Take a persona and stretch out over time. What does a user do before/during/after the task? What action lead to the task? What do they do later? What are they thinking when they are doing the task? Scenarios and Storyboards - Examining the specific scenarios users will encounter while using our interfaces (a more specific approach). Timelines tend to be pretty general, scenarios are more specific. Particular user, particular events, while performing particular task. Ambulance/Audiobook example. Pretty close to mockups. User modeling - Where personas are personal and will give us an empathetic view of the UX, user models are more objective and meant to give us measurable and comparable view of the UX. GOMS model. How does the user achieve a goal? Identification of phases and efficiencies. Prototyping Basics Of Prototyping Some basics to prototyping:\nEarly prototyping - rapid revision on preliminary ideas Late prototyping - finishing touches on final design/revising already live design Representation (Types) Different prototype representations from early (low-fidelity) to late (high fidelity):\nVerbal prototype: A verbal prototype is a concept or idea that is described in words, without any visual or physical representation. It can be used to quickly test and refine ideas without investing time or resources in creating a physical prototype. Paper prototype: A paper prototype is a simple, low-fidelity mockup of a product that is made using paper or cardboard. It is used to test the layout and functionality of a design, and can be easily modified as needed. Wizard of Oz prototype: A Wizard of Oz prototype is a type of interactive mockup that simulates the behavior of a fully functioning product by using human operators to perform the actions that would normally be performed by the product itself. This type of prototype can be used to test user interaction and identify usability issues before building a fully functional prototype. Wireframe prototype: A wireframe prototype is a visual representation of a product that shows the layout and structure of the interface, without including detailed graphics or other design elements. It can be used to test the flow and functionality of a design. Physical prototype: A physical prototype is a working model of a product that is built using real materials. It can be used to test the form and function of a design and identify any manufacturing or engineering issues that may need to be addressed. Functional prototype: A functional prototype is a working model of a product that includes some or all of the features and functionality of the final product. It can be used to test the performance and usability of a design before beginning mass production. Live prototype: A live prototype is a fully functional version of a product that is released to a limited audience for testing and feedback. It can be used to identify any remaining bugs or usability issues before launching the product to a wider audience. Fidelity Fidelity - Completeness/Maturity of prototype. Far from being complete. Correlated to representation.\nEvaluation Prototype evaluation from early to late: Function - Low fidelity - what button to press ? Can it do what it is meant to do? Can user figure out what to do by looking at it? Interface - Readability Performance - Higher fidelity - working/closing to working output Scope Prototype scope from early to late: Horizontal prototype - covers the design as a whole but in a more shallow way Entire FB website Vertical prototype - great detail on a small portion of the interaction Status posting screen Tradeoffs In Prototyping We must note tradeoffs in prototyping:\nLow-fidelity prototypes:\nPros: easy to create and modify Cons: not as effective for detailed comprehensive evaluations High-fidelity prototypes:\nPros: can be used for detailed feedback and evaluation Cons: difficult to actually put together Remember that we are designing to get more feedback.\nStart easy get ideas and move to higher fidelity.\nNot complete interfaces.\n5 Tips: Prototyping Keep prototypes easy to change - Enable rapid revision (paper vs code) Make it clear that it\u0026rsquo;s a prototype - Don\u0026rsquo;t make too good, make it look like prototype Be creative - Do whatever it takes to get feedback. Find ones that get feedback. Evaluate risks - Minimize time spent on bad design by getting feedback early. Don\u0026rsquo;t waste time. Prototype for feedback - Goal of prototype is feedback. Prototype for kind of feedback. Design Life Cycle Revisited We do not just move to the evaluation stage after we are done with prototyping, rather a single prototype corresponds to a single iteration through the cycle. Success of prototype -\u0026gt; Raise the fidelity Multi-Level Prototyping All prototypes do not have to be at the same level at the same time. Instead, prototyping can and should exist at multiple levels of fidelity. Don\u0026rsquo;t do everything, part could be done from low to high fidelity. Evaluation Three Types Of Evaluation Qualitative evaluation - evaluation that emphasizes the totality of a phenomenon. Likes dislike need doesn\u0026rsquo;t need, easy vs hard etc. Empirical evaluation - evaluation based on numeric summaries or observations of a phenomenon. More participants and qualitative is done in prior. Predictive evaluation - evaluation based on systematic application of pre-established principles and heuristics (no users). Evaluation Terminology Reliability - whether a measure consistently returns the same results for the same phenomenon. Validity - whether a measure\u0026rsquo;s results actually reflect the underlying phenomenon (reality and results) Generalizability - whether a measure\u0026rsquo;s results can be used to predict phenomena beyond what it measured. Broader Audience (may or may not be applicable to all). Precision - the level of detail a measure supplies 5 Tips: What To Evaluate Efficiency - how long does user take to achieve text (Expert) Accuracy - how many users does users commit while executing a task (Expert) Learnability - How long does user take to reach expertise. Memorability - users ability to remember on how to use interface over time. Satisfaction - Cognitive load, how many actually download the app? - Social desirability bias. Important things needed to address the research: What data are you gathering? What are you evaluating ? What approach will you use to evaluate? Evaluation Timeline Change in evaluation with time, The evaluation timeline usually is as follows:\nRegarding purpose: Formative - primary purpose is to help redesign and improve our interface (early) Summative - the intention of conclusively saying at the end what the difference was (late, hopefully we only do formative) Regarding approach: Ways to fullfil purpose Qualitative - the goal is to help us improve and understand tasks (early) Predictive - inform how we revise and improve our interface over time (Similar to qualitative evaluation) Empirical - the goal is to demonstrate or assess change (late) Regarding data: Qualitative - always useful to improve our interfaces (early and late) Quantitative - while always useful, can only arise when we have rigorous evaluations (late) Regarding setting: where does it take place. Lab testing - helps us focus exclusively on the interface early on Field testing - helps us focus more on the interface in context Evaluation Design Define the task - very large or very small task. Define performance measures - how are we going to measure this. Define it and avoid confirmation bias. Create metrics. Qualitative vs Quantitative. Develop the experiment - How we find user performance on the measures. Survey or Interview/ What to control or vary empirically. Recruit participants - Ethics - right awareness Do the experiment Analyze the data - what data tells about performance measures. Do followups if you find something extra than expected. Summarize the data - Informs ongoing process Qualitative Evaluation Get qualitative feedback about the interface. There are some questions we want to ask in this evaluation: (Similar to interviews) What did you like/dislike? What were you thinking while using this interface? What was your goal when you took that particular action? Methods - Interview/Survey/ Think out load protocol/Focus Groups Use these techniques to get feedback on how our prototype changes the task. Designing A Qualitative Evaluation There are options when designing a qualitative evaluation:\nPrior experience or live demonstration? - bring user in to test. Mostly later case Synchronous or asynchronous? - watch live or complete and send One interface or multiple prototypes? - Vary the order based on bias. Think aloud protocol or post-event protocol? - explain while doing or do later at the end. Individuals or groups? - Focus groups (build and expand)/ Only source of knowledge (bad) but no bias. Capturing Qualitative Evaluation Options to capture qualitative evaluation:\nVideo recording Pros: Automated, comprehensive and passive (focus on administering) Cons: Intrusive, non-analyzable and screen-less. Overwhelming on analyses. Note-taking Pros: Cheap, Non intrusive (Capture what we do/not everything) and Analyzable Cons: Slow, Manual and Limited Software logging Pros: Automated, passive and analyzable Cons: Limited (only some parts could be captured), Narrow and Tech Sensitive (prototype needs to reach certain level of fidelity) 5 Tips: Qualitative Evaluation Run pilot studies - Recruiting is hard, gather useful data . Use friends and coworkers Focus on feedback - Don\u0026rsquo;t explain rationale, don\u0026rsquo;t teach. Take it and design. Use questions - when user get stuck? Guide user Instruct users what to do, not how - Reduce bias Capture satisfaction - Do they like it? Empirical Evaluation Something numerical in evaluation. What layout of button is useful? Comparing design and showing improvement in industry. Build new theories (gesture has tuf curve than voice) How can we show there is a difference between these designs? The goal of empirical evaluation is to come up with strong conclusions. Most empirical evaluations are comparisons. Designing Empirical Evaluation Treatment - what a participant does in an experiment. Difference interface or design and comparison between them. Difference between two logo should be based on design color should be only comparable. Between subjects design - comparison between two groups of subjects receiving different treatments. What do participants do or both treatment? Within subjects design - comparison within one group experiencing multiple treatments. Both treatments are given, what are seen first? order is randomized. Random assignment - using random chance to decide what treatment each participant receives. Control bias. Hypothesis Testing Reaction time study? Data is generated and compare this. Hypothesis testing - testing whether or not the data allows us to conclude a difference exists. Null - Assume oppose is true Alternative if data doesn\u0026rsquo;t support that. Less than 5% chance. Quantitative Data And Empirical Tests Recall that there are a number of tests for quantitative data:\nNominal Recommended - Chi-squared test Alternatively: Fisher\u0026rsquo;s exact test, G-test Ordinal: Recommended - Kolmogorov-Smirnov test Alternatively - Chi-squared test, median test Interval and ratio: Recommended - Student\u0026rsquo;s t-test Alternatively - MWW test, Kruskal-Wallis test Special Test Three independent variable (hypothesis) Do pairwise Repeated testing False positive Falsely reject null and agree alternative hypothesis. Type I Error (False Positive): False rejection of null hypothesis Type II Error (False Negative): False retention of null hypothesis Fishers exact and G-test Where is the difference? ANOVA and Kruskal Wallis (Interval and Ratio) Where is the difference? Independent variable is mostly categorical. GPA is interval data. Binomial Data - Two sample binomial test Summary Of Empirical Tests Below is a summary of empirical tests:\n5 Tips: Empirical Evaluation Control what you can, document what you can\u0026rsquo;t - Try to make treatments identical as possible Limit your variables - Noisy data false conclusion and monitor handful of things at a point. Work backwards in designing experiments - Decide what question you want to answer, the anaysis you want to use and the question you want to ask Script your analyses in advance - Do not torture data. Pay attention to power - Size of difference the test can detect. Predictive Evaluation Predictive evaluation should only be used where we wouldn\u0026rsquo;t otherwise be doing any evaluation. Rapid feedback, appropriately and when users are not available. Types Of Predictive Evaluation Heuristic evaluation - each individual evaluator inspects the interface alone, and identifies places where the interface violates some heuristic. Sit with an expert and get the report. Model-based evaluation - tracing through models in the context of the interface we designed (e.g., GOMS model). We can also compare interfaces. Also profiles of users could be used. Simulation-based evaluation - where we might construct an AI agent that interacts with our interface in the way a human would. The human project - IIIT Germany. Cognitive Walkthrough The most common type of predictive evaluation is actually cognitive walkthrough. Cognitive walkthrough - stepping through the process of interacting with an interface, mentally simulating in each stage what the user is seeing and thinking and doing. To do this, we start with task and goal. predict what action will user take Noting system response Investigate gulf for each step it may be fine for us, but we can put to user shoes we can identify something missing. Evaluating Prototypes Our goal is to constantly apply multiple evaluation techniques to center our designs on the user.\nQualitative evaluation. Some quantitative evaluation. For all the prototypes. HCI and Agile Development The Demand For Rapid HCI Before, costs for development, distribution, and feedback regarding software and products were expensive compared to now. Specialized development skills Distribution is physical Fix was hard By have users to come into testing How do we take the principles we have covered so far and apply them to a rapid agile development process? Cheap development Internet distribution - Free and update Usage data is free and live. Lots of feedback More incentive to build and save When To Go Agile? We can make the decision to go agile by considering the following:\nTraditional Agile Criticality is\u0026hellip; High Low Requirements change.. Rarely Frequently Team size is\u0026hellip; Large Small Team embraces\u0026hellip; Order Change Towards a framework for integrating agile development and user-centred design (UCD). Similarities and Differences Between UCD and Agile Development\nThey rely on an iterative development process, building on empirical information from previous cycles or rounds. For instance, one of XP’s values is feedback, and the idea of refactoring code is an embodiment of this value. In UCD one of its founding principles is iterative design Agile techniques place an emphasis on the user, encouraging participation throughout the development process. For instance, in Scrum, user evaluation of the product is encouraged on a monthly basis as users are ideally present during the sprint review and the “Product Owner” is responsible for the requirements and feature prioritization for the product. A second founding principle of UCD, is early and continual focus on users. Both approaches emphasis the importance of team coherence. Beck states that one of the purposes of the planning game is to “bring the team together”. One of the features of the UCD approach is that the whole team should have the user in mind while developing the product. The two main differences are:\nUCD advocates maintain that certain design products are required to support communication with developers, while agile methods seek minimal documentation. UCD encourages the team to understand their users as much as possible before the product build begins, whereas agile methods are largely against an up-front period of investigation at the expense of writing code. Live Prototyping Optimizers - Drag and drop interface. Small revision its awesome. Benefit is high. Final interface vs Prototype Allows to get feedback A/B Testing Rapid software testing between two changes. B version to small users and change as positive before to all. Real user testing Agile HCI In The Design Life Cycle Agile development techniques don\u0026rsquo;t replace the design life cycle, they just change the rate at which we go through it and the types of prototypes and evaluation that we actually do. We\u0026rsquo;re still going to do the initial need-finding step. 5 Tips: Mitigating Risk In HCI And Agile Development Start more traditional - Once you have something up and running, move to agile Focus on small changes - Don\u0026rsquo;t make huge change Adopt a parallel track method - 2 week sprints, have HCI one sprint ahead Be careful with consistency - Don\u0026rsquo;t mess with user expectation Nest your design cycles - Small cycles rapidly in Agile Approaches To User-Centered Design There are different approaches to user-centered design:\nParticipatory design - all the stakeholders including the users themselves, are involved as part of the design team but we must be careful not to over represent the few users that are participating in the design with the rest of the users out there Action research - addresses an immediate problem and researches it by trying to simultaneously solve it Design-based research - similar to action research but it can be done by outside practitioners as well. Common in learning science research ","date":"2023-04-25T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/human_computer_interaction/","section":"posts","tags":["user-interface","user-interaction","user-experience","processor-mode","predictor-model"],"title":"Human-Computer Interaction"},{"categories":["gatech","hci"],"contents":"","date":"2023-04-22T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/redesigning_goodreads/","section":"posts","tags":["cs6750","user-interface","user-interaction","user-experience","processor-mode","predictor-model"],"title":"Redesigning the Goodreads bookshelf interaction"},{"categories":["codefornepal"],"contents":"Code for Nepal and DataCamp Partnership: Empowering Data Literacy and Career Growth Code for Nepal is a non-profit organization that aims to increase digital literacy and access to technology in Nepal. We focus on providing training and resources for coding and data literacy, as well as advocating for policies and programs that support technology education and access. Code for Nepal also works to promote the use of technology to address social issues and improve the lives of Nepali people.\nWe introduced the Code for Nepal Data Fellowship in 2020, and this has been one of our success stories. Our partnership with DataCamp enables our fellows access to an amazing learning platform that offers interactive courses in data science, analytics, and programming. Additionally, as part of the fellowship they also join a supportive community, collaborate with like-minded individuals, receive guidance from experienced mentors, and even earn an income. Fellows have the opportunity to share their knowledge and insights by writing blogs. We believe this program offers a unique chance for individuals to develop their skills and contribute to the tech industry in Nepal and globally.\nDataCamp provides a range of courses on topics such as Python programming, data analysis, data visualization, machine learning, and many more. DataCamp courses are interactive and include hands-on coding exercises, quizzes, and projects that allow learners to apply their skills in real-world scenarios. The platform is designed to be user-friendly and accessible to learners of all levels, from beginners to experts. Additionally, DataCamp offers career tracks, skill assessments, and a personalized learning experience through its adaptive learning technology.\nRecently, we surveyed our alumni to request information regarding if the Code for Nepal partnership with DataCamp helped them kickstart their career in data.\nMost of the respondents stated that DataCamp helped them to learn or enhance their programming skills, specifically in Python, SQL, and data analysis. Some respondents are students who are still learning and looking for internships, while others secured jobs or internships after completing DataCamp courses. A few respondents also mentioned that DataCamp provided them with a platform to learn about AI/ML and big data. One respondent mentioned that DataCamp has been a crucial resource for them in developing their data analysis skills, and they were able to secure a full-time position at a clinical tech company.\nFrom the survey responses, the following companies were mentioned where participants secured jobs or internships after learning through DataCamp: Accenture, Fusemachines, Cloudfactory, Cotiviti Nepal, Plieger Group, Mediflow Solutions, and Nimble Clinical Research. However, some participants did not mention the companies they secured jobs/internships with or were still seeking opportunities.\nThe responses provided by different individuals suggest that the partnership has helped them in various ways to enhance their knowledge and skills in the field of data science, engineering, and analysis. Many individuals reported that DataCamp helped them to learn Python programming language, which is widely used in the field of data science, and also introduced them to various tools and modules available in Python, such as Numpy, Pandas, and Matplotlib.\nSome individuals mentioned that DataCamp helped them gain SQL skills and provided them with a brief overview of the field of data engineering. Others reported that DataCamp helped them to hone their AI/ML skills, learn important concepts such as data manipulation, data visualization, machine learning algorithms, and deep learning techniques, and build practical projects to showcase their abilities to potential employers.\nMany individuals expressed their gratitude towards DataCamp for providing them with an excellent platform to learn and improve their skills in data science, engineering, and analysis. They appreciated the interactive and impactful nature of the courses and projects, which helped them to test their understanding of the material and identify their areas of weakness. Some individuals also mentioned that DataCamp helped them to advance in their careers in data science and analysis.\nThe responses suggest that DataCamp has been a valuable resource for individuals interested in data science, engineering, and analysis, providing them with the necessary tools and knowledge to succeed in these fields.\n","date":"2023-03-29T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/code_for_nepal/","section":"posts","tags":["codefornepal","dcdonates","datacamp","datascience","dataanalysis","digitalliteracy","careerdevelopment","pythonprogramming","sql","machinelearning","datavisualization","fellowshipprogram","socialimpact"],"title":"Code for Nepal and DataCamp Partnership: Empowering Data Literacy and Career Growth"},{"categories":["AWS Certified ML - Specialty exam (MLS-C01)"],"contents":"Data Engineering S3 Kinesis Glue Redshift RDS DynamoDB OpenSearch AWS Data Pipeline AWS Batch AWS DMS Step Functions EFS EBS EMR Misc This domain requires understanding of creating data repositories for machine learning, identification and implementation of data ingestion solution, and identification and implementation of a data transformation solution.\nData engineering is the process of building and maintaining the infrastructure and systems that are used to store, process, and analyze data. In the context of Amazon Web Services (AWS), data engineering involves the use of various AWS services and tools to build and operate data pipelines, data lakes, and other data processing systems.\nSome common AWS services and tools that are used in data engineering on AWS include:\nAmazon S3: A fully managed object storage service that is used to store and retrieve data. Amazon EMR: A fully managed big data processing service that is used to process and analyze large datasets using Apache Hadoop and Apache Spark. AWS Glue: A fully managed extract, transform, and load (ETL) service that is used to move and transform data between data stores. Amazon Redshift: A fully managed data warehouse service that is used to store and analyze large amounts of data using SQL and business intelligence tools. Amazon RDS: A fully managed database service that is used to set up, operate, and scale relational databases in the cloud. By using these and other AWS services, data engineers can build and maintain robust, scalable, and cost-effective data processing systems on the AWS Cloud.\nS3 Amazon S3 (Simple Storage Service) is a cloud storage service that allows you to store and retrieve data at any time, from anywhere on the web. It is designed to make web-scale computing easier for developers by providing a simple, highly scalable, and cost-effective way to store and retrieve any amount of data. With S3, you can store and retrieve any amount of data, at any time, from anywhere on the web. S3 is designed to provide 99.999999999% durability and scale past trillions of objects worldwide. It is used to store and retrieve any amount of data, at any time, from anywhere on the web. It is an object storage service that offers industry-leading scalability, data availability, security, and performance.\nfoundational for machine learning projects since it is a cost effective solution for datasets storage object based storage, bucket name need to be globally unique, however the storage itself is unique to regions key is the full path of the file and even though it looks like there is a folder based heirarchy, that is not how it works you can have a very long file name, in the sense that the path (key) can be very long individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 TB. The largest object that can be uploaded in a single PUT is 5 GB. object tags can be added, helpful with classification and security lifecycle (these are key value pairs) decoupling of compute and storage side perfect use case of data lake, since it can store various formats of data (object storage) it is possible to partition the storage, which is helpful (speedy) when querying via athena. Kinesis partitions the data automatically. 11 9\u0026rsquo;s durability (for all storage classes) availability differs between availability classes Storage classes S3, Standard / General Purpose: for frequently accessed data S3, Infrequent Access: lower cost than standard, for data accessed monthly, and requires milliseconds retrival, but there is a cost associated with retrival S3, Infrequent Access, One Zone, good for secondary copies of backup, or data that can be recreated, infrequent access for cost saving S3, Glacier Instant Retrival, price per storage + price per retrival, can access within milliseconds, for low cost storage for long-lived data S3, Glacier Flexible Retrival, expedited: 1-5 mins, standard: 3-5 hrs, bulk: 5-12 hrs (free), for long term low cost storage for backups and archives with different retrival options S3, Glacier Deep Archive: lowest cost, 180 days of minimum storage, for rarely accessed archive data S3, Intelligent Tiering: move objects between tiers with monthly monitoring and auto-tiering fee It is possible to move objects between these storage classes more info\nLifecycle rules Amazon S3 Lifecycle rules allow you to define policies for how Amazon S3 stores objects. You can use Lifecycle rules to specify when objects transition to different storage classes, or when they expire and are deleted. This can help you reduce your storage costs by moving objects to lower-cost storage classes or deleting them when they are no longer needed. You can set up Lifecycle rules at the bucket level or at the object level (for individual objects or for groups of objects). You can also specify different rules for different prefixes or object tags.\nTransitioning objects between classes is possible Transition Actions can be used to configure objects to transition to another storage class Transition Actions can also be used for expiration, incomplete multi part uploads etc. Rules can be applied to buckets, specific paths of the project or also to tags Amazon S3 analytics works exclusively on S3 standard, and S3 IA, and provides analytics on usage Performance Chart Encryption Amazon S3 supports several encryption options to help users secure their data at rest. These options include:\nSSE-S3: This option uses server-side encryption with Amazon S3-managed keys. With this option, Amazon S3 encrypts the data as it is written to disks in its data centers and decrypts it when it is accessed. SSE-KMS: This option uses server-side encryption with AWS KMS-managed keys. With this option, users can create, rotate, and manage the keys used to encrypt their data. SSE-C: This option allows users to use their own encryption keys to encrypt their data. Users are responsible for securely managing their keys and rotating them as needed. Users can enable encryption when creating a new bucket or when uploading an object to an existing bucket. They can also enable encryption for all objects in an existing bucket by enabling bucket-level encryption.\nSecurity Policy Amazon S3 bucket policies allow users to add additional security controls to their S3 buckets and objects. A bucket policy is a JSON document that defines the permissions for an S3 bucket. It can be used to grant permissions to other AWS accounts, or to grant public access to a bucket and its objects.\nWith a bucket policy, users can:\nGrant read and write permissions to a specific AWS account for all objects in a bucket. Grant read-only permissions to the anonymous user for all objects in a bucket. Grant read and write permissions to a specific AWS account for all objects with a specific prefix (such as \u0026ldquo;private/\u0026rdquo;). Deny all access to a specific AWS account for all objects in a bucket. It is important for users to carefully consider the permissions they grant in their bucket policy, as it can have wide-ranging effects on the security of the bucket and its contents. Misc Amazon S3 VPC Endpoints allow users to access Amazon S3 from within their virtual private cloud (VPC) without the need for an Internet gateway, NAT device, or VPN connection. With VPC Endpoints, users can access S3 from their VPC over an optimized network path, reducing Internet traffic and improving performance. Users can create a VPC Endpoint for Amazon S3 in their VPC, and then configure their VPC security groups and IAM policies to allow access to the endpoint. They can then use the endpoint to access Amazon S3 using the Amazon S3 APIs or the AWS Management Console, just as they would over the Internet. VPC Endpoints for Amazon S3 are supported in all regions and are available in two types: Gateway Endpoints and Interface Endpoints. Gateway Endpoints are powered by a highly available network gateway, while Interface Endpoints are powered by a highly available Network Load Balancer. Users can choose the endpoint type that best meets their needs. Amazon S3 CloudTrail is a service that enables users to record API calls made to Amazon S3 and log the events to an Amazon S3 bucket. This allows users to track changes to their objects, buckets, and Amazon S3 configurations, and to identify and troubleshoot issues. With CloudTrail, users can: Track changes to their Amazon S3 objects and bucket metadata. Determine who made a change and when it was made. Audit changes to their Amazon S3 bucket and object permissions. CloudTrail logs are stored in an Amazon S3 bucket that the user specifies, and they can be delivered to an Amazon CloudWatch Logs log group or an Amazon SNS topic. Users can use the CloudTrail logs to monitor their S3 resources and to ensure compliance with their policies. Kinesis Amazon Kinesis is a fully managed (alternative to Kafka), cloud-based service that enables users to process and analyze streaming data in real-time. With Kinesis, users can build custom applications that process and analyze data as it arrives, and they can scale these applications to process any volume of data, at any time. Kinesis consists of three main components: Producers: Producers are sources of data that send data records to Kinesis streams. Kinesis streams: A Kinesis stream is a sequence of data records that are persisted for a set period of time. Users can create and delete streams, and they can specify the number of shards in a stream. Consumers: Consumers are applications that read and process data records from Kinesis streams. Kinesis is designed to be highly available and durable, and it can automatically scale to handle increases in traffic. Users can use Kinesis to build custom applications that can process and analyze real-time data streams, and they can use the service to support a wide range of use cases, such as real-time analytics, fraud detection, and Internet of Things (IoT) applications. Data is replicated to at least 3 AZ Kinesis Streams Amazon Kinesis Streams is a fully managed, cloud-based service that allows real-time processing of streaming data at high scale. It can continuously capture and store terabytes of data per hour from hundreds of thousands of sources, such as website clickstreams, financial transactions, social media feeds, IT logs, and location-tracking events. With Kinesis Streams, users can build custom applications that process or analyze the data as it arrives, or they can use the provided Kinesis Data Streams API to load the data into other AWS services, such as Amazon S3, Amazon Redshift, or Amazon Elasticsearch Service, for long-term storage and analysis. Streams are divided into shards and partitions The maximum throughput of a single shard 1 mb/seconds or 1000 messages/seconds Data retention: 24 hours by default. It can go up to 365 days. This is useful for reprocessing/replaying data Immutable, 1 mb in size Provisioned mode: choose number of shards and scale manually or using an API Each shard gets 1mb/s in, 2mb/s out On demand mode: each capacity provisioned is 4mb/s If you can plan capacity, use provisioned. however, use on demand if capacity is unknown Custom code for producer or consumer is possible Real time (200 ms latency, possible all the way up to 70ms) Automatic scaling with on-demand mode Multi consumers is possible from one source Kinesis Analytics Amazon Kinesis Analytics is a fully managed, cloud-based service that allows users to process and analyze streaming data in real-time with SQL. With Kinesis Analytics, users can run ad-hoc queries on the data, or they can set up a SQL-based stream processing application to perform transformations on the data as it arrives. SQL or Apache Flink can be used here. The output of these queries and transformations can be fed back into Kinesis Streams for further processing, or it can be stored in other AWS services, such as Amazon S3 or Amazon Redshift, for long-term analysis. Select columns, continious metric generation, responsive analytics, etc. Serverless, scales automatically, pay for resouces consumed but expensive Schema discovery Lambda can be used for preprocessing Two machine learning algorithms: Random cut forest for anomaly detection on numeric columns in a stream, uses recent data to compute the model. A random cut forest (RCF) is a machine learning algorithm that is used for anomaly detection in streaming data. It works by constructing a number of decision trees on randomly selected subsets of the data, and then comparing the score for each new data point to the scores of similar points in the trees. If the score for a new data point is significantly lower than the scores of similar points in the trees, it is considered to be an anomaly. The number of trees in the forest and the size of the subsets of data used to train each tree can be adjusted to control the sensitivity of the model. RCFs are particularly well-suited for detecting anomalies in large, high-dimensional datasets, and they are often used in conjunction with streaming data platforms, such as Amazon Kinesis Streams. Hotspots: A hotspots algorithm is a type of machine learning algorithm that is used to identify spatial clusters of events or observations in a dataset. These clusters, which are also known as hotspots, are areas in which the concentration of events or observations is significantly higher than the surrounding areas. Hotspots algorithms are often used in a variety of applications, such as crime mapping, disease surveillance, and marketing analysis. There are several different approaches to identifying hotspots, including spatial clustering methods, spatial scan statistics, and kernel density estimation. These methods can be applied to a variety of types of data, including point data, such as crime incidents or disease cases, and areal data, such as census tracts or zip codes. Kinesis Firehose Amazon Kinesis Firehose is a fully managed service makes it easy to load streaming data into data stores and analytics tools It can capture, transform, and load data streams into Amazon S3, Amazon Redshift, and Amazon Elasticsearch Service, Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards Kinesis Firehose is a simple and reliable way to load streaming data into data stores and analytics tools. most common is firehose reading from kinesis streams near realtime service because it batch writes data desitination can be s3, redshift, elastisearch, splunk, new relic, or http endpoint 60 seconds latency minimum for non full batches data ingestion into redshift, s3, elasticsearch, splunk automatic scaling conversions from csv/json to parquet and orc and requires the use of glue and transformation through lambda csv to json is possible compression is possible automates scaling no data storage no replay capability it is a serverless transformation tool Kinesis Video Streams Amazon Web Services (AWS) Kinesis Video Streams is a fully managed service that allows users to stream live video from connected devices to the cloud. This service is designed to make it easy to build applications that process and analyze live video streams, as well as store and transmit videos securely at scale. With Kinesis Video Streams, users can stream live video from millions of devices and easily build applications for real-time video analytics and machine learning. In addition, the service allows users to stream video directly to other AWS services, such as Amazon S3, Amazon Kinesis Data Streams, and Amazon Rekognition, for further processing and analysis. Producers: security camera, body-worn cam, aws deeplens, radar data, camera One producer per video stream Video playback capability Sagemaker, rekognition video, 1 hour to 10 years of storage Checkpointing via dynamodb, frames to Sagemaker for ML inference, publish to stream, lambda can be used for notification. Glue Glue Data Catalog and Glue Data Crawlers AWS Glue Data Catalog is a fully managed, cloud-native metadata store that provides a central place to store, annotate, and share metadata across AWS services, applications, and tools. It makes it easy to discover and understand data, and facilitates the development of data-driven applications. With the Glue Data Catalog, users can create, maintain, and access metadata such as database and table definitions, column names and data types, and data lineage. The Glue Data Catalog is integrated with other AWS services such as Amazon Redshift, Amazon Athena, and Amazon EMR, and is accessible through the AWS Management Console, the AWS Glue API, and the AWS Glue ETL (extract, transform, and load) library. Schemas are versioned Glue crawlers help build the Catalog Glue will also extract the partitions, this is helpful for query optimization Glue Data Crawlers are a tool within the Amazon Glue service that allows users to extract metadata from their data stores and create table definitions in the Glue Data Catalog. This enables the creation of ETL jobs and development endpoints in Glue, which can be used to move and transform data. Glue Data Crawlers can connect to various data stores, including Amazon S3 and RDS, as well as any JDBC-compliant data store. Custom connectors for other data stores can also be created using the Glue ETL library. To use Glue Data Crawlers, a Glue ETL job or development endpoint must first be created, after which the Glue ETL library can be utilized for data movement and transformation tasks. Glue ETL Transform data, clean data, enrich data before doing analysis Generate ETL code in python or scala, you can modify the code Possible to provide your own spark or pyspark scripts Target can be S3, JDBC or in glue data catalog Fully managed, cost effective, pay only for the resources consumed Jobs are run on a serverless Spark platform Glue scheduler to schedule the jobs Glue triggers to automate job runs based on events Transformations can be bundled (drop, filter, join, map) Machine learning transformation (find matches, duplicates even when data do not match exactly, dedup) Any apache spark transformation is possible, and changing in format is possible. Multiple ways to create glue jobs including visual editors, python notebooks, python script, etc. Redshift Amazon Redshift is a fully managed data warehouse service offered by Amazon Web Services (AWS). It is designed to handle petabyte-scale data warehouses and make it easy to analyze data using SQL and business intelligence tools. Amazon Redshift is based on PostgreSQL, and it supports many of the same data types and functions as PostgreSQL. To use Amazon Redshift, users first need to set up a cluster of compute nodes. They can then load data into the cluster and perform SQL queries on the data. Amazon Redshift integrates with various data sources and destinations, including Amazon S3, Amazon EMR, and Amazon RDS. It also integrates with a variety of business intelligence tools, such as Quicksight, Tableau, Qlik, and MicroStrategy. Amazon Redshift offers a number of features to help users manage their data warehouses, including automatic data compression, data replication, and data security. It also provides a number of performance enhancements, such as columnar storage, data caching, and parallel query execution. Overall, Amazon Redshift is a powerful and scalable data warehouse solution for analyzing large datasets in the cloud. OLAP Uses SQL to analyze structured and semi-structured data across data warehouses, operational databases, and data lakes Redshift Spectrum can directly query from S3 RDS Amazon Relational Database Service (RDS) is a fully managed database service offered by Amazon Web Services (AWS). It makes it easy to set up, operate, and scale a relational database in the cloud. Amazon RDS supports a variety of database engines, including MySQL, MariaDB, PostgreSQL, Oracle, and Microsoft SQL Server. With Amazon RDS, users can create and manage a database without the need to install and maintain database software. Amazon RDS handles tasks such as hardware provisioning, database setup, patching, and backups. It also provides features such as automated failover and read replicas to help users improve availability and scalability. Amazon RDS is a popular choice for applications that require a relational database, such as e-commerce, content management, and customer relationship management systems. It is particularly well-suited for use cases that require high availability and low latency, such as online transaction processing (OLTP). Must provision servers in advance DynamoDB Amazon DynamoDB is a fully managed NoSQL database service offered by Amazon Web Services (AWS). It is designed to be scalable, fast, and flexible, making it a good choice for applications that need high performance and low latency. DynamoDB stores data in tables, and each table has a primary key that uniquely identifies each item. The primary key can be either a simple primary key (a single attribute) or a composite primary key (a combination of two or more attributes). DynamoDB supports both key-value and document data models, and it offers a number of powerful features, such as global secondary indexes, auto scaling, and stream-based data replication. DynamoDB is a popular choice for applications that need to store large amounts of data that is frequently read or written, such as online gaming, real-time analytics, and IoT applications. It is also well-suited for applications that need to scale rapidly, as it can automatically adjust capacity to meet changing demand. Useful to store ML model (or checkpoints) OpenSearch Previously ElasticSearch Amazon OpenSearch is a search service that makes it easy to build and run search applications. It is based on the open source Apache Lucene search engine, and it provides a number of features to help users build sophisticated search experiences, such as full-text search, faceted search, and hit highlighting. With Amazon OpenSearch, users can index and search large datasets, such as websites, documents, and logs. They can also customize the search experience by adding search criteria, filters, and facets, and by displaying search results in various formats. Amazon OpenSearch also provides analytics and monitoring capabilities to help users understand how their search applications are being used. Amazon OpenSearch is a flexible and scalable search solution that is well-suited for a wide range of applications, such as e-commerce, content management, and data analysis. It is fully managed, so users do not need to worry about infrastructure or maintenance. AWS Data Pipeline Amazon Data Pipeline is a fully managed data processing service that helps users move and transform data between data stores. It is designed to be easy to use and highly reliable, and it can handle data processing tasks of any size. With Amazon Data Pipeline, users can create pipelines that move data between data stores, such as Amazon S3, Amazon RDS, and Amazon Redshift. They can also use Data Pipeline to transform data, such as by aggregating, filtering, or joining data from different sources. Data Pipeline supports a variety of data formats and sources, and it can be used to schedule and automate data processing tasks. Amazon Data Pipeline is a useful tool for a wide range of data processing tasks, such as data warehousing, ETL, and analytics. It is particularly well-suited for use cases that involve moving and transforming large amounts of data, as it can scale to handle data processing needs of any size. Data sources can be on premise Runs on EC2 but fully managed Orchestration service Glue is managed, serverless, spark focused, ETL focused, has catalog Data Pipeline is orchestation tool, and can do more AWS Batch For any non-ETL batch is usually better than glue Amazon Web Services Batch is a fully managed batch processing service that makes it easy to run batch computing workloads on the AWS Cloud. It is designed to be scalable, fault-tolerant, and flexible, and it supports a wide range of workloads, such as machine learning, data processing, and scientific simulations. With AWS Batch, users can define batch computing workloads as \u0026ldquo;jobs\u0026rdquo; and \u0026ldquo;tasks,\u0026rdquo; and the service automatically provisions the required compute resources and executes the tasks. Users can specify the desired level of concurrency and resource allocation for their jobs, and AWS Batch will automatically scale up or down as needed. AWS Batch also integrates with other AWS services, such as Amazon S3 and Amazon ECS, to provide a complete batch processing solution. AWS Batch is a useful tool for organizations that need to run large-scale batch computing workloads, such as financial analysis, scientific simulations, and media processing. It is fully managed, so users do not need to worry about infrastructure or maintenance. Batch can be scheduled using cloudwatch, step functions Not just for ETL but absolutely anything at all AWS DMS Amazon Web Services Database Migration Service (AWS DMS) is a fully managed service that makes it easy to migrate databases to the AWS Cloud. It is designed to be reliable, efficient, and flexible, and it supports a wide range of database platforms, including Oracle, MySQL, and Microsoft SQL Server. With AWS DMS, users can migrate their databases to the AWS Cloud with minimal downtime. The service handles tasks such as data extraction, transformation, and load, and it supports both one-time and ongoing migrations. AWS DMS also provides a number of features to help users manage their database migrations, such as change data capture, data transformation, and task scheduling. AWS DMS is a useful tool for organizations that want to migrate their databases to the cloud, or that need to replicate their databases across multiple regions for disaster recovery or other purposes. It is fully managed, so users do not need to worry about infrastructure or maintenance. Supports homogeneous migrations and heterogeneous migrations Step Functions Amazon Web Services Step Functions is a fully managed service that makes it easy to coordinate the various components of complex, distributed applications. It is based on the concepts of tasks and state machines, and it provides a visual workflow editor to help users design and manage their applications. With AWS Step Functions, users can define and execute workflows that coordinate multiple AWS services, such as AWS Lambda, Amazon ECS, and AWS Batch. The service automatically scales to meet the needs of the workflow, and it provides features such as error handling and retry logic to help users build resilient applications. AWS Step Functions is a useful tool for organizations that need to coordinate the various components of complex, distributed applications, such as data pipelines, machine learning workflows, and microservices architectures. It is fully managed, so users do not need to worry about infrastructure or maintenance. Audit of history of workflow Allows waiting Maximum execution time of 1 year Can be used to train/tune a ML model EFS Amazon Elastic File System (EFS) is a fully managed, cloud-native file storage service that makes it easy to store and access files from multiple Amazon Elastic Compute Cloud (EC2) instances. It is designed to be scalable, highly available, and easy to use, and it supports the Network File System (NFS) protocol. With AWS EFS, users can create file systems and store files in them, and they can access the files from multiple EC2 instances at the same time. EFS automatically scales up or down as needed to meet the storage and performance needs of the applications, and it provides features such as file system access control and data durability to help users manage their file storage. AWS EFS is a useful tool for organizations that need to store and access files from multiple EC2 instances, such as web servers, application servers, and development environments. It is fully managed, so users do not need to worry about infrastructure or maintenance. EBS Amazon Elastic Block Store (EBS) is a fully managed, cloud-native block storage service that makes it easy to store and access data from Amazon Elastic Compute Cloud (EC2) instances. It is designed to be scalable, highly available, and easy to use, and it supports a variety of storage types and performance levels. With AWS EBS, users can create storage volumes and attach them to EC2 instances, and they can use the volumes to store and access data. EBS provides a number of features to help users manage their storage, such as snapshotting, data replication, and encryption. It also supports a variety of storage types, including SSD-backed volumes for high performance and HDD-backed volumes for lower cost. AWS EBS is a useful tool for organizations that need to store and access data from EC2 instances, such as databases, file systems, and applications. It is fully managed, so users do not need to worry about infrastructure or maintenance. EBS volumes are attached to specific EC2 instances, and they scale with the needs of the applications running on those instances. EFS file systems, on the other hand, can be accessed concurrently by multiple EC2 instances, and they scale automatically to meet the needs of the workload. EMR Amazon Elastic MapReduce (EMR) is a fully managed, cloud-native big data processing service that makes it easy to process large amounts of data using Apache Hadoop and Apache Spark. It is designed to be scalable, highly available, and easy to use, and it integrates with a variety of data sources and destinations. With AWS EMR, users can create clusters of Amazon Elastic Compute Cloud (EC2) instances and use them to process and analyze large datasets stored in Amazon S3 or other data stores. EMR supports a variety of big data processing frameworks, including Hadoop, Spark, and Flink, and it provides tools to help users manage and monitor their clusters. AWS EMR is a useful tool for organizations that need to process and analyze large amounts of data, such as log files, scientific simulations, and machine learning models. It is fully managed, so users do not need to worry about infrastructure or maintenance. Misc AWS DataSync: for data migrations from on-premises to AWS storage services MQTT: IOT protocol, Standard messaging protocol Apache Spark, Apache Hive, Apache Hadoop, and Apache Pig are all open-source technologies that are used for data processing and analysis. However, they are designed for different purposes and have different strengths and weaknesses. Apache Spark is a fast, in-memory data processing engine that is used for real-time data processing and analytics. It is particularly well-suited for use cases that require fast processing times, such as streaming data and interactive data exploration. Apache Hive is a data warehousing and SQL-like query language that is used to process and analyze large datasets stored in the Hadoop Distributed File System (HDFS). It is particularly well-suited for use cases that involve complex data transformations and aggregations. Apache Hadoop is a distributed computing platform that is used to store and process large amounts of data. It is composed of several modules, including HDFS for storing data, YARN for resource management, and MapReduce for parallel data processing. Hadoop is a popular choice for batch processing and offline data analysis. Apache Pig is a high-level data processing language that is used to write and execute MapReduce jobs on Apache Hadoop. It is particularly well-suited for use cases that involve complex data transformations and complex data structures. ","date":"2023-01-15T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/aws_ml_speciality_data_engineering/","section":"posts","tags":["analytics","machine-learning","certification","Data Engineering","AWS","certification","s3","kinesis","glue","redshift","rds","step function","data pipeline","elasticsearch","efs","ebs","emr","spark","hadoop"],"title":"AWS Certified ML - Specialty exam (MLS-C01) - 1. Data Engineering"},{"categories":["AWS Certified ML - Specialty exam (MLS-C01)"],"contents":"Exploratory Data Analysis Python in data science and machine learning Types of Data Data Distributions Trends and seasonality Athena Quicksight Types of visualization EMR Hadoop Apache Spark EMR Notebooks, Security and Instance Types Feature Engineering Imputing Missing Data Unbalanced Data Handling Outliers Binning, Transoforming, Encoding, Scaling, and Shuffling Amazon Sagemaker Ground Truth and Label Generation Misc This section requires understanding of sanitizing and preparing data for modeling, performing feature engineering, and analyzing and visualizing data for machine learning.\nExploratory Data Analysis (EDA) is a process of analyzing and summarizing a dataset in order to understand its structure and relationships. In the context of Amazon Web Services (AWS), EDA is often performed on large datasets that are stored in AWS storage services such as Amazon S3 or Amazon EBS.\nTo perform EDA on AWS, users can use various tools and services provided by AWS. For example, users can use Amazon Elastic MapReduce (EMR) to process and analyze large datasets using tools such as Apache Spark or Hive. Users can also use Amazon Athena to query datasets stored in Amazon S3 using SQL.\nIn addition to these tools, users can also use various AWS services and libraries to visualize and explore the data. For example, users can use Amazon QuickSight to create interactive charts and dashboards, or use libraries such as pandas and matplotlib to create custom visualizations.\nOverall, EDA on AWS involves using a combination of tools and services to understand the structure and relationships within a dataset, and to gain insights that can inform further analysis and decision making.\nPython in data science and machine learning Python code will not be tested in the exam. Python is a popular language for data exploration, analysis, and machine learning. It has a number of useful libraries for loading, manipulating, and visualizing data, as well as for building and training machine learning models. For data exploration and visualization, some popular libraries include pandas, numpy, and matplotlib. Pandas is a library for working with tabular data, numpy is a library for working with numerical data, and matplotlib is a library for creating charts and plots. For machine learning, some popular libraries include scikit-learn, tensorflow, and pytorch. These libraries include a wide range of tools for tasks such as classification, regression, clustering, and deep learning. Overall, Python is a powerful and flexible language for data analysis and machine learning, and is widely used in the field. Types of Data There are many different types of data, and the type of data can often influence the analysis and techniques used to understand it. Some common types of data include:\nNumeric data: This includes data that is represented as numbers, such as integers or floating point values. Categorical data: This includes data that consists of categories or groups, such as gender or eye color. Ordinal data: This is similar to categorical data, but the categories have a natural ordering, such as low, medium, and high. Binary data: This is data that has only two categories, such as true/false or 0/1. Time series data: This is data that is collected over time, such as daily stock prices or monthly sales figures. Text data: This is data that is represented as text, such as emails or social media posts. Image data: This is data that is represented as images, such as photographs or videos. Data Distributions Normal Distribution The normal distribution, also known as the Gaussian distribution, is a continuous probability distribution that is defined by a symmetrical bell-shaped curve. It is one of the most widely used and well-known probability distributions in statistics, and is commonly used to model real-valued random variables. The normal distribution is completely defined by its mean and standard deviation. The mean is the center of the distribution and determines the location of the peak of the curve. The standard deviation is a measure of the spread of the distribution and determines the width of the curve. A larger standard deviation means that the data is more spread out, while a smaller standard deviation means that the data is more concentrated around the mean. The normal distribution has a number of useful properties. For example, the empirical rule states that for a normal distribution, approximately 68% of the data lies within one standard deviation of the mean, 95% of the data lies within two standard deviations of the mean, and 99.7% of the data lies within three standard deviations of the mean. This means that if a dataset follows a normal distribution, a large percentage of the data will be concentrated around the mean. Overall, the normal distribution is a widely used and important distribution in statistics, and is often used to model real-valued data. Probability Mass function (discrete data type) A probability mass function (PMF) is a function that gives the probability of a discrete random variable taking on a particular value. For a random variable X, the PMF is denoted as f(x), and it is defined as the probability that X takes on the value x. The PMF is a useful tool for describing the probability distribution of a discrete random variable. It specifies the probability of each possible outcome, and can be used to calculate various statistical quantities such as the mean, variance, and skewness of the distribution. Bernoulli (discrete data type) The Bernoulli distribution is a discrete probability distribution that models the probability of a binary outcome, such as the result of a coin flip or a yes/no question. It is defined by a single parameter p, which represents the probability of success (the probability of the outcome being \u0026ldquo;yes\u0026rdquo;). The Bernoulli distribution is a special case of the binomial distribution, where the number of trials is fixed at n=1. In other words, it models a single binary event, such as the flip of a coin. The probability mass function (PMF) of a Bernoulli-distributed random variable X is given by the following formula: f(x) = p^x * (1-p)^(1-x)\nwhere x is the outcome (0 for \u0026ldquo;no\u0026rdquo; and 1 for \u0026ldquo;yes\u0026rdquo;), and p is the probability of success.\nThe Bernoulli distribution has a mean of p and a variance of p(1-p). It is a simple but widely used distribution, and is often used as a building block for more complex models. Overall, the Bernoulli distribution is a useful tool for modeling the probability of a binary outcome, and is widely used in a variety of applications. Binomial (discrete data type) The binomial distribution is a discrete probability distribution that is used to model the probability of a certain number of successes in a fixed number of independent trials. It is defined by two parameters: the number of trials (n) and the probability of success in each trial (p). The binomial distribution can be used to model a wide variety of situations, such as the probability of flipping a coin and getting a certain number of heads in a row, or the probability of a certain number of defects occurring in a batch of products. The probability mass function (PMF) of a binomial-distributed random variable X is given by the following formula: f(x) = (n choose x) * p^x * (1-p)^(n-x)\nwhere x is the number of successes, n is the number of trials, p is the probability of success in each trial, and \u0026ldquo;choose\u0026rdquo; represents the binomial coefficient.\nThe binomial distribution has a number of useful properties, such as the fact that the mean and variance can be easily calculated from the parameters n and p. It is also a limiting distribution, meaning that it can be used to approximate other distributions under certain conditions. Overall, the binomial distribution is a useful tool for modeling the probability of a certain number of successes in a fixed number of independent trials, and is widely used in a variety of applications. Geometric Distribution The geometric distribution is a discrete probability distribution that models the number of Bernoulli trials (i.e., a series of independent \u0026ldquo;success-failure\u0026rdquo; experiments) needed to get a success. It is defined by a single parameter p, which is the probability of success on each trial. The probability mass function (PMF) of the geometric distribution is given by: f(k) = (1 - p)^(k-1) * p\nwhere k is the number of trials needed to get a success and p is the probability of success on each trial.\nThe geometric distribution has several characteristics: It is a one-sided distribution, which means that it is defined only for k \u0026gt; 0. It is a discrete distribution, which means that it is defined for a specific set of values rather than for a continuous range of values. It has a mean of 1/p, which is the expected number of trials needed to get a success. The geometric distribution is often used in modeling the number of trials needed to get a success, such as the number of ads that need to be shown before a customer clicks on one, or the number of patients that need to be treated before a certain medical condition is cured. It is also used in reliability engineering to model the number of failures before a unit fails. Poission (discrete data type) The Poisson distribution is a discrete probability distribution that is used to model the number of times an event occurs within a certain period of time or space. It is commonly used to model events that occur randomly and independently, such as the number of customers arriving at a store or the number of defects in a manufactured product. The Poisson distribution is defined by a single parameter, called the rate parameter or the mean rate of occurrence. This parameter is denoted as lambda (λ) and represents the average number of times the event occurs per unit of time or space. The probability mass function (PMF) of a Poisson-distributed random variable X is given by the following formula: f(x) = (λ^x * e^(-λ)) / x!\nwhere x is the number of times the event occurs, λ is the rate parameter, and e is the base of the natural logarithm.\nThe Poisson distribution has a number of useful properties, such as the fact that the mean and variance are equal to the rate parameter λ. It is also a limiting distribution, meaning that it can be used to approximate other distributions under certain conditions. Overall, the Poisson distribution is a useful tool for modeling the number of times an event occurs within a certain period of time or space, and is widely used in a variety of applications. Exponential Distribution The exponential distribution is a continuous probability distribution that describes the time between events in a Poisson process, which is a process in which events occur continuously and independently at a constant average rate. It is defined by a single parameter λ (lambda), which is the rate at which the events occur. The probability density function (PDF) of the exponential distribution is given by: f(x) = λ * e^(-λx)\nwhere x is the time between events and e is the base of the natural logarithm.\nThe exponential distribution has several characteristics: It is a memoryless distribution, which means that the probability of an event occurring at time t+x, given that it has not occurred by time t, is the same as the probability of the event occurring at time x. It has a constant hazard rate, which means that the probability of an event occurring at any given time is constant. It is a one-sided distribution, which means that it is defined only for x \u0026gt; 0. The exponential distribution is often used in modeling the time between failures of equipment, the time between arrivals at a service facility, and the time between phone calls at a call center. It is also used in survival analysis to model the time until an event occurs, such as death or failure. Weibull Distribution The Weibull distribution is a continuous probability distribution that is often used to model time-to-failure data in reliability engineering. It is defined by two parameters: shape and scale.\nThe probability density function (PDF) of the Weibull distribution is given by:\nf(x) = (shape/scale) * (x/scale)^(shape-1) * e^(-(x/scale)^shape)\nwhere x is the time to failure, shape is the shape parameter, and scale is the scale parameter.\nThe Weibull distribution has several characteristics: It is a one-sided distribution, which means that it is defined only for x \u0026gt; 0. It has a shape parameter that controls the shape of the curve. If the shape parameter is less than 1, the curve is \u0026ldquo;skewed\u0026rdquo; to the right, meaning that it has a longer tail on the right side. - If the shape parameter is greater than 1, the curve is \u0026ldquo;skewed\u0026rdquo; to the left, meaning that it has a longer tail on the left side. If the shape parameter is equal to 1, the curve is symmetrical. It has a scale parameter that controls the spread of the curve. If the scale parameter is large, the curve is spread out and has a longer tail. If the scale parameter is small, the curve is more concentrated and has a shorter tail. The Weibull distribution is often used in reliability engineering to model the time until failure of a component or system. It is also used in other fields, such as meteorology, to model wind speed and in economics to model stock returns. Trends and seasonality Trends and seasonality are two common patterns that can occur in time series data. A trend is a long-term increase or decrease in the data. It can be either linear, meaning that the data increases or decreases at a constant rate, or nonlinear, meaning that the rate of change varies over time. Trends can be caused by various factors such as changes in consumer demand, economic conditions, or technological innovations. Seasonality is a pattern that repeats over a specific time period, such as annually or monthly. It can be caused by factors such as weather patterns, holidays, or consumer behavior. Both trends and seasonality can have important implications for forecasting and decision making. For example, if a company sees a trend of increasing sales, it may decide to ramp up production or hire more staff. If a company sees seasonal fluctuations in demand, it may need to adjust its inventory or staffing levels accordingly. To analyze trends and seasonality in time series data, various techniques can be used such as smoothing methods, decomposition methods, and autoregressive models. It is important to correctly identify and account for these patterns in order to make accurate forecasts and informed decisions. Additive time series data is characterized by a constant trend and constant seasonality over time. This means that the trend and seasonality do not change, and the data can be modeled using a linear function. The relationship between the data and the trend and seasonality components can be represented as follows: Data = Trend + Seasonality + Noise\nMultiplicative time series data, on the other hand, is characterized by a varying trend and varying seasonality. This means that the trend and seasonality change over time, and the data cannot be modeled using a linear function. The relationship between the data and the trend and seasonality components can be represented as follows: Data = Trend * Seasonality * Noise\nIt is important to correctly identify whether a time series is additive or multiplicative, as this can influence the choice of modeling techniques and the interpretation of the results.\nAlso this is possible:\nAdditive trend and additive seasonality Additive trend and multiplicative seasonality Multiplicative trend and additive seasonality Multiplicative trend and multiplicative seasonality Athena Amazon Athena is a serverless, interactive query service that allows users to analyze data stored in Amazon S3 using SQL. It is designed to be fast and easy to use, and can be used to analyze data from a wide variety of sources such as logs, streaming data, and data lakes. To use Athena, users first define a data schema by creating tables that point to the data stored in Amazon S3. They can then use SQL to query the data and analyze it using various functions and aggregations. Athena supports a wide range of SQL functions and data types, and users can also use custom user-defined functions (UDFs) to extend its capabilities. It is also highly scalable, and can handle queries on large datasets with minimal performance degradation. Presto under the hood Supports multiple formats Unstructured, semi structured or structured Ad hoc queries, querying data before loading to Redshift, analyze Cloudtrail, integration with Jupyter, Zepplin, Integration with quicksight, integration with ODBC, JDBC AWS Glue datalog can extract the schema for Athena to use Pay as you go, inexpensive, converting to columner saves a lot of money, Glue and S3 have their own charges IAM policies, encryption is possible, TLS is possible Quicksight Amazon QuickSight is a business intelligence and data visualization service provided by Amazon Web Services (AWS). It allows users to create interactive dashboards and charts to visualize and analyze data from a wide variety of sources. To use QuickSight, users first need to connect it to their data sources, which can include data stored in Amazon S3, Amazon Redshift, Amazon RDS, and other AWS data stores, as well as external data sources such as spreadsheets and databases. Once the data is connected, users can use QuickSight\u0026rsquo;s visual interface to create charts, graphs, and other visualizations to explore and analyze the data. QuickSight offers a range of features and tools to help users analyze and understand their data. These include built-in analytics functions, support for custom SQL queries, and the ability to share and collaborate on dashboards with other users. Overall, Amazon QuickSight is a powerful and easy-to-use tool for creating interactive dashboards and visualizations, and is widely used in a variety of applications such as business intelligence, data exploration, and data reporting. Ad-hoc analysis Can do calculated columns etc. SPICE: Super Fast Parallel, In memory Calculation engine 10 gb Quicksight is quick because of SPICE Quicksights machine learning insights: Anomaly detection using Random cut forest, Forecasting and auto narratives (not too mature). Multifactor authentication Works with vpc, and provides row-level security Users defined via IAM or email signup AugoGraph feature selects the best graph for the respective data type Types of visualization Bar charts: These are used to compare categories or groups of data. They can be vertical or horizontal, and can be used to show both quantitative and categorical data. Line charts: These are used to show trends over time or other continuous variables. They can be used to show multiple data series on the same chart. Scatter plots: These are used to show the relationship between two numeric variables. They can be used to show correlations, patterns, and trends in the data. Pie charts: These are used to show proportions or percentages. They are most commonly used to show how a whole is divided into parts. Histograms: These are used to show the distribution of a continuous variable. They show the frequency or density of data points within different ranges or bins. Box plots: These are used to show the distribution and spread of a continuous variable. They show the minimum, first quartile, median, third quartile, and maximum values of the data. Heatmaps: These are used to show patterns and trends in data organized in a grid. They use color to represent the data, with warmer colors indicating higher values and cooler colors indicating lower values. EMR Amazon Elastic MapReduce (EMR) is a fully managed, cloud-native big data processing service that makes it easy to process large amounts of data using Apache Hadoop and Apache Spark. It is designed to be scalable, highly available, and easy to use, and it integrates with a variety of data sources and destinations. With AWS EMR, users can create clusters of Amazon Elastic Compute Cloud (EC2) instances and use them to process and analyze large datasets stored in Amazon S3 or other data stores. EMR supports a variety of big data processing frameworks, including Hadoop, Spark, and Flink, and it provides tools to help users manage and monitor their clusters. AWS EMR is a useful tool for organizations that need to process and analyze large amounts of data, such as log files, scientific simulations, and machine learning models. It is fully managed, so users do not need to worry about infrastructure or maintenance. Provides notebooks Master nodes (manages the cluster), Core node (holds HDFS data and run tasks), Task node (only runs tasks) HDFS is epimerical Transient cluster vs Long running cluster IAM configure permissions CloudTrail: audit requests Data Pipeline: schedule and start clusters EMRFS: access s3 as if it were HDFS, uses DynamoDB to track consistency EBS for HDFS is also possible Hadoop Apache Hadoop is an open-source software framework for storing and processing large amounts of data in a distributed computing environment. It is designed to handle data that is too large or complex for traditional database systems, and can process and analyze data in parallel across a large number of servers. Hadoop consists of two main components: the Hadoop Distributed File System (HDFS) and the MapReduce programming model. HDFS is a distributed file system that stores data across a large number of servers, and MapReduce is a programming model that allows developers to write programs that can process and analyze large amounts of data in parallel. Hadoop is commonly used for tasks such as data analysis, machine learning, and log processing. It is also often used in conjunction with other tools and technologies such as Apache Spark, Apache Hive, and Apache Pig to build more complex data processing pipelines. Apache Spark Apache Spark is an open-source, distributed computing system that is designed for fast and flexible data processing. It is a popular choice for tasks such as data analytics, machine learning, and real-time stream processing. Spark is built on top of the Hadoop distributed file system (HDFS) and is designed to be highly scalable and efficient. It can process and analyze data in parallel across a large number of servers, and supports a wide range of programming languages including Python, Java, R, and Scala. One of the main benefits of Spark is its ability to process data in memory, which allows it to be much faster than other distributed computing systems that rely on disk-based storage. It also includes a number of useful libraries and tools for tasks such as machine learning, graph processing, and stream processing. in memory caching, DAGs Batch processing and real time analytics, graph processing, machine learning Spark context, cluster manager via spark or yarn, executors Spark core Spark RDD, DataFrames and Datasets are built on top of RDD, and they are most commonly used at the moment Spark Streaming is possible (works in mini batches). Unbounded database table MlLib (distributed machine learning) Graphx (distributed graph processing) Zepplin can run spark code interactively, and can also use charts/plots Spark MLlib MLlib is a machine learning library for Apache Spark. It is designed to provide scalable and efficient machine learning algorithms that can be used on big data. MLlib includes a wide range of machine learning algorithms and utility functions, including algorithms for classification, regression, clustering, collaborative filtering, and dimensionality reduction. It also includes tools for feature engineering, such as feature extraction, transformation, and selection. MLlib is designed to be easy to use, and includes APIs for several programming languages including Python, Java, R, and Scala. It is also designed to be highly scalable, and can be used to build machine learning models on large datasets distributed across multiple servers. Classification: logistic regression and naive bayes Regression Decision trees Recommendation engine (ALS) Clustering (K-means) LDA (topic modeling) ML workflow utilities (pipelines, feature transformation, persistence) SVD, PCA and statistics EMR Notebooks, Security and Instance Types Amazon EMR Notebooks is a service that allows users to create and manage Jupyter notebooks on Amazon Elastic MapReduce (EMR) clusters. EMR is a cloud-based big data processing service, and Jupyter notebooks are interactive, web-based documents that can contain code, text, and visualizations. EMR Notebooks provides a simple and flexible way to analyze and visualize data stored in Amazon S3 or other data stores using a variety of tools and libraries such as Apache Spark, Python, and R. Users can create and edit notebooks using a web-based editor, and can also use notebooks to run and debug code, create visualizations, and collaborate with other users. EMR Notebooks is fully integrated with EMR, which means that users can easily spin up and down EMR clusters to process and analyze large datasets, and can also access other EMR features such as security and data access controls. Similar concept to Zeppelin, with more AWS integration Notebooks backed up to s3 Provision clusters from the notebooks Hosted inside a vpc Accessed only via aws console IAM policies, Kerberos (a computer-network authentication protocol that works on the basis of tickets to allow nodes communicating over a non-secure network to prove their identity to one another in a secure manner), SSH, IAM roles Spot instances are good choice for task nodes, only use on core or master if you are testing or very cost sensitive, however, you are risking partial data loss Feature Engineering Feature engineering is the process of creating new features or transforming existing features in a dataset in order to improve the performance of a machine learning model. It is a crucial step in the machine learning process, and can have a significant impact on the model\u0026rsquo;s accuracy and effectiveness. There are many different techniques that can be used in feature engineering, including: Feature selection: This involves selecting a subset of the most relevant features from a dataset to use in a model. This can help to reduce overfitting, improve model interpretability, and reduce training time. Feature extraction: This involves creating new features from existing data by combining or transforming the original features. For example, a new feature could be created by taking the square root of an existing feature. Feature transformation: This involves transforming the scale or distribution of a feature in order to improve model performance. For example, data may need to be normalized or standardized in order to be used in some models. Imputing Missing Data There are several ways to impute (or fill in) missing data, and the best method will depend on the specific dataset and the nature of the missing data. Some common methods for imputing missing data include:\nMean imputation: This involves replacing missing values with the mean (or average) of the non-missing values. This is a simple method that can be useful for numerical data, but can be biased if the data has a skewed distribution. Median imputation: This is similar to mean imputation, but uses the median (or middle value) instead of the mean. It can be less affected by outliers than mean imputation and may be a better choice for skewed data. Mode imputation: This involves replacing missing values with the most frequent (or mode) value in the dataset. It is often used for categorical data. Regression imputation: This involves using a regression model to predict the missing values based on the other available features. It can be a more powerful method, but requires a good understanding of the relationships between the features and the target variable. Nearest Neighbors (k-NN) imputation is a method for imputing missing data that uses the k-NN algorithm to fill in missing values based on the values of the nearest neighbors. It is a simple and intuitive method that is often used in machine learning and data analysis. To use k-NN imputation, the first step is to identify the nearest neighbors of the data point with missing values. This is typically done using a distance measure such as Euclidean distance, and the number of neighbors (k) is a user-defined parameter. Once the nearest neighbors have been identified, the missing values can be imputed by averaging the values of the neighbors. k-NN imputation can be a useful method for filling in missing data, especially when the data is highly correlated and the relationships between the features are well understood. However, it can be sensitive to the choice of k, and may not always produce the best results. Dropping NA\u0026rsquo;s: This method can be useful in some cases, such as when the number of missing values is small and removing them does not significantly affect the overall size of the dataset. However, it can also lead to a loss of information and may not be appropriate if the missing values are prevalent or if they are likely to be informative. MICE (multiple imputation by chained equations): Multiple imputation by chained equations (MICE) is a method for imputing missing data that involves creating multiple imputed datasets and combining them to produce a final result. It is a more advanced method that can be more robust and accurate than other imputation methods, especially when the data has a complex structure and the relationships between the features are not well understood. The final result is produced by combining the imputed datasets using appropriate statistical methods, such as averaging or weighted averaging. MICE can be a powerful method for imputing missing data, but it can also be time-consuming and requires a good understanding of statistical modeling. Categorical is usually not trivial Just get more data (if possible) Unbalanced Data There are several approaches for handling imbalanced data in machine learning, including:\nBalancing the data: This can be done by oversampling the minority class or undersampling the majority class. Using a different evaluation metric: Instead of using accuracy, try using precision, recall, or F1 score, which are more sensitive to imbalanced data. Adjusting the class weight: Some algorithms allow you to adjust the weight of each class, which can be used to give more importance to the minority class. Using a different algorithm: Some algorithms are more robust to imbalanced data than others. For example, tree-based algorithms like random forests and gradient boosting tend to perform better on imbalanced data than algorithms like logistic regression. Using data augmentation: If you are working with image data, you can use data augmentation techniques to generate additional minority class examples. Anomaly detection: If the minority class represents anomalies or rare events, you can treat the problem as an anomaly detection problem rather than a classification problem. Synthetic minority oversampling technique (SMOTE): This is a popular method for oversampling the minority class by synthesizing new examples rather than simply replicating existing ones. Cost-sensitive learning: In this approach, the cost of misclassifying examples from the minority class is higher than the cost of misclassifying examples from the majority class. Handling Outliers There are several ways to identify outliers in a dataset:\nVisualization: One of the easiest ways to identify outliers is to plot the data using a box plot or histogram. Outliers will typically be located outside the \u0026ldquo;whiskers\u0026rdquo; of the box plot or outside the range of the histogram. Statistical tests: You can use statistical tests to identify outliers in a dataset. For example, you can use the Z-score method to identify outliers by calculating the distance of each data point from the mean in terms of standard deviations. Data points with a Z-score greater than a certain threshold (such as 3 or 4) can be considered outliers. Data transformation: Transforming the data, such as taking the log of the data, can make outliers more obvious. Anomaly detection algorithms: There are also machine learning algorithms specifically designed for detecting anomalies or outliers in a dataset. These algorithms include density-based methods, distance-based methods, and model-based methods. There are several approaches for handling outliers in a dataset:\nIgnore the outlier: This is an option if the outlier does not have a significant impact on the model or if the outlier is the result of a data entry error. Drop the outlier: This is an option if the outlier is not representative of the population being studied and if the outlier has a significant impact on the model. Transform the data: Some algorithms are more robust to outliers, such as decision trees and random forests. Transforming the data, such as using the log transformation, can also make the model more robust to outliers. Use robust models: Some models, such as linear regression with the Huber loss function, are less sensitive to outliers than other models. Anomaly detection: If the outlier represents an anomaly or rare event, you can treat the problem as an anomaly detection problem rather than a classification or regression problem. Binning, Transforming, Encoding, Scaling, and Shuffling Binning Binning is a process of transforming numerical data into categorical data by dividing the data into a set of bins or intervals. This can be useful for reducing the number of unique values in a dataset, which can make it easier to visualize and analyze the data. For example, if you have a dataset with a large range of numerical values, you could use binning to group the values into a smaller set of intervals. This would allow you to plot the data on a histogram or bar chart, which would be more informative than a scatter plot. There are several ways to determine the size and number of bins to use for binning data, including: Fixed width bins: In this approach, you specify the size of the bins and the data is divided into intervals of that size. Fixed number of bins: In this approach, you specify the number of bins and the data is divided into that number of intervals. Optimal bin width: In this approach, the optimal bin width is determined using a statistical method, such as the Scott\u0026rsquo;s normal reference rule or the Freedman-Diaconis rule. Transforming Data transformation is a process of converting data from one format or representation to another. This can be useful for several reasons, such as:\nData cleaning: Data transformation can be used to fix or remove errors or inconsistencies in the data. Data preparation: Data transformation can be used to prepare the data for analysis by formatting the data in a specific way or creating new variables. Data reduction: Data transformation can be used to reduce the size or complexity of the data, such as by aggregating the data or removing unnecessary variables. Data normalization: Data transformation can be used to scale the data to a common range, such as by normalizing the data to have a mean of 0 and a standard deviation of 1. Data transformation can also be used to make the data more amenable to a specific algorithm or technique, such as by binning numerical data or encoding categorical data. There are many different types of data transformation techniques, including scaling, centering, normalization, aggregation, imputation, and encoding. The appropriate transformation technique will depend on the specific characteristics of the data and the goals of the analysis. Encoding Encoding is the process of converting data from one format into another, often for the purpose of efficient storage or transmission. In the context of machine learning, encoding is often used to convert categorical data, which can\u0026rsquo;t be represented as numerical values, into numerical form. There are several types of encoding techniques that can be used, including: One-hot encoding: This technique converts each categorical value into a new binary column, with a value of 1 indicating the presence of the categorical value and a value of 0 indicating its absence. Label encoding: This technique converts each categorical value into a numerical value, such as an integer. However, this can lead to problems if the numerical values are interpreted as having a meaningful order. Count encoding: This technique encodes the categorical values by the count of each value in the dataset. Binary encoding: This technique encodes the categorical values as binary code. Target encoding: This technique encodes the categorical values using the mean of the target variable for each value. Scaling and normalizing Scaling and normalization are techniques used to transform variables so that they have a comparable scale. This can be useful for a variety of reasons, such as: Some machine learning algorithms are sensitive to the scale of the input variables, and can perform poorly if the variables are on a different scale. Scaling the variables to the same scale can improve the performance of these algorithms. Scaling the variables can also make it easier to compare the magnitude of the variables. There are several ways to scale and normalize data: Min-max scaling: This scales the variables to a specific range, such as 0-1 or -1 to 1. Standardization: This scales the variables so that they have a mean of 0 and a standard deviation of 1. Normalization: This scales the variables so that they have a unit norm (a length of 1). Shuffling Shuffling is the process of randomly rearranging the rows of a dataset. This can be useful for several reasons: Machine learning algorithms often expect the data to be in a random order. Shuffling the data before training a model can help ensure that the model is not biased by the order of the data. Shuffling the data can also help ensure that the training and test sets are representative of the overall dataset. If the data is not shuffled and the rows are ordered in a certain way, the training and test sets may not be representative of the overall dataset. Tokenization Tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements, known as tokens. Tokenization is an important preprocessing step for many natural language processing (NLP) tasks, such as text classification and information retrieval. There are several approaches to tokenization, including: Word tokenization: This involves dividing the text into words. Sentence tokenization: This involves dividing the text into sentences. Word-level tokenization: This involves dividing the text into words and punctuation, such as \u0026ldquo;don\u0026rsquo;t\u0026rdquo; being split into \u0026ldquo;do\u0026rdquo; and \u0026ldquo;n\u0026rsquo;t.\u0026rdquo; N-gram tokenization: This involves dividing the text into contiguous sequences of n items, such as bigrams (pairs of words) or trigrams (triplets of words). Stemming and lemmatization Stemming and lemmatization are techniques used to normalize words to their base form, known as a stem or lemma. These techniques are often used as a preprocessing step for natural language processing (NLP) tasks, such as text classification and information retrieval. Stemming involves removing the suffixes from a word to obtain the root form of the word. For example, the stem of the word \u0026ldquo;jumping\u0026rdquo; might be \u0026ldquo;jump,\u0026rdquo; and the stem of the word \u0026ldquo;stemmer,\u0026rdquo; might be \u0026ldquo;stem.\u0026rdquo; Lemmatization, on the other hand, involves determining the base form of a word based on its part of speech and meaning. For example, the lemma of the word \u0026ldquo;was\u0026rdquo; might be \u0026ldquo;be,\u0026rdquo; and the lemma of the word \u0026ldquo;better\u0026rdquo; might be \u0026ldquo;good.\u0026rdquo; Stemming and lemmatization can be useful for reducing the dimensionality of the data and improving the performance of NLP models, but they can also remove some of the context and meaning of the words. Amazon Sagemaker Ground Truth and Label Generation Amazon SageMaker Ground Truth is a fully managed data labeling service that allows users to build highly accurate training datasets for machine learning. It offers a variety of labeling methods, including human labeling, active learning, and automatic labeling. SageMaker Ground Truth also includes workflows for common data labeling tasks, such as image classification, object detection, and semantic segmentation. It also provides tools for managing and tracking the data labeling process, including the ability to set up labeling jobs, track their progress, and review the results. This helps users ensure that their data is labeled accurately and efficiently. Ambiguous data is sent to humans Mechanical Turk: Amazon Mechanical Turk (MTurk) is a cloud platform that enables organizations to use a network of human workers to perform tasks that are typically difficult or time-consuming for computers to perform. These tasks, known as Human Intelligence Tasks (HITs), can include data labeling, transcription, image annotation, and many other types of work. Amazon SageMaker Ground Truth Plus helps you to create high-quality training datasets without having to build labeling applications or manage a labeling workforce. Misc Synthetic Features Synthetic features are artificially created features that are derived from existing features in a dataset. They are often used to improve the performance of machine learning models by providing additional information that may not be present in the original features. Synthetic features can be created using various techniques, such as combining or transforming existing features, or by applying statistical or mathematical operations to the data. One example of a synthetic feature is a polynomial feature, which is created by taking the product of a feature with itself or with other features. Polynomial features can capture nonlinear relationships between features and the target variable, and can improve the performance of linear models on nonlinear problems. Other examples of synthetic features include interactions between features, binned features, and dummy variables. Synthetic features can be useful for improving the performance of machine learning models, especially when the original features are not sufficient for making accurate predictions. However, care should be taken when creating synthetic features, as adding too many of them can lead to overfitting and degrade model performance. Stop words Stop words are common words that are typically filtered out before natural language processing (NLP) tasks, such as text classification or text mining, because they do not provide meaningful information. Examples of stop words include words like \u0026ldquo;a,\u0026rdquo; \u0026ldquo;an,\u0026rdquo; \u0026ldquo;the,\u0026rdquo; \u0026ldquo;and,\u0026rdquo; and \u0026ldquo;but.\u0026rdquo; There are several ways to identify and handle stop words: Use a list of stop words: Many NLP libraries and frameworks, such as NLTK and scikit-learn, include a pre-defined list of stop words that can be used to filter out common words. Identify stop words using term frequency-inverse document frequency (TF-IDF): This method involves calculating the importance of each word in a document or corpus and filtering out the least important words, which are often stop words. Customize the stop word list: You can customize the list of stop words based on the specific needs of your task. For example, if you are working with domain-specific language, you may need to add domain-specific words to the stop word list. To handle stop words, you can simply filter them out of the dataset before performing the NLP task. This can be done by comparing the words in the dataset to the stop word list and removing any words that are on the list. It\u0026rsquo;s important to carefully consider whether or not to filter out stop words, as they can sometimes provide important context or meaning. For example, in the phrase \u0026ldquo;not good,\u0026rdquo; the word \u0026ldquo;not\u0026rdquo; is a stop word that changes the meaning of the phrase. TF-IDF Term Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure that is used to evaluate the importance of a word in a document or corpus. The importance of a word is determined by its frequency in the document and in the corpus as a whole. TF-IDF is calculated as the product of the term frequency (TF) and the inverse document frequency (IDF). The term frequency is the number of times a word appears in the document, and the inverse document frequency is the logarithm of the number of documents in the corpus divided by the number of documents that contain the word. TF-IDF is often used as a weighting factor in information retrieval and text mining, and can be useful for tasks such as document classification, clustering, and keyword extraction. Correlation Correlation is a statistical measure that indicates the strength and direction of a linear relationship between two variables. A positive correlation indicates that as one variable increases, the other variable also increases. A negative correlation indicates that as one variable increases, the other variable decreases. The correlation coefficient, denoted by \u0026ldquo;r,\u0026rdquo; is a measure of the strength of the relationship between the variables. It ranges from -1 to 1, where -1 indicates a strong negative correlation, 0 indicates no correlation, and 1 indicates a strong positive correlation. Correlation can be useful for understanding the relationship between two variables and predicting one variable based on the other. However, it\u0026rsquo;s important to remember that correlation does not necessarily imply causation, meaning that a correlation between two variables does not necessarily mean that one variable causes the other. There are several methods for calculating the correlation between variables, including Pearson\u0026rsquo;s correlation coefficient, Spearman\u0026rsquo;s rank correlation coefficient, and Kendall\u0026rsquo;s tau. The appropriate method will depend on the characteristics of the data and the goals of the analysis. p-value The p-value is a statistical measure that is used to assess the significance of a hypothesis test. It is the probability of obtaining a test statistic at least as extreme as the one observed, assuming that the null hypothesis is true. The null hypothesis is a statement that there is no statistical relationship between the variables being tested. The alternative hypothesis is the opposite of the null hypothesis and states that there is a statistical relationship between the variables. To interpret the p-value, you compare it to a significance level, which is a predetermined cutoff value. If the p-value is less than the significance level, you can reject the null hypothesis and conclude that there is a statistical relationship between the variables. If the p-value is greater than the significance level, you fail to reject the null hypothesis and cannot conclude that there is a statistical relationship between the variables. It\u0026rsquo;s important to carefully consider the appropriate significance level and the limitations of the p-value, as it can be affected by factors such as sample size and the distribution of the data. Elbow plot An elbow plot is a graphical method used to determine the appropriate number of clusters to use in a cluster analysis. It plots the within-cluster sum of squared distances (WCSS) for each possible number of clusters, and the number of clusters is chosen at the \u0026ldquo;elbow\u0026rdquo; point, where the change in WCSS begins to level off. To create an elbow plot, you first perform a cluster analysis for a range of possible number of clusters, and then plot the WCSS for each number of clusters. The WCSS can be calculated as the sum of the squared distance between each point and its cluster centroid. The \u0026ldquo;elbow\u0026rdquo; point is generally considered to be the point where the WCSS starts to decrease more slowly, indicating that adding more clusters is not significantly improving the fit of the model. It\u0026rsquo;s important to carefully consider the limitations of the elbow method, as it can be affected by the shape of the data and may not always clearly identify the appropriate number of clusters. Other methods, such as the silhouette method, can also be used to determine the number of clusters in a dataset. Summary Statistics Summary statistics are quantitative measures that describe and summarize a dataset. They provide a quick and easy way to get a sense of the characteristics and patterns in the data. Some common summary statistics include: Mean: The mean is the arithmetic average of the data. It is calculated by summing all the values and dividing by the number of values. Median: The median is the middle value of the data when it is sorted in ascending order. It is a measure of central tendency that is resistant to outliers. Mode: The mode is the most frequent value in the data. Range: The range is the difference between the maximum and minimum values in the data. Variance: The variance is a measure of the spread or dispersion of the data. It is calculated as the sum of the squared differences between each value and the mean, divided by the number of values. Standard deviation: The standard deviation is the square root of the variance. It is a measure of the spread of the data that is in the same units as the original data. Distance Norms Distance norms, also known as metrics, are functions that define a distance between two points in a space. These functions take two points as inputs and return a non-negative number that represents the distance between them. Different distance norms can be used depending on the characteristics of the data and the requirements of the application. Some common distance norms include: Euclidean distance: This is the most common distance norm and is based on the Pythagorean theorem. It is defined as the square root of the sum of the squares of the differences between the coordinates of the two points. Manhattan distance: This distance norm is based on the sum of the absolute differences of the coordinates of the two points. It is also known as the \u0026ldquo;taxi cab\u0026rdquo; distance because it represents the distance a taxi cab would need to travel to get from one point to the other. Minkowski distance: This is a generalization of the Euclidean and Manhattan distances. It is defined as the sum of the absolute differences of the coordinates of the two points, raised to a power. The value of the power determines whether the distance is more similar to the Euclidean or Manhattan distance. Cosine distance: This distance norm is based on the cosine similarity between two vectors. It is often used in text analysis to measure the similarity between documents. Jaccard distance: This distance norm is based on the Jaccard similarity coefficient and is often used to compare the similarity of sets. It is defined as the size of the intersection of the sets divided by the size of the union of the sets. QQ plots A Q-Q plot (short for \u0026ldquo;quantile-quantile plot\u0026rdquo;) is a graphical way to compare two probability distributions by plotting their quantiles against each other. It is a plot of the sorted data against an idealized distribution with a uniform distribution. The purpose of a Q-Q plot is to check whether two datasets come from the same distribution. To create a Q-Q plot, you first need to specify the distribution that you want to use as the reference distribution. Then, you sort both datasets and plot the quantiles of one dataset against the quantiles of the other dataset. If the two datasets come from the same distribution, the points in the Q-Q plot will lie approximately on a straight line. If the points do not lie on a straight line, it suggests that the two datasets come from different distributions. Q-Q plots are often used to check whether a dataset follows a particular distribution, such as a normal distribution. They are also useful for comparing datasets to see whether they come from the same distribution. Parametric and Non-Parametric test A parametric test is a statistical test that assumes that the data comes from a population with a known probability distribution, such as a normal distribution. The test uses parameters of the distribution (such as the mean and standard deviation) to make statistical inferences about the population. Parametric tests are based on the assumption that the data follows a particular probability distribution, and they are generally more powerful (i.e., able to detect differences with smaller sample sizes) than nonparametric tests, which do not make any assumptions about the distribution of the data. However, if the assumption of a known distribution is not met, the results of a parametric test may be less reliable. Examples of parametric tests include the t-test, the ANOVA test, and the linear regression analysis. These tests are commonly used to compare means, variances, and relationships between variables. Nonparametric tests, on the other hand, do not assume that the data comes from a particular distribution and are more robust to departures from normality. Examples of nonparametric tests include the Wilcoxon rank-sum test and the Kruskal-Wallis test. Markov Chain model A Markov chain is a mathematical system that undergoes transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the system arrived at its current state, the possible future states are fixed. A Markov chain is often represented by a state transition diagram, which shows all the possible states that the system can be in, and the transitions between these states. The transitions are governed by transition probabilities, which specify the probability of moving from one state to another. Markov chains have many applications in various fields, including economics, computer science, and physics. They are used to model systems that change over time and have a finite number of states. Some examples of systems that can be modeled using Markov chains include: The behavior of a customer moving through a website, where the states represent the different pages on the website and the transitions represent the clicks that the customer makes to move from one page to another. The weather, where the states represent different weather conditions (such as sunny, cloudy, or rainy) and the transitions represent the probability of the weather changing from one condition to another. The movement of a particle through a lattice, where the states represent the different positions that the particle can occupy and the transitions represent the probability of the particle moving from one position to another. Optimization Optimization is the process of finding the best solution to a problem, given certain constraints. In mathematics, optimization problems involve finding the maximum or minimum value of a function, subject to certain constraints. These constraints can be equality constraints, which specify that a certain relationship must hold among the variables, or inequality constraints, which specify that a certain relationship must not hold among the variables. There are many different methods for solving optimization problems, including gradient descent, the simplex method, and the interior point method. The choice of method depends on the specific problem and the desired properties of the solution. Optimization is used in many fields, including engineering, economics, and machine learning. Some examples of optimization problems include: Finding the optimal allocation of resources, such as the allocation of capital to different investments, or the allocation of production capacity to different products. Finding the optimal design of a system, such as the design of an aircraft or the design of a supply chain. Finding the optimal parameters of a machine learning model, such as the weights of a neural network or the regularization parameters of a linear model. Objective Function An objective function is a mathematical function that is used to represent the goal of an optimization problem. The goal of the optimization problem is to find the values of the variables that either maximize or minimize the objective function, subject to certain constraints. The objective function is also known as the cost function, the loss function, or the criterion function. It is a measure of how well a given solution meets the requirements of the problem. In general, the objective function is a scalar-valued function, meaning that it maps a vector of variables to a single scalar value. The objective function is an important component of an optimization problem, as it defines the goal of the optimization and determines the solution of the problem. The objective function is often defined in terms of the decision variables (the variables that are being optimized) and the parameters of the problem (the constants that define the problem). For example, in a linear programming problem, the objective function is a linear function that represents the cost or profit associated with a particular allocation of resources. In a nonlinear programming problem, the objective function is a nonlinear function that represents the cost or performance of a system. Exponential smoothing for outlier detection Exponential smoothing is a time series forecasting method that is used to predict future values based on historical data. It is based on the idea of giving more weight to more recent observations and less weight to observations that are further in the past. Exponential smoothing can be used for outlier detection by analyzing the residuals (i.e., the differences between the predicted values and the actual values) of the time series. If the residuals contain outliers (i.e., values that are significantly different from the majority of the residuals), it may indicate that there is something unusual or unexpected happening in the time series. Box-Cox Transformation The Box-Cox transformation is a way to transform non-normal dependent variables into a normal shape. It is named after George Box and David Cox, who introduced the transformation in 1964. The transformation is defined as: Y = (X^L - 1) / L\nwhere X is the variable to be transformed, Y is the transformed variable, and L is a parameter that needs to be estimated. When L = 0, the Box-Cox transformation becomes the log transformation.\nThe Box-Cox transformation can be useful in regression analysis, ANOVA, and other statistical tests when the assumptions of normality are not met. It can also be useful in improving the interpretability of the model by making the relationship between the dependent and independent variables more linear. PCA for visualization and EDA Principal component analysis (PCA) is a statistical technique that is used to reduce the dimensionality of a data set. It does this by identifying the directions in which the data vary the most, and then projecting the data onto a lower-dimensional space. This can be useful for visualization, as it can allow you to plot high-dimensional data in a 2D or 3D space. ","date":"2023-01-15T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/aws_ml_speciality_eda/","section":"posts","tags":["analytics","machine-learning","certification","Exploratory Data Analysis","AWS","types-of-data","data-distributions","athena","quicksight","emr","apache-spark","feature-engineering","unbalanced-data","handling-outliers"],"title":"AWS Certified ML - Specialty exam (MLS-C01) - 2. Exploratory Data Analysis"},{"categories":["AWS Certified ML - Specialty exam (MLS-C01)"],"contents":"Modeling Activation Functions Convolutional Neural Network Recurrent Neural Networks Modern NLP with BERT and GPT, and Transfer Learning Deep Learning on EC2 and EMR Tuning Neural Networks Regularization Techniques for Neural Networks (Dropout, Early Stopping) L1 and L2 Regularization Grief with Gradients The Vanishing Gradient problem The Confusion Matrix Precision, Recall, F1, AUC, and more Ensemble Methods Bagging and Boosting Introducing Amazon SageMaker Linear Learner in SageMaker XGBoost in SageMaker Seq2Seq in SageMaker DeepAR in SageMaker BlazingText in SageMaker Object2Vec in SageMaker Object Detection in SageMaker Image Classification in SageMaker Semantic Segmentation in SageMaker Random Cut Forest in SageMaker Neural Topic Model in SageMaker Latent Dirichlet Allocation (LDA) in SageMaker K-Nearest-Neighbors (KNN) in SageMaker K-Means Clustering in SageMaker Principal Component Analysis (PCA) in SageMaker Factorization Machines in SageMaker IP Insights in SageMaker Reinforcement Learning in SageMaker Automatic Model Tuning Apache Spark with SageMaker SageMaker Studio, and SageMaker Experiments SageMaker Debugger SageMaker Autopilot / AutoML SageMaker Model Monitor Other recent features (JumpStart, Data Wrangler, Features Store, Edge Manager) SageMaker Canvas Bias Measures in SageMaker Canvas SageMaker Training Compiler Amazon Comprehend Amazon Translate Amazon Transcribe Amazon Polly Amazon Rekognition Amazon Forecast Amazon Forecast Algorithms Amazon Lex Amazon Personalize Lightning round! TexTract, DeepLens, DeepRacher, Lookout, and Monitron TorchServe, AWS Neuron, and AWS Panorama Deep Composer, Fraud Detection, CodeGuru, and Contact Lens Amazon Kendra and Amazon Augmented AI (A2I) This section covers framing business problems as machine learning problems, selecting the appropriate model(s) for a given machine learning problem, training machine learning models, performing hyperparameter optimization, and evaluate machine learning models.\nDeeplearning Frameworks Tensorflow/Keras (Google) PyTorch (Meta) MXNet (Apache, and therefore AWS leans towards this) Scikit-Learn (for simple DL) Activation Functions Apply a non linear transformation Given the input, what should by output be Can be applied in between layers, or in the output layer Step Function, Sigmoid, TanH, ReLU, Leaky ReLU Binary Step Function is either on or off, cannot handle multiple classification, vertical slopes do not work with calculus Sigmoid: 0 to 1 TanH: -1 to 1 For Sigmoid and TanH there is a vanishing gradient problem (value changes slowly for high or low value) Sigmoid and TanH are computationally expensive ReLu: fast to compute, for inputs that are zero or negative, it is a linear function (dying relu problem) Leaky ReLU solves this Parametric ReLU, slope in the negative part is learned via backpropagation, complicated Exponential Linear Unit (ELU) Maxout: usually not worth the effort Softmax: usually the final layer of a classification model RNN\u0026rsquo;s do well with Tanh Sigmoid if more that one classification is required for the same thing For everything else, start with ReLU Convolutional Neural Network CNN vs MLP (Multilayer perceptron) They have convolutional layers Some filters may detect edges, lines, shapes etc. and deeper layers can detect objects Feature location invariant, Shift Invariant, Space Invariant Artificial Neural Networks Image and video recognition, recommender systems, image classification, image segmentations, Machine translation, Sentence Classification, Sentiment analysis AlexNet, LeNet, GoogLeNet, ResNet as an example source data must be of appropriate dimensions Recurrent Neural Networks deals with sequences in time (predict stock prices, understand words in a sentence, translation etc) time series data, sequence of arbitrary length captions for images, order matters structure and context is relevant machine generated music past behaviour of neuron impacts the future Sequence to Sequence: predict stock prices based on series of historic data Sequence to vector: words in a sentence to sentiment Vector to sequence: create captions from an image Encoder -\u0026gt; Decoder: Sequence -\u0026gt; vector -\u0026gt; sequence, machine translation Backpropogation through time Ends up looking like a really really deep neural network Therefore, we use truncated backpropagation through time State from earlier time steps get diluted over time, Long Short-Term memory cell LSTM cell GRU cell: Gated Recurrent Unit, Simplified LSTM which performs almost as well Traning RNN\u0026rsquo;s is hard, very sensitive to topologies, choice of hyperparameters, very resource intensive, a wrong choice can lead to a RNN that does not converge at all. Modern NLP with BERT and GPT, and Transfer Learning Transformer deep learning architectures BERT, RoBERTa, T5, GPT2, GPT3, etc DistilBERT: uses knowledge distillation to reduce model size by 40% BERT: Bi-directional Encoder Representations from Transformers GPT: Generative Pre-trained Transformer Transfer Learning Model zoos: hugging face offer pre trained models to start with Hugging face DLC (deep learning containers) Transfer Learning, retrain=True vs False Deep Learning on EC2/EMR EMR supports Apache MXNet and GPU instance types Appropriate instance types for deep learning P3, P2, G3 Deep Learning AMI\u0026rsquo;s Tuning Neural Networks Neural nets are trained by gradient descent or sth similar We start at some random point, and sample different solutions seeking to minimize some cost functions, over many epochs how far apart these samples are is the learning rate learning rate is an example of a hyperparameter batch size is also a hyperparameter, smaller batch size can work out of local minima small batch size tend to not get stuck in local minima large batch sizes can converge on the wrong solution at random large learning rates can overshoot the correct solution small learning rates increate training time Regularization Techniques for Neural Networks (Dropout, Early Stopping) Regularization helps with avoiding overfitting build simple model, dropout, early stopping can also help with avoiding overfitting L1 and L2 Regularization L1: sum of abs value of weights: perform feature selection, computationally inefficient, sparse output L2: sum of square of weights, all features considered but weighted, computationally efficient, dence output Grief with Gradients The Vanishing Gradient problem vanishing gradient propogate to deeper layer slope is approaching zero it could be the local miminum or global where the convergence is happening long short term memory RNN can be used resnet also helps with vanishing gradient problem better activation function (relu is a good choice) The Confusion Matrix sometimes accuracy does not tell the whole story TP, TN, FP, FN Confusion matrix shows this multi class confusion matrix: heatmap Precision, Recall, F1, AUC, and more Precision/Correct Positives/Percent of relevant results: when you are a lot about false positives: TP/(TP+FP) Recall/Sensitivity/True Positive Rate: TP/(TP + FN): when you care about false negatives F1 score: harmonic mean of Precision and Recall Specificity: TN/(TN+FP) RMSE, AMSE, etc. ROC curve: Receiver Operating Characteristic Curve: Plot of true positive rate (recall) vs false positive rate at various threshold setting. AUC curve: area under the ROC curve. Ensemble Methods Bagging and Boosting Bagging: Generate N new training sets by random sampling with replacement, each resampled model can be trained in parallel Boosting: Observations are weighted, training is sequential XGBoost is the latest hotness, boosting generally yields better accuracy, bagging avoids overfitting, bagging is easier to parallelize Introducing Amazon SageMaker built to handle the entire machine learning workflow deploy model, evaluate results in production, fetch, clean and prepare data, train and evaluate a model training data will be in s3, sagemakaker docker EC2 for inference spins as many hosts, spins as many endpoints Sagemaker notebook: notebook instance on EC2, has access to s3, scikit learn, spark, tensorflow, ability to deploy trained models for making predictions at scale hyperparameter tuning from notebook Sagemaker console Data comes from S3, ideal format is RecordIO/Protobuf/csv Can also ingest from Athena, EMR, Redshift, Amazon Keyspaces DB Apache Spark integrates with Sagemaker Scikit learn, numpy, pandas all work Create training job save your trained model to s3 can be deployed using persistent endpoint for making individual predictions on demand or batch transform to get prediction for and entire dataset inference pipelines sagemaker neo for deploying to edge devices elastic inference for accelerating deep learning models automatic scaling of endpoints as needed Linear Learner in SageMaker Linear learer can handle both classification and regression can do classification using Linear Learner threshold as long as a line will fit RecordIO wrapped protobuf float32, or csv (first column assumed to be the label) File or pipe mode both supported pipe mode will be more efficient if s3 is taking to long to train, pipe is a simple optimization training data should be normalized input data should be shuffled uses SGD multiple models are optimized in parallel tune l1, l2 regularization balance multiclass weights: give each class equal importance in loss functions learning rate, mini batch size, l1 regualization multi gpu does not help XGBoost in SageMaker eXtreme gradient boosting boosted group of decision trees gradient descent winning a lot of kaggle competitions fast classification/regression CSV/libsvm/recordIO-protobuf/parquet models are searilized/deserialized with pickle can use as a framework withing notebooks or as a built in sagemaker algorithm subsample (prevent overfitting) ETA (step size shrinkage, prevents overfitting) Gamma (minimul loss reduction to create a partition) Alpha (L1 regularization term, larger = more conservative) Lambda (L2 regularization term, larger = more conservative) eval_metric: Optimize on AUC, example: if you care about false positives more than accuracy scale_pos_weight: adjusts balance of positive and negative weights, helpful for unbalanced classes max_depth : too high may overfit Xgboost with cpu: M5 is a good choice (optimize for memory and not compute) Xgboost with gpu: tree_method hyperparameter: gpu_hist, cheaper and faster, P3 is good choice Seq2Seq in SageMaker sequence to sequence (example machine translation, text summarization, speech to text) implemented with RNN\u0026rsquo;s and CNN\u0026rsquo;s with attention RecordIO-Protobuf tokens must be integers start with tokenized text files convert to protobuf using sample code must provide training data, validation data and vocabulary files training machine translation can take days, pretrained models are available public training datasets are avaialable for specific translation tasks batch_size, optimizer_type, learning_rate, num_layers_encoder, num_layers_decoder, can optimize on accuracy, bleu score (compares against multiple reference translations), perplexity (cross-entropy) cannot be parallelized can only use gpu instance can use multi gpu within an instance machine DeepAR in SageMaker Forecasting one dimensional time series data uses rnn\u0026rsquo;s allows you to train the same model over several related time series finds frequencies and seasonality json lines format, Gzip or Parquet each record must contain, start and target each record can contain dynamic features and categorical features always include entire time series for training, testing and inference use entire dataset as test set do not use very large values for prediction (\u0026gt;400) train on many time series contect length, epochs, mini batch size, learning rate, num cells can use cpu or gpu single or multi machine cpu only for inferene may need larger instances for tuning BlazingText in SageMaker Text classification: predict labels for a sentence, useful in web searches, information retrieal, supervised Word2vec: creates a vector representation of workds semantically similar words are represented by vectors close to each otehr this is called a word embedding it is useful for nlp, but is not an nlp algorithm itself it only works on individual words, not sentences or documents for supervised mode, one sentence per line, first word in the sentence is the string label followed by the label Also, \u0026ldquo;augmented manifest text format\u0026rdquo; Word3vec just wants a text file with one training sentence per line There are multiple modes: Cbow (Continuous Bag of Words) Skip-gram Batch skip-gram (Distributed computation over many CPU nodes) Word2vec: mode, learning rate, window size, verctor dim, negative samples Text classification: epochs, learning rate, word ngrams, vector dim For cbow and skipgram, recommend a single ml.p3.2xlarge, any single CPU or single GPU instance will work for batch_skipgram, can use single or multiple CPU instances for text classification C5 recommended if less than 2GB training data, for larger datasets use a single GPU instance ml.p2.xlarge or ml.p3.2xlarge Object2Vec in SageMaker creates low-dimensional dense embeddings of high-dimensional objects compute nearest neighbors of objects visualize clusters genre prediction recommendations data must be tokenized into integers training data consists of pairs of tokens and or sequenses of tokens process data into json lines and shuffle it train with two input channels, two encoders, and a comparator encoder choices: average-pooled embeddings, cnn\u0026rsquo;s, bidirectional lstm comparator is followed by feed-fowrard neural network usual suspect: dropout, early stopping, epochs, learning rate, bbatch size, layers, activation function, optimizer, weight decay Enc1_network, enc2_network instance types: can only train on a single machine (cpu or gpu, multi-gpu ok) inference: use ml.p2.2xlarge Object Detection in SageMaker identify all objects in an image with bounding box detects and classifies objects with a single deep neural network classes are accompanied by confidence scores can train from scratch, or use pretrained models based on imagenet recodrio or image format with image format, supply a json file for annotation data for each image takes and image input, outputs all instances of objects in teh imagte with categories and confidence scores uses cnn with single shot multibox detector ssd algorithm, the base being vgg-16 or resnet-50 transfer learning mode/incrementatl training: use pretrained model for the base network instead of random inintial weights uses flip, rescale, and jitter internally to avoid overfitting mini batch size, learning rate, optimizer gpu instances for training multi gpu multi machines for inference cpu is enough Image Classification in SageMaker assign one or more labels to an image does not tell you where objects are mxnet recordio (not protobuf) raw jpg or png .lst files to associate image index and class augmented manifest image format enables pipe mode resnet cnn under the hood full training mode transfer learning mode default image is 224 224 3 bbatch size, learning rate, optimizer weight decay, beta 1, beta 2, eps, gamma gpu instance fr training cpu or gpu for inference Semantic Segmentation in SageMaker pixel level object classificaion different from image classification useful for self driving vehicles, medical imaging, robot sensing produces a semantic mask jpg or img with annotations augmented manifest image format supported for pipe mode jpg images accepted for inference mxnet gluon and gluon cv fully convolution network, pyramid scene parsing, deeplabv3 resnet50, renet101, both rained on imagenet incremental training, or scratch epochs, learning rate, batch size, optimizer, algorithm, backbone only gpu for training (p2 or p3), and only on one maching cpu or gpu for inference Random Cut Forest in SageMaker anomaly detection unsupervised detect unexpected spikes in time series data breaks in periodicity unclassifiable data points assigns and anamoly score to each data points recordio protobuf or csv can use file or pipe mode on either optional test channel for computation creates a forest of trees where each tree is a partition of the training data, looks at expected change in complexity of the tree as a result of adding a point into it data is sampled randomly and then trained rcf shows up in kinesis analytics as well, it can work on streaming data as well. num_trees, num_samples_per_tree (should be chosen such that 1/num_samples_per_tree approximates the ratio of anomalous to normal data) does not take advantage of gpu ml.c5.xl for inference Neural Topic Model in SageMaker organize documents into topics classify or summarize documents based on topics it is not just tf/idf unsupervised: algorithm is neural variational inference four data channels, train, validation, test and auxiliary record io or csv words muyst be tokenized into integers file or pipe mode you define how many topics you want, these topics are latent representation based on top ranking words one of two modelling algorithms sagemaker offers batch size, num_topics gpu or cpu Latent Dirichlet Allocation (LDA) in SageMaker latent dirichlet allocation another topic modeling algorithm but not based on deep learning unsupervised: topics are unlabeled, they are just grouping of documents with a shared subseet of words can be used for other purposes as well train channel, optional test channel protobuf or csv each document has counts for every word in vocabulary pipe mode: only supported with proto unsupervised, generates however many topics you specify per-word log likelyhood num_topics, alpha0 cpu single instance, cannot parallelize K-Nearest-Neighbors (KNN) in SageMaker simple classification or regression algorithm classification: k closest points regression: average values train channel, test channel emits accuracy or MSE protobuf or csv (first column is label) data is sampled, sagemaker includes dimensionality reduction stage, build an index for looking up neighbors, serialize the model, query the model for given K hyperparameter K, sample_size cpu or gpu cpu or gpu for inference K-Means Clustering in SageMaker unsupervised clustering divide data into k groups, where members of a group are as similar as possible to each other web scale k means clustering training input: train channel, train in shardedbys3key and testing: fullyreplicated recordio or csv file or pipemode every ovservation is mapped to n-dimensional space works to optimize the center of k clusters algorithm: k means++ tries to make initial clusters far away, lloyd\u0026rsquo;s method mini_batch_size, extra_center_factor, init_method cpu or gpu, but cpu recommended only one gpu per instance used on gpu Principal Component Analysis (PCA) in SageMaker dimensionality reduction unsupervised covariance matrix is created, then SVD two modesL regular: for sparse matrix, randomized: for large number of observations and features algorithm_mode and subtract_mean gpu or cpu Factorization Machines in SageMaker dealing with sparse data item recommendations supervised: classification or regression limited to pair-wise interactions protobuf with float32 bias, factors, and linear terms cpu or gpu, cpu recommended IP Insights in SageMaker finding fishy behaviour unsupervised learning of ip address identifies suspicious behaviour from ip addresses user names, account ids, not need to pre process training channel, optional validation (computes auc score) csv only (entity, ips) neural network to learn latent vector representations of entities and ip addresses entities are hashed and embedded automatically generates negative samples during training by randomly pairing entities and ips num_entity vectors, vector_dim, epochs, learning rate, batch size cpu or gpu gpu recommended multiple gpu can be used withing an instance Reinforcement Learning in SageMaker agent and environment supply chain management, hvac systems, industrial robots, dialog systems, autonomous vehicles yields fast on-line performance once the space has been explored Q learning: environment, actions, state/action part uses a deep learning framework with tensorflow and mxnet supports intel coach and ray rllib toolkits custom, open-source or commercial environments supported can distribute trining and environment rollout multi core and multi instance Automatic Model Tuning define the hyperparameters you care about sagemaker spins up a hyperparameter tuning job that trains as many combinations as you will allow it learns as it goes, so it does not have to try every possible combination intelligent do not optimize too many hyperparameters at once limit your ranges to as samall range use logarithmic scales do not run too many training jobs concurently make sure training jobs running on multiple instance report the correct objective metric in the end Apache Spark with SageMaker apache spark allows for preprocessing and also has mllib combination of sagemaker and spark is possible preprocess with spark, and instead of using mllib, you can use sagemaker estimator, you can use kmeans, pca, xgboost sagemakermodel, can be used to make inferences connect notebook to a remote emr fit, transform in sagemaker SageMaker Studio, and SageMaker Experiments visual ide sagemaker notebooks sagemaker experiments SageMaker Debugger saves internal model state at periodical intervals gradients/tensors over time is saved define rules for detecting unwanted conditions while training a debug job is run for each rule logs and fires a cloudwatch event when the rule is hit sagemaker studio debugger dashboards auto generated training reports built in rules: monitor system bottlenecks, profile model framework operations, debug model parameters supported framewords and algorithms: tensorflow, pytorch, mxnet, xgboost, sagemaker generic estimator debugger api\u0026rsquo;s available in github smdebug is the library Sagemaker debugger insights dashboard profiler report, hardware system metrics, framework metrics built in actions to receive notifications or stop training profiling system resource usage and training SageMaker Autopilot / AutoML automates algorithm selection, data preprocessing, model tuning it does all the trial and error for you automl automatic model creation model leaderboard ranks can add in human guidance human in the loop with or without code in sagemaker studio problem types: binary/multiclass classification linear learner, xgboost, mlp data must be tabular csv autopilot explainability integrates with sagemaker clarify transparency on how models arrive at predictions feature attributions: uses shap baselines/shapley values, research from cooperative game theory, assigns each feature an importance value for a give prediction SageMaker Model Monitor get alery on quality deviations on your deployed models via cloudwatch visualize data drift detect anomalies and outliers detect new features no code required data is stored in s3, monitoring jobs are scheduled via a monitoring schedule, metrics are emitted to cloudwatch, integrates with quicksight, tensorboard etc. drift in statistical properties of the features drift in model quality bias drift feature attribution drift Other recent features (JumpStart, Data Wrangler, Features Store, Edge Manager) jumpstart: one click models and algorithms from model zoos: 150 open source models in nlp, object detection, image classification etc data wrangler: import transform analayze and export data withing sagemaker studio feature studio: find, discover and share features in studio:online and offline modes sagemaker edge manager: software agent for edge devices, models optimized with agemaker neo, collects and samples data for monitoring, labeling and retraining SageMaker Canvas no code machine learning for business analysts upload csv data, select a column to predict, build it and make predictions can also join datasets classification or regressions automatic data cleaning, missing values, outlier and duplicates share models and datasets with sagemaker studio import from redshift is possible time series must be enabled via IAM vpc a little expensive Bias Measures in SageMaker Clarify class imbalance difference in proportions of labels kullback-leibler divergence, jensen-shannon divergence lp-norm total variation distance kolmogorov-smirnov conditional demographic disparity SageMaker Training Compiler integrates into AWS deep learning containers compile and optimize training jobs on gpu can accelerate training up to 50% converts models into hardware-optimized instructions tested with hugging face transformers library, or bring your own model ensure gpu instance are used in ml.p3, ml.p4 pytorch models must use pytorch xla\u0026rsquo;s model save function enable dubug flask in compiler_config parameter to enable debugging Amazon Comprehend nlp and text analytics input social media, emails, web pages, documents, transcripts, medical records (comprehend medical) extract key phrases, entities, sentiment, language, syntax, topics, and document classifications Amazon Translate translates text uses deep learning supports custom terminology for proper names Amazon Transcribe speech to text speaker identification channel identification language identification custom vocabularies Amazon Polly text to speech polly is parrot lexicons ssml (speech synthesis markup language) speech marks Amazon Rekognition compute vision object and scene detection image moderation, facial analysis, celebrity recognition, face comparison, text in image, video analysis kinesis video stream h.264 encoded, 5-30 fps can use lambda to trigger image analysis upon upload Amazon Forecast fully managed service to deliver highly accurate forecasts with ml automl chooses the best model for your time series data arima, deepar, ets, npts, prophet works with any time series inventory planning, financial planning, resource planning, based on dataset groups, predictors and forecasts Amazon Forecast Algorithms cnnqr: convolutional neural network quantile regression, best for large datasets with hundreds of time series, accepts related historical time series data and metadata deepar+ : recurrent neural network, best for large datasets, accepts related forward-looking time series and metadata prophet: additive model with non linear trends and seasonality npts: non parametric time series: good for sparse data arima: simple datasets ets: exponential smoothing Amazon Lex chatbot engine lambda to fulfill intent from text can deploy to aws mobile sdk, facebook messenger, slack, twilio Amazon Personalize fully managed recommendation engine api access: feed in data, provide schema in avro, javascript or sdk, get recommendations, get personalized ranking real time or batch recommendations recommendations for new users and new items contextual recommendations similar items datasets, recipes, solutions, compaignhs hidden_dimensions, bptt, recency_mask, min/max_user_history_length_percentile, exploration_weight, exploration_item_age_cut_off necessary to maintain recency bucket policy data ingestion: per gb, training per training hour, inference per tps-hour, batch recommendations: per user or per item Lightning round! TexTract, DeepLens, DeepRacher, Lookout, and Monitron TexTract: ocr with forms, fields, tables support DeepLens: deep learning enabled video camera, integrated with rekognition, sagemaker, polly, tensorflow, mxnet, caffe DeepRacer: reinforcement learning powered 1/18 scale race car Lookout: equipment, metrics and vision: detect defects in silicon wafers, circuit boards etc. Monitron: end to end system for monitoring equipment and predictive maintenance TorchServe, AWS Neuron, and AWS Panorama TorchServe: model serving framework for pytorch AWS Neuron: ml inferentia chip, Ec2 inf1 instance type Panorama: computer vision at the edge Deep Composer, Fraud Detection, CodeGuru, and Contact Lens DeepComposer: ai powered keyboard fraud detection: upload your own data Codeguru: automated code reviews, finds lines of code that hurt performance contact lens: for customer support call centers, ingests audio, sentiment analysis finds utterances that correlate with successful calls categorize calls automatically measure talk speed and interruptions theme detection: discovers emerging issues Amazon Kendra and Amazon Augmented AI (A2I) Enterprise search with natural languate combines data from sharepoint, intranet, sharing services, jdbc, s4 into one searchable repo ml powered, uses thumbs up/down relevance tuning, boost strength of document freshness Kendra: Alexa\u0026rsquo;s sister AugmentedAI: human review of ml predictions, mechanical turk workforce or vendors integrated into textract and rekognition integrates with sagemaker ","date":"2023-01-15T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/aws_ml_speciality_modeling/","section":"posts","tags":["analytics","machine-learning","certification","AWS","certification","deepar-in-sagemaker","sagemaker-debugger","sagemaker-canvas","amazon-comprehend","amazon-translate","Modeling","amazon-transcribe","amazon-polly","amazon-rekognition","amazon-forecast","amazon-lex","amazon-personalize"],"title":"AWS Certified ML - Specialty exam (MLS-C01) - 3b. Modeling"},{"categories":["AWS Certified ML - Specialty exam (MLS-C01)"],"contents":"Machine Learning Implementation and Operations Section Intro: Machine Learning Implementation and Operations SageMaker\u0026rsquo;s Inner Details and Production Variants SageMaker On the Edge: SageMaker Neo and IoT Greengrass SageMaker Security: Encryption at Rest and In Transit SageMaker Security: VPC\u0026rsquo;s, IAM, Logging, and Monitoring SageMaker Resource Management: Instance Types and Spot Training SageMaker Resource Management: Elastic Inference, Automatic Scaling, AZ\u0026rsquo;s SageMaker Serverless Inference and Inference Recommender SageMaker Inference Pipelines This section covers building machine learning solutions for performance, availability, scalability, resiliency, and fault tolerance, recommending and implementing the appropriate machine learning services and features for a given problem, applying basic AWS security practices to machine learning solutions and deploying and operationalizing machine learning solutions.\nSection Intro: Machine Learning Implementation and Operations scaling, productionalization and security SageMaker\u0026rsquo;s Inner Details and Production Variants all models in agemaker are hosted in docker containers the docker container is registered with ECR pre built deep learning, scikit learn and spark ml pre built tensorflow, mxnet, chainer, pytorch etc. horovod or parameter server is a way to distribute tensorflow training you can also use any script or algorithm within sagemaker the containers are isolated and contain all dependencies Dockerfile structure using your own image you can test muliple models on live traffic using Production Variants (Roll out variant weights) A/B test is posible SageMaker On the Edge: SageMaker Neo and IoT Greengrass Train once, run anywhere supports multiple architecture optimizes code for specific devices consists of a compiler and a runtime Neo compiled models can be deployed to an https endpoint, must be the same instance type used for compilation or you can deploy to iot greengrass SageMaker Security: Encryption at Rest and In Transit IAM MFA SSL/TLS Cloudtrail to log API and user activity encryption AWS key mangement service is accepted by notebooks and all sagemaker jobs s3 can be encrypted as well All traffic supports TLS/SSL inter node training communication may be optionally encrypted SageMaker Security: VPC\u0026rsquo;s, IAM, Logging, and Monitoring training jobs run in VPC private VPC s3 vpc endpoints IAM user permissions for CreateTrainingJob, CreateModel, CreateEndpointConfig, CreateTransformJob, CreateHyperParameterTuningJob, CreateNotebookInstance, UpdateNotebookInstance Predefined policies for AmazonSageMakerReadOnly, AmazonSageMakerFullAccess, AdministratorAccess, DataScientist cloudwatch can log, monitor and alarm on invocations and latency of endpoints, health of instance nodes, ground truth (active workers) cloudtrail records actions from users, roles, and services within Sagemaker: log files are delivered to s3 for auditing purposes SageMaker Resource Management: Instance Types and Spot Training depends but usually gpu for training and cpu for inference EC2 spot training: checkpoints to s3 so training can resume can increate training time SageMaker Resource Management: Elastic Inference, Automatic Scaling, AZ\u0026rsquo;s accelerates deep learning inference at a fraction of a cost EI accelerator may be added alongside a CPU instance EI accelerators can also be applied to notebooks works with tensorflow, pytorch, mxnet, onnx may be used to export models to mxnet works with custom containers built with El-enabled Tensorflow, Pytorch or mxnet works with image classification and object detection built in algorithms Automatic scaling: can be used to define target metrics, min or max capacity, cooldown periods, works with cloudwatch, dynamically adjusts number of instances for a production variant, load test your configuration before using it. SageMaker Serverless Inference and Inference Recommender specify your container, memory requirement, concurrency requirements underlying capacity is automatically provisioned and scaled good for infrequent or unpredictable traffic, will scale down to zero when there are no requests chared based on usage monitor via cloudwatch Inference Recommender recommends what instance type and configuration for your model automates load testing and model tuning SageMaker Inference Pipelines linear sequence of 2-15 containers any combination of pre-trained built-in algorithms or your own algorithms in Docker combine pre-processing, predictions, post-processing Spark ML and scikit-learn chaining multiple inference containers into a pipeline of results ","date":"2023-01-15T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/aws_ml_speciality_ml/","section":"posts","tags":["certification","sagemaker","greengrass","pipelines","analytics","machine-learning","certification","AWS","Machine Learning Implementation and Operations"],"title":"AWS Certified ML - Specialty exam (MLS-C01) - 4. Machine Learning Implementation and Operations"},{"categories":["ethics"],"contents":"Ethics in Human Research What CITI (Collaborative Institutional Training Initiative) is a program that provides training on ethical and regulatory issues related to human subjects research. It offers online courses and resources to researchers, students, and staff at institutions around the world. The training covers topics such as the history and ethical principles of human subjects research, federal regulations and guidelines, and the responsibilities of researchers and institutional review boards (IRBs). Completing CITI training is often required for researchers who plan to conduct human subjects research at institutions that participate in the program.\nCITI training for Social/Behavioral Research Investigators and Key Personnel covers the ethical and regulatory issues specific to social and behavioral research involving human subjects.\nWhy By getting certified on the CITI program, individuals can demonstrate that they have received training on responsible conduct of research and are aware of the ethical principles involved in their work.\nImportant Topics The Belmont Report The Belmont Report is a document that was created by the National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research in 1979. The report sets out the ethical principles and guidelines that govern the conduct of research involving human subjects. The report is considered a seminal document in the field of research ethics, and its principles continue to be widely referenced today.\nThe report\u0026rsquo;s main principles are:\nRespect for persons: This principle acknowledges the autonomy of individuals and the need for informed consent in research. Beneficence: This principle requires that research should maximize benefits and minimize harm. Non-maleficence: This principle states that researchers should not cause harm to subjects. Justice: This principle requires that the benefits and burdens of research be distributed fairly among subjects. The report also elaborates on the concept of informed consent and the protection of vulnerable populations such as children, prisoners, pregnant women, and individuals with cognitive impairment.\nIn summary, The Belmont Report is a widely recognized guide for ethical conduct of research involving human subjects, which provides the foundation for Federal Policy for the Protection of Human Subjects (also known as the Common Rule) and many institutional and national guidelines on human subjects research.\nThe Belmont Report was written in response to the infamous Tuskegee Syphilis Study:\nThe Nuremberg Code The Nuremberg Code is a set of ethical principles for human experimentation that was developed as a result of the Nuremberg Trials after World War II. The trials were held to prosecute Nazi doctors and scientists who had conducted brutal and inhumane medical experiments on concentration camp prisoners during the war. The Nuremberg Code was developed to ensure that such atrocities would never happen again.\nThe code consists of 10 principles that were intended to guide the conduct of medical research involving human subjects. The principles are:\nThe voluntary, informed consent of the human subject is absolutely essential. The experiment should be such as to yield fruitful results for the good of society, unprocurable by other methods or means of study, and not random and unnecessary in nature. The experiment should be so designed and based on the results of animal experimentation and a knowledge of the natural history of the disease or other problem under study that the anticipated results will justify the performance of the experiment. The experiment should be so conducted as to avoid all unnecessary physical and mental suffering and injury. No experiment should be conducted where there is an a priori reason to believe that death or disabling injury will occur. The degree of risk to be taken should never exceed that determined by the humanitarian importance of the problem to be solved by the experiment. Proper preparations should be made and adequate facilities provided to protect the experimental subject against even remote possibilities of injury, disability, or death. The experiment should be conducted only by scientifically qualified persons. The highest degree of skill and care should be required through all stages of the experiment of those who conduct or engage in the experiment. During the course of the experiment, the human subject should be at liberty to bring the experiment to an end if he has reached the physical or mental state where continuation of the experiment seems to him to be impossible. The Nuremberg Code is considered a landmark document in the history of research ethics, and its principles continue to be widely referenced today. It serves as a foundation for the protection of human subjects in medical research and other fields. The principles of informed consent, beneficence and non-maleficence were for the first time defined and established as the foundation for ethical conduct in research involving human subjects.\nThe Declaration of Helsinki The Declaration of Helsinki is a set of ethical principles for medical research involving human subjects, developed by the World Medical Association (WMA). The first version of the Declaration was adopted in 1964, and it has been revised several times since then. The most recent version was adopted in 2013.\nThe Declaration of Helsinki sets out ethical principles for the conduct of medical research involving human subjects, including guidelines for obtaining informed consent, protecting vulnerable populations, and ensuring the safety and well-being of research participants. The Declaration also includes principles for the conduct of research involving medical interventions, such as the use of placebo controls and the reporting of adverse events.\nThe Declaration of Helsinki includes the following principles:\nMedical research involving human subjects must conform to generally accepted scientific principles and should be based on a thorough knowledge of the scientific literature.\nThe well-being of the individual research subject must take precedence over all other considerations.\nInformed consent must be obtained from all research subjects, or their legally authorized representatives, before the research begins.\nMedical research involving human subjects should only be conducted if the importance of the objective outweighs the inherent risks and burdens to the research subjects.\nMedical research involving human subjects should not be conducted if there are other research methods available that are likely to yield essentially the same results.\nResearchers have a duty to protect the privacy and confidentiality of research subjects.\nThe design and performance of every medical study involving human subjects must be clearly formulated in an experimental protocol.\nEvery medical study involving human subjects must be preceded by the appropriate ethical and regulatory review.\nResearchers have a duty to publish the results of their research, whether positive or negative, and must make all data available for further research.\nResearchers have a duty to respect the rights and welfare of their research subjects, even after the study has been completed.\nThe Declaration of Helsinki is widely recognized as an important guide for the ethical conduct of medical research involving human subjects, and is considered a cornerstone of research ethics in the international medical community. It is also considered as a foundation of many national and institutional guidelines on human subjects research.\nThe Common Rule The Common Rule is a set of federal regulations for the protection of human subjects in research, developed by the U.S. Department of Health and Human Services (HHS) and other federal agencies. The regulations are officially known as the Federal Policy for the Protection of Human Subjects, and are commonly referred to as the \u0026ldquo;Common Rule.\u0026rdquo;\nThe Common Rule was first established in 1991 and applies to all research conducted or supported by HHS and other federal agencies that involve human subjects. The Common Rule sets out requirements for informed consent, institutional review board (IRB) review, and record-keeping, and it also provides guidance on the protection of vulnerable populations, such as children, prisoners, and individuals with cognitive impairments.\nThe Common Rule includes the following key requirements:\nInformed consent: Researchers must obtain the informed consent of research participants, or their legally authorized representatives, before beginning any research involving human subjects.\nInstitutional Review Board (IRB) review: All research involving human subjects must be reviewed and approved by an IRB, an independent body that is responsible for evaluating the ethical and scientific aspects of the research.\nRecord-keeping: Researchers must maintain records of all research involving human subjects, including the informed consent forms and the IRB approval documents.\nProtection of vulnerable populations: The Common Rule includes special provisions for protecting vulnerable populations, such as children, prisoners, and individuals with cognitive impairments.\nReporting: The Common Rule requires that researchers must report certain information to the relevant federal agencies, such as any unanticipated problems involving risks to subjects or others, and any serious or continuing noncompliance with the regulations.\nThe Common Rule is considered a fundamental framework for the protection of human subjects in the United States, serving as a foundation for the ethical conduct of research involving human subjects in the country. The Common Rule was revised in January 19, 2017, and the changes went into effect in 2018. The revisions are aimed at modernizing, strengthening, and making the regulations more effective in protecting human subjects involved in research.\nResearch with human subjects Research with human subjects involves conducting studies or experiments on living individuals to gain knowledge or understanding about a certain topic. Such research can be conducted in a variety of fields, including medicine, psychology, sociology, and anthropology. Because research with human subjects has the potential to cause harm or impact individuals\u0026rsquo; rights and well-being, it is subject to strict ethical guidelines and regulations.\nResearchers are required to obtain informed consent from participants and ensure that the potential risks of the research are minimized and that any potential benefits outweigh the risks. They must also ensure that the research is conducted in a way that protects participants\u0026rsquo; privacy and confidentiality. Research involving vulnerable populations, such as children or individuals with disabilities, may require additional safeguards.\nInformed Concent Informed consent is a process by which a researcher obtains permission from a research participant to participate in a study. Informed consent is considered a fundamental ethical principle in research involving human subjects. It is important because it ensures that participants are aware of the nature of the research, the risks and benefits involved, and any alternatives to participation. They also have the right to discontinue participation at any time.\nDuring the informed consent process, the researcher is responsible for providing the participant with a clear and comprehensive explanation of the study, including its purpose, procedures, risks, benefits, and any potential consequences. Participants must also be informed of their right to withdraw from the study at any time, and any potential impact on their medical or legal rights.\nInformed consent is typically documented by having the participant sign a written consent form, which should be written in language that is easy for the participant to understand. Research involving vulnerable populations, such as children or individuals with cognitive impairment, may require additional safeguards to ensure that informed consent is obtained in a way that is appropriate for the population.\nResearch with Children Research with children involves conducting studies or experiments on individuals who have not yet reached the age of legal adulthood. Such research can be conducted in a variety of fields, including medicine, psychology, sociology, and anthropology. Because children are considered a vulnerable population, research involving them is subject to additional ethical guidelines and regulations to ensure their safety, well-being and rights are protected.\nWhen conducting research with children, researchers must obtain informed consent from both the child (if they are capable of understanding the nature of the research and giving consent) and the child\u0026rsquo;s parent or legal guardian. In addition, the research must be designed in a way that is appropriate for the age and developmental level of the children.\nIt is also important to minimize any potential harm to the children and to ensure that the potential benefits of the research outweigh any risks. Researchers must also take steps to protect the children\u0026rsquo;s privacy and confidentiality.\nAn Institutional Review Board (IRB) An Institutional Review Board (IRB) is a committee that is responsible for reviewing and approving research studies involving human subjects. The primary role of an IRB is to ensure that the rights and welfare of human subjects are protected in research studies. IRBs are required by federal regulations (such as the Common Rule) and are usually established by institutions that conduct or support research involving human subjects.\nIRBs are made up of a diverse group of individuals, including researchers, medical professionals, ethicists, and community members, who have specific expertise in the field of research. They review the proposed research study to ensure that it meets the ethical guidelines set out in federal regulations and institutional policies.\nThe IRB will review the following aspects of the study:\nRisks and benefits of the study Informed consent process and the information provided to the subjects Protection of vulnerable populations Privacy and confidentiality of the participants Data security and management Proposed recruitment procedures Study design, objectives, and methods After reviewing the study, the IRB may approve the study, approve the study with modifications, or disapprove the study if it determines that the risks outweigh the potential benefits or that the rights of the participants are not adequately protected. IRBs play a critical role in ensuring that research involving human subjects is conducted in an ethical and responsible manner. They are responsible for ensuring that the rights and welfare of human subjects are protected in research studies, and they act as a safeguard to ensure that research is conducted in compliance with federal regulations and institutional policies.\nThere are several types of Institutional Review Board (IRB) review that are typically used to evaluate research studies involving human subjects. These include:\nExempt Review: Some research studies are considered to be low-risk and may be eligible for an exempt review. These studies usually involve minimal risk to participants and may include activities such as surveys, interviews, or observation of public behavior. IRBs can grant exempt status to a study after determining that it meets the criteria for exemption as defined by federal regulations.\nExpedited Review: Some research studies may involve more than minimal risk but still be considered appropriate for expedited review. These studies may include interventions such as blood draws or physical exams that are considered to be low-risk or are already being performed as part of routine medical care. In this type of review, the IRB can make a decision based on a review by one or more members rather than a full board review.\nFull Board/Convened Review: Research studies that involve more than minimal risk or that involve vulnerable populations, such as children or pregnant women, require full board review. The full board will review the study, discuss it, and vote on whether to approve, disapprove, or approve with modifications. This type of review may also be necessary when a study has been previously reviewed and the new proposed changes needs to be evaluated.\nContinuing Review: Once a study is approved, the IRB will require that the study be reviewed on a regular basis to ensure that the rights and welfare of human subjects are being protected. This review may be expedited or full board, depending on the level of risk of the study and the nature of the changes proposed.\nIn summary, the type of IRB review that is required depends on the level of risk involved in the research study and the type of participants involved. The IRB will determine the appropriate level of review based on federal regulations, institutional policies, and the specific characteristics of the study.\nCertificate of confidentiality A certificate of confidentiality is a legal document issued by the U.S. Department of Health and Human Services (HHS) that is intended to protect the privacy of research participants by preventing the disclosure of identifying information without the participant\u0026rsquo;s consent. Certificates of confidentiality are issued to researchers who are conducting sensitive research that involves the collection of potentially sensitive information, such as information about an individual\u0026rsquo;s mental health, substance use, or genetic information.\nThe main purpose of a certificate of confidentiality is to protect the privacy of research participants by allowing researchers to refuse to disclose identifying information in any legal proceeding, including civil, criminal, administrative, legislative, or other proceedings. This can include subpoena, court order, or any other legal process. Researchers who hold a certificate of confidentiality are not required to disclose identifying information about research participants in any of these proceedings, unless the participant consents to the disclosure.\nA certificate of confidentiality applies to the data collected during the study, and also to any documents or records that could identify participants, such as consent forms, medical records, and study questionnaires.\nIn addition to protecting the privacy of research participants, a certificate of confidentiality can also help to increase trust and participation in sensitive research studies by assuring participants that their information will be kept confidential.\nCertificates of confidentiality are issued by the National Institutes of Health (NIH) and the Office of Human Research Protections (OHRP) on behalf of the U.S. Department of Health and Human Services (HHS). Researchers who wish to obtain a certificate of confidentiality must submit an application, which will be reviewed by NIH or OHRP to determine if the research meets the criteria for the certificate.\nIn summary, a certificate of confidentiality is a legal document that protects the privacy of research participants by preventing the disclosure of identifying information without their consent. It is intended for sensitive research studies that involve the collection of potentially sensitive information, such as mental health, substance use, or genetic information.\nInternet Based Research Delves on how to address the ethical challenges of conducting research in this way, including guidelines for obtaining informed consent, protecting privacy and confidentiality, and avoiding bias in the sample of participants. The module covers the following topics:\nEthical considerations in Internet-based research Informed consent in Internet-based research Protecting the privacy and confidentiality of research participants Recruiting participants for Internet-based research Addressing bias in Internet-based research Ethical considerations in data collection, management, and sharing Special considerations for conducting Internet-based research with vulnerable - populations Specific guidance for conducting research using social media and other online - platforms The module is designed to help researchers understand the unique ethical - considerations involved in internet-based research, and to provide them with the - knowledge and skills they need to conduct their research in an ethical and - responsible manner. HIPPA The Health Insurance Portability and Accountability Act (HIPAA) is a federal law that protects the privacy of individuals\u0026rsquo; medical information. It applies to certain entities, known as \u0026ldquo;covered entities,\u0026rdquo; which include healthcare providers, health plans, and healthcare clearinghouses.\nHIPAA includes several provisions to protect the privacy of personal medical information, such as the Privacy Rule and the Security Rule. The Privacy Rule establishes national standards for protecting the confidentiality, integrity, and availability of personal medical information. It gives individuals certain rights with respect to their medical information, including the right to access and receive a copy of their own information, and the right to request that their information be corrected or amended. The Security Rule establishes standards for protecting the confidentiality, integrity, and availability of electronic personal health information.\nResearch is generally subject to HIPAA\u0026rsquo;s privacy protections, meaning that researchers must obtain informed consent from participants and must protect the confidentiality of the information obtained. However, there are some exceptions and modifications to the rules that apply to research, such as the \u0026ldquo;limited data set\u0026rdquo; and \u0026ldquo;de-identified information\u0026rdquo; provisions, which allow for the use and disclosure of certain information without specific authorization from the individual.\nIt is important for researchers to work closely with their institution\u0026rsquo;s privacy and compliance office to ensure that they are in compliance with HIPAA and other applicable laws and regulations related to the protection of personal health information in research.\nVulnerable Subjects Vulnerable subjects refer to individuals or groups who may be at increased risk of harm or exploitation as a result of their participation in research. This includes individuals or groups who may have limited ability to protect their own interests, such as children, individuals with cognitive or developmental disabilities, pregnant women, prisoners, and individuals from economically or educationally disadvantaged backgrounds.\nResearch involving vulnerable subjects requires additional safeguards to protect their rights and well-being. Researchers must take extra care to obtain informed consent from these individuals or their legal representatives and to ensure that the risks of the research are minimized and any potential benefits outweigh the risks. Researchers must also be mindful of the potential for exploitation and must be prepared to modify their research design to address any special needs or concerns that may arise.\nThe CITI program has a specific module for Vulnerable Populations, it covers the following topics:\nEthical considerations in research involving vulnerable populations Informed consent in research involving vulnerable populations Protecting the rights and welfare of vulnerable populations Special considerations for conducting research with children, pregnant women, - prisoners and individuals with cognitive or developmental impairments Addressing cultural and language barriers in research involving vulnerable - populations This module provides researchers with the knowledge and skills they need to conduct research with vulnerable populations in an ethical and responsible manner. Conflicts of Interest in Human Subjects Research Conflicts of interest (COI) in human subjects research refer to any situation in which a researcher\u0026rsquo;s personal, financial, or professional interests may interfere with their ability to conduct the research in an unbiased, ethical, and objective manner. Conflicts of interest can take many forms and can occur at any stage of the research process, from the design and conduct of the study to the dissemination and interpretation of the results.\nExamples of conflicts of interest in human subjects research include:\nFinancial conflicts of interest, such as when a researcher has a financial stake in the outcome of the research, such as stock options in a company whose products are being studied, or when the researcher is being paid by a third party with a vested interest in the research outcome. Intellectual conflicts of interest, such as when a researcher\u0026rsquo;s professional reputation or career advancement is closely tied to the success of the research. Personal conflicts of interest, such as when a researcher has a close personal relationship with a participant in the study or a stake in the outcome of the research. To minimize conflicts of interest in human subjects research, institutions and funding agencies often have policies in place that require researchers to disclose any potential conflicts of interest and to take steps to manage or eliminate them. This can include measures such as limiting the researcher\u0026rsquo;s involvement in certain aspects of the study, providing independent oversight of the research, or requiring that the researcher divest themselves of any conflicting financial interests. It is important for researchers to be aware of their own potential conflicts of interest and to be transparent about any that they may have. CITI program also have a specific module on Conflict of Interest which provides guidance on how to identify and manage conflicts of interest in research involving human subjects.\n","date":"2023-01-14T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/ethics_human_research/","section":"posts","tags":["learning","certificate","research","hippa"],"title":"Ethics in Human Research"},{"categories":["analytics","machine-learning","certification","AWS","AWS Certified ML - Specialty exam (MLS-C01)","Data Engineering"],"contents":"Wikipedia search using TFIDF Term Frequecy Inverse Document Frequency please, call, the, number, below, do, not, us, please call, call the, the number, number below, please do, do not, not call, call us\ndimension = [2, 16]\nExample of unigram TFIDF Imports import pandas as pd import numpy as np import os import pyspark from pyspark.sql import SparkSession from pyspark.sql.types import * from pyspark.sql.functions import udf from pyspark.ml.feature import HashingTF, IDF, Tokenizer SparkSession spark = SparkSession.builder \\ .appName(\u0026#39;tfidf\u0026#39;)\\ .config(\u0026#39;spark.jars\u0026#39;, \u0026#39;../jars/snowflake-jdbc-3.13.6.jar, ../jars/spark-snowflake_2.12-2.9.0-spark_3.1.jar\u0026#39;) \\ .getOrCreate() spark.sparkContext.setLogLevel(\u0026#34;WARN\u0026#34;) 22/12/27 13:35:58 WARN Utils: Your hostname, SPMBP136.local resolves to a loopback address: 127.0.0.1; using 192.168.0.101 instead (on interface en6) 22/12/27 13:35:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address 22/12/27 13:35:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Setting default log level to \u0026quot;WARN\u0026quot;. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). file_path = \u0026#34;../datasets/wiki.csv\u0026#34; wiki = spark.read.format(\u0026#34;csv\u0026#34;).option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;).load(file_path) wiki.show() +---+--------------------+-------------------+--------------------+ | ID| Title| Time| Document| +---+--------------------+-------------------+--------------------+ | 12| Anarchism|2008-12-30 06:23:05|\u0026quot;Anarchism (somet...| | 25| Autism|2008-12-24 20:41:05|\u0026quot;Autism is a brai...| | 39| Albedo|2008-12-29 18:19:09|\u0026quot;The albedo of an...| |290| A|2008-12-27 04:33:16|\u0026quot;The letter A is ...| |303| Alabama|2008-12-29 08:15:47|\u0026quot;Alabama (formall...| |305| Achilles|2008-12-30 06:18:01|\u0026quot;thumb\\n\\nIn Gree...| |307| Abraham Lincoln|2008-12-28 20:18:23|\u0026quot;Abraham Lincoln ...| |308| Aristotle|2008-12-29 23:54:48|\u0026quot;Aristotle (Greek...| |309|An American in Paris|2008-09-27 19:29:28|\u0026quot;An American in P...| |324| Academy Award|2008-12-28 17:50:43|\u0026quot;The Academy Awar...| |330| Actrius|2008-05-23 15:24:32|Actrius (Actresse...| |332| Animalia (book)|2008-12-18 11:12:34|thumbAnimalia (IS...| |334|International Ato...|2008-11-21 22:40:20|International Ato...| |336| Altruism|2008-12-27 03:57:17|\u0026quot;Altruism is self...| |339| Ayn Rand|2008-12-30 08:03:06|\u0026quot;Ayn Rand (, – M...| |340| Alain Connes|2008-09-03 13:41:39|Alain Connes (bor...| |344| Allan Dwan|2008-11-14 05:28:58|Allan Dwan (April...| |358| Algeria|2008-12-29 02:54:36|\u0026quot;Algeria (, al-Ja...| |359|List of character...|2008-12-23 20:20:21|\u0026quot;This is a list o...| |569| Anthropology|2008-12-28 23:04:30|\u0026quot;Anthropology (, ...| +---+--------------------+-------------------+--------------------+ only showing top 20 rows wiki.filter(wiki.Document.isNull()).count() 1 wiki = wiki.filter(~wiki.Document.isNull()) wiki.show() +---+--------------------+-------------------+--------------------+ | ID| Title| Time| Document| +---+--------------------+-------------------+--------------------+ | 12| Anarchism|2008-12-30 06:23:05|\u0026quot;Anarchism (somet...| | 25| Autism|2008-12-24 20:41:05|\u0026quot;Autism is a brai...| | 39| Albedo|2008-12-29 18:19:09|\u0026quot;The albedo of an...| |290| A|2008-12-27 04:33:16|\u0026quot;The letter A is ...| |303| Alabama|2008-12-29 08:15:47|\u0026quot;Alabama (formall...| |305| Achilles|2008-12-30 06:18:01|\u0026quot;thumb\\n\\nIn Gree...| |307| Abraham Lincoln|2008-12-28 20:18:23|\u0026quot;Abraham Lincoln ...| |308| Aristotle|2008-12-29 23:54:48|\u0026quot;Aristotle (Greek...| |309|An American in Paris|2008-09-27 19:29:28|\u0026quot;An American in P...| |324| Academy Award|2008-12-28 17:50:43|\u0026quot;The Academy Awar...| |330| Actrius|2008-05-23 15:24:32|Actrius (Actresse...| |332| Animalia (book)|2008-12-18 11:12:34|thumbAnimalia (IS...| |334|International Ato...|2008-11-21 22:40:20|International Ato...| |336| Altruism|2008-12-27 03:57:17|\u0026quot;Altruism is self...| |339| Ayn Rand|2008-12-30 08:03:06|\u0026quot;Ayn Rand (, – M...| |340| Alain Connes|2008-09-03 13:41:39|Alain Connes (bor...| |344| Allan Dwan|2008-11-14 05:28:58|Allan Dwan (April...| |358| Algeria|2008-12-29 02:54:36|\u0026quot;Algeria (, al-Ja...| |359|List of character...|2008-12-23 20:20:21|\u0026quot;This is a list o...| |569| Anthropology|2008-12-28 23:04:30|\u0026quot;Anthropology (, ...| +---+--------------------+-------------------+--------------------+ only showing top 20 rows tokenizer = Tokenizer(inputCol=\u0026#34;Document\u0026#34;, outputCol=\u0026#34;words\u0026#34;) wordsData = tokenizer.transform(wiki) wordsData.show() +---+--------------------+-------------------+--------------------+--------------------+ | ID| Title| Time| Document| words| +---+--------------------+-------------------+--------------------+--------------------+ | 12| Anarchism|2008-12-30 06:23:05|\u0026quot;Anarchism (somet...|[\u0026quot;anarchism, (som...| | 25| Autism|2008-12-24 20:41:05|\u0026quot;Autism is a brai...|[\u0026quot;autism, is, a, ...| | 39| Albedo|2008-12-29 18:19:09|\u0026quot;The albedo of an...|[\u0026quot;the, albedo, of...| |290| A|2008-12-27 04:33:16|\u0026quot;The letter A is ...|[\u0026quot;the, letter, a,...| |303| Alabama|2008-12-29 08:15:47|\u0026quot;Alabama (formall...|[\u0026quot;alabama, (forma...| |305| Achilles|2008-12-30 06:18:01|\u0026quot;thumb\\n\\nIn Gree...|[\u0026quot;thumb\\n\\nin, gr...| |307| Abraham Lincoln|2008-12-28 20:18:23|\u0026quot;Abraham Lincoln ...|[\u0026quot;abraham, lincol...| |308| Aristotle|2008-12-29 23:54:48|\u0026quot;Aristotle (Greek...|[\u0026quot;aristotle, (gre...| |309|An American in Paris|2008-09-27 19:29:28|\u0026quot;An American in P...|[\u0026quot;an, american, i...| |324| Academy Award|2008-12-28 17:50:43|\u0026quot;The Academy Awar...|[\u0026quot;the, academy, a...| |330| Actrius|2008-05-23 15:24:32|Actrius (Actresse...|[actrius, (actres...| |332| Animalia (book)|2008-12-18 11:12:34|thumbAnimalia (IS...|[thumbanimalia, (...| |334|International Ato...|2008-11-21 22:40:20|International Ato...|[international, a...| |336| Altruism|2008-12-27 03:57:17|\u0026quot;Altruism is self...|[\u0026quot;altruism, is, s...| |339| Ayn Rand|2008-12-30 08:03:06|\u0026quot;Ayn Rand (, – M...|[\u0026quot;ayn, rand, (,, ...| |340| Alain Connes|2008-09-03 13:41:39|Alain Connes (bor...|[alain, connes, (...| |344| Allan Dwan|2008-11-14 05:28:58|Allan Dwan (April...|[allan, dwan, (ap...| |358| Algeria|2008-12-29 02:54:36|\u0026quot;Algeria (, al-Ja...|[\u0026quot;algeria, (,, al...| |359|List of character...|2008-12-23 20:20:21|\u0026quot;This is a list o...|[\u0026quot;this, is, a, li...| |569| Anthropology|2008-12-28 23:04:30|\u0026quot;Anthropology (, ...|[\u0026quot;anthropology, (...| +---+--------------------+-------------------+--------------------+--------------------+ only showing top 20 rows hashingTF = HashingTF(inputCol=\u0026#34;words\u0026#34;, outputCol=\u0026#34;rawFeatures\u0026#34;) featuredData = hashingTF.transform(wordsData) featuredData.show() +---+--------------------+-------------------+--------------------+--------------------+--------------------+ | ID| Title| Time| Document| words| rawFeatures| +---+--------------------+-------------------+--------------------+--------------------+--------------------+ | 12| Anarchism|2008-12-30 06:23:05|\u0026quot;Anarchism (somet...|[\u0026quot;anarchism, (som...|(262144,[15157,27...| | 25| Autism|2008-12-24 20:41:05|\u0026quot;Autism is a brai...|[\u0026quot;autism, is, a, ...|(262144,[15,1546,...| | 39| Albedo|2008-12-29 18:19:09|\u0026quot;The albedo of an...|[\u0026quot;the, albedo, of...|(262144,[7853,240...| |290| A|2008-12-27 04:33:16|\u0026quot;The letter A is ...|[\u0026quot;the, letter, a,...|(262144,[6037,942...| |303| Alabama|2008-12-29 08:15:47|\u0026quot;Alabama (formall...|[\u0026quot;alabama, (forma...|(262144,[1797,256...| |305| Achilles|2008-12-30 06:18:01|\u0026quot;thumb\\n\\nIn Gree...|[\u0026quot;thumb\\n\\nin, gr...|(262144,[10758,16...| |307| Abraham Lincoln|2008-12-28 20:18:23|\u0026quot;Abraham Lincoln ...|[\u0026quot;abraham, lincol...|(262144,[2564,460...| |308| Aristotle|2008-12-29 23:54:48|\u0026quot;Aristotle (Greek...|[\u0026quot;aristotle, (gre...|(262144,[2767,356...| |309|An American in Paris|2008-09-27 19:29:28|\u0026quot;An American in P...|[\u0026quot;an, american, i...|(262144,[2366,670...| |324| Academy Award|2008-12-28 17:50:43|\u0026quot;The Academy Awar...|[\u0026quot;the, academy, a...|(262144,[2931,328...| |330| Actrius|2008-05-23 15:24:32|Actrius (Actresse...|[actrius, (actres...|(262144,[6558,674...| |332| Animalia (book)|2008-12-18 11:12:34|thumbAnimalia (IS...|[thumbanimalia, (...|(262144,[2284,609...| |334|International Ato...|2008-11-21 22:40:20|International Ato...|[international, a...|(262144,[847,925,...| |336| Altruism|2008-12-27 03:57:17|\u0026quot;Altruism is self...|[\u0026quot;altruism, is, s...|(262144,[5675,680...| |339| Ayn Rand|2008-12-30 08:03:06|\u0026quot;Ayn Rand (, – M...|[\u0026quot;ayn, rand, (,, ...|(262144,[528,1091...| |340| Alain Connes|2008-09-03 13:41:39|Alain Connes (bor...|[alain, connes, (...|(262144,[154,1595...| |344| Allan Dwan|2008-11-14 05:28:58|Allan Dwan (April...|[allan, dwan, (ap...|(262144,[1578,181...| |358| Algeria|2008-12-29 02:54:36|\u0026quot;Algeria (, al-Ja...|[\u0026quot;algeria, (,, al...|(262144,[3852,492...| |359|List of character...|2008-12-23 20:20:21|\u0026quot;This is a list o...|[\u0026quot;this, is, a, li...|(262144,[14376,19...| |569| Anthropology|2008-12-28 23:04:30|\u0026quot;Anthropology (, ...|[\u0026quot;anthropology, (...|(262144,[57138,10...| +---+--------------------+-------------------+--------------------+--------------------+--------------------+ only showing top 20 rows idf = IDF(inputCol=\u0026#34;rawFeatures\u0026#34;, outputCol=\u0026#34;features\u0026#34;) idfModel = idf.fit(featuredData) rescaledData = idfModel.transform(featuredData) rescaledData.show() 22/12/27 13:36:11 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB +---+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+ | ID| Title| Time| Document| words| rawFeatures| features| +---+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+ | 12| Anarchism|2008-12-30 06:23:05|\u0026quot;Anarchism (somet...|[\u0026quot;anarchism, (som...|(262144,[15157,27...|(262144,[15157,27...| | 25| Autism|2008-12-24 20:41:05|\u0026quot;Autism is a brai...|[\u0026quot;autism, is, a, ...|(262144,[15,1546,...|(262144,[15,1546,...| | 39| Albedo|2008-12-29 18:19:09|\u0026quot;The albedo of an...|[\u0026quot;the, albedo, of...|(262144,[7853,240...|(262144,[7853,240...| |290| A|2008-12-27 04:33:16|\u0026quot;The letter A is ...|[\u0026quot;the, letter, a,...|(262144,[6037,942...|(262144,[6037,942...| |303| Alabama|2008-12-29 08:15:47|\u0026quot;Alabama (formall...|[\u0026quot;alabama, (forma...|(262144,[1797,256...|(262144,[1797,256...| |305| Achilles|2008-12-30 06:18:01|\u0026quot;thumb\\n\\nIn Gree...|[\u0026quot;thumb\\n\\nin, gr...|(262144,[10758,16...|(262144,[10758,16...| |307| Abraham Lincoln|2008-12-28 20:18:23|\u0026quot;Abraham Lincoln ...|[\u0026quot;abraham, lincol...|(262144,[2564,460...|(262144,[2564,460...| |308| Aristotle|2008-12-29 23:54:48|\u0026quot;Aristotle (Greek...|[\u0026quot;aristotle, (gre...|(262144,[2767,356...|(262144,[2767,356...| |309|An American in Paris|2008-09-27 19:29:28|\u0026quot;An American in P...|[\u0026quot;an, american, i...|(262144,[2366,670...|(262144,[2366,670...| |324| Academy Award|2008-12-28 17:50:43|\u0026quot;The Academy Awar...|[\u0026quot;the, academy, a...|(262144,[2931,328...|(262144,[2931,328...| |330| Actrius|2008-05-23 15:24:32|Actrius (Actresse...|[actrius, (actres...|(262144,[6558,674...|(262144,[6558,674...| |332| Animalia (book)|2008-12-18 11:12:34|thumbAnimalia (IS...|[thumbanimalia, (...|(262144,[2284,609...|(262144,[2284,609...| |334|International Ato...|2008-11-21 22:40:20|International Ato...|[international, a...|(262144,[847,925,...|(262144,[847,925,...| |336| Altruism|2008-12-27 03:57:17|\u0026quot;Altruism is self...|[\u0026quot;altruism, is, s...|(262144,[5675,680...|(262144,[5675,680...| |339| Ayn Rand|2008-12-30 08:03:06|\u0026quot;Ayn Rand (, – M...|[\u0026quot;ayn, rand, (,, ...|(262144,[528,1091...|(262144,[528,1091...| |340| Alain Connes|2008-09-03 13:41:39|Alain Connes (bor...|[alain, connes, (...|(262144,[154,1595...|(262144,[154,1595...| |344| Allan Dwan|2008-11-14 05:28:58|Allan Dwan (April...|[allan, dwan, (ap...|(262144,[1578,181...|(262144,[1578,181...| |358| Algeria|2008-12-29 02:54:36|\u0026quot;Algeria (, al-Ja...|[\u0026quot;algeria, (,, al...|(262144,[3852,492...|(262144,[3852,492...| |359|List of character...|2008-12-23 20:20:21|\u0026quot;This is a list o...|[\u0026quot;this, is, a, li...|(262144,[14376,19...|(262144,[14376,19...| |569| Anthropology|2008-12-28 23:04:30|\u0026quot;Anthropology (, ...|[\u0026quot;anthropology, (...|(262144,[57138,10...|(262144,[57138,10...| +---+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+ only showing top 20 rows Search def search_article(keyword): # get the hash val from keyword schema = StructType([StructField(\u0026#34;words\u0026#34;, ArrayType(StringType()))]) temp = spark.createDataFrame(([[[keyword]]]), schema).toDF(\u0026#34;words\u0026#34;) temp_unhashed = hashingTF.transform(temp).select(\u0026#34;rawFeatures\u0026#34;).collect() val = int(temp_unhashed[0].rawFeatures.indices[0]) # termExtractor = udf(lambda x:float(x[val]), FloatType()) final = rescaledData.withColumn(\u0026#39;score\u0026#39;, termExtractor(rescaledData.features)) final = final.filter(\u0026#34;score\u0026gt;0\u0026#34;).orderBy(\u0026#34;score\u0026#34;, ascending=False) return final.select(\u0026#39;ID\u0026#39;, \u0026#39;Title\u0026#39;, \u0026#39;score\u0026#39;) search_article(\u0026#39;mystery\u0026#39;).show() 22/12/27 13:36:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB [Stage 11:===================\u0026gt; (1 + 2) / 3] +----+--------------------+--------+ | ID| Title| score| +----+--------------------+--------+ | 984| Agatha Christie|5.521461| | 986| The Plague|5.521461| |1307|The Alan Parsons ...|5.521461| +----+--------------------+--------+ search_article(\u0026#39;comic\u0026#39;).show() 22/12/27 13:36:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB +----+--------------------+----------+ | ID| Title| score| +----+--------------------+----------+ | 931|The Amazing Spide...|14.4849415| |2101| Asterix| 9.656628| |1549| Agathon| 9.656628| |2023| Aeschylus| 9.656628| |1028| Aristophanes| 9.656628| |1614| Alexis| 4.828314| |1784| Athenian democracy| 4.828314| +----+--------------------+----------+ search_article(\u0026#39;revolution\u0026#39;).show() 22/12/27 13:36:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB +----+--------------------+---------+ | ID| Title| score| +----+--------------------+---------+ |1973| American Revolution|12.052151| |2273| AFC Ajax|4.0173836| | 339| Ayn Rand|4.0173836| | 572|Agricultural science|4.0173836| | 771|American Revoluti...|4.0173836| | 915| Andrey Markov|4.0173836| | 930| Alvin Toffler|4.0173836| |1030| Austrian School|4.0173836| |1057| Anatole France|4.0173836| |1192| Artistic revolution|4.0173836| |1316| Annales School|4.0173836| |1676|Alfonso XII of Spain|4.0173836| |1363| André-Marie Ampère|4.0173836| |2075| Aircraft hijacking|4.0173836| |1784| Athenian democracy|4.0173836| |1844| Archimedes|4.0173836| |2070|Act of Settlement...|4.0173836| +----+--------------------+---------+ search_article(\u0026#39;football\u0026#39;).show() 22/12/27 13:36:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB +----+--------------------+---------+ | ID| Title| score| +----+--------------------+---------+ |2273| AFC Ajax|54.596165| |2357|American Football...|46.196754| |2174| Arsenal F.C.|29.397936| |2358| A.S. Roma| 25.19823| |2102| Arizona Cardinals|20.998526| |2103| Atlanta Falcons| 16.79882| | 615|American Football...| 16.79882| | 925|Alumni Athletic Club|12.599115| |2289| AZ (football club)| 4.199705| |2310| Arthur Miller| 4.199705| |1797| Acre| 4.199705| |2363|Alessandro Scarlatti| 4.199705| |2382| Aalen| 4.199705| |1016| Achill Island| 4.199705| +----+--------------------+---------+ search_article(\u0026#39;emirates\u0026#39;).show() 22/12/27 13:36:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB +----+------------+--------+ | ID| Title| score| +----+------------+--------+ |2174|Arsenal F.C.|6.214608| +----+------------+--------+ search_article(\u0026#39;the\u0026#39;).show() 22/12/27 13:36:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB +----+--------------------+---------+ | ID| Title| score| +----+--------------------+---------+ |1854| Geography of Africa|56.093544| |2273| AFC Ajax|43.326492| |2023| Aeschylus|41.968296| |1216| Athens|30.287798| | 717| Alberta|26.213207| |2358| A.S. Roma|23.904272| | 841| Attila the Hun|23.360992| |1285|Geography of Alabama|23.089354| |2338|Rise and Fall of ...|21.323696| |1440| Abydos, Egypt|19.150581| | 904| Aluminium| 18.87894| |1905| Ambush|18.199842| |1962| Apparent magnitude|17.928204| |1557|Agrippina the You...|17.792383| |1613| Alexios I Komnenos|17.792383| |1234| Acoustic theory|17.520744| |2064| Antonio Canova|15.619268| |1686| Alfonso V of Aragon| 15.07599| |1451|APL (programming ...| 15.07599| |2274|Arthur Stanley Ed...| 14.80435| +----+--------------------+---------+ only showing top 20 rows ","date":"2022-12-25T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/tfidf/","section":"posts","tags":["certification","tfidf"],"title":"Term Frequecy Inverse Document Frequency (TFIDF)"},{"categories":["georgia-tech","capstone-project"],"contents":"Flight delay prediction and exploration in the US As a part of Georgia Tech OMSA, CSE 6242: Data and Visual Analytics Poster Presentation\nPoster\nFinal Report\n","date":"2022-12-15T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/airlines_delay/","section":"posts","tags":["correlation","dimensionality reduction","PCA","Random Forest","Deep Learning","XGBoost","AdaBoost","analytics","group-project","machine-learning","random-forest","XGBoost","data-viz"],"title":"Flight delay prediction and exploration in the United States"},{"categories":["analytics","data-viz","georgia-tech","capstone-project"],"contents":"","date":"2022-04-30T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/mushrooms/","section":"posts","tags":["correlation","dimensionality reduction","PCA","MCA","t-SNE","Correspondence Analysis","Contingency Table","Cramers V","Decision Tree"],"title":"Detecting the edibility of Mushrooms in the wild"},{"categories":null,"contents":"Certifications Name Issuing Organization Issue date Expiry date Credential URL Group 2 Social / Behavioral Research Investigators and Key Personnel CITI Program Jan, 2023 Jan, 2026 link AWS Certified Machine Learning Specialty 2023-Hands On! Udemy January 4, 2023 NA link D3.js Essential Training for Data Scientists Linkedin Learning September 12, 2022 NA link Academy Accreditation - Databricks Lakehouse Fundamentals Databricks August 24, 2022 NA link Mathematical Thinking with Terence Tao Masterclass July, 2022 NA Predictive Analytics Essential Training: Estimating and Ensuring ROI Linkedin Learning June, 2022 NA link Big Data with PySpark Track Datacamp March 4, 2022 NA link Building Recommendations Engines with PySpark Datacamp March 4, 2022 NA link Machine Learning with PySpark Datacamp March 2, 2022 NA link Feature Engineering with PySpark Datacamp March 1, 2022 NA link Cleaning Data with PySpark Datacamp Feb 16, 2022 NA link Big Data Fundamentals with PySpark Datacamp Jan 10, 2022 NA link Introduction to PySpark Datacamp Jan 05, 2022 NA link Intermediate SQL Datacamp July 20, 2021 NA link Joining Data in SQL Datacamp July 13, 2021 NA link Introduction to SQL Datacamp July 08, 2021 NA link Reporting with R Markdown Datacamp June 15, 2021 NA link Introduction to Data Visualization with ggplot2 Datacamp June 14, 2021 NA link Joining Data with dplyr Datacamp June 07, 2021 NA link Data Manipulation with dplyr Datacamp June 01, 2021 NA link Introduction to the Tidyverse Datacamp June 01, 2021 NA link Intermediate R Datacamp May 28, 2021 NA link Introduction to R Datacamp May 27, 2021 NA link Intro to AI Ethics Kaggle May 20, 2021 NA link Computer Vision Kaggle February 28, 2021 NA link Natural Language Processing Kaggle February 23, 2021 NA link Intro to Deep Learning Kaggle January 11, 2021 NA link Geospatial Analysis Kaggle August 25, 2020 NA link Machine Learning Explainability Kaggle August 25, 2020 NA link Intermediate Machine Learning Kaggle August 24, 2020 NA link Advanced SQL Kaggle August 20, 2020 NA link Intro to SQL Kaggle August 19, 2020 NA link Data Visualization Kaggle August 18, 2020 NA link Data Cleaning Kaggle August 17, 2020 NA link Pandas Kaggle August 11, 2020 NA link Intro to Machine Learning Kaggle August 7, 2020 NA link Python Kaggle August 3, 2020 NA link AWS Fundamentals in AWS Machine Learning Udacity July 5, 2020 NA link ","date":"2022-01-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/certifications/","section":"","tags":null,"title":"Certifications"},{"categories":["cheers hospital","data viz","analytics"],"contents":"Cheers AI Demo for Diabetic Retinopathy and Glaucoma Detection Features Efficient Prediction Models Efficient models trained on Inception-v3, with weightage on recall. Powerful hospital-MIS to create and track patient, and patient\u0026rsquo;s historical predictions. Inputs reviewed by opthalmologists and added to training. Diabetic Retinopathy Diabetic Retinopathy is an eye illness caused by diabetes that may lead to vision impairment and even to blindness if it isn\u0026rsquo;t identified and treated early. Of the estimated 422 million diabetics globally, more than 148 million have DR and 48 million have Vision Threating DR (VTDR).\nHowever, because of insufficient specialists and eye care health workers globally as well as locally to screen everyone at risk, the situation seems acute especially in developing countries like Nepal. Besides, Nepal has difficult geographical terrain and people living in remorse remote areas with limited or no access to clinics and screening facilities making the condition even worse.\nGlaucoma Glaucoma is a diverse group of disorders representing the second prominent cause of blindness. It has already affected 91 million individuals all over the world. It has multiple risk factors such as older age, elevated intraocular pressure (IOP), and thinner central corneal thickness etc. However, one or more of these risk factors may or may not develop glaucoma making it difficult for accurate prediction of the disease. Additionally, since glaucoma can be asymptomatic, its detection before significant vision loss is critical. Hence, automated methods for predicting glaucoma could have a significant impact.\nAn intuitive app Easy to use, access managed platform, with the primary focus on providing assistance to our opthalmologists.\nSteps involved in Research, Model Creation, and Deployment Glaucoma Prediction What worked? (90% accuracy) densenet sequential with ben on himanchu dataset, using NLLLoss criterion, Adam optimizer Limitation very much dependent on dataset disk extraction is good but is very subjective to the dataset trained on very small dataset Preliminary understand the difference between possibility of glaucoma by classification (vs measurements) Preprocessing ben transformation extract disk from fundus images improve extraction algorithms perform EDA on disk image to find troubling images (cases where crop does not work) convert python function to extract disk to torch transform class (failed) transformation to disk during training failed. create a disk dataset before training the model. train on new dataset with and without ben transformation handle imbalanced class with class weighting convert Kaggle dataset to the format that we have templated our notebooks with for kaggle dataset get disks using new algorithm Obseverations in regards to disk generation extraction of disk does not help (too many vague areas left unfilled) however, cropping shows very good promise but, cropping requires somewhat similar of fundus images Datasets find datasets https://deepblue.lib.umich.edu/data/concern/data_sets/3b591905z, https://www.kaggle.com/andrewmvd/ocular-disease-recognition-odir5k create a dataset from Magrabia create a dataset from Messidor create a dataset from Ocular Disease Recognition create EDA on non measurement dataset (Ocular Disease Recognition) create a dataset from ocular disease recognition to include normal and glaucoma images (Kaggle dataset, custom generated, filtered)https://www.kaggle.com/sshikamaru/glaucoma-detection?select=glaucoma.csv train on Kaggle dataset (without changing anything) Training inception v3 with and without ben on ocular, kaggle, and himanchu dataset inception v3 with ben on ocular, kaggle, and himanchu dataset (disk extracted, normal, and cropped dataset) densenet linear with ben on ocular, kaggle, and himanchu dataset densenet linear with ben on ocular, kaggle, and himanchu dataset (disk extracted, normal, and cropped dataset) densenet sequential with ben on ocular, kaggle, and himanchu dataset densenet sequential with ben on ocular, kaggle, and himanchu dataset (disk extracted, normal, and cropped dataset) add datasets from cheers for testing add datasets from cheers for training Diabetic Retinopathy Prediction What worked? (90% accuracy) Large dataset from EyePACS (Kaggle competition used training (30%) and testing data (70%) from Kaggle. After the competition, the labels were published). Flipped the ratios for our use case. Remove out of focus images Remove too bright, and too dark images. Link to clean dataset https://www.kaggle.com/ayushsubedi/drunstratified To handle class imbalanced issue, used weighted random samplers. Undersampling to match no of images in the least class (4) did not work. Pickled weights for future use. Ben Graham transformation and augmentations Inception v3 fine tuning, with aux logits trained (better results compared to other architecture) Perform EDA on inference to observe what images were causing issues Removed the images and created another dataset (Link to the new dataset https://www.kaggle.com/ayushsubedi/cleannonstratifieddiabeticretinopathy See 5, 6, and 7 Datasets Binary Stratified (cleaned): https://drive.google.com/drive/folders/12-60Gm7c_TMu1rhnMhSZjrkSqqAuSsQf?usp=sharing Categorical Stratified (cleaned): https://drive.google.com/drive/folders/1-A_Mx9GdeUwCd03TUxUS3vwcutQHFFSM?usp=sharing Non Stratified (cleaned): https://www.kaggle.com/ayushsubedi/drunstratified Recleaned Non Stratified: https://www.kaggle.com/ayushsubedi/cleannonstratifieddiabeticretinopathy\nPriliminary https://www.youtube.com/watch?v=VIrkurR446s\u0026amp;ab_channel=khanacademymedicine What is diabetic retinopathy? collect all previous analysis notebooks conduct preliminary EDA (for balanced dataset, missing images etc) create balanced test train split for DR (stratify) store the dataset in drive for colab identify a few research papers, create a file to store subsequently found research papers identify right technology stack to use (for ML, training, PM, model versioning, stage deployment, actual deployment) perform basic augmentation create a version 0 base model apply a random transfer learning model create a metric for evaluation store the model in zenodo, or find something for version control create a model that takes image as an input create a streamlit app that reads model streamlit app to upload and test prediction test deployment to free tier heroku identify gaps create priliminary test set create folder structures for saved model in the drive figure out a way to move files from kaggle to drive (without download/upload) research saving model (the frugal way) research saving model to google drive after each epoch so that during unforseen interuptions, the training of the model can be continued Resource upgrade to 25GB RAM in Google Colab possibly w/ Tesla P100 GPU upgrade to Colab Pro Baseline medicmind grading (accuracy: 0.8) medicmind classification (0.47) Transfer Learning resnet alexnet vgg squeezenet densenet inception efficient net Dataset clean images create a backup of primary dataset (zip so that kaggle kernels can consume them too) find algorithms to detect black/out of focus images identify correct threshold for dark and out of focus images remove black images remove out of focus images create a stratified dataset with 2015 data only (convert train and test both to train and use), remove black images and out of focus images (also create test set) create non-stratified dataset with 2015 clean data only (train, test, valid) (upload in kaggle if google drive full) create a binary dataset (train, test, valid) create confusion matrices (train, test, valid) after clean up (dark and blurry) the model is confusing labels 0 and 1 as 2, is this due to disturbance in image in 0. concluded that the result is due to the model not capturing class 0 enough (due to undersampling) Inference create a csv with preds probability and real label calculate recall, precision, accuracy, confusion matrix identify different prediction issues relationship between difference in preds and accuracy inference issue: labels 0 being predicted as 4 inference issue: Check images from Grade 2, 3 being predicted as Grade 0 inference issue: Check images from Grade 4 being predicted as Grade 0 inference issue: Check images from Grade 0 being predicted as Grade 4 inference issue: A significant Grade 2 is being predicted as Grade 0 inference issue: More than 50% of Grade 1 is being predicted as Grade 0 create a new dataset Model Improvement research kaggle winning augmentation for DR research appropriate augmentation: optical distortion, grid distortion, piecewise affine transform, horizontal flip, vertical flip, random rotation, random shift, random scale, a shift of RGB values, random brightness and contrast, additive Gaussian noise, blur, sharpening, embossing, random gamma, and cutout train on various pretrained models or research which is supposed to be ideal for this case https://pytorch.org/vision/stable/models.html create several neural nets (test different layers) experiment with batch size Reducing lighting-condition effects Cropping uninformative area Create custom dataloader based on ben graham kaggle winning strategy finetune vs feature extract oversample undersample add specificity and sensitivity to indicators create train loss and valid loss charts test regression models (treat this as a grading problem) pickle weights Additional Models check if left/right eye classification model is required Additional datasets make datasets more extensive (add test dataset with recoverd labels to train dataset 2015) add APTOS dataset request labelled datasets from cheers add datasets from cheers for testing add datasets from cheers for training Test datasets find datasets for testing (dataset apart from APTOS and EyePACS) update folder structures to match our use case find dataset for testing after making sure old test datasets are not in vaid/train (4 will be empty) Concepts/Research Papers read reports from kaggle competition winning authors Deep Learning Approach to Diabetic Retinopathy Detection https://arxiv.org/pdf/2003.02261.pdf Google research https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45732.pdf Nature article https://www.nature.com/articles/s41746-019-0172-3 https://deim.urv.cat/~itaka/itaka2/PDF/acabats/PhD_Thesis/TESI_doctoral_Jordi_De_la_Torre.pdf what can go wrong https://yerevann.github.io/2015/08/17/diabetic-retinopathy-detection-contest-what-we-did-wrong/ https://arxiv.org/pdf/1902.07208.pdf ","date":"2022-01-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/cheers_ai_demo/","section":"posts","tags":["python","ml","ai","nepal","flask"],"title":"Diabetic Retinopathy and Glaucoma Detection (Cheers AI Demo)"},{"categories":["georgia tech"],"contents":"The Beginner\u0026rsquo;s Trap The stages in the analytics process is filled with moments of success and failures. There are some instant gratifications during the process, where a begginer like myself might construe a non success as success, due to some kind of judgement error.\nThis project will go through one of those instances and discuss some of the things I need to keep in mind so that I (and people new to analytics) do not make these mistakes.\nThe dataset is from https://www.kaggle.com/sanjeetsinghnaik/most-expensive-footballers-2021\nSince learning is a never ending process, I will be updating the notebook (or creating a variation of it) as I discover more best practices. So, consider this to be an excerise on SOTA (State of the art) POC (proof of concept).\nSteps involved:\nIntroduction Exploratory Data Analysis Feature Engineering Linear Regression Decision Tree Regression Validation Conclusion 1. Introduction The description of the dataset from Kaggle:\nThis file consists of data of Top 500 Most Expensive Footballer In 2021. The data is according to the prices listed in transfer market along with data like goals, assists, matches, age, etc.\nThe question we are trying to answer is:\n** Based on different predictors (goals, assists, matches, age etc.) is it possible to predict the market value of footballers? **\nThe source of the image above is the Kaggle repository hosting the dataset.\nImports import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set(rc={\u0026#39;figure.figsize\u0026#39;:(12, 6)}) from matplotlib.ticker import MaxNLocator import numpy as np from sklearn.linear_model import LinearRegression from sklearn.tree import DecisionTreeRegressor from sklearn.model_selection import cross_val_score The dataset is in the file players_data.csv\ndf = pd.read_csv(\u0026#39;players_data.csv\u0026#39;) Let us take a look at the columns. We have a few ways of doing this but I always found using list easy.\nlist(df) ['Name', 'Position', 'Age', 'Markey Value In Millions(£)', 'Country', 'Club', 'Matches', 'Goals', 'Own Goals', 'Assists', 'Yellow Cards', 'Second Yellow Cards', 'Red Cards', 'Number Of Substitute In', 'Number Of Substitute Out'] Since we know the dataset is a ranking of 500 highest paid footballers, the expected rows should be 500. And the number of columns is 15\ndf.shape (500, 15) Let\u0026rsquo;s look at the first few rows.\ndf.head() Name Position Age Markey Value In Millions(£) Country Club Matches Goals Own Goals Assists Yellow Cards Second Yellow Cards Red Cards Number Of Substitute In Number Of Substitute Out 0 Kylian Mbappé Centre-Forward 22 144.0 France Paris Saint-Germain 16 7 0 11 3 0 0 0 8 1 Erling Haaland Centre-Forward 21 135.0 Norway Borussia Dortmund 10 13 0 4 1 0 0 0 1 2 Harry Kane Centre-Forward 28 108.0 England Tottenham Hotspur 16 7 0 2 2 0 0 2 2 3 Jack Grealish Left Winger 26 90.0 England Manchester City 15 2 0 3 1 0 0 2 8 4 Mohamed Salah Right Winger 29 90.0 Egypt Liverpool FC 15 15 0 6 1 0 0 0 3 Let\u0026rsquo;s look at the last few rows.\ndf.tail() Name Position Age Markey Value In Millions(£) Country Club Matches Goals Own Goals Assists Yellow Cards Second Yellow Cards Red Cards Number Of Substitute In Number Of Substitute Out 495 Giorgian de Arrascaeta Attacking Midfield 27 16.2 Uruguay Clube de Regatas do Flamengo 0 0 0 0 0 0 0 0 0 496 Ayoze Pérez Second Striker 28 16.2 Spain Leicester City 8 1 0 3 0 0 1 2 5 497 Alex Meret Goalkeeper 24 16.2 Italy SSC Napoli 5 0 0 0 0 0 0 0 0 498 Duje Caleta-Car Centre-Back 25 16.2 Croatia Olympique Marseille 8 0 0 0 2 0 0 0 2 499 Aritz Elustondo Centre-Back 27 16.2 Spain Real Sociedad 15 3 0 1 4 0 0 1 1 2. Exploratory Data Analysis Let\u0026rsquo;s use Pandas built in function to generate descriptive statistics\ndf.describe() Age Markey Value In Millions(£) Matches Goals Own Goals Assists Yellow Cards Second Yellow Cards Red Cards Number Of Substitute In Number Of Substitute Out count 500.000000 500.000000 500.000000 500.000000 500.000000 500.00000 500.000000 500.000000 500.000000 500.000000 500.000000 mean 24.968000 31.537800 12.396000 2.160000 0.030000 1.51200 1.592000 0.036000 0.046000 2.394000 3.744000 std 3.165916 17.577697 4.342453 2.880102 0.170758 1.85276 1.445585 0.186477 0.209695 2.517825 3.293046 min 16.000000 16.200000 0.000000 0.000000 0.000000 0.00000 0.000000 0.000000 0.000000 0.000000 0.000000 25% 23.000000 19.800000 10.000000 0.000000 0.000000 0.00000 0.000000 0.000000 0.000000 0.000000 1.000000 50% 25.000000 25.200000 13.000000 1.000000 0.000000 1.00000 1.000000 0.000000 0.000000 2.000000 3.000000 75% 27.000000 36.000000 16.000000 3.000000 0.000000 2.00000 2.000000 0.000000 0.000000 3.250000 6.000000 max 36.000000 144.000000 24.000000 23.000000 1.000000 12.00000 7.000000 1.000000 1.000000 13.000000 20.000000 Let us look into the data types for each of the columns\ndf.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 500 entries, 0 to 499 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Name 500 non-null object 1 Position 500 non-null object 2 Age 500 non-null int64 3 Markey Value In Millions(£) 500 non-null float64 4 Country 500 non-null object 5 Club 500 non-null object 6 Matches 500 non-null int64 7 Goals 500 non-null int64 8 Own Goals 500 non-null int64 9 Assists 500 non-null int64 10 Yellow Cards 500 non-null int64 11 Second Yellow Cards 500 non-null int64 12 Red Cards 500 non-null int64 13 Number Of Substitute In 500 non-null int64 14 Number Of Substitute Out 500 non-null int64 dtypes: float64(1), int64(10), object(4) memory usage: 58.7+ KB Count of missing values\ndf.isna().sum() Name 0 Position 0 Age 0 Markey Value In Millions(£) 0 Country 0 Club 0 Matches 0 Goals 0 Own Goals 0 Assists 0 Yellow Cards 0 Second Yellow Cards 0 Red Cards 0 Number Of Substitute In 0 Number Of Substitute Out 0 dtype: int64 df.isna().sum().sum() 0 We do not have any values missing from the dataset.\ndf.groupby(\u0026#39;Club\u0026#39;).size().sort_values(ascending=False).head(10) Club Manchester United 19 Manchester City 18 Chelsea FC 16 Tottenham Hotspur 16 Real Madrid 16 Paris Saint-Germain 16 RB Leipzig 15 Arsenal FC 15 Liverpool FC 15 Atlético de Madrid 15 dtype: int64 ax = sns.countplot(x=\u0026#39;Club\u0026#39;, data=df, order=df.Club.value_counts().iloc[:10].index) ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) ax.yaxis.set_major_locator(MaxNLocator(integer=True)) ax.set(xlabel=\u0026#39;Club\u0026#39;, ylabel=\u0026#39;Count of players\u0026#39;) plt.title(\u0026#34;Top 10 Clubs based on player representation (count)\u0026#34;) plt.show() club_grouped = df.groupby([\u0026#39;Club\u0026#39;])[[\u0026#39;Markey Value In Millions(£)\u0026#39;]].sum().sort_values(by=\u0026#39;Markey Value In Millions(£)\u0026#39;, ascending=False).head(10) club_grouped Markey Value In Millions(£) Club Manchester City 940.5 Paris Saint-Germain 775.8 Manchester United 760.5 Chelsea FC 709.2 Bayern Munich 685.8 Liverpool FC 681.3 Atlético de Madrid 616.5 Real Madrid 594.0 Tottenham Hotspur 536.4 Juventus FC 450.0 ax = sns.barplot(x=club_grouped.index, y=club_grouped[\u0026#39;Markey Value In Millions(£)\u0026#39;]) ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) ax.set(xlabel=\u0026#39;Club\u0026#39;, ylabel=\u0026#39;Markey Value In Millions(£)\u0026#39;) plt.title(\u0026#34;Top 10 Clubs based on total market value\u0026#34;) plt.show() df.groupby(\u0026#39;Country\u0026#39;).size().sort_values(ascending=False).head(10) Country England 67 France 58 Spain 52 Brazil 41 Germany 29 Portugal 26 Italy 26 Argentina 22 Netherlands 17 Belgium 14 dtype: int64 ax = sns.countplot(x=\u0026#39;Country\u0026#39;, data=df, order=df.Country.value_counts().iloc[:10].index) ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) ax.yaxis.set_major_locator(MaxNLocator(integer=True)) ax.set(xlabel=\u0026#39;Country\u0026#39;, ylabel=\u0026#39;Count of players\u0026#39;) plt.title(\u0026#34;Top 10 Country based on player representation (count)\u0026#34;) plt.show() country_grouped = df.groupby([\u0026#39;Country\u0026#39;])[[\u0026#39;Markey Value In Millions(£)\u0026#39;]].sum().sort_values(by=\u0026#39;Markey Value In Millions(£)\u0026#39;, ascending=False).head(10) country_grouped Markey Value In Millions(£) Country England 2248.2 France 1895.4 Spain 1565.1 Brazil 1275.3 Germany 1005.3 Portugal 890.1 Italy 854.1 Argentina 650.7 Netherlands 571.5 Belgium 522.9 ax = sns.barplot(x=country_grouped.index, y=country_grouped[\u0026#39;Markey Value In Millions(£)\u0026#39;]) ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) ax.set(xlabel=\u0026#39;Country\u0026#39;, ylabel=\u0026#39;Markey Value In Millions(£)\u0026#39;) plt.title(\u0026#34;Top 10 Country based on total market value\u0026#34;) plt.show() df.groupby(\u0026#39;Position\u0026#39;).size().sort_values(ascending=False) Position Centre-Back 87 Central Midfield 74 Centre-Forward 70 Right Winger 48 Left Winger 46 Attacking Midfield 41 Defensive Midfield 41 Right-Back 30 Left-Back 23 Goalkeeper 19 Left Midfield 8 Second Striker 8 Right Midfield 5 dtype: int64 ax = sns.countplot(x=\u0026#39;Position\u0026#39;, data=df, order=df.Position.value_counts().iloc[:10].index) ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) ax.yaxis.set_major_locator(MaxNLocator(integer=True)) ax.set(xlabel=\u0026#39;Position\u0026#39;, ylabel=\u0026#39;Count of players\u0026#39;) plt.title(\u0026#34;Top 10 Position based on player representation (count)\u0026#34;) plt.show() position_grouped = df.groupby([\u0026#39;Position\u0026#39;])[[\u0026#39;Markey Value In Millions(£)\u0026#39;]].sum().sort_values(by=\u0026#39;Markey Value In Millions(£)\u0026#39;, ascending=False).head(10) position_grouped Markey Value In Millions(£) Position Centre-Back 2583.9 Central Midfield 2421.9 Centre-Forward 2369.7 Left Winger 1647.0 Right Winger 1461.6 Attacking Midfield 1332.0 Defensive Midfield 1275.3 Right-Back 784.8 Left-Back 704.7 Goalkeeper 585.9 ax = sns.barplot(x=position_grouped.index, y=position_grouped[\u0026#39;Markey Value In Millions(£)\u0026#39;]) ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) ax.set(xlabel=\u0026#39;Position\u0026#39;, ylabel=\u0026#39;Markey Value In Millions(£)\u0026#39;) plt.title(\u0026#34;Top 10 Position based on total market value\u0026#34;) plt.show() 3. Feature Engineering Since we only have 500 rows of data, 10 different positions, let us aggregate some of these positions together.\nI created a simple position mapping and transformed it so that it was possible to use map\nposition_mapping = { \u0026#34;forward\u0026#34; : [\u0026#39;Centre-Forward\u0026#39;, \u0026#39;Second Striker\u0026#39;], \u0026#34;midfield\u0026#34; : [\u0026#39;Central Midfield\u0026#39;, \u0026#39;Attacking Midfield\u0026#39;, \u0026#39;Defensive Midfield\u0026#39;, \u0026#39;Left Midfield\u0026#39;, \u0026#39;Right Midfield\u0026#39;, \u0026#39;Right Winger\u0026#39;, \u0026#39;Left Winger\u0026#39;], \u0026#34;back\u0026#34; : [\u0026#39;Centre-Back\u0026#39;, \u0026#39;Right-Back\u0026#39;, \u0026#39;Left-Back\u0026#39;] } The only position we have not mapped here is Goalkeeper, which we will remove later on.\nmapping_dict = {} for key in position_mapping: for item in position_mapping[key]: mapping_dict[item] = key mapping_dict {'Centre-Forward': 'forward', 'Second Striker': 'forward', 'Central Midfield': 'midfield', 'Attacking Midfield': 'midfield', 'Defensive Midfield': 'midfield', 'Left Midfield': 'midfield', 'Right Midfield': 'midfield', 'Right Winger': 'midfield', 'Left Winger': 'midfield', 'Centre-Back': 'back', 'Right-Back': 'back', 'Left-Back': 'back'} I could have just typed in the mapping_dict directly but this felt more natural.\ndf[\u0026#39;Position_Agg\u0026#39;] = df.Position.map(mapping_dict) df Name Position Age Markey Value In Millions(£) Country Club Matches Goals Own Goals Assists Yellow Cards Second Yellow Cards Red Cards Number Of Substitute In Number Of Substitute Out Position_Agg 0 Kylian Mbappé Centre-Forward 22 144.0 France Paris Saint-Germain 16 7 0 11 3 0 0 0 8 forward 1 Erling Haaland Centre-Forward 21 135.0 Norway Borussia Dortmund 10 13 0 4 1 0 0 0 1 forward 2 Harry Kane Centre-Forward 28 108.0 England Tottenham Hotspur 16 7 0 2 2 0 0 2 2 forward 3 Jack Grealish Left Winger 26 90.0 England Manchester City 15 2 0 3 1 0 0 2 8 midfield 4 Mohamed Salah Right Winger 29 90.0 Egypt Liverpool FC 15 15 0 6 1 0 0 0 3 midfield ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 495 Giorgian de Arrascaeta Attacking Midfield 27 16.2 Uruguay Clube de Regatas do Flamengo 0 0 0 0 0 0 0 0 0 midfield 496 Ayoze Pérez Second Striker 28 16.2 Spain Leicester City 8 1 0 3 0 0 1 2 5 forward 497 Alex Meret Goalkeeper 24 16.2 Italy SSC Napoli 5 0 0 0 0 0 0 0 0 NaN 498 Duje Caleta-Car Centre-Back 25 16.2 Croatia Olympique Marseille 8 0 0 0 2 0 0 0 2 back 499 Aritz Elustondo Centre-Back 27 16.2 Spain Real Sociedad 15 3 0 1 4 0 0 1 1 back 500 rows × 16 columns\nNow we can remove goalkeepers (which are NaNs because of missing mapping)\ndf.dropna(inplace=True) Let\u0026rsquo;s create similar charts for the new column as well\ndf.groupby(\u0026#39;Position_Agg\u0026#39;).size().sort_values(ascending=False) Position_Agg midfield 263 back 140 forward 78 dtype: int64 ax = sns.countplot(x=\u0026#39;Position_Agg\u0026#39;, data=df, order=df.Position_Agg.value_counts().iloc[:10].index) ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) ax.yaxis.set_major_locator(MaxNLocator(integer=True)) ax.set(xlabel=\u0026#39;Position\u0026#39;, ylabel=\u0026#39;Count of players\u0026#39;) plt.title(\u0026#34;Top 10 Position based on player representation\u0026#34;) plt.show() position_agg_grouped = df.groupby([\u0026#39;Position_Agg\u0026#39;])[[\u0026#39;Markey Value In Millions(£)\u0026#39;]].sum().sort_values(by=\u0026#39;Markey Value In Millions(£)\u0026#39;, ascending=False).head(10) position_agg_grouped Markey Value In Millions(£) Position_Agg midfield 8460.0 back 4073.4 forward 2649.6 ax = sns.barplot(x=position_agg_grouped.index, y=position_agg_grouped[\u0026#39;Markey Value In Millions(£)\u0026#39;]) ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) ax.set(xlabel=\u0026#39;Position\u0026#39;, ylabel=\u0026#39;Markey Value In Millions(£)\u0026#39;) plt.title(\u0026#34;Top 10 Position based on total market value\u0026#34;) plt.show() 4. Linear Regression First of all, let us only focus on the midfield players. Midfield is the largest group of players we have, and it might make more sense to use regression the same group of players.\ndf_mid = df[df.Position_Agg==\u0026#39;midfield\u0026#39;] df_mid.reset_index(drop=True, inplace=True) list(df_mid) ['Name', 'Position', 'Age', 'Markey Value In Millions(£)', 'Country', 'Club', 'Matches', 'Goals', 'Own Goals', 'Assists', 'Yellow Cards', 'Second Yellow Cards', 'Red Cards', 'Number Of Substitute In', 'Number Of Substitute Out', 'Position_Agg'] df_mid = df_mid[[\u0026#39;Age\u0026#39;, \u0026#39;Goals\u0026#39;, \u0026#39;Assists\u0026#39;, \u0026#39;Markey Value In Millions(£)\u0026#39;]] df_mid = df_mid.apply(lambda x: x/x.max(), axis=0) df_mid Age Goals Assists Markey Value In Millions(£) 0 0.764706 0.133333 0.3 1.00 1 0.852941 1.000000 0.6 1.00 2 0.882353 0.200000 0.1 1.00 3 0.852941 0.200000 0.3 1.00 4 0.617647 0.000000 0.0 0.90 ... ... ... ... ... 258 0.676471 0.066667 0.1 0.18 259 0.764706 0.000000 0.0 0.18 260 0.823529 0.000000 0.1 0.18 261 0.764706 0.200000 0.1 0.18 262 0.794118 0.000000 0.0 0.18 263 rows × 4 columns\nX = df_mid.iloc[:, :-1] y = df_mid.iloc[:, -1] linear_regressor = LinearRegression().fit(X, y) linear_regressor.score(X, y) 0.06775382996413992 linear_regressor.predict(X) array([0.37728587, 0.61138884, 0.3870485 , 0.40721923, 0.28447775, 0.326131 , 0.4879743 , 0.40387802, 0.36451519, 0.34205403, 0.4380145 , 0.42209148, 0.26836585, 0.40466779, 0.3113309 , 0.39576034, 0.35685413, 0.39922502, 0.36962481, 0.31978173, 0.3962891 , 0.36759539, 0.3706756 , 0.41769948, 0.37093661, 0.47298534, 0.36451519, 0.34021348, 0.45451087, 0.33229141, 0.30798969, 0.39103519, 0.34179302, 0.30058964, 0.34866431, 0.5442388 , 0.41146694, 0.34021348, 0.31415009, 0.32823257, 0.38036608, 0.34637388, 0.34663489, 0.3274428 , 0.37473444, 0.34716365, 0.33871282, 0.49826569, 0.30904047, 0.32718179, 0.30798969, 0.30058964, 0.38370729, 0.45300347, 0.54202051, 0.32823257, 0.38573671, 0.408009 , 0.39110734, 0.45484402, 0.39549933, 0.38167787, 0.39437641, 0.31415009, 0.42385989, 0.35174451, 0.3144111 , 0.4689265 , 0.32921121, 0.39523832, 0.33255242, 0.37296602, 0.31644052, 0.3422429 , 0.33589363, 0.36451519, 0.35174451, 0.36680562, 0.32410158, 0.37525645, 0.40616845, 0.39136835, 0.32181115, 0.36248577, 0.28447775, 0.32076037, 0.42183047, 0.31106989, 0.31212067, 0.31952073, 0.36222476, 0.38986769, 0.33995247, 0.30596027, 0.36346441, 0.32718179, 0.30058964, 0.36019534, 0.35711514, 0.3484033 , 0.4241209 , 0.30032863, 0.41567006, 0.36969696, 0.32384057, 0.36556597, 0.37525645, 0.36988582, 0.31670153, 0.40669721, 0.31741916, 0.36444305, 0.34401131, 0.33890169, 0.40590744, 0.31336032, 0.36654461, 0.32947222, 0.31336032, 0.34637388, 0.36248577, 0.30596027, 0.35711514, 0.35711514, 0.32410158, 0.36346441, 0.40079782, 0.38265651, 0.34761353, 0.37859767, 0.35200552, 0.29829921, 0.39294789, 0.30261906, 0.30058964, 0.3144111 , 0.38573671, 0.35050486, 0.40151545, 0.34100325, 0.31336032, 0.37735802, 0.27373649, 0.3144111 , 0.330523 , 0.27576591, 0.26836585, 0.29521901, 0.41638769, 0.4781671 , 0.40571857, 0.43238286, 0.31873095, 0.39726774, 0.34637388, 0.42288125, 0.3113309 , 0.42175832, 0.32286194, 0.35050486, 0.32286194, 0.30596027, 0.43257173, 0.30058964, 0.45052417, 0.35174451, 0.39313676, 0.38802714, 0.34126426, 0.40079782, 0.3678564 , 0.40721923, 0.34558411, 0.34558411, 0.36222476, 0.3366834 , 0.47095592, 0.38900577, 0.38960668, 0.3706756 , 0.4166487 , 0.34663489, 0.30569926, 0.29724843, 0.35482471, 0.36962481, 0.48876408, 0.29829921, 0.33255242, 0.3422429 , 0.28984838, 0.30904047, 0.34408345, 0.40590744, 0.30904047, 0.31336032, 0.34637388, 0.33589363, 0.31670153, 0.36248577, 0.34558411, 0.3274428 , 0.33897383, 0.3854757 , 0.47167354, 0.28650717, 0.28447775, 0.3484033 , 0.29521901, 0.30261906, 0.30366984, 0.35685413, 0.32384057, 0.32718179, 0.40676935, 0.38678749, 0.33995247, 0.28984838, 0.3144111 , 0.29521901, 0.30596027, 0.31873095, 0.31336032, 0.3113309 , 0.30798969, 0.31873095, 0.33484285, 0.33890169, 0.35174451, 0.39005656, 0.37191524, 0.31670153, 0.41893913, 0.3113309 , 0.40827001, 0.40086996, 0.34021348, 0.35940557, 0.37525645, 0.34126426, 0.38494693, 0.39136835, 0.41488029, 0.35606436, 0.31415009, 0.36943595, 0.31846994, 0.38141686, 0.36864618, 0.28447775, 0.31538974, 0.29521901, 0.36969696, 0.30904047, 0.330523 , 0.39850739, 0.31336032, 0.30596027, 0.32181115, 0.3113309 , 0.33484285, 0.36556597, 0.31670153]) 5. Decision Tree Regressor decision_tree_regressor = DecisionTreeRegressor(random_state=42) decision_tree_regressor.fit(X, y) DecisionTreeRegressor(random_state=42) decision_tree_regressor.predict(X) array([1. , 1. , 1. , 0.625 , 0.4075 , 0.9 , 0.9 , 0.9 , 0.66666667, 0.85 , 0.85 , 0.85 , 0.525 , 0.8 , 0.326 , 0.8 , 0.475 , 0.7 , 0.46 , 0.7 , 0.7 , 0.7 , 0.465 , 0.7 , 0.7 , 0.7 , 0.66666667, 0.5 , 0.65 , 0.65 , 0.45 , 0.65 , 0.6 , 0.396 , 0.6 , 0.6 , 0.6 , 0.5 , 0.38333333, 0.5 , 0.55 , 0.33 , 0.39 , 0.385 , 0.5 , 0.5 , 0.5 , 0.5 , 0.28 , 0.35 , 0.45 , 0.396 , 0.5 , 0.48 , 0.47 , 0.5 , 0.365 , 0.45 , 0.45 , 0.45 , 0.45 , 0.45 , 0.45 , 0.38333333, 0.42 , 0.3175 , 0.29 , 0.42 , 0.4 , 0.4 , 0.31 , 0.4 , 0.4 , 0.31 , 0.31 , 0.66666667, 0.3175 , 0.4 , 0.35 , 0.30666667, 0.4 , 0.3 , 0.28 , 0.3 , 0.4075 , 0.35 , 0.35 , 0.35 , 0.35 , 0.35 , 0.295 , 0.35 , 0.275 , 0.256 , 0.325 , 0.35 , 0.396 , 0.35 , 0.31666667, 0.275 , 0.35 , 0.33 , 0.33 , 0.25 , 0.26 , 0.25 , 0.30666667, 0.32 , 0.23 , 0.3 , 0.3 , 0.3 , 0.3 , 0.25 , 0.26 , 0.245 , 0.3 , 0.3 , 0.245 , 0.33 , 0.3 , 0.256 , 0.31666667, 0.31666667, 0.35 , 0.325 , 0.275 , 0.3 , 0.3 , 0.3 , 0.3 , 0.25 , 0.28 , 0.24 , 0.396 , 0.29 , 0.365 , 0.26 , 0.27 , 0.27 , 0.245 , 0.27 , 0.26 , 0.29 , 0.22 , 0.25 , 0.525 , 0.2075 , 0.25 , 0.25 , 0.25 , 0.25 , 0.21666667, 0.25 , 0.33 , 0.25 , 0.326 , 0.25 , 0.25 , 0.26 , 0.25 , 0.256 , 0.25 , 0.396 , 0.25 , 0.3175 , 0.25 , 0.25 , 0.225 , 0.275 , 0.25 , 0.625 , 0.24 , 0.24 , 0.295 , 0.24 , 0.24 , 0.24 , 0.23 , 0.465 , 0.23 , 0.39 , 0.22 , 0.22 , 0.22 , 0.46 , 0.22 , 0.25 , 0.31 , 0.31 , 0.21 , 0.28 , 0.22 , 0.26 , 0.28 , 0.245 , 0.33 , 0.31 , 0.23 , 0.3 , 0.24 , 0.385 , 0.21 , 0.2 , 0.2 , 0.2 , 0.4075 , 0.275 , 0.2075 , 0.24 , 0.2 , 0.475 , 0.26 , 0.35 , 0.2 , 0.2 , 0.275 , 0.21 , 0.29 , 0.2075 , 0.256 , 0.21666667, 0.245 , 0.326 , 0.45 , 0.21666667, 0.19 , 0.25 , 0.3175 , 0.2 , 0.2 , 0.23 , 0.2 , 0.326 , 0.2 , 0.2 , 0.5 , 0.2 , 0.30666667, 0.225 , 0.2 , 0.3 , 0.2 , 0.19 , 0.38333333, 0.18 , 0.18 , 0.18 , 0.18 , 0.4075 , 0.18 , 0.2075 , 0.25 , 0.28 , 0.22 , 0.18 , 0.245 , 0.256 , 0.28 , 0.326 , 0.19 , 0.25 , 0.23 ]) decision_tree_regressor.score(X, y) 0.7410570792901311 6. Validation from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42) decision_tree_regressor = DecisionTreeRegressor(random_state=42) decision_tree_regressor.fit(X_train, y_train) DecisionTreeRegressor(random_state=42) decision_tree_regressor.score(X_train, y_train) 0.7325965837309238 decision_tree_regressor.score(X_test, y_test) -1.0994712392955237 7. Conclusion The Trap The trap here is using same data for training and validation. As a beginner, it is common to look into a few models (here we looked into Linear Regression and Decision Tree Regressor), and concluded one is better than the other because the latter had a good determination coefficient score.\nHowever, after validation we realize that the Decision Tree Regressor is up to no good.\nhttps://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor.score\nThe score is negative, and based on the documentation above:\nThe best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse)\nThis is just a very basic example of misjudgement on my side. As a peer reviewer, you might feel like this is something very trivial. Finally, I just demonstrated one example here. There are pleanty of these in analytics.\n","date":"2021-12-10T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/beginners_trap/","section":"posts","tags":["cse-6040","analytics","eda"],"title":"The Beginner's Trap in analytics"},{"categories":["georgia tech","data viz","analytics","tootle"],"contents":"Analytics for Ride Hailing Services Introduction to Ride Hailing At present, it is pretty common to hail a ride to get from one place to the other at a tap of a button. Almost all major cities in the world have some sort of ride-hailing service. Uber, Lyft, Didi, Ola, Gojek, etc. are some examples of service providers that come to mind. Additionally, the service is also proliferating to smaller cities and has become commonplace in many parts of the world. Analytics is a key component in making sure the service is provided efficiently. All of the aforementioned companies invest heavily in data science and analytics to be competitive and to provide better services.\nFor this post, I will focus on Ride-Hailing services (not Ride Sharing services). See the difference here.\nPredominantly, ride-hailing functions as a Gig Economy. The drivers (sometimes referred to as partners, captains, etc.) are mostly independent contractors who bring their own vehicle and work at their own time and are paid based on their time commitment. This variability requires monitoring, sophisticated algorithms, good incentives, competitive pricing to passengers, etc. which is also common in other gig economy jobs. In most cases, the analytics models that will be built for one gig economy can be tweaked to fit another one as well.\nLet\u0026rsquo;s look at a few components of Ride-hailing that will be relevant for how we frame our models and the data we use.\nFor this post, \u0026ldquo;passengers\u0026rdquo; are referred to as service requesters/receivers and \u0026ldquo;drivers\u0026rdquo; are referred to as service providers.\nComponents of the problem Balancing act: Supply and Demand, and Chicken and Egg Problem There is a balancing act that all of these ride-sharing platforms need to perform to be efficient. A healthy ratio between driver and passenger (to go more granular, for a segment of geographic area at a given time) is very important. The balancing act is even crucial when a ride-hailing service decides to introduce itself to a new city (especially one that is new to ride-hailing).\nIf an area has more drivers than demand from passengers, the drivers might not get ride requests causing them to lose interest and find a different job or move to a different competition. If an area has more passengers than a supply of drivers, the passengers might not get their ride requests accepted causing them to move onto another (direct/indirect) competition. From an analytics perspective, this is a difficult problem to solve. However, good analytics can also be a competitive advantage here.\nPricing Pricing is a by-product of the balancing act described above. The pricing must be competitive enough to lure the supply and the demand pool. The driver should feel like the pricing justifies the time, effort, and resources supplied. The passenger should feel the amount paid for the service justifies the service received.\nFew ride-hailing services opt-out for transparent and fixed payment (i.e the price is only dictated by the distance to destination), while some have complex pricing strategies to stand out, lure passengers or drivers, and manage supply and demand effectively.\nDynamic Pricing Some ride-hailing services implement dynamic pricing as a way to balance the chicken and egg problem described above. This is a large-scale, complex analytics problem involving several variables. Additionally, driver bonuses, discounts, and referrals might constitute the pricing strategy as well.\nCredit: Forbes\nCompetition (Direct and Indirect) Direct Competition (Passenger) other ride hailing services Direct Competition (Driver) other ride-hailing services Indirect Competition (Passenger) public transportation taxi/cab etc. Indirect Competition (Driver) other employment opportunities Credit: Forbes\nDescriptive analysis Before we build complex models, it is essential to understand how the business/service is performing. These descriptive analyses will lay the foundation for us when we build complex and combined models later on.\nRide Completion/Cancellation rate What is the ride completion rate? To be more granular, what is the ride completion rate at a geographic segment of the city at a particular time? What is the ride cancellation rate? Similar to before, what is the ride cancellation rate at a geographic segment of the city at a particular time? Why do passengers cancel rides? Is cancellation more prominent in one area compared to the other? Is this dependent on the time of the day? DATA passenger_id driver_id latitude (pickup, drop) longitude (pickup, drop) timestamps (requested, accepted, picked up, dropped, canceled) completion_status cancellation_reason Late arrival rate What is the late arrival rate? what is the late arrival rate at a geographic segment of the city at a particular time? Is the late arrival rate prominent for some time of the day or for a particular geographical area? DATA passenger_id driver_id latitude (pickup, drop) longitude (pickup, drop) timestamps (requested, accepted, picked up, dropped, canceled) completion_status cancellation_reason Activation, Acquisition, Retention, Referral, Revenue What does the pirate metric funnel look like? Is there a specific area where the business should focus to improve business/efficiency? Is the funnel leaking somewhere? What is the passenger/driver churn rate? DATA passenger_id/driver_id timestamps (created_date, last_ride_date) total_amount_spent_on_platform / total_money_made total_rides num_of_referrals acquisition_channel Credit: hygger.io\nChannels What is the acquisition rate from different marketing channels for drivers or for passengers? What marketing channel is more apt/effective for different demography/user segments? Can we use the multi-arm bandits model to identify a balance between exploration and exploitation to test on different channels? DATA passenger_id/driver_id timestamps (created_date) acquisition_channel total_amount_spent_on_platform / total_money_made passenger/driver demographic information (age, gender, etc.) User Analysis What does the demography (social, cultural, economic) of the driver look like? What does the demography (social, cultural, economic) of the passenger look like? What does the demography of the city look like? What does the demography of the segment that uses the service the most look like? DATA passenger_id/driver_id timestamps (created_date) acquisition_channel total_amount_spent_on_platform / total_money_made passenger/driver/city demographic information (age, gender etc.) Driver Ranking/Driver Performance How is a driver performing? (this could be based on multiple factors including customer rating, and other factors) Based on the index for performance, what is the rank of a driver? What is the rank of a driver among a segment of drivers? (this will be useful for priority queue for driver dispatching) DATA driver_id timestamps average_rating rides_complete_rate last_ride_date Predictive analysis If we are looking to make the system more efficient, it is also very important to understand what the future holds.\nGrowth in rides What is the number of expected daily rides next day/week/month/year? What is the expected revenue for the next day/week/month/year? Is there a daily/weekly/monthly seasonality? DATA timestamp (daily) num_of_ride (completed rides or ride requests) Passenger growth What is the number of expected passenger growth next day/week/month/year? Is there a daily/weekly/monthly seasonality? DATA timestamp (daily) num_of_unique_passengers (acquisition or ride request) Driver growth What is the number of expected driver growth next day/week/month/year? Is there a daily/weekly/monthly seasonality? DATA timestamp (daily) num_of_unique_drivers (acquisition or ride request) Churn over the period of time What is the expected churn in the next day/week/month/year? Is there a daily/weekly/monthly seasonality? DATA timestamp (passenger acquisition) passenger\u0026rsquo;s number of rides each month (grouped acquisition to present) Prescriptive analysis Descriptive and Predictive analysis will help us move towards prescriptive analysis, especially for optimization models. These models will help the service provider in decision making, especially with regards to an increase in efficiency for drivers and passengers.\nRatio of drivers to passengers What is the ideal ratio of the passenger to the driver to maximize rides completion rate? Given Voronoi clustering for geographic indexing based on geographic hotspots (other indexing methods are more efficient like h3 developed by Uber, but Voronoi can be used to build something similar as well.) rides data (requested, canceled, completed) passenger data (raw data and data after descriptive analysis performed: Pirate metrics etc.) driver data Use Optimization with constraints: num_of_rides should be greater than a threshold (comes from future rides data) with objective functions: maximize rides completion rate for each geographic segment to find an optimal driver to passenger ratio Notes\nRegression (or logistic regression if we only care about a healthy/unhealthy ratio) can also be used to do something similar as well. Additionally, the result from the model can also be used to model advertisement campaigns for the future if we find the number of driver or passenger (in a particular geographic area) need to be increased for a stable ratio. This is an important indicator because it allows the service provider to focus on growth while keeping this indicator at a healthy level. Dynamic pricing What should the dynamic/surge pricing be at a given time? Given ratio of the driver to passenger paying capacity of passengers (based on descriptive analysis of users, useful for capping at some multiplier so that it does not go wild) number of requests in the queue in a geographic segment competition surge at the moment number of requests completed in the geographic segment (and neighboring segment) in last x minutes (arbitrary but can be defined by waiting for time analysis from descriptive analysis) geographic location information (grid-based on Voronoi for the availability of drivers in other cells) number of drivers that will be free (complete a ride soon or are predicted to come online soon) in the grid or neighboring grids Use Linear regression to find ideal dynamic pricing multiplier Notes\nThe cap might/might not be necessary, and that might be another analytics problem altogether. There have been some cases where a natural disaster/terrorist attack increased surge multiplier to an exorbitant number causing massive backlash. grid above refers to one unit of Voronoi based geographic segmentation It is necessary to study the correlation of some of the predictors mentioned above. Ride Dispatching What is a robust ride dispatching mechanism that will increase passengers and drivers? Given Drivers in Geographic Grid (and neighboring Grid) Driver Rating/Driver Ranking Geographic Grid Pickup/Drop location (distance and Grid) Use Optimization with constraints: the probability of each driver getting ride should be close to 1, waiting time should be less than some threshold for the request to be accepted or not accepted (which comes from descriptive analysis), the time between request dispatching (time window a driver gets before the request is passed on to a different driver, also comes from descriptive analysis) should be equal to the acceptable waiting time divided by some constant (integer) with objective functions: maximize rides completion rate for each geographic segment ** Notes **\nQueuing models can also be here to identify correct values for the dispatching system (waiting time, dynamic geographic grid, etc.). However, there is a need to check the distribution of different events (booking created, booking accepted, waiting time, etc.) Conclusion It seems analytics is extremely relevant in all aspects of ride-hailing. In this project, I merely covered a few use cases, with one or two relevant models. Even with this brief exploration, I can conclude that analytics can lead to better outcomes for both drivers and passengers.\n","date":"2021-12-02T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/ride_hailing_analytics/","section":"posts","tags":["analytics in ride sharing","tootle","nepal","analytics","isye-6501","ride hailing","ride sharing","regression","optimization","eda"],"title":"Analytics for Ride Hailing Services"},{"categories":["british college"],"contents":"Business Intelligence (British College, 2021 Fall) Objectives To ensure students understand the concepts of business analytics and data visualization To develop students\u0026rsquo; knowledge, understanding. and skills in the real life scenario tackling real world business problems using data To ensure students learn practical skills leveraging few essential tools Outcomes Understand the essentials of business intelligence, statistics and the corresponding terminologies. Be able to create data visualizations using spreadsheets/tableau. Be able to interpret real world problems using data. Be familiar with the steps involved in the BI process. Be able to interact competently on the topic of BI. Have had some hands-on experience in using BI techniques. Tools Google Sheets/Excel, Tableau Github Basic Python and Pandas (if participants want) Weekly Breakdown Week # Topics Covered Week 1 Introduction to BI (Definition, Context, Process, Use Cases) Week 2 Decision Making and Decision Support Systems\nDiscuss the decision making challenges faced by different managers\nDiscuss various DSS and its evolution Week 3 Introduction to Business Metrics, KPIs\nBecome familiar with business metrics used by business analysts in the area of marketing, sales, growth, engagement, and financial analysis\nCalculate and interpret key performance metrics Week 4 Business statistics\nBasic statistics\nTypes of visualization\nInterpreting visualization\nGood visualization Week 5 Descriptive Analysis with Excel/Google Sheets\nFundamentals of data and statistics with use case Week 6 Descriptive Analysis with Excel/Google Sheets\nFundamentals of data and statistics with use case Week 7 Time Series Analysis, Simple Regression, Trends, Seasonality concepts with relevant instances Week 8 Design Principles\nTableau Basics\nTelling stories with Tableau Week 9 Workshop on building Tableau Dashboard Week 10 Current trends in BI, ending notes, complete projects, wrap up Potpourri Terminilogies (will be added thoughout the course) A/B testing ACID compliancy Ad Hoc Agile Anonymization API AWS Quicksight, Tableau, Power BI, Datastudio Backend Batch processing vs streaming Biases Big Data Bots Brand Awareness+Equity+Loyalty Call to action Channel Churn Cloud Columnar Database Competitive Analysis Conversion Correlation CRO (Conversion Rate Optimization) Customer Lifetime Value Dashboard Data Engineering Data Lake Data Mart Data Pipeline (Luigi, Apache airflow) Data Scraping Data Storytelling Data Warehouse Data Wrangling Data Cleansing, Referential integrity, Domain integrity, Entity integrity Database Demand Generation Demography Denormalization DevOps Dimensions and measures Discrete, Continious, Categorical, Ordinal Data Domain knowledge ETL EDA Forecasting Funnels Git Goals Grant funding Hadoop, Spark etc Heuristics Impressions In-Memory BI Infographic Joins KPIs Leads Lookalike audiences Metadata Metrics Mission Near Real Time Normal distribution Normalization Net Promoter Score OKRs OLAP OLTP outlier Pandas, Tidyverse, PySpark Pareto Pirate Metrics Qualatitive data Quantitative data Real Time RDBMS Roadmap ROI Runway period SaaS Schema Seasonality Single source of truth Semi structured data Structured data Spreadsheets Snapshot SQL Surveys Synthetic Data Trend Unstructured data Unique Value Preposition Valuation View Vision Volume Velocity Variety Veracity Variability Don\u0026rsquo;t worry, some of the acronyms confuse everyone Source: https://www.perdoo.com/resources/merkel-asks-what-does-okr-mean/\nDataset eHamroPasalmandu.com (we will be using a fake dataset for this fake ecommerce startup in nepal, see the datasets folder) Tables clients table items table transactions table merged (complete transactions table) clients table sample client_id name gender dob email phone channel first_contact lat lon location_name created_at 100000000 Kaushal Bashyal Male 14/03/2003 kaushal.bashyal@fakeemail.com 9841791565 Google Search browser 27.7768 85.3622 Golfutar Main Rd 48:09.8 100000001 Sona Hayanju Female 03/07/1996 sona.hayanju@fakeemail.com 9841685812 Other browser 27.6954 85.3447 ACE Institute Of Management 46:03.5 100000002 Suman Pokherel Male 04/08/2005 suman.pokherel@fakeemail.com 9841526187 Facebook/Ads browser 27.659 85.368 Changathali Rd 05:06.3 100000003 Samita Yogol Female 22/08/2001 samita.yogol@fakeemail.com 9841131562 Facebook/Ads browser 27.7126 85.283 Ring Road 56:28.8 100000004 Mahima Marasaini Female 20/03/1985 mahima.marasaini@fakeemail.com 9841859344 Other app 27.7137 85.3245 Bhagawati Marg 06:58.6 items table sample item_id item_name price category image_url inventory 10000 iPhone 12 (256GB) 152900 Phone https://dummyimage.com/600x400/000/fff\u0026amp;text=iPhone+12+(256GB) 36 10001 VivoBook 14 X415JA (14” FHD, Intel i7-1065G7, Intel UHD, 8GB, 512GB SSD) 115000 Laptop https://dummyimage.com/600x400/000/fff\u0026amp;text=VivoBook+14+X415JA+(14”+FHD,+Intel+i7-1065G7,+Intel+UHD,+8GB,+512GB+SSD) 39 10002 Helios 300 2020 (15.6″ FHD 144Hz, Core i7-10750H, GTX 1660 Ti, 16GB, 512GB SSD) 190000 Laptop https://dummyimage.com/600x400/000/fff\u0026amp;text=Helios+300+2020+(15.6″+FHD+144Hz,+Core+i7-10750H,+GTX+1660+Ti,+16GB,+512GB+SSD) 27 10003 Sony Alpha A6500 (With 18-135mm zoom lens) 245000 Camera https://dummyimage.com/600x400/000/fff\u0026amp;text=Sony+Alpha+A6500+(With+18-135mm+zoom+lens) 71 10004 iPhone SE 2 (256GB) 96000 Phone https://dummyimage.com/600x400/000/fff\u0026amp;text=iPhone+SE+2+(256GB) 24 transactions table sample created_at item_id client_id 2020-09-01 09:49:12.285421 10142 100000015 2020-09-01 16:44:09.888896 10113 100000010 2020-09-01 22:40:07.678792 10105 100000010 2020-09-02 18:48:22.620671 10177 100000022 2020-09-02 11:04:38.703035 10039 100000019 merged table sample item_id client_id created_at name gender dob email phone channel first_contact lat lon location_name created_at_client item_name price category image_url inventory 10001 100000127 2020-09-06 16:03:41.236993 Pralhad Biskiwakarma Male 2002-05-25 pralhad.biskiwakarma@fakeemail.com 9841968059 Word of Mouth app 27.712 85.322 Hattisar Sadak 2020-09-06 04:41:23.065429 VivoBook 14 X415JA (14” FHD, Intel i7-1065G7, Intel UHD, 8GB, 512GB SSD) 115000 Laptop https://dummyimage.com/600x400/000/fff\u0026amp;text=VivoBook+14+X415JA+(14”+FHD,+Intel+i7-1065G7,+Intel+UHD,+8GB,+512GB+SSD) 39 10001 100000262 2021-03-03 04:08:21.342722 Prazol Harlalka Male 1999-08-22 prazol.harlalka@fakeemail.com 9841213863 Word of Mouth app 27.7415 85.3127 G4 Futsal 2020-09-13 21:25:24.950483 VivoBook 14 X415JA (14” FHD, Intel i7-1065G7, Intel UHD, 8GB, 512GB SSD) 115000 Laptop https://dummyimage.com/600x400/000/fff\u0026amp;text=VivoBook+14+X415JA+(14”+FHD,+Intel+i7-1065G7,+Intel+UHD,+8GB,+512GB+SSD) 39 10001 100000901 2020-11-03 06:43:52.725155 Pramod Shapakota Male 1979-09-14 pramod.shapakota@fakeemail.com 9841852187 Google Search browser 27.7244 85.322 Lazimpat Rd 1079 2020-10-09 13:10:51.153269 VivoBook 14 X415JA (14” FHD, Intel i7-1065G7, Intel UHD, 8GB, 512GB SSD) 115000 Laptop https://dummyimage.com/600x400/000/fff\u0026amp;text=VivoBook+14+X415JA+(14”+FHD,+Intel+i7-1065G7,+Intel+UHD,+8GB,+512GB+SSD) 39 10001 100002190 2020-11-25 08:56:35.801396 Samita Panjiyar Female 1987-10-20 samita.panjiyar@fakeemail.com 9841003522 Word of Mouth browser 27.7057 85.3337 Maiti Devi Marg 2020-11-20 05:14:59.540079 VivoBook 14 X415JA (14” FHD, Intel i7-1065G7, Intel UHD, 8GB, 512GB SSD) 115000 Laptop https://dummyimage.com/600x400/000/fff\u0026amp;text=VivoBook+14+X415JA+(14”+FHD,+Intel+i7-1065G7,+Intel+UHD,+8GB,+512GB+SSD) 39 10001 100002773 2021-04-29 08:08:06.426326 Kusum Katiwada Female 2003-03-01 kusum.katiwada@fakeemail.com 9841201400 Google Search browser 27.7224 85.3101 Rayamajhi Marga 2020-12-07 01:10:09.243962 VivoBook 14 X415JA (14” FHD, Intel i7-1065G7, Intel UHD, 8GB, 512GB SSD) 115000 Laptop https://dummyimage.com/600x400/000/fff\u0026amp;text=VivoBook+14+X415JA+(14”+FHD,+Intel+i7-1065G7,+Intel+UHD,+8GB,+512GB+SSD) 39 Find the complete course here: https://github.com/ayushsubedi/bi\n","date":"2021-10-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/british_bi/","section":"posts","tags":["teaching","curriculum","syllabus"],"title":"Business Intelligence for Nepal"},{"categories":["british college","analytics"],"contents":"eHamroPasalmandu.com eHamroPasalmandu is a fictional eCommerse based in Nepal. If it were operating for a year, this is what the data would look like (kind of, sort of).\nSome \u0026ldquo;real world\u0026rdquo; patterns here: Upward Trend Weekly seasonality Holiday System shutdowns Pirate metrics Pattens usually visible in Nepali startups (demography and distributions) Customer Churn others Raw sources http://www.studentsoftheworld.info/penpals/stats.php?Pays=NEP https://raw.githubusercontent.com/amitness/Thar/master/surnames_en.csv https://www.gadgetbytenepal.com/ Clients table For rest of the data generation, go to https://github.com/ayushsubedi/eHamroPasalmandu\nfrom faker import Faker import pandas as pd import random import matplotlib.pyplot as plt import datetime df = pd.read_csv(\u0026#39;../datasets/nepali_first_name_gender.csv\u0026#39;) filename = \u0026#39;../datasets/lastnames.txt\u0026#39; with open(filename) as f: content = f.readlines() content = [x.strip()[:-1] for x in content] last_names = [x.capitalize() for x in content] first_name_male = [x.capitalize() for x in list(df[df.gender==\u0026#39;M\u0026#39;].first_name)] first_name_female = [x.capitalize() for x in list(df[df.gender==\u0026#39;F\u0026#39;].first_name)] [random.choice(first_name_male) + \u0026#34; \u0026#34; + random.choice(last_names) for x in range(10)] ['Anjan Bhul', 'Ayus Kaliraj', 'Manish Praveen', 'Yubaraj Kareem', 'Bhakta Bagala', 'Sabin Rajbansi', 'Susshanzt Tinkari', 'Sajit Rasaili', 'Dipak Kahrel', 'Sam Remimagar'] total_customer = 30000 male_percent = 58 male = {\u0026#39;name\u0026#39;: [random.choice(first_name_male) + \u0026#34; \u0026#34; + random.choice(last_names) for x in range(int(total_customer*male_percent/100))]} female = {\u0026#39;name\u0026#39;: [random.choice(first_name_female) + \u0026#34; \u0026#34; + random.choice(last_names) for x in range(int(total_customer*(100-male_percent)/100))]} df_male = pd.DataFrame(data=male) df_male[\u0026#39;gender\u0026#39;] = \u0026#39;Male\u0026#39; df_female = pd.DataFrame(data=female) df_female[\u0026#39;gender\u0026#39;] = \u0026#39;Female\u0026#39; df = df_male.append(df_female) df.loc[df.sample(frac=.04).index, \u0026#39;gender\u0026#39; ] = \u0026#39;Other\u0026#39; df.loc[df.sample(frac=.02).index, \u0026#39;gender\u0026#39; ] = \u0026#39;Prefer not to say\u0026#39; df.gender.value_counts() Male 15670 Female 11145 Other 2091 Prefer not to say 1094 Name: gender, dtype: int64 def email_gen(name): name = name.lower() return (name.split(\u0026#39; \u0026#39;)[0]+\u0026#39;.\u0026#39;+name.split(\u0026#39; \u0026#39;)[1]+\u0026#39;@fakeemail.com\u0026#39;) df[\u0026#39;email\u0026#39;] = df.name.apply(email_gen) df name gender email 0 Suman Chyhetri Male suman.chyhetri@fakeemail.com 1 Susshanzt Achrya Male susshanzt.achrya@fakeemail.com 2 Asutosh Awal Male asutosh.awal@fakeemail.com 3 Aditya Madwari Male aditya.madwari@fakeemail.com 4 Pradeep raj nepal Lamichahne Male pradeep.raj@fakeemail.com ... ... ... ... 12595 Prati Bhatt Other prati.bhatt@fakeemail.com 12596 Sneha Jyakhwo Female sneha.jyakhwo@fakeemail.com 12597 Shambhav Shai Female shambhav.shai@fakeemail.com 12598 Soneeya Vonjon Female soneeya.vonjon@fakeemail.com 12599 Yahnaa Bishwas Female yahnaa.bishwas@fakeemail.com 30000 rows × 3 columns\ndf[\u0026#39;phone\u0026#39;] = [random.randint(9841000000, 9842000000) for x in range(df.shape[0])] df name gender email phone 0 Suman Chyhetri Male suman.chyhetri@fakeemail.com 9841137209 1 Susshanzt Achrya Male susshanzt.achrya@fakeemail.com 9841064824 2 Asutosh Awal Male asutosh.awal@fakeemail.com 9841162884 3 Aditya Madwari Male aditya.madwari@fakeemail.com 9841298742 4 Pradeep raj nepal Lamichahne Male pradeep.raj@fakeemail.com 9841204451 ... ... ... ... ... 12595 Prati Bhatt Other prati.bhatt@fakeemail.com 9841152214 12596 Sneha Jyakhwo Female sneha.jyakhwo@fakeemail.com 9841550701 12597 Shambhav Shai Female shambhav.shai@fakeemail.com 9841067695 12598 Soneeya Vonjon Female soneeya.vonjon@fakeemail.com 9841045051 12599 Yahnaa Bishwas Female yahnaa.bishwas@fakeemail.com 9841352244 30000 rows × 4 columns\ndf[\u0026#39;channel\u0026#39;] = \u0026#39;Word of Mouth\u0026#39; df.loc[df.sample(frac=.5).index, \u0026#39;channel\u0026#39; ] = \u0026#39;Facebook/Ads\u0026#39; df.loc[df.sample(frac=.2).index, \u0026#39;channel\u0026#39; ] = \u0026#39;Google Search\u0026#39; df.loc[df.sample(frac=.1).index, \u0026#39;channel\u0026#39; ] = \u0026#39;Other\u0026#39; df name gender email phone channel 0 Suman Chyhetri Male suman.chyhetri@fakeemail.com 9841137209 Facebook/Ads 1 Susshanzt Achrya Male susshanzt.achrya@fakeemail.com 9841064824 Google Search 2 Asutosh Awal Male asutosh.awal@fakeemail.com 9841162884 Facebook/Ads 3 Aditya Madwari Male aditya.madwari@fakeemail.com 9841298742 Facebook/Ads 4 Pradeep raj nepal Lamichahne Male pradeep.raj@fakeemail.com 9841204451 Other ... ... ... ... ... ... 12595 Prati Bhatt Other prati.bhatt@fakeemail.com 9841152214 Word of Mouth 12596 Sneha Jyakhwo Female sneha.jyakhwo@fakeemail.com 9841550701 Facebook/Ads 12597 Shambhav Shai Female shambhav.shai@fakeemail.com 9841067695 Facebook/Ads 12598 Soneeya Vonjon Female soneeya.vonjon@fakeemail.com 9841045051 Facebook/Ads 12599 Yahnaa Bishwas Female yahnaa.bishwas@fakeemail.com 9841352244 Facebook/Ads 30000 rows × 5 columns\ndf.channel.value_counts() Facebook/Ads 11414 Google Search 8131 Other 5318 Word of Mouth 5137 Name: channel, dtype: int64 df[\u0026#39;first_contact\u0026#39;] = \u0026#39;app\u0026#39; df.loc[df.sample(frac=.676).index, \u0026#39;first_contact\u0026#39; ] = \u0026#39;browser\u0026#39; df name gender email phone channel first_contact 0 Suman Chyhetri Male suman.chyhetri@fakeemail.com 9841137209 Facebook/Ads browser 1 Susshanzt Achrya Male susshanzt.achrya@fakeemail.com 9841064824 Google Search browser 2 Asutosh Awal Male asutosh.awal@fakeemail.com 9841162884 Facebook/Ads browser 3 Aditya Madwari Male aditya.madwari@fakeemail.com 9841298742 Facebook/Ads browser 4 Pradeep raj nepal Lamichahne Male pradeep.raj@fakeemail.com 9841204451 Other browser ... ... ... ... ... ... ... 12595 Prati Bhatt Other prati.bhatt@fakeemail.com 9841152214 Word of Mouth browser 12596 Sneha Jyakhwo Female sneha.jyakhwo@fakeemail.com 9841550701 Facebook/Ads app 12597 Shambhav Shai Female shambhav.shai@fakeemail.com 9841067695 Facebook/Ads browser 12598 Soneeya Vonjon Female soneeya.vonjon@fakeemail.com 9841045051 Facebook/Ads browser 12599 Yahnaa Bishwas Female yahnaa.bishwas@fakeemail.com 9841352244 Facebook/Ads browser 30000 rows × 6 columns\ndf[\u0026#39;first_contact\u0026#39;].value_counts() browser 25780 app 4220 Name: first_contact, dtype: int64 df = df.sample(frac=1) df name gender email phone channel first_contact 8250 Kaushal Bashyal Male kaushal.bashyal@fakeemail.com 9841791565 Google Search browser 10606 Sona Hayanju Female sona.hayanju@fakeemail.com 9841685812 Other browser 5076 Suman Pokherel Male suman.pokherel@fakeemail.com 9841526187 Facebook/Ads browser 1824 Samita Yogol Female samita.yogol@fakeemail.com 9841131562 Facebook/Ads browser 6878 Mahima Marasaini Female mahima.marasaini@fakeemail.com 9841859344 Other app ... ... ... ... ... ... ... 9653 Ansu Adhikary Female ansu.adhikary@fakeemail.com 9841430787 Facebook/Ads browser 3735 Lune Bisht Female lune.bisht@fakeemail.com 9841969651 Other browser 1387 Rohit Gajurel Other rohit.gajurel@fakeemail.com 9841940773 Google Search browser 2197 Kusum J.b.rana Female kusum.j.b.rana@fakeemail.com 9841904502 Google Search browser 14831 Satish Rangata Male satish.rangata@fakeemail.com 9841982263 Word of Mouth app 30000 rows × 6 columns\nfrom timeseries_generator import LinearTrend, Generator, WhiteNoise, RandomFeatureFactor, WeekdayFactor import pandas as pd start=\u0026#34;09-01-2020\u0026#34; end=\u0026#34;08-31-2021\u0026#34; # setting up a linear tren lt = LinearTrend(coef=5.0, offset=1., col_name=\u0026#34;my_linear_trend\u0026#34;) weekday_factor = WeekdayFactor( col_name=\u0026#34;weekend_boost_factor\u0026#34;, factor_values={4: 1.1, 5: 1.2, 6: 1.2} # Here we assign a factor of 1.1 to Friday, and 1.2 to Sat/Sun ) # weekday_factor.plot(start_date=start, end_date=end) g = Generator(factors={lt, weekday_factor}, features=None, date_range=pd.date_range(start=start, end=end)) g.generate() # update by adding some white noise to the generator wn = WhiteNoise(stdev_factor=0.06) g.update_factor(wn) g.generate() g.plot() data = g.generate() data[\u0026#39;rows\u0026#39;] = (10*data.value).astype(int) data.rows.sum() 17326 data date base_amount my_linear_trend white_noise weekend_boost_factor total_factor value rows 0 2020-09-01 1.0 2.000000 1.033445 1.0 2.066891 2.066891 20 1 2020-09-02 1.0 2.013699 0.956114 1.0 1.925326 1.925326 19 2 2020-09-03 1.0 2.027397 0.906669 1.0 1.838179 1.838179 18 3 2020-09-04 1.0 2.041096 0.957273 1.1 2.149275 2.149275 21 4 2020-09-05 1.0 2.054795 0.962236 1.2 2.372637 2.372637 23 ... ... ... ... ... ... ... ... ... 360 2021-08-27 1.0 6.931507 0.986548 1.1 7.522090 7.522090 75 361 2021-08-28 1.0 6.945205 0.981777 1.2 8.182375 8.182375 81 362 2021-08-29 1.0 6.958904 1.021142 1.2 8.527237 8.527237 85 363 2021-08-30 1.0 6.972603 1.005213 1.0 7.008952 7.008952 70 364 2021-08-31 1.0 6.986301 1.063856 1.0 7.432417 7.432417 74 365 rows × 8 columns\ndef nepali_holiday(row): date = datetime.datetime.strptime(str(row.date), \u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;) num = row.rows if date\u0026gt;datetime.datetime(2020, 10, 16) and date\u0026lt;datetime.datetime(2020, 10, 25): num = int (num*1.2) if date\u0026gt;datetime.datetime(2020, 12, 24) and date\u0026lt;datetime.datetime(2021, 1, 2): num = int (num*1.4) if date\u0026gt;datetime.datetime(2021, 1, 4) and date\u0026lt;datetime.datetime(2021, 1, 7): num = num - num if date\u0026gt;datetime.datetime(2021, 2, 13) and date\u0026lt;datetime.datetime(2021, 2, 15): num = int (num*1.4) if date\u0026gt;datetime.datetime(2021, 7, 1): num = int(num * 2) return num data[\u0026#39;rows\u0026#39;] = data.apply(nepali_holiday, axis=1) # [\u0026#34;2020-09-01\u0026#34;]*100 + [\u0026#34;2020-09-02\u0026#34;] * 200 from datetime import datetime as dt import datetime def create_list(row): return [str(row.date)]*row.rows final_dates = (data.apply(create_list, axis=1).sum()) clients = df.head(len(final_dates)) clients.reset_index(inplace = True, drop=True) clients[\u0026#39;created_at\u0026#39;] = final_dates clients name gender email phone channel first_contact created_at 0 Kaushal Bashyal Male kaushal.bashyal@fakeemail.com 9841791565 Google Search browser 2020-09-01 00:00:00 1 Sona Hayanju Female sona.hayanju@fakeemail.com 9841685812 Other browser 2020-09-01 00:00:00 2 Suman Pokherel Male suman.pokherel@fakeemail.com 9841526187 Facebook/Ads browser 2020-09-01 00:00:00 3 Samita Yogol Female samita.yogol@fakeemail.com 9841131562 Facebook/Ads browser 2020-09-01 00:00:00 4 Mahima Marasaini Female mahima.marasaini@fakeemail.com 9841859344 Other app 2020-09-01 00:00:00 ... ... ... ... ... ... ... ... 21638 Bibek Barakoti Male bibek.barakoti@fakeemail.com 9841288299 Facebook/Ads browser 2021-08-31 00:00:00 21639 Ansu Huzdar Female ansu.huzdar@fakeemail.com 9841694532 Google Search browser 2021-08-31 00:00:00 21640 Sumit Sanjel Male sumit.sanjel@fakeemail.com 9841638790 Google Search browser 2021-08-31 00:00:00 21641 Prapti Mushyakhow Female prapti.mushyakhow@fakeemail.com 9841359950 Facebook/Ads browser 2021-08-31 00:00:00 21642 Prazol Swarnakar Male prazol.swarnakar@fakeemail.com 9841429598 Facebook/Ads browser 2021-08-31 00:00:00 21643 rows × 7 columns\nclients.gender.value_counts() Male 11306 Female 8040 Other 1500 Prefer not to say 797 Name: gender, dtype: int64 clients.channel.value_counts() Facebook/Ads 8247 Google Search 5849 Other 3827 Word of Mouth 3720 Name: channel, dtype: int64 clients[\u0026#39;first_contact\u0026#39;].value_counts() browser 18601 app 3042 Name: first_contact, dtype: int64 def random_times(input_time): random_hour = random.uniform(0, 23) return dt.strptime(input_time, \u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;) + datetime.timedelta(hours=random_hour) clients[\u0026#39;created_at\u0026#39;] = clients[\u0026#39;created_at\u0026#39;].apply(random_times) clients.sort_values(\u0026#39;created_at\u0026#39;).reset_index(drop=True, inplace=True) clients[\u0026#39;client_id\u0026#39;] = clients.index + 100000000 clients name gender email phone channel first_contact created_at client_id 0 Kaushal Bashyal Male kaushal.bashyal@fakeemail.com 9841791565 Google Search browser 2020-09-01 10:48:09.763278 100000000 1 Sona Hayanju Female sona.hayanju@fakeemail.com 9841685812 Other browser 2020-09-01 11:46:03.489814 100000001 2 Suman Pokherel Male suman.pokherel@fakeemail.com 9841526187 Facebook/Ads browser 2020-09-01 14:05:06.316626 100000002 3 Samita Yogol Female samita.yogol@fakeemail.com 9841131562 Facebook/Ads browser 2020-09-01 20:56:28.775488 100000003 4 Mahima Marasaini Female mahima.marasaini@fakeemail.com 9841859344 Other app 2020-09-01 16:06:58.593803 100000004 ... ... ... ... ... ... ... ... ... 21638 Bibek Barakoti Male bibek.barakoti@fakeemail.com 9841288299 Facebook/Ads browser 2021-08-31 21:01:37.440162 100021638 21639 Ansu Huzdar Female ansu.huzdar@fakeemail.com 9841694532 Google Search browser 2021-08-31 01:11:26.657379 100021639 21640 Sumit Sanjel Male sumit.sanjel@fakeemail.com 9841638790 Google Search browser 2021-08-31 20:41:11.488311 100021640 21641 Prapti Mushyakhow Female prapti.mushyakhow@fakeemail.com 9841359950 Facebook/Ads browser 2021-08-31 18:17:39.263566 100021641 21642 Prazol Swarnakar Male prazol.swarnakar@fakeemail.com 9841429598 Facebook/Ads browser 2021-08-31 02:46:50.362896 100021642 21643 rows × 8 columns\ndef date_gen(start, end): start_date = datetime.date(start, 1, 1) end_date = datetime.date(end, 12, 31) time_between_dates = end_date - start_date days_between_dates = time_between_dates.days random_number_of_days = random.randrange(days_between_dates) random_date = start_date + datetime.timedelta(days=random_number_of_days) return str(random_date) dob = [date_gen(1975, 1990) for x in range(clients.shape[0]//5)] + \\ [date_gen(1991, 1995) for x in range(clients.shape[0]//5)] + \\ [date_gen(1996, 2000) for x in range(clients.shape[0]//2)] + \\ [date_gen(2001, 2005) for x in range(clients.shape[0]//2)] + \\ [date_gen(2006, 2008) for x in range(clients.shape[0]//5)] random.shuffle(dob) clients[\u0026#39;dob\u0026#39;] = dob[:clients.shape[0]] clients name gender email phone channel first_contact created_at client_id dob 0 Kaushal Bashyal Male kaushal.bashyal@fakeemail.com 9841791565 Google Search browser 2020-09-01 10:48:09.763278 100000000 2003-03-14 1 Sona Hayanju Female sona.hayanju@fakeemail.com 9841685812 Other browser 2020-09-01 11:46:03.489814 100000001 1996-07-03 2 Suman Pokherel Male suman.pokherel@fakeemail.com 9841526187 Facebook/Ads browser 2020-09-01 14:05:06.316626 100000002 2005-08-04 3 Samita Yogol Female samita.yogol@fakeemail.com 9841131562 Facebook/Ads browser 2020-09-01 20:56:28.775488 100000003 2001-08-22 4 Mahima Marasaini Female mahima.marasaini@fakeemail.com 9841859344 Other app 2020-09-01 16:06:58.593803 100000004 1985-03-20 ... ... ... ... ... ... ... ... ... ... 21638 Bibek Barakoti Male bibek.barakoti@fakeemail.com 9841288299 Facebook/Ads browser 2021-08-31 21:01:37.440162 100021638 1999-10-05 21639 Ansu Huzdar Female ansu.huzdar@fakeemail.com 9841694532 Google Search browser 2021-08-31 01:11:26.657379 100021639 2004-03-30 21640 Sumit Sanjel Male sumit.sanjel@fakeemail.com 9841638790 Google Search browser 2021-08-31 20:41:11.488311 100021640 1997-07-11 21641 Prapti Mushyakhow Female prapti.mushyakhow@fakeemail.com 9841359950 Facebook/Ads browser 2021-08-31 18:17:39.263566 100021641 2006-05-08 21642 Prazol Swarnakar Male prazol.swarnakar@fakeemail.com 9841429598 Facebook/Ads browser 2021-08-31 02:46:50.362896 100021642 1975-05-24 21643 rows × 9 columns\nclients name gender email phone channel first_contact created_at client_id dob 0 Kaushal Bashyal Male kaushal.bashyal@fakeemail.com 9841791565 Google Search browser 2020-09-01 10:48:09.763278 100000000 2003-03-14 1 Sona Hayanju Female sona.hayanju@fakeemail.com 9841685812 Other browser 2020-09-01 11:46:03.489814 100000001 1996-07-03 2 Suman Pokherel Male suman.pokherel@fakeemail.com 9841526187 Facebook/Ads browser 2020-09-01 14:05:06.316626 100000002 2005-08-04 3 Samita Yogol Female samita.yogol@fakeemail.com 9841131562 Facebook/Ads browser 2020-09-01 20:56:28.775488 100000003 2001-08-22 4 Mahima Marasaini Female mahima.marasaini@fakeemail.com 9841859344 Other app 2020-09-01 16:06:58.593803 100000004 1985-03-20 ... ... ... ... ... ... ... ... ... ... 21638 Bibek Barakoti Male bibek.barakoti@fakeemail.com 9841288299 Facebook/Ads browser 2021-08-31 21:01:37.440162 100021638 1999-10-05 21639 Ansu Huzdar Female ansu.huzdar@fakeemail.com 9841694532 Google Search browser 2021-08-31 01:11:26.657379 100021639 2004-03-30 21640 Sumit Sanjel Male sumit.sanjel@fakeemail.com 9841638790 Google Search browser 2021-08-31 20:41:11.488311 100021640 1997-07-11 21641 Prapti Mushyakhow Female prapti.mushyakhow@fakeemail.com 9841359950 Facebook/Ads browser 2021-08-31 18:17:39.263566 100021641 2006-05-08 21642 Prazol Swarnakar Male prazol.swarnakar@fakeemail.com 9841429598 Facebook/Ads browser 2021-08-31 02:46:50.362896 100021642 1975-05-24 21643 rows × 9 columns\nlocation = pd.read_csv(\u0026#34;../datasets/location.csv\u0026#34;) location = location[[\u0026#39;lat\u0026#39;, \u0026#39;lon\u0026#39;, \u0026#39;name\u0026#39;]] location_list = [location.sample().values.tolist() for x in range(clients.shape[0])] location_df = pd.DataFrame(location_list) location_df 0 0 [27.7768, 85.3622, Golfutar Main Rd] 1 [27.6954, 85.3447, ACE Institute Of Management] 2 [27.659, 85.368, Changathali Rd] 3 [27.7126, 85.283, Ring Road] 4 [27.7137, 85.3245, Bhagawati Marg] ... ... 21638 [27.6787, 85.3103, Jhamsikhel Marg] 21639 [27.7305, 85.3405, Banshidhar Marg] 21640 [27.7206, 85.3194, Kumari Mai Marg] 21641 [27.7351, 85.3056, Gongabu New Buspark] 21642 [27.6962, 85.3393, BanaGanga Marga] 21643 rows × 1 columns\nlocation_df[[\u0026#39;lat\u0026#39;,\u0026#39;lon\u0026#39;, \u0026#39;location_name\u0026#39;]] = pd.DataFrame(location_df[0].tolist(), index= location_df.index) location_df[[\u0026#39;lat\u0026#39;, \u0026#39;lon\u0026#39;, \u0026#39;location_name\u0026#39;]] lat lon location_name 0 27.7768 85.3622 Golfutar Main Rd 1 27.6954 85.3447 ACE Institute Of Management 2 27.6590 85.3680 Changathali Rd 3 27.7126 85.2830 Ring Road 4 27.7137 85.3245 Bhagawati Marg ... ... ... ... 21638 27.6787 85.3103 Jhamsikhel Marg 21639 27.7305 85.3405 Banshidhar Marg 21640 27.7206 85.3194 Kumari Mai Marg 21641 27.7351 85.3056 Gongabu New Buspark 21642 27.6962 85.3393 BanaGanga Marga 21643 rows × 3 columns\nclients name gender email phone channel first_contact created_at client_id dob 0 Kaushal Bashyal Male kaushal.bashyal@fakeemail.com 9841791565 Google Search browser 2020-09-01 10:48:09.763278 100000000 2003-03-14 1 Sona Hayanju Female sona.hayanju@fakeemail.com 9841685812 Other browser 2020-09-01 11:46:03.489814 100000001 1996-07-03 2 Suman Pokherel Male suman.pokherel@fakeemail.com 9841526187 Facebook/Ads browser 2020-09-01 14:05:06.316626 100000002 2005-08-04 3 Samita Yogol Female samita.yogol@fakeemail.com 9841131562 Facebook/Ads browser 2020-09-01 20:56:28.775488 100000003 2001-08-22 4 Mahima Marasaini Female mahima.marasaini@fakeemail.com 9841859344 Other app 2020-09-01 16:06:58.593803 100000004 1985-03-20 ... ... ... ... ... ... ... ... ... ... 21638 Bibek Barakoti Male bibek.barakoti@fakeemail.com 9841288299 Facebook/Ads browser 2021-08-31 21:01:37.440162 100021638 1999-10-05 21639 Ansu Huzdar Female ansu.huzdar@fakeemail.com 9841694532 Google Search browser 2021-08-31 01:11:26.657379 100021639 2004-03-30 21640 Sumit Sanjel Male sumit.sanjel@fakeemail.com 9841638790 Google Search browser 2021-08-31 20:41:11.488311 100021640 1997-07-11 21641 Prapti Mushyakhow Female prapti.mushyakhow@fakeemail.com 9841359950 Facebook/Ads browser 2021-08-31 18:17:39.263566 100021641 2006-05-08 21642 Prazol Swarnakar Male prazol.swarnakar@fakeemail.com 9841429598 Facebook/Ads browser 2021-08-31 02:46:50.362896 100021642 1975-05-24 21643 rows × 9 columns\nclients = pd.concat([clients, location_df[[\u0026#39;lat\u0026#39;, \u0026#39;lon\u0026#39;, \u0026#39;location_name\u0026#39;]]], axis=1) list(clients) ['name', 'gender', 'email', 'phone', 'channel', 'first_contact', 'created_at', 'client_id', 'dob', 'lat', 'lon', 'location_name'] clients[ [\u0026#39;client_id\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;gender\u0026#39;, \u0026#39;dob\u0026#39;, \u0026#39;email\u0026#39;, \u0026#39;phone\u0026#39;, \u0026#39;channel\u0026#39;, \u0026#39;first_contact\u0026#39;, \u0026#39;lat\u0026#39;, \u0026#39;lon\u0026#39;, \u0026#39;location_name\u0026#39;, \u0026#39;created_at\u0026#39;] ].to_csv(\u0026#39;../datasets/clients.csv\u0026#39;, index=False) Find the analysis notebooks here: https://github.com/ayushsubedi/eHamroPasalmandu\n","date":"2021-09-12T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/synthetic_data_generation/","section":"posts","tags":["synthetic","python"],"title":"Synthetic Data Generation: eHamroPasalmandu.com"},{"categories":["open source","pet projects","proof of concept"],"contents":"Twitter Sentiment Analysis Getting data from twitter Using Tweepy (uses official twitter API) easy several limitations check https://developer.twitter.com/en/docs/rate-limits Using Twint (unofficial) a little difficult less limitation amazing for large volume Analysis Vader sentiment using python package https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664 Naive bayes for sentiment analysis (a little of DIY) https://www.datacamp.com/community/tutorials/simplifying-sentiment-analysis-python Spacy package Gensim package etc. Installation Clone the repo git clone https://github.com/cloudfactory/sentiment_analysis_twitter_starter_code\nCD into the cloned directory and create a virtualenv python -m venv env\nEnable virtualenv source env/bin/activate\nInstall dependency packages from requirements.txt pip install -r requirements.txt\nOper jupyter lab session jupyter-lab\nA simple twitter sentiment analysis poc import tweepy import json from tweepy import OAuthHandler import pandas as pd Full documentation here: https://docs.tweepy.org/en/stable/client.html#tweets\nAccess keys Apply at twitter developer and receive these: API_SECRET_KEY = \u0026#34;fill this in\u0026#34; API_KEY = \u0026#34;fill this in\u0026#34; ACCESS_TOKEN = \u0026#34;fill this in\u0026#34; ACCESS_TOKEN_SECRET = \u0026#34;fill this in\u0026#34; class TwitterClient(object): \u0026#39;\u0026#39;\u0026#39; Twitter Client \u0026#39;\u0026#39;\u0026#39; def __init__(self): \u0026#39;\u0026#39;\u0026#39; Class constructor or initialization method. \u0026#39;\u0026#39;\u0026#39; # read keys from the secret credentials file api_key = API_KEY api_secret =API_SECRET_KEY access_token = ACCESS_TOKEN access_token_secret = ACCESS_TOKEN_SECRET try: self.auth = OAuthHandler(api_key, api_secret) self.auth.set_access_token(access_token, access_token_secret) self.api = tweepy.API(self.auth) except: print(\u0026#39;Error: Authentication error\u0026#39;) def get_tweets(self): tweet = self.api.user_timeline(screen_name =\u0026#39;kathmandupost\u0026#39;, count=20) return tweet raw = TwitterClient().get_tweets() df = pd.json_normalize([r._json for r in raw]) df.head() created_at id id_str text truncated source in_reply_to_status_id in_reply_to_status_id_str in_reply_to_user_id in_reply_to_user_id_str ... user.profile_text_color user.profile_use_background_image user.has_extended_profile user.default_profile user.default_profile_image user.following user.follow_request_sent user.notifications user.translator_type user.withheld_in_countries 0 Tue Oct 26 01:20:38 +0000 2021 1452807348309868564 1452807348309868564 EDITORIAL: Protect paddy farmers\\n\\nEase of ac... True \u0026lt;a href=\"https://about.twitter.com/products/tw... None None None None ... 333333 True False False False False False False none [] 1 Tue Oct 26 00:45:00 +0000 2021 1452798380527161348 1452798380527161348 Despite some opposition, Oli appears to have p... True \u0026lt;a href=\"https://about.twitter.com/products/tw... None None None None ... 333333 True False False False False False False none [] 2 Mon Oct 25 23:15:00 +0000 2021 1452775731050782726 1452775731050782726 Over 100,000 doses of Pfizer-BioNtech vaccine ... True \u0026lt;a href=\"https://about.twitter.com/products/tw... None None None None ... 333333 True False False False False False False none [] 3 Mon Oct 25 21:45:00 +0000 2021 1452753082111209475 1452753082111209475 Congress may appoint deputy Speaker, leaving s... True \u0026lt;a href=\"https://about.twitter.com/products/tw... None None None None ... 333333 True False False False False False False none [] 4 Mon Oct 25 20:15:00 +0000 2021 1452730433033015296 1452730433033015296 Everything you need to know about the Covid-19... True \u0026lt;a href=\"https://about.twitter.com/products/tw... None None None None ... 333333 True False False False False False False none [] 5 rows × 70 columns\ndf.text 0 EDITORIAL: Protect paddy farmers\\n\\nEase of ac... 1 Despite some opposition, Oli appears to have p... 2 Over 100,000 doses of Pfizer-BioNtech vaccine ... 3 Congress may appoint deputy Speaker, leaving s... 4 Everything you need to know about the Covid-19... 5 United States to provide 100,620 doses of Pfiz... 6 Paddy damage by freak rains estimated at Rs8.2... 7 Dalit representatives complain of social discr... 8 Consult Delhi for census in Kalapani, census b... 9 Supreme Court justices to boycott full court m... 10 London expands vehicle levy to improve air qua... 11 ‘Children are going to die’, UN agency warns a... 12 EDITORIAL: Railblock ahead\\n\\nDelay in operati... 13 Nepal reports 673 new Covid-19 cases, 13 death... 14 Whether it’s supply or demand, oil era heads f... 15 Justices to decide their further step after me... 16 Climate change: what are the economic stakes?\\... 17 Oslo opens museum to “The Scream” painter Munc... 18 Everything you need to know about the Covid-19... 19 Fauci says vaccines for kids between 5-11 like... Name: text, dtype: object Sentiment analysis from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer def sentiment_scores(sentence): sid_obj = SentimentIntensityAnalyzer() return sid_obj.polarity_scores(sentence) df[\u0026#39;sentiment_scores\u0026#39;] = df.text.apply(sentiment_scores) df[[\u0026#39;text\u0026#39;, \u0026#39;sentiment_scores\u0026#39;]] text sentiment_scores 0 EDITORIAL: Protect paddy farmers\\n\\nEase of ac... {'neg': 0.0, 'neu': 0.779, 'pos': 0.221, 'comp... 1 Despite some opposition, Oli appears to have p... {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound... 2 Over 100,000 doses of Pfizer-BioNtech vaccine ... {'neg': 0.0, 'neu': 0.872, 'pos': 0.128, 'comp... 3 Congress may appoint deputy Speaker, leaving s... {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound... 4 Everything you need to know about the Covid-19... {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound... 5 United States to provide 100,620 doses of Pfiz... {'neg': 0.0, 'neu': 0.865, 'pos': 0.135, 'comp... 6 Paddy damage by freak rains estimated at Rs8.2... {'neg': 0.276, 'neu': 0.724, 'pos': 0.0, 'comp... 7 Dalit representatives complain of social discr... {'neg': 0.134, 'neu': 0.753, 'pos': 0.113, 'co... 8 Consult Delhi for census in Kalapani, census b... {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound... 9 Supreme Court justices to boycott full court m... {'neg': 0.164, 'neu': 0.582, 'pos': 0.255, 'co... 10 London expands vehicle levy to improve air qua... {'neg': 0.145, 'neu': 0.683, 'pos': 0.173, 'co... 11 ‘Children are going to die’, UN agency warns a... {'neg': 0.277, 'neu': 0.723, 'pos': 0.0, 'comp... 12 EDITORIAL: Railblock ahead\\n\\nDelay in operati... {'neg': 0.113, 'neu': 0.887, 'pos': 0.0, 'comp... 13 Nepal reports 673 new Covid-19 cases, 13 death... {'neg': 0.17, 'neu': 0.83, 'pos': 0.0, 'compou... 14 Whether it’s supply or demand, oil era heads f... {'neg': 0.061, 'neu': 0.939, 'pos': 0.0, 'comp... 15 Justices to decide their further step after me... {'neg': 0.076, 'neu': 0.762, 'pos': 0.162, 'co... 16 Climate change: what are the economic stakes?\\... {'neg': 0.0, 'neu': 0.901, 'pos': 0.099, 'comp... 17 Oslo opens museum to “The Scream” painter Munc... {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound... 18 Everything you need to know about the Covid-19... {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound... 19 Fauci says vaccines for kids between 5-11 like... {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound... sentiments = pd.concat([df, df.sentiment_scores.apply(pd.Series)], axis=1)[[\u0026#39;text\u0026#39;, \u0026#39;neg\u0026#39;, \u0026#39;neu\u0026#39;, \u0026#39;pos\u0026#39;]] sentiments text neg neu pos 0 EDITORIAL: Protect paddy farmers\\n\\nEase of ac... 0.000 0.779 0.221 1 Despite some opposition, Oli appears to have p... 0.000 1.000 0.000 2 Over 100,000 doses of Pfizer-BioNtech vaccine ... 0.000 0.872 0.128 3 Congress may appoint deputy Speaker, leaving s... 0.000 1.000 0.000 4 Everything you need to know about the Covid-19... 0.000 1.000 0.000 5 United States to provide 100,620 doses of Pfiz... 0.000 0.865 0.135 6 Paddy damage by freak rains estimated at Rs8.2... 0.276 0.724 0.000 7 Dalit representatives complain of social discr... 0.134 0.753 0.113 8 Consult Delhi for census in Kalapani, census b... 0.000 1.000 0.000 9 Supreme Court justices to boycott full court m... 0.164 0.582 0.255 10 London expands vehicle levy to improve air qua... 0.145 0.683 0.173 11 ‘Children are going to die’, UN agency warns a... 0.277 0.723 0.000 12 EDITORIAL: Railblock ahead\\n\\nDelay in operati... 0.113 0.887 0.000 13 Nepal reports 673 new Covid-19 cases, 13 death... 0.170 0.830 0.000 14 Whether it’s supply or demand, oil era heads f... 0.061 0.939 0.000 15 Justices to decide their further step after me... 0.076 0.762 0.162 16 Climate change: what are the economic stakes?\\... 0.000 0.901 0.099 17 Oslo opens museum to “The Scream” painter Munc... 0.000 1.000 0.000 18 Everything you need to know about the Covid-19... 0.000 1.000 0.000 19 Fauci says vaccines for kids between 5-11 like... 0.000 1.000 0.000 Depending on the threshold you create, you can also label something as positive or negative sentiment now ","date":"2021-08-11T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/sentiment_analysis/","section":"posts","tags":["twitter","sentiment","python"],"title":"Sentiment Analysis starter code"},{"categories":["open source","pet projects","proof of concept"],"contents":"Nepal Vaccine Progress Twitter Bot https://twitter.com/NepalVaccine Data Source: https://github.com/owid/covid-19-data/blob/master/public/data/vaccinations/country_data/Nepal.csv\nThis is a Twitter bot that shows progress on covid vaccination (full and partial) in Nepal.\nInstallation for contribution Clone the repository https://github.com/ayushsubedi/vaccine_progress_bot_nepal/ CD into the cloned directory and create a virtualenv python -m venv env Enable virtualenv .\\env\\Scripts\\activate Enable virtualenv (windows) .\\env\\Scripts\\activate Install dependency packages from requirements.txt pip install -r requirements.txt Run flask app source FLASK_APP=\u0026#34;app.py\u0026#34; flask run Run flask app (windows) $env:FLASK_APP=\u0026#34;app.py\u0026#34; flask run .env Add Twitter keys and BasicAuth keys to the .env_sample file. Once complete rename it to .env\n","date":"2021-07-20T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/vaccine_progress/","section":"posts","tags":["flask","python","bot"],"title":"Nepal Vaccine Progress Twitter Bot"},{"categories":["pet projects","data viz","analytics"],"contents":"The Emojis of Kathmandu 50K tweets between 2012 to 2021 (containing emojis, and geocoded to Kathmandu) show that we really love 😂\nRecently though, we have been 😂 less and 🙏 more, 😁 less and ❤️ more. Just goes to show Kathmandu is healing from Covid.\nGithub repo: https://github.com/ayushsubedi/emojan\nCreate the dataset using twint import twint import pandas as pd import nest_asyncio nest_asyncio.apply() emoji_list = [\u0026#39;😁\u0026#39;, \u0026#39;😒\u0026#39;, \u0026#39;😊\u0026#39;, \u0026#39;😈\u0026#39;, \u0026#39;🇳🇵\u0026#39;, \u0026#39;😌\u0026#39;, \u0026#39;☕\u0026#39;, \u0026#39;👶\u0026#39;, \u0026#39;👍\u0026#39;, \u0026#39;😷\u0026#39;, \u0026#39;👌\u0026#39;, \u0026#39;🌞\u0026#39;, \u0026#39;😑\u0026#39;, \u0026#39;😉\u0026#39;, \u0026#39;😍\u0026#39;, \u0026#39;☺️\u0026#39;, \u0026#39;😴\u0026#39;, \u0026#39;😱\u0026#39;, \u0026#39;🙏\u0026#39;, \u0026#39;😘\u0026#39;, \u0026#39;🙌\u0026#39;, \u0026#39;😔\u0026#39;, \u0026#39;😋\u0026#39;, \u0026#39;😂\u0026#39;, \u0026#39;😩\u0026#39;, \u0026#39;💕\u0026#39;, \u0026#39;😎\u0026#39;, \u0026#39;😭\u0026#39;, \u0026#39;😳\u0026#39;, \u0026#39;😇\u0026#39;, \u0026#39;😏\u0026#39;, \u0026#39;😜\u0026#39;, \u0026#39;☕️\u0026#39;, \u0026#39;✌️\u0026#39;, \u0026#39;🙈\u0026#39;, \u0026#39;❤️\u0026#39;, \u0026#39;😄\u0026#39;, \u0026#39;💞\u0026#39;] for emoji in emoji_list: c = twint.Config() c.Search = emoji c.Pandas = True c.Store_csv = True c.Output = emoji c.Hide_output= True c.Near= \u0026#34;kathmandu\u0026#34; c.Since=\u0026#34;2010-01-01\u0026#34; c.Until = \u0026#34;2021-05-31\u0026#34; twint.run.Search(c) Perform analysis import pandas as pd df = pd.read_csv(\u0026#34;../datasets/merged.csv\u0026#34;, lineterminator=\u0026#39;\\n\u0026#39;, parse_dates=[\u0026#39;date\u0026#39;]) list(df) ['id', 'emoji', 'date', 'username', 'tweet', 'likes_count', 'place'] df.username.value_counts().head(20) ms_madhur 1433 sristee44 1121 beingsamikshya 871 anuskashresthax 618 nepalplanettrek 594 scousergirl 572 dreamingdr 569 nepaligentleman 476 itsme_shivangi 433 iamnabinraj75 410 sim_shrestha 402 milan_pu1 353 raunakbasnet1 332 thulokanxo 325 chetan_karki 302 saampokhrel 301 mongolianheartk 300 tenzintsetenbhu 290 rana1997rohit 275 fatyangri 256 Name: username, dtype: int64 df.emoji.value_counts().head(20) 😂 7576 😊 4016 😍 3781 😁 3482 🇳🇵 2785 😎 2270 ❤️ 2233 🙏 2122 😉 2000 😜 1463 😘 1432 😋 1241 💕 1173 😄 1095 👍 1001 👌 730 😭 637 😏 554 ✌️ 505 😒 483 Name: emoji, dtype: int64 df.date.min() Timestamp('2012-01-28 00:00:00') df.set_index(\u0026#39;date\u0026#39;).resample(\u0026#39;M\u0026#39;)[\u0026#39;id\u0026#39;].count() date 2012-01-31 2 2012-02-29 16 2012-03-31 13 2012-04-30 3 2012-05-31 11 ... 2021-01-31 212 2021-02-28 134 2021-03-31 183 2021-04-30 160 2021-05-31 119 Freq: M, Name: id, Length: 113, dtype: int64 df.set_index(\u0026#39;date\u0026#39;).resample(\u0026#39;M\u0026#39;)[\u0026#39;id\u0026#39;].count().plot() \u0026lt;AxesSubplot:xlabel='date'\u0026gt; df[df.emoji==\u0026#34;❤️\u0026#34;].set_index(\u0026#39;date\u0026#39;).resample(\u0026#39;M\u0026#39;)[\u0026#39;id\u0026#39;].count().plot() \u0026lt;AxesSubplot:xlabel='date'\u0026gt; df[df.emoji==\u0026#34;😷\u0026#34;].set_index(\u0026#39;date\u0026#39;).resample(\u0026#39;M\u0026#39;)[\u0026#39;id\u0026#39;].count().plot() \u0026lt;AxesSubplot:xlabel='date'\u0026gt; df[df.emoji==\u0026#34;🇳🇵\u0026#34;].set_index(\u0026#39;date\u0026#39;).resample(\u0026#39;M\u0026#39;)[\u0026#39;id\u0026#39;].count().plot() \u0026lt;AxesSubplot:xlabel='date'\u0026gt; df[df.emoji==\u0026#34;😂\u0026#34;].set_index(\u0026#39;date\u0026#39;).resample(\u0026#39;M\u0026#39;)[\u0026#39;id\u0026#39;].count().plot() \u0026lt;AxesSubplot:xlabel='date'\u0026gt; location_tweet = df.dropna(subset=[\u0026#39;place\u0026#39;]) import json import math def getlatlon(row): place = row.place place = place.replace(\u0026#34;\\\u0026#39;\u0026#34;, \u0026#34;\\\u0026#34;\u0026#34;) place = json.loads(place) lat = place[\u0026#39;coordinates\u0026#39;][0] lon = place[\u0026#39;coordinates\u0026#39;][1] return pd.Series([lat, lon],index=[\u0026#39;lat\u0026#39;,\u0026#39;lon\u0026#39;]) df = df.join(location_tweet.apply(getlatlon, axis=1, result_type=\u0026#34;expand\u0026#34;)) df.to_csv(\u0026#39;location.csv\u0026#39;, index=False) df id emoji date username tweet likes_count place lat lon 0 1398437418768818176 👌 2021-05-29 dendikapan शुभ बिहानी ☕️☕️ शुभदिनकाे कामना🙏😷👌🇳🇵💞 Hopefull... 3 NaN NaN NaN 1 1397792077149216769 👌 2021-05-27 dendikapan @damanbro66 Yes I like it 👌so much your post ... 0 NaN NaN NaN 2 1397341721566990336 👌 2021-05-26 dendikapan @KiranCh77 Really nice 👌thanks for sharing 👌 ... 0 NaN NaN NaN 3 1394863068035813379 👌 2021-05-19 iammuhnaj It's still Bull season👌 @ 𝙃𝙊𝙈𝙀 https://t.co/O... 1 {'type': 'Point', 'coordinates': [27.713776, 8... 27.713776 85.310244 4 1393821979032133635 👌 2021-05-16 chetan_karki #just #fun with #baby #myrahsofkarki 😊❤️ looki... 0 {'type': 'Point', 'coordinates': [27.67733347,... 27.677333 85.307636 ... ... ... ... ... ... ... ... ... ... 45362 389751359874289664 ❤️ 2013-10-14 ashmoo_moo #littleone #cuteness #dashain #tika #sister #c... 0 {'type': 'Point', 'coordinates': [27.73048071,... 27.730481 85.330964 45363 389704517815902208 ❤️ 2013-10-14 ashmoo_moo Baba. Prajjwal Dai. ❤️ #dashain #tika #family ... 0 {'type': 'Point', 'coordinates': [27.73855019,... 27.738550 85.338760 45364 387917234490048512 ❤️ 2013-10-09 ashmoo_moo #brother #cousins #mamaghar #nagpokhari #morni... 0 {'type': 'Point', 'coordinates': [27.71356987,... 27.713570 85.324463 45365 387542385145954304 ❤️ 2013-10-08 ashmoo_moo #cuteness #babysister #cousins #smile ❤️😘💋 @ M... 0 {'type': 'Point', 'coordinates': [27.7090305, ... 27.709031 85.326469 45366 386357639460179968 ❤️ 2013-10-05 ashmoo_moo on our way back homeeeeee.... ❤️😘 #lastnight #... 0 {'type': 'Point', 'coordinates': [27.73868383,... 27.738684 85.338705 45367 rows × 9 columns\ndf.emoji.value_counts() 😂 7576 😊 4016 😍 3781 😁 3482 🇳🇵 2785 😎 2270 ❤️ 2233 🙏 2122 😉 2000 😜 1463 😘 1432 😋 1241 💕 1173 😄 1095 👍 1001 👌 730 😭 637 😏 554 ✌️ 505 😒 483 💞 461 😇 432 🙌 423 ☺️ 375 😔 373 😌 332 😱 314 😑 296 😈 292 🙈 291 ☕ 277 🌞 213 😳 185 😩 181 😴 163 😷 135 👶 45 Name: emoji, dtype: int64 ","date":"2021-07-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/emojis_of_kathmandu/","section":"posts","tags":["tableau","viz"],"title":"The Emojis of Kathmandu"},{"categories":["world bank","data viz","analytics"],"contents":"Data in News Revisited Ayush Subedi developed this dashboard with the guidance from Ravi Kumar. The dashboard is based on the methodology proposed by Klein, Galdin, and Mohamedou in 2016. This dashboard was initiated to assess the use of data in media in Nepal to provide an input to a World Bank report titled \u0026ldquo;Use of Data in the Private Sector of Nepal : The Current State and Opportunities in Finance, Education, and the Media\u0026rdquo;. This publication was funded by the Trust Fund for Statistical Capacity Building, a global grant facility administered by the Development Data Group of the World Bank on behalf of the contributing donors, and the Partnership for Knowledge-Based Poverty Reduction and Shared Prosperity, a World Bank project with support from the UK\u0026rsquo;s Foreign, Commonwealth and Development Office (FCDO) to increase the production and usage of data and statistics in Nepal.\nIntroduction How journalists use data in their reporting or in their writing can be viewed as a reflection of the country\u0026rsquo;s demand for statistics and data. The World Bank team developed an indicator model to assess the use of data by Nepali news portals based on the methodology proposed by Klein, Galdin, and Mohamedou 2016. As of 07 Jun 2021, the results show that very few news articles indicate the source of data or mention development indicators. However, articles that discuss data, reports, research, statistics, and related topics do critically engage with these ideas.\n101060 articles were analyzed to assess the use of data in media. In the previous implementation of this project, the source of data was RSS feeds from Nepali news portal. Unfortunately, due to most of the Nepali news portals deprecating their RSS feature, for this implementation, the news articles are scraped from article URLs posted by the official news portal\u0026rsquo;s account on twitter. The news portals that use twitter to post articles regularly are The Himalayan Times, Onlinekhabar, The Nepali Times, myRepublica, and The Kathmandu Post.\nImplementation Data in NEWS portal The results from the project was used in the research article published at http://documents1.worldbank.org/curated/en/805261601023506163/pdf/Use-of-Data-in-the-Private-Sector-of-Nepal-The-Current-State-and-Opportunities-in-Finance-Education-and-the-Media.pdf\nAs of 9th February 2020, the results showed that very few news articles indicate the source of data or mention development indicators. However, articles that discuss data, reports, research, statistics, and related topics do critically engage with these ideas.\nUnfortunately, the data collection process was halted because we ran out of our AWS Activate for Startups credits.\nTODOS create a simple architecture diagram curate a list of twitter handles of Nepali newspapers that put URL to their newspapers use twint to create datasets (historic) check if url is present, url does not 404, and url belongs to the publisher use Newspaper to collect important information (author, etc) and complete the dataset research dash, swifter and pandarallel optimize regular expression functions (if possible this should be performed during scraping) create a sqlite database to hold access information (and design schema around other meta) or find and alternative add checkpoints (using twint resume or build it manually) POC create a full working version of the product research downloading twint as pandas to merge with previous data Twitter handles @kathmandupost @thehimalayan @NepaliTimes @OnlineKhabar_En @RepublicaNepal The Frugal way This project leverages on Github Actions for scheduling and scraping purposes. This project is hosted on Heroku free tier (free ssl) Uses amazing open source projects, primariliy (Twint, Newspaper) etc Previous iteration Find the previous iteration here: https://ayushsubedi.github.io/posts/data_in_news/\n","date":"2021-03-03T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/data_in_news_revisited/","section":"posts","tags":["analysis","proxy"],"title":"Data in News Revisited"},{"categories":["open source","pet projects","proof of concept"],"contents":"kohokoho (को हो को हो !) Are you an aspiring data analyst? Do clients hesitate to give you their data because of all the sensitive information in it?\nOr, are you an entrepreneur hesitant to hire a data consultant due to the nature of your data?\nMask your data using kohokoho (को हो को हो !)\nKohokoho (को हो को हो) is a python package that anonymizes a dataset. Currently, it is still in its infancy and only supports a select few data types.\nFeel free to contribute with code (it\u0026rsquo;s all open source), or by giving it a better name :D\nInstallation pip install kohokoho\nInstallation (from source) Clone the repository git clone https://github.com/ayushsubedi/kohokoho\nCD into the cloned directory and create a virtualenv python -m venv env\nEnable virtualenv Windows\n.\\env\\Scripts\\activate\nMac/Linux\nsource env/bin/activate\nInstall dependency packages from requirements.txt pip install -r requirements.txt\nRun the app python kohokoho.py --csv={location of csv file}\nPackage link: https://pypi.org/project/kohokoho/\nTest it here https://kohokoho.herokuapp.com/\nVersion-O Demo ","date":"2021-02-03T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/kohokoho/","section":"posts","tags":["cli","python","web app","flask"],"title":"Kohokoho"},{"categories":["open source","pet projects","proof of concept"],"contents":"Choto pip install choto\nIf you spend most of your time on the terminal, and you need to update yourself on the news, use \u0026ldquo;choto\u0026rdquo;. Choto will summarize the news for you.\nPython package: https://pypi.org/project/choto/\nInstallation (from source) Clone the repository git clone https://github.com/ayushsubedi/choto\nCD into the cloned directory and create a virtualenv python -m venv env\nEnable virtualenv .\\env\\Scripts\\activate\nInstall dependency packages from requirements.txt pip install -r requirements.txt\nRun the app python choto.py --url=https://abc.xyz --ratio=0.5 --algorithm=bert\nUsage: choto.py [OPTIONS]\nOptions: --url TEXT Enter a valid URL --ratio FLOAT Ratio to summarize to. --algorithm [gensim|spacy|bert] Algorithm to use --help Show this message and exit. ","date":"2021-02-02T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/choto/","section":"posts","tags":["cli","python"],"title":"Choto"},{"categories":["moonlit","data viz","analytics"],"contents":"Developers in Nepal vs the world Link to the dashboard: https://devsinnepal.github.io/\nThis dashboard facilitates a comparative study of the coding landscape in Nepal with the rest of the world. This tool will help visualize and understand developers, their demographic and geographic profile, motivation, education, coding preference and several other factors.\nThe survey data has been utilized from StackOverflow. For almost a decade, StackOverflow\u0026rsquo;s annual Developer Survey has been the largest survey of people who code around the world. The survey is taken by nearly 65,000 people in 2020. 242 people participated from Nepal.\nWhile Stackoverflow also publishes its analysis on the survey annually, it primarily has a wide focus on aggregated global trends.\nInsights Nepal is one of the top destination to outsource technology. As we can see in the chart below, the annual median developer remuneration is the lowest in Nepal ($6300). It is still more than 6 times the median per capita income in Nepal. Even with fairly low remuneration, productivity is notably high. In the last 20 yrs, the quality of technology services these pool of talents can/have built is very close to the international standards.\nThe annual median remuneration in the US is USD. 115,000. Broken into hourly pay (40 hours in a week, 52 weeks in a year), a Nepali developer makes USD. 3.03 an hour whereas a developer in the US makes USD 55.30 an hour. There is a stark difference in the pay between the developers in US vs. Nepal which suggests that there is a great value for employers in outsourcing technology in Nepal. It\u0026rsquo;s a win-win for both, promotes high-paying growth-oriented jobs for talented Nepali developers and quality service at a lower cost for employers.\nSimilar to other countries as well.\nThe trend of developers participating in StackOverflow Survey from Nepal has increased over the years. This implies growing popularity of Stackoverflow among the developers and a possibility of more inclusive and accurate results.\nIn the trend below, a certain spike is seen during 2018. However, the number is not largely deviated. Considering, 2018 as an outlier, we can see a growing trend.\nJavaScript and Python are the two most desired language among developers of Nepal. CURRENTLY USED LANGUAGE DESIRED LANGUAGE NUMBER OF DEVELOPERS HTML/CSS JavaScript 86 JavaScript Python 83 HTML/CSS Python 82 SQL Python 62 This analysis also shows that most of the developers are currently familiar with HTML/CSS . However, most developers desire to learn JavaScript in future.\nFurthermore, most developers who have worked with JavaScript and HTML/CSS desire to learn Python indicating its popularity amongst the community too.\n","date":"2020-12-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/devs_in_nepal/","section":"posts","tags":["tableau","stackoverflow","analysis"],"title":"Devs in Nepal"},{"categories":["open source","pet projects","proof of concept"],"contents":"Affirmations with flutter It had been a while since I (subconsciously or not so subconsciously) pivoted my career from app development to data science. Man, I did not miss Android Studio clogging my base model laptop’s memory. However, my little cousin (who just ventured into app development) wanted help with one of the issues he was having. But, as someone who had not used semicolons and braces for almost couple of years, Dart was almost nauseating upon first glance. I promised him I would get around answering his queries, but after I built something simple first.\nAffirmations.dev provides quotes that are motivating to the devs. So, as a newbie in Flutter, it good to get all the positive reinforcement especially when Android Studio and the virtual device doesn’t leave much space for many Stackoverflow tabs.\nFew examples of what affirmations.dev returns: // 20200927192236 // https://www.affirmations.dev/ { \u0026quot;affirmation\u0026quot;: \u0026quot;Your life is already a miracle of chance waiting for you to shape its destiny\u0026quot; } // 20200927194030 // https://www.affirmations.dev/ { \u0026quot;affirmation\u0026quot;: \u0026quot;It is not a sprint, it is a marathon. One step at a time\u0026quot; } // 20200927194048 // https://www.affirmations.dev/ { \u0026quot;affirmation\u0026quot;: \u0026quot;You are learning valuable lessons from yourself every day\u0026quot; } Complete App import 'dart:convert'; import 'package:flutter/material.dart'; import 'package:http/http.dart' as http; class AffirmationsModel{ final String affirmation; AffirmationsModel({this.affirmation}); factory AffirmationsModel.fromJson(final json){ return AffirmationsModel( affirmation : json[\u0026quot;affirmation\u0026quot;] ); } } Future\u0026lt;AffirmationsModel\u0026gt; fetchAffirmations() async { final response = await http.get('https://www.affirmations.dev/'); if (response.statusCode == 200) { final jsonAffirmations = jsonDecode(response.body); return AffirmationsModel.fromJson(jsonAffirmations); } else { throw Exception('Failed'); } } void main() { runApp(AffirmationsApp()); } class AffirmationsApp extends StatefulWidget { AffirmationsApp({Key key}) : super(key: key); @override _AffirmationsAppState createState() { return _AffirmationsAppState(); } } class _AffirmationsAppState extends State\u0026lt;AffirmationsApp\u0026gt; { Future\u0026lt;AffirmationsModel\u0026gt; _futureAffirmationsModel; @override void initState() { super.initState(); _futureAffirmationsModel = fetchAffirmations(); } @override Widget build(BuildContext context) { return MaterialApp( title: 'Affirmations', theme: ThemeData( primarySwatch: Colors.pink, visualDensity: VisualDensity.adaptivePlatformDensity, ), debugShowCheckedModeBanner: false, home: Scaffold( backgroundColor: Colors.deepPurpleAccent, appBar: AppBar( title: Text('Affirmations'), centerTitle: true, ), body: Column(children: \u0026lt;Widget\u0026gt;[ Container( alignment: Alignment.center, decoration: BoxDecoration( shape: BoxShape.circle, color: Colors.lightBlueAccent, ), // width: 300, height: 300, padding: EdgeInsets.all(20.0), margin: EdgeInsets.all(80.0), child: FutureBuilder\u0026lt;AffirmationsModel\u0026gt;( future: _futureAffirmationsModel, builder: (context, snapshot) { if (snapshot.connectionState == ConnectionState.done \u0026amp;\u0026amp; snapshot.hasData) { return Text(snapshot.data.affirmation, textAlign: TextAlign.center, textScaleFactor: 1.4, style: TextStyle(color: Colors.white)); } else if (snapshot.hasError) { return Text(\u0026quot;${snapshot.error}\u0026quot;); } return CircularProgressIndicator(); }, ), ), Divider(), Container( child: RaisedButton( onPressed: () { setState(() { _futureAffirmationsModel = fetchAffirmations(); }); }, textColor: Colors.white, padding: const EdgeInsets.all(0.0), child: Container( decoration: const BoxDecoration( gradient: LinearGradient( colors: \u0026lt;Color\u0026gt;[ Color(0xEEFF1461), Color(0xDDFF1483), Color(0xFFFF1493), ], ), ), padding: const EdgeInsets.all(30.0), child: const Text('INSPIRE ME', style: TextStyle(fontSize: 15)), ), ), ) ])), ); } } ","date":"2020-07-27T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/affirmations_dev/","section":"posts","tags":["flutter","affirmations","android"],"title":"Affirmations with Flutter"},{"categories":["pet projects","data viz"],"contents":"K-Shaped recovery A K-shaped recovery encapsulates the idea that some industries in the economy will recover (or actually prevail in the context of Nepal), while others will decline or fail to recover nearly as quickly.\nOne example of the former includes the software service sector (outsourcing, e-commerce, delivery, etc.). On the unfortunate side, an example would be the hospitality/tourism sector.\nThe challenge for time series analysts is to figure out ways to fit this into their model depending on the COVID elasticity for their sectors. COVID, a \u0026ldquo;black swan event\u0026rdquo; has screwed up the Business As Usual.\n","date":"2020-07-15T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/k_shaped_recovery/","section":"posts","tags":["data viz"],"title":"K-Shaped recovery"},{"categories":["moonlit","analytics"],"contents":"Synthetic data based on pareto principle Nowadays, the phrase \u0026ldquo;data is the new oil\u0026rdquo; seems a little cliche. However, it is true. The demand for data has grown significantly, as people and businesses have realized the possibilities of leveraging data for informed decision making. Often, it is not possible to get data to our specific needs. As a ML teacher, I have heard my students claim that it is really challenging for them to find data to their specifications for several of the projects that they want to pursue (in this case for learning data science or for school projects). In other cases, where confidentiality is crucial, the possibility of open data is minimal (anonamized data is there but that is another difficult avenue). Synthetic data can help solve these issues. Synthetic data is \u0026ldquo;any production data applicable to a given situation that are not obtained by direct measurement\u0026rdquo; according to the McGraw-Hill Dictionary of Scientific and Technical Terms. However, synthetic does not imply random. The patterns in synthetic data should mimic real world patterns. This is a priliminary exercise.\nCreating a synthetic dataset for users and users transactions that demonstrate the Pareto principle. Compulsory fields include basic demographic information, email address, phone number, item number and price. Imports import pandas as pd import numpy as np, numpy.random import random import matplotlib.pyplot as plt Brute Force Solution Brute Force Assumption: All ticket size the same, Customers only purchase once.\nCreate a naive list for users This will be modified to look real soon. In real world case, this can be thought of user/client table with incremental id.\n# variables final_dataframe_size = 1000 # in this case, these are the 1000 customers who have bought something. # Generate users users = [x for x in range (1000, 10000)] users[:10] [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009] # shuffle the original list random.shuffle(users) users[:10] [8815, 2924, 6563, 3568, 3604, 6821, 1466, 9813, 3153, 6637] Sampling random 1000 users who have transacted. This is because of the assumption that not every user will have transacted. tran_users = random.sample(users, final_dataframe_size) print (len(tran_users)) print (tran_users[:10]) 1000 [6113, 5352, 4711, 2600, 5643, 3009, 7765, 7301, 1399, 2336] pareto_tran_users = random.sample(tran_users, int(.2 * final_dataframe_size)) len(pareto_tran_users) 200 df = pd.DataFrame({\u0026#39;user\u0026#39;: tran_users}) df.head() user 0 6113 1 5352 2 4711 3 2600 4 5643 If the sum of all transactions is 100000,\npareto users = 0.8*100000 = 80000 to be divided into 200 users, each user gets = 400\nnon pareto users = .2*100000 = 20000 to be divided into 800 users, each user gets = 25\ndef assign_amount(val): if val in pareto_tran_users: return 400 return 25 df[\u0026#39;transaction\u0026#39;] = df.user.apply(assign_amount) # validate pareto df.sort_values(\u0026#39;transaction\u0026#39;, ascending=False).head(int(.2 * final_dataframe_size)).sum()/df.sum() user 0.20033 transaction 0.80000 dtype: float64 Here we see 20 percen generating 80 percent of the revenue.\nImprovement 1 Make the sales figure more dynamic (instead of 25 and 400) def sum_to_x(n, x): values = [0.0, x] + list(np.random.uniform(low=0.0,high=x,size=n-1)) values.sort() return [values[i+1] - values[i] for i in range(n)] df2 = df.copy() df2_p = df2[df2.user.isin(pareto_tran_users)] df2_p_n = df2[~df2.user.isin(pareto_tran_users)] # .apply(sum_to_x(20000, 80000)) df2_p[\u0026#39;transaction\u0026#39;] = sum_to_x(200, 80000) df2_p_n[\u0026#39;transaction\u0026#39;] = sum_to_x(800, 20000) df2 = df2_p.append(df2_p_n) df2 user transaction 4 5643 271.252382 16 1403 64.145236 20 2671 150.894671 27 1881 3.295457 29 4574 129.600777 ... ... ... 991 7792 58.155219 992 4682 18.239724 994 5986 4.711870 996 8145 0.914484 998 4102 20.064127 1000 rows × 2 columns\ndf2.sum() user 5553121.0 transaction 100000.0 dtype: float64 # validate pareto df2.sort_values(\u0026#39;transaction\u0026#39;, ascending=False).head(200).sum().apply(lambda x: \u0026#39;%.3f\u0026#39; % x) user 1136745.000 transaction 82287.864 dtype: object Upload some data To make the data more realistic, we can add more features. In this case, mockaroo was used to create the features. df_users = pd.read_csv(\u0026#39;synthetic_users.csv\u0026#39;) df_users.head() id first_name last_name email gender 0 1 Pierette Deners pdeners0@1und1.de Female 1 2 Gage Opdenorth gopdenorth1@hugedomains.com Male 2 3 Raquel Gabriel rgabriel2@studiopress.com Female 3 4 Matti Sullly msullly3@soundcloud.com Female 4 5 Zita Doggart zdoggart4@ftc.gov Female The client table seems complete. Let us create an item table which follows normal distribution. df_items = pd.read_csv(\u0026#39;synthetic_items_catalog.csv\u0026#39;) df_items.drop_duplicates(subset=[\u0026#39;product\u0026#39;], keep=\u0026#39;first\u0026#39;, inplace=True) df_items[\u0026#39;id\u0026#39;] = df_items[\u0026#39;id\u0026#39;] + 1000 df_items id product 0 1001 Bread Ww Cluster 1 1002 Glove - Cutting 2 1003 Ice Cream - Super Sandwich 3 1004 Wine - Chateau Bonnet 4 1005 Ecolab - Ster Bac ... ... ... 992 1993 Table Cloth 53x53 White 993 1994 Durian Fruit 995 1996 Chinese Foods - Plain Fried Rice 996 1997 Wine - White, Ej Gallo 998 1999 Sugar - Fine 836 rows × 2 columns\nprices = np.random.normal(50, 20, 836) df_items[\u0026#39;prices\u0026#39;] = prices df_items[\u0026#39;prices\u0026#39;] = np.where(df_items[\u0026#39;prices\u0026#39;] \u0026lt; 0 , 2.0, df_items[\u0026#39;prices\u0026#39;]) df_items[\u0026#39;prices\u0026#39;] = round(df_items[\u0026#39;prices\u0026#39;]) df_items.describe() id prices count 836.000000 836.000000 mean 1469.366029 50.340909 std 285.036373 20.400653 min 1001.000000 2.000000 25% 1220.500000 36.000000 50% 1460.500000 50.000000 75% 1710.250000 64.000000 max 1999.000000 120.000000 Now we have users and product table. The product table consists of items that are uniformly distributed.\nnp.random.normal(0, 1, 10) array([ 0.7365823 , 0.18626006, -0.10850225, -0.8307533 , 0.18490917, 0.51846682, -1.20012347, 0.31953487, -0.12546108, 0.75146225]) ## Could not figure out how to generate normal distribution frame, ## so opting out for uniform distribution # weights_users = np.random.normal(1000, 1000, len(df_users.id)) weights_items = np.random.normal(1000, 1000, len(df_items.id)) tran_user = random.choices(random.sample(list(df_users.id), 268), k=10000) tran_items = random.choices(list(df_items.id), weights = weights_items, k=10000) df = pd.DataFrame.from_dict({\u0026#39;user_id\u0026#39;: tran_user, \u0026#39;item_id\u0026#39;: tran_items})\\ .set_index(\u0026#39;item_id\u0026#39;)\\ .join(df_items.set_index(\u0026#39;id\u0026#39;)) df_ = df.groupby(\u0026#39;user_id\u0026#39;)[[\u0026#39;prices\u0026#39;]].sum() df_.describe() prices count 268.000000 mean 1889.985075 std 343.707196 min 1161.000000 25% 1649.750000 50% 1896.500000 75% 2114.500000 max 2860.000000 # validate pareto # regardless of transaction table, hamro users 200 jana df_.reset_index(inplace=True) (df_.sort_values(\u0026#39;prices\u0026#39;, ascending=False).head(200).sum())/df_.sum() user_id 0.732714 prices 0.803493 dtype: float64 merged = df.reset_index()\\ .set_index(\u0026#39;user_id\u0026#39;)\\ .join(df_users.set_index(\u0026#39;id\u0026#39;), how=\u0026#39;outer\u0026#39;)\\ .reset_index()\\ .rename(columns={\u0026#39;index\u0026#39;:\u0026#39;product_id\u0026#39;, \u0026#39;level_0\u0026#39;: \u0026#39;user_id\u0026#39;}) merged user_id product_id product prices first_name last_name email gender 0 1 1013.0 Shrimp - Black Tiger 13/15 78.0 Pierette Deners pdeners0@1und1.de Female 1 1 1025.0 Longos - Lasagna Veg 48.0 Pierette Deners pdeners0@1und1.de Female 2 1 1032.0 Wine - Pinot Grigio Collavini 54.0 Pierette Deners pdeners0@1und1.de Female 3 1 1067.0 Shiro Miso 85.0 Pierette Deners pdeners0@1und1.de Female 4 1 1073.0 Pea - Snow 69.0 Pierette Deners pdeners0@1und1.de Female ... ... ... ... ... ... ... ... ... 10727 1000 1866.0 Figs 56.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10728 1000 1902.0 Wine - Harrow Estates, Vidal 62.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10729 1000 1924.0 Pancetta 25.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10730 1000 1933.0 Vodka - Smirnoff 23.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10731 1000 1994.0 Durian Fruit 51.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10732 rows × 8 columns\nmerged.groupby(\u0026#39;user_id\u0026#39;)[[\u0026#39;prices\u0026#39;]].sum().sort_values(\u0026#39;prices\u0026#39;, ascending=False).head(200).sum()/merged[\u0026#39;prices\u0026#39;].sum() prices 0.803493 dtype: float64 merged.to_csv(\u0026#39;merged.csv\u0026#39;, index=False) df_users.to_csv(\u0026#39;users.csv\u0026#39;, index=False) df_items.to_csv(\u0026#39;items.csv\u0026#39;, index=False) merged user_id product_id product prices first_name last_name email gender 0 1 1013.0 Shrimp - Black Tiger 13/15 78.0 Pierette Deners pdeners0@1und1.de Female 1 1 1025.0 Longos - Lasagna Veg 48.0 Pierette Deners pdeners0@1und1.de Female 2 1 1032.0 Wine - Pinot Grigio Collavini 54.0 Pierette Deners pdeners0@1und1.de Female 3 1 1067.0 Shiro Miso 85.0 Pierette Deners pdeners0@1und1.de Female 4 1 1073.0 Pea - Snow 69.0 Pierette Deners pdeners0@1und1.de Female ... ... ... ... ... ... ... ... ... 10727 1000 1866.0 Figs 56.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10728 1000 1902.0 Wine - Harrow Estates, Vidal 62.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10729 1000 1924.0 Pancetta 25.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10730 1000 1933.0 Vodka - Smirnoff 23.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10731 1000 1994.0 Durian Fruit 51.0 Hillery Dickings hdickingsrr@gizmodo.com Male 10732 rows × 8 columns\ndf_users id first_name last_name email gender 0 1 Pierette Deners pdeners0@1und1.de Female 1 2 Gage Opdenorth gopdenorth1@hugedomains.com Male 2 3 Raquel Gabriel rgabriel2@studiopress.com Female 3 4 Matti Sullly msullly3@soundcloud.com Female 4 5 Zita Doggart zdoggart4@ftc.gov Female ... ... ... ... ... ... 995 996 Web Beauly wbeaulyrn@drupal.org Male 996 997 Joachim Silber jsilberro@cam.ac.uk Male 997 998 Kearney Huntly khuntlyrp@hud.gov Male 998 999 Robb Eads readsrq@sfgate.com Male 999 1000 Hillery Dickings hdickingsrr@gizmodo.com Male 1000 rows × 5 columns\ndf_items id product prices 0 1001 Bread Ww Cluster 55.0 1 1002 Glove - Cutting 34.0 2 1003 Ice Cream - Super Sandwich 26.0 3 1004 Wine - Chateau Bonnet 52.0 4 1005 Ecolab - Ster Bac 46.0 ... ... ... ... 992 1993 Table Cloth 53x53 White 3.0 993 1994 Durian Fruit 51.0 995 1996 Chinese Foods - Plain Fried Rice 23.0 996 1997 Wine - White, Ej Gallo 57.0 998 1999 Sugar - Fine 84.0 836 rows × 3 columns\n","date":"2020-07-03T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/synthetic_data_pareto/","section":"posts","tags":["python","pareto","synthetic"],"title":"Synthetic data based on pareto principle"},{"categories":["moonlit","data viz","analytics"],"contents":"1 million seconds equal 11.5 days, whereas 1 billion seconds equal 31.75 years. Very large numbers are baffling to us, humans. From an evolutionary point of view, we never had to deal with anything colossal. With the recent news of Amazon boss Jeff Bezos now being worth about as much as New Zealand’s economy, it is quite interesting to see how he compares to rest of the celebrities we consider rich. It is absolutely shocking that the wealth difference between Elon Musk and YOU is smaller than between Elon Musk and Jeff Bezos.\n","date":"2020-07-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/bezos/","section":"posts","tags":["tableau","data viz"],"title":"You vs Bezos - Weber Fencher law"},{"categories":["cheers hospital","data viz","analytics"],"contents":"Data analysis of Cheers hospital Since it’s inception, Hospital for Children Eye ENT and Rehabilitation Service (CHEERS) has amassed data from various sources such as all the software used in daily operations, research department and the finance department. With priority in budgeting which is always a significant factor, CHEERS wanted Moonlit Solutions to analyze the data from different sectors within the hospital and create forecasting models to help with their operational efficiency.\nScope of work Data curation from different sectors of the hospital (but only limited to their Hospital Management Information System) Data wrangling and principal component analysis Exploratory data analysis Creation and validation of different forecasting models Assist in the creation of the forecasting section in the hospital master plan that is currently being developed. Methodology After the acquisition stage, all analysis presented in notebooks followed the following methodology:\nGaining domain expertise Data wrangling and cleaning (a lot of imputing using the domain knowledge and meeting stakeholders) Exploratory Data Analysis Forecasting with multiple models (AR, MA, ARIMA, SARIMA, Bayesian, Deep Learning)and different KPIs (Daily sales, Daily patient visits) Optimization (Parameter tuning) Outputs Robust forecasting models predicting future sales A report document Dashboard showcasing interactive charts and tables (and all analysis notebooks for research reproducibility) The exploratory data analysis and forecasts generated went through various levels of quality assurance to validate the correctness. Unfortunately, the COVID-19 epidemic and subsequent lockdowns was going to affect the business. The model created wasBAU (Business As Usual). It did not forecast out of the world scenarios like COVID-19 (a Black Swan event). However, it will create a benchmark for measuring the effects of COVID-19 in the future. The forecasting can still be used to understand the impact of COVID-19 on the business. The exploratory data on the other hand, provides significant insights for the hospital\u0026rsquo;s strategy development.\n","date":"2020-03-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/cheers_analysis/","section":"posts","tags":["python","analysis","ML","ARIMA"],"title":"Cheers Hospital Analysis"},{"categories":["world bank","data viz","analytics"],"contents":"Data in News Link to the website\nHow journalists use data in their reporting or in their writing can be viewed as a reflection of the country’s demand for statistics and data. The report team, in partnership with the Society of Economic Journalists-Nepal (SEJON), organized a roundtable with 38 economic journalists, editors, and bureau chiefs in Nepal to discuss data use in news organizations. The team also administered an online questionnaire to 28 media professionals (see appendix A for the questionnaire and appendix B for the results). To complement these findings, the World Bank team developed an indicator model to assess the use of data by Nepali news portals based on the methodology proposed by Klein, Galdin, and Mohamedou 2016. The results show that very few news articles indicate the source of data or mention development indicators. However, articles that discuss data, reports, research, statistics, and related topics do critically engage with these ideas.\nMore than 4,100 articles were analyzed to assess the use of data in media. The data was collected between 9th December 2019 to 9th February 2020. The data source for the model was Nepali news portals that support RSS feeds and are published in English. The news portals that meet these criteria are the Himalayan Times, Onlinekhabar, the Nepali Times, the Telegraph Nepal, the Kathmandu Tribune, and Lokaantar.\nThe results of the analysis are as follows:\nLevel 1 analysis: The team analyzed whether articles indicated the presence of a data source, a statistical/development indicator, or keywords from papers on statistical capacity–building projects. Examples of keywords included household survey, population census, geospatial data, GDP, GNP, pay gap, trade balance, and unemployment rate. The keywords for data sources are comprised of word sequences of length two and scraped from https://klein.uk/literacy.html. The share of news articles that included at least one keyword that indicated “consistent non-critical” use of data averaged 7.6 percent (scores ranged from 2 percent to 12 percent).\nLevel 2 analysis: This level of analysis assessed whether an article critically engaged with topics of data or statistics or made arguments by using mathematical concepts. Examples of keywords included ambiguous, error, bias, fake, impartial, precise, scientific, and unreliable Seventeen percent of the more than 4,100 articles studied directly referenced data, records, research, statistics, or studies. Two-thirds of those articles included keywords.\nLevel 3 analysis: “Critical mathematical” use of data occurs when an article indicates questioning or interpreting of data and statistics. Examples of keywords included data manipulation, lead question, report bias, sample select, and sample size. No article met this criterion.\n","date":"2020-01-03T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/data_in_news/","section":"posts","tags":["analysis","proxy"],"title":"Data in News"},{"categories":["world bank"],"contents":"Nepal Data Literacy Program Nepal Data Literacy Program is designed to catalyze stronger data-driven decision-making by government and non-government actors (mass media, civil society, and academia) through targeted Data Literacy Workshop to help Nepal achieve its development goals.\nThe program comprises a 100 hour modular, customizable pedagogy to support both technical skills-building and efforts to enhance a ‘culture of data use’ among Nepalis.\nFor the event, my responsibilities were to develop course materials for Python and Machine Learning. Additionally, I was also tasked with reviewing course materials for other modules (Tableau, Statistics, Survey Methodologies etc.).\nSimilarly, I also developed the Nepal Data Literacy website that hosts the entire curriculum, instructor\u0026rsquo;s note, student\u0026rsquo;s workbook, analysis notebooks, datasets, participants projects, etc.\nDuring the event, I instructed Python and Machine Learning, including topics of Regression and Classification to industry experts on different fields (Academics, CSO, Media, Entrepreneurs, Managers, NGOs). This was exciting because two of my teachers who taught me in grade 8 were my students in this event. They were proud.\nThe content created for Unit 7 Supplement Materials Student Workbook Instructor Note Downloadable Slide Google Colab Files Python Basics Python Basics (Solutions) Probabilities in Python Probabilities in Python (Solutions) Introduction to Seaborn for visualization Introduction to Pandas Linear Regression Logistic Regression ","date":"2019-12-31T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/nepal_data_literacy/","section":"posts","tags":["data","literacy","teaching materials"],"title":"Nepal Data Literacy Program"},{"categories":["community","world bank"],"contents":"Journey to Spam Detection Spiralogics is hosting a first of it\u0026rsquo;s kind, AI Conference in Nepal this February - a platform for AI veterans and students to come together and network. This year, Spiralogics is focused on exploring and explaining the ways how AI can create positive impact on different aspects of our life. It is a platform which will act as a driving force in encouraging the implementation of AI (especially in the Nepalese environment). Their purpose is to bring together professionals and students under one roof to rediscover the possibilities that AI can create in our day to day lives.\nHow was I involved? I was asked to teach a 5 hour session to introduce the students to a concept of AI. I decided to conduct a workshop based session on Naive Bayes Classifier to detect spam.\nTopics covered Probability Primer: We look into some few probability problems and Monte Carlo Simulations to get a basic primer on probability and Python. Conditional Probability: We look into few examples on conditional probability. Bayes Theorem: We connect Conditional Probability with Bayes Theorem. We move from what we know to what we can infer and connect the dots. Sensitivity, Specificity and Confusion matrix: We look into concepts of Sensitivity, Specificity, TP, TN, FP and FN. We also look into Type I and Type II errors. Accuracy, Precision and Recall: We look into different ways to evaluate our models and realize when one is more important than the other. We also study F1 score which combines Precision and Recall. Naive Bayes: We move from Bayes Theorem to Naive Bayes and define what Naive means. NB Spam detection classifier: We build a binary classifier using Scikit learn and publicly available SMS datasets. We also evaluate our model’s performance. NB vs Bagging, Random Forest and Adaboost: We will not spend much time here. This notebook basically compares Naive bayes with other algorithmns for the same dataset. Link to the github repo.\n","date":"2019-12-20T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/spirathon/","section":"posts","tags":["bayes","python"],"title":"Spirathon AI Conference 2019"},{"categories":["world bank","community"],"contents":"What is a Solve-a-thon? A two-day Solveathon, the first of its kind, was organized by the World Bank and The Asia Foundation with support from UKAID at the Kathmandu University School of Management on 14-15 December. The event brought together data scientists, programmers, developers, researchers, and professionals with diverse backgrounds and provided them a platform to work collaboratively on data-driven projects to tackle developmental challenges.\nDuring the event, the participating teams worked with mentors and respective domain experts to come up with solutions, refine them and devise various prototypes. The participants worked on nine different projects – three culminated from the 100-hour Nepal Data Literacy Program organized by the World Bank, three proposed by the participants, and three selected by the team based on the pertinent developmental issues and availability of data. The Solveathon will award a total amount of 10,000 USD, divided among two to five projects, to further develop their prototypes. In this phase, the awardees will assess and validate the feasibility of proposed prototype– assumptions in the theory of change, analyze user needs, develop sustainability and action plan–, identify potential partners, and analyze whether similar projects are existent or being planned in the market. Implementation and Review Phase: Under this phase, the awardees will implement their idea or prototype, evaluate its success and demonstrate plans for sustainability of impact.\nI was part of the planning and execution committee, and a technical mentor to the participating team. Additionally, once the projects were submitted, I also was part of the team that judged the team based on various criteria such as complexity, sustainability, cost and team members. Three of the projects from the event have started taking good shape at the present.\nI was also responsible for creating the event website which you can find here.\n","date":"2019-12-05T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/solveathon_nepal/","section":"posts","tags":["community","mentorship","organizer"],"title":"Solveathon Nepal 2019"},{"categories":["community","british college"],"contents":"Nepal, students and AI The conference titled “Artificial Intelligence for Transforming Business and Society (AITB2019)”, brought together more than 100 international and national academics, industry experts, researchers, policy makers and students under one roof to share their research findings, experience, and expertise in the field of AI.\nAccording to the AITB2019 Organizing Committee, the topic of Artificial Intelligence(AI) was chosen to discuss the relevance of AI revolution happening all over the world and in the context of Nepal. It was also intended to bring the national and international intellect to share their ideas through their research and experience in the field of AI. The initiation of discussion about the importance for Nepal to keep up with the AI revolution and to make sure that the human resources are prepared for the future skills was the main aim of the conference.\nDiscussion Topics I was one of the panelist and we were asked the following questions:\nIn terms of the impact of AI, how big do you think the impact will be in our lives in the future? You are free to choose your timelines here. How important is it to be updated about AI today? Why does everyone need to know about AI? How can we inspire the young generation to be part of the AI revolution? or is it the other way around? (ie. do we need to inspire the older generation, policymakers to be part of the revolution to make the progress faster?) From the academic perspective, do you see a need to do something radically different on how we impart knowledge now vs when AI will be prevalent? There is a debate about jobs when AI will really come in effect? What is your stand in this debate? will there be more jobs or fewer jobs? Final question, do we need to fear AI? will we be taken over by machines? or will we all be cyborgs that will be completely in sync with the machines? The press release for the program concluded the following regarding our panel discussion.\n“The second panel moderated by Mr. Amod Niroula, Co-founder and Project Manager at ACT360, brought industry practitioners, entrepreneurs and academics together to discuss the need for developing AI strategies for tomorrow. Panelist Dr Bal Krishna Bal, Head of Department, Computer Science at Kathmandu University emphasized the crucial role of academia in providing the needed talent as well as the need for adaptation and updating of curriculum. Mr. Ayush Subedi, Co-founder/CTO of Moonlit Solutions stressed the need to first increase access to disaggregated data to be able to adopt AI. Mr. Ravi Bajracharya, Co- founder of Wiseyak shared his own experiences about adopting AI in the field of healthcare for increased access to rural Nepal. Chief Technology Architect of Makura Creations, Mr. Chandan Gupta shared his views about not taking the AI revolution as a threat to jobs but instead a need to be aware of the changing nature of jobs.”\n","date":"2019-11-05T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/british_ai_conference/","section":"posts","tags":["panels"],"title":"British College AI Conference"},{"categories":["trove"],"contents":"The extension to Trove Trove Summary is a web app (React) that generates summary of a web article (from its URL) and allows users to securely store them. Technically speaking, it follows the same philosophy of Trove (build in Blockstack and Gaia), and therefore inherits, decentralization, anonymity and encryption.\nHowever, the other aspect of Trove Summary is its backend that uses abstractive and extractive summarization models to generate reliable summary.\nExamples\nRequest\n/api/get_content?url=abc.xyz Result\n{ \u0026quot;authors\u0026quot;: [ ], \u0026quot;code_content\u0026quot;: 200, \u0026quot;description\u0026quot;: \u0026quot;Alphabet Inc. is a holding company that gives ambitious projects the resources, freedom, and focus to make their ideas happen — and will be the parent company of Google, Nest, and other ventures. Alphabet supports and develops companies applying technology to the world’s biggest challenges.\u0026quot;, \u0026quot;keywords\u0026quot;: [ ], \u0026quot;movies\u0026quot;: [ ], \u0026quot;publish_date\u0026quot;: null, \u0026quot;summary\u0026quot;: \u0026quot;We’ve long believed that over time companies tend to get comfortable doing the same thing, just making incremental changes.\\nI am really excited to be running Alphabet as CEO with help from my capable partner, Sergey, as President.\\nThis newer Google is a bit slimmed down, with the companies that are pretty far afield of our main internet products contained in Alphabet instead.\\nIn addition, with this new structure we plan to implement segment reporting for our Q4 results, where Google financials will be provided separately than those for the rest of Alphabet businesses as a whole.\\nSundar has been saying the things I would have said (and sometimes better!) for quite some time now, and I’ve been tremendously enjoying our work together.\\nSergey and I are seriously in the business of starting new things.\\nFor Sergey and me this is a very exciting new chapter in the life of Google—the birth of Alphabet.\u0026quot;, \u0026quot;summary_flag\u0026quot;: 1, \u0026quot;text\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Alphabet\u0026quot;, \u0026quot;top_image\u0026quot;: \u0026quot;https://abc.xyz/favicon-194x194.png\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Website\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://abc.xyz/\u0026quot; } Request\n/api/get_content?url=bbc.com/news/world-europe-43841194 Result\n{ \u0026quot;authors\u0026quot;: [ ], \u0026quot;code_content\u0026quot;: 200, \u0026quot;description\u0026quot;: \u0026quot;One of the world's biggest dance music stars dies in Oman, with no cause of death announced.\u0026quot;, \u0026quot;keywords\u0026quot;: [ ], \u0026quot;movies\u0026quot;: [ ], \u0026quot;publish_date\u0026quot;: null, \u0026quot;summary\u0026quot;: \u0026quot;Avicii, top electronic dance music artist, found dead at 28 Published duration 21 April 2018\\nSwedish DJ Avicii, one of the world's biggest dance music stars, has died in Oman at the age of 28.\\nThe electronic dance music (EDM) star, who reportedly made $250,000 (£180,000) a night on tour, had struggled with some health issues in the past, having his gall bladder and appendix removed in 2014\\n\\\u0026quot;I know I am blessed to be able to travel all around the world and perform, but I have too little left for the life of a real person behind the artist,\\\u0026quot; he said at the time.\\nAs well as working with the likes of Aloe Blacc and Rita Ora, Avicii collaborated with artists including Madonna and Coldplay.\\nFormer Radio 1 DJ Judge Jules, who often performed alongside him, said his biggest achievement was being the first electronic dance star to break America.\u0026quot;, \u0026quot;summary_flag\u0026quot;: 1, \u0026quot;text\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Avicii, top electronic dance music artist, found dead at 28\u0026quot;, \u0026quot;top_image\u0026quot;: \u0026quot;https://ichef.bbci.co.uk/news/1024/branded_news/54F2/production/_100964712_mediaitem100964710.jpg\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Article\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://www.bbc.com/news/world-europe-43841194\u0026quot; } Request\n/api/get_content?url=medium.com/@trovenow/the-underlying-architecture-of-trove-cbfa4ba18581 Result\n{ \u0026quot;authors\u0026quot;: [ ], \u0026quot;code_content\u0026quot;: 200, \u0026quot;description\u0026quot;: \u0026quot;After releasing Trove, our inboxes and DMs flooded with questions pertaining to the underlying architecture of Trove, and most importantly, about DAaps (Decentralized Apps) and the Blockstack…\u0026quot;, \u0026quot;keywords\u0026quot;: [ ], \u0026quot;movies\u0026quot;: [ ], \u0026quot;publish_date\u0026quot;: \u0026quot;Sun, 30 Jun 2019 16:21:44 GMT\u0026quot;, \u0026quot;summary\u0026quot;: \u0026quot;After releasing Trove, our inboxes and DMs flooded with questions pertaining to the underlying architecture of Trove, and most importantly, about DAaps (Decentralized Apps) and the Blockstack platform.\\nWe quickly put together a decentralized app that allowed users to save their favorite words.\\nThe primary purpose of Gaia is to store any relevant data for the apps that the user uses in the Blockstack ecosystem.\\nHowever, Blockstack also allows users to choose their own Gaia hub and to configure the back-end provider to store data with.\\nThe data for an app is stored in one or many text format files (JSON) within the app’s hub in Gaia.\\nThere is no possible way for the makers of an app to have all of the data of all of the users in a centralized repository.\\nEvery Trove user has three files associated with their Gaia hub to store bookmark, collections and archived objects.\u0026quot;, \u0026quot;summary_flag\u0026quot;: 1, \u0026quot;text\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;The Underlying Architecture of Trove\u0026quot;, \u0026quot;top_image\u0026quot;: \u0026quot;https://miro.medium.com/max/1200/0*CUrk-zRTYflhc_Ja.jpeg\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Article\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://medium.com/@trovenow/the-underlying-architecture-of-trove-cbfa4ba18581\u0026quot; } ","date":"2019-09-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/trove_summary/","section":"posts","tags":["Blockchain","Blockstack","Gaia"],"title":"Trove Summary"},{"categories":["trove"],"contents":"After releasing Trove, our inboxes and DMs flooded with questions pertaining to the underlying architecture of Trove, and most importantly, about DAaps (Decentralized Apps) and the Blockstack platform. The purpose of this post is to answer those burning questions and to take you on a journey of figuring all of this out ourselves. The engineers @Trove have strong understanding and experience of centralized applications. We followed the decentralized space, and also built small prototypes in Blockstack and Ethereum platforms. But, Trove is our first endeavour to create a full end-to-end Decentralized Application from scratch. We had questions. Actually, a lot of them.\nProof of Concept A proof of concept entails investigating to be convinced of an idea. In our case, the idea was a bookmark manager. However, we also had to deep dive into the Blockstack platform to get a gist of the second part of our idea, i.e., decentralization. We started with a very very simple decentralized app.\nWe quickly put together a decentralized app that allowed users to save their favorite words. Although the app does not make sense from a usability perspective, and might be hilarious to even think about deploying it to production, it helped answer a lot of our questions:\nHow will the users login/signup to use the app? The users will create an ID with Blockstack. The ID will be used to sign in all apps in the Blockstack ecosystem.\nWhere is the data stored? By default, when an ID is created in Blockstack, each user ID is also issued some storage space. This storage system is called Gaia. The primary purpose of Gaia is to store any relevant data for the apps that the user uses in the Blockstack ecosystem. However, Blockstack also allows users to choose their own Gaia hub and to configure the back-end provider to store data with. Learn more here.\nHow is the data stored in Gaia? Gaia is not a DBMS. The data for an app is stored in one or many text format files (JSON) within the app’s hub in Gaia.\nIs it encrypted? This depends on the developers. The developers can choose whether or not a file should be encrypted. For our case, the words in the proof of concept app were encrypted and the bookmarks for Trove are encrypted as well.\nHow is it decentralized? One of the most common questions we have been receiving is regarding decentralization. Several users new to the Blockstack ecosystem have asked, “If all of my bookmarks are in the same place, how is it decentralized?”. Well, there are a few ways of thinking about this. From the perspective of the developers, all of the data pertaining to the app are scattered in several Gaia storage all over. There is no possible way for the makers of an app to have all of the data of all of the users in a centralized repository. This makes it decentralized. Also, no central repository implies no machine learning algorithm tinkering to garner patterns and trends on collective data.\nNow, can we now build a MVP for our purposes? Yes. At this point, we had a better understanding of the approach to make Trove possible and optimized to leverage on Blockstack and Gaia. Moreover, we realized that a bookmark manager would be an ideal exploration for this platform because bookmarks are sacred to a specific user, and they are not normally shared. We might eventually work on the functionality of publicly shared bookmarks in the future if our users request it.\nProduct Specification and Minimum Viable Product The team collectively decided to pursue these functionalities for the first version of the app:\nCRUD bookmarks CRUD buckets/categories for bookmarks CRUD tags Favorite bookmarks Archive bookmarks Archive buckets/categories Extract meta tags of bookmarks Extract HTML body of bookmarks for Read Mode (Parked for future version of the app) Extract keywords from bookmarks (Parked for future version of the app) Filter using buckets/categories, tags, favourites Search Browser extensions Architecture How are the files stored in Gaia for Trove?\nEvery Trove user has three files associated with their Gaia hub to store bookmark, collections and archived objects. This schema is also backward compatible. Backward compatibility is a major issue with Gaia because schema changes in the future to incorporate any other features where we do not have access to user’s Gaia (by definition), is a major constraint and therefore needs precautions. **If we were to add Read Mode in the future (which will have massive content and makes no sense to be stored inside bookmark object anyway), it can be stored on a separate file {bookmark_id}.json and referenced with the id. This implies, in the future, a user will have 3+ files, while everything remains maintainable, compatible and scalable. A randomly generated id in base 36 (0–9, a-z) with a length of 9 is used for each category and each bookmark. The probability of collision for a user limits to 0 (1/36⁹). This decision was made so that ids can be assigned on the go without having to keep track of array sizes for incremental ids. When a bookmark is created, and assigned to a category, the bookmark object stores the id of the category and not the name. This referential association allows for seamless category name edits. For the sake of blog post brevity, we are leaving out a lot of the trivial implementation that is common with centralized architecture design pattern (archive, tags, filter, search etc). Extension\nWe initially started with one click bookmark save on our extension. This implied a seamless experience for the users. However, server-less comes with its limitation. In a centralized system, once the user clicked on the extension, we would have sent the URL to our back-end, scraped meta-tags and stored it in a database record referencing the user. We do not have the liberty here. The URL is sent to our open-sourced back-end for meta tag extraction, but that is all it does. It returns the response back to the extension, which does the rest. This implies a user would have to wait for a few seconds while:\nWe collect meta tags from back-end Collect all bookmarks from Gaia Change 2 to an array Append response from back-end to the array Save array to Gaia as a JSON Therefore, we decided to make it a two-step process. We are still researching on ways to make this part seamless to our users. URL Meta Extractor A simple Flask app using Newspaper package has been used for the purpose of URL extraction. The back-end extractor is open sourced at https://gitlab.com/trovenow/trovenow_url_parser. Use the app and tweet us ****for more clarification. We would love to hear your feedback. Also, please show your support by up-voting us here.\nFinal Version (v1) of Trove\n","date":"2019-06-30T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/trove_architecture/","section":"posts","tags":["Blockchain","Blockstack","Gaia"],"title":"Underlying Architecture of Trove"},{"categories":["moonlit","analytics"],"contents":"Fraud Detection Research Items Data Sets Relevant Papers Available Solutions Machine learning Pre-processing Features analysis Modelling Evaluation Suggested Solution 1. Data Sets The first step is to find fraud data sets for modeling purposes. Unfortunately, fraud data sets are really difficult to find publicly because of the confidential information that they contain. Listed below are some of the data sets found and notes on them.\nReal world data set from Kaggle (ULB)\nhttps://www.kaggle.com/mlg-ulb/creditcardfraud\nThe data set contains labelled credit card transactions labeled as fraudulent or genuine. Unfortunately, the column labels do not make sense because PCA has been applied for dimensional reduction. Therefore, it is very difficult to understand what each of the columns represent. Nonetheless, it is real world data. The data sets contains transactions made by credit cards in September 2013 by European cardholders. This data set presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The data set is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\nThis data set has been analysed and models have been created below in the document, with F1 score of 94%.\nSynthetic data set from Kaggle (NTNU)\nhttps://www.kaggle.com/ntnu-testimon/paysim1\nThe data set contains synthetic (created) transaction data. The advantage of using this data set is that PCA has not been pre-performed, thus allowing extraction of all useful information. However, the data set is scaled down to 1/4th of the original data set.\n2. Papers: Link Title Summary https://www.aaai.org/Papers/KDD/1998/KDD98-026.pdf Toward Scalable Learning with Non-uniform Class and Cost Distributions: A Case Study in Credit Card Fraud Detection Handling skewed datasets, compares credit card fraud detection models, and evaluate how the different sets of features have an impact on the results with the help of a real credit card fraud dataset provided by a large European card processing company (the dataset above). The results show an average increase in savings of 13% by including the proposed periodic features into the methods. Using 50-50 split in fraud, non-fraud leads to better models.\nvon Mises distribution: https://en.wikipedia.org/wiki/Von_Mises_distribution https://www.aaai.org/Papers/Workshops/1997/WS-97-07/WS97-07-015.pdf Credit Card Fraud Detection using Meta-Learning: Issues and Initials Results Apart from the finding like above (using balanced training), the paper talks about using metrics other than accuracy for model evaluation. http://journal.utem.edu.my/index.php/jtec/article/view/3571/2466 Credit Card Fraud Detection Using Machine Learning As Data Mining Technique 95% accuracy based on Naive based derivatives. https://www.jair.org/index.php/jair/article/view/10302/24590 SMOTE: Synthetic Minority Over-sampling Technique Using SMOTE method as described in the paper is another alternative of getting around the skewness problem. 3. Research on available solutions: Airbnb https://medium.com/airbnb-engineering/architecting-a-machine-learning-system-for-risk-941abbba5a60\nWays to mitigate potential bad actors to carry out different types of attacks:\nProduct changes: 2FA, email verification, etc etc Anomaly detection: Scripted attacks that can cause anomaly heuristics/machine learning model based on different factors Framework\nFast and robust Agile (catch up game) PMML: Predictive model markup language Openscoring: encodes several common types of machine learning models\nThey do not provide fraud detection as a service.\nPaypal\nhttps://venturebeat.com/2018/06/21/paypal-to-acquire-machine-learning-powered-fraud-detection-startup-simility/\nPaypal recently acquired Simility for fraud detection.\nSimility looks at various session, device, and behavioral bio-metrics and builds a profile for what constitutes “normal” user login behavior; if an anomaly is spotted, it can act to prevent the action.\nhttps://www.dropbox.com/s/ft3wu5ix15xukhc/Mobile%20Fintech%20Fraud.pdf?dl=0\nStripe\nhttps://stripe.com/us/radar\nEven if a card is new to your business, there’s an 89% chance it’s been seen before on the Stripe network.\n4. Machine Learning Supervised learning was applied to the PCA data set discussed in the data sets section. Different ensemble machine learning algorithms were tested, rather than using one particular algorithm for modelling. Metrics like Precision, Recall, F1 score were used to evaluate the model and get a better understanding of True Positives, True Negatives, False Positive and False Negatives. Link to complete notebook\nResults:\nRandom Forest\nAccuracy score for Random Forest : 0.9538461538461539 Precision score Random Forest : 0.98 Recall score Random Forest : 0.9245283018867925 F1 score Random Forest : 0.9514563106796116 Bagging\nAccuracy score for Bagging : 0.963076923076923 Precision score Bagging : 0.9867549668874173 Recall score Bagging : 0.9371069182389937 F1 score Bagging : 0.9612903225806452 AdaBoost\nAccuracy score for Ada Boost Classifier : 0.9446153846153846 Precision score Ada Boost Classifier : 0.9795918367346939 Recall score Ada Boost Classifier : 0.9056603773584906 F1 score Ada Boost Classifier : 0.9411764705882353 False Positives vs False Negatives\n","date":"2019-01-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/fraud_detection/","section":"posts","tags":["ml","python"],"title":"Fraud Detection"},{"categories":["proof of concept"],"contents":"Torpe Blockchain (तोर्पे ब्लोक्कचैन) A minimal blockchain data structure to understand the blockchain basics like hashing, nonce, states, genesis blocks etc. Hash Function A hash function is any function that can be used to map data of arbitrary size to data of a fixed size. The values returned by a hash function are called hash values, hash codes, digests, or simply hashes.\nimport hashlib def hash_sha256(raw): raw = str(raw).encode(\u0026#39;utf-8\u0026#39;) return hashlib.sha256(raw).hexdigest() hash_sha256(\u0026#39;torpe_blockchain\u0026#39;) 'e5367197a1f12480ec761306f2fa9d15b494d5a80e5a806713df9e60943f4faf' hash_sha256(\u0026#39;torpe_blockchain\u0026#39;) 'e5367197a1f12480ec761306f2fa9d15b494d5a80e5a806713df9e60943f4faf' hash_sha256(\u0026#39;torpe_blockchaiN\u0026#39;) '71fe90af906a9afa507ee54287595df6d7267df05428b3f91429371ebd27cb3e' Hashes for the same text are the same as seen above. Even when one character of the string is changed, the hash generated as a result seems to be completely random.\nSHA-256 collisions have not been found yet.\nNonce Number that can only be used once\nAn arbitrary numer used in cryptography to ensure uniqueness and prevent the rerunning of transactions (known as replay attack).\ndef hash_sha256_nonce(raw): raw_bytes = str(raw).encode(\u0026#39;utf-8\u0026#39;) hashed = hashlib.sha256(raw_bytes).hexdigest() nonce = 0 while (hashed[:5] != \u0026#39;00000\u0026#39;): nonce = nonce+1 raw.update({\u0026#39;nonce\u0026#39;: nonce}) raw_bytes = str(raw).encode(\u0026#39;utf-8\u0026#39;) hashed = hashlib.sha256(raw_bytes).hexdigest() return raw, hashed print (hash_sha256_nonce({\u0026#39;hello\u0026#39;: \u0026#39;proof_of_work\u0026#39;, \u0026#39;nonce\u0026#39;: 0})) ({'hello': 'proof_of_work', 'nonce': 623228}, '00000a9d45728c6f4d1eff383dab4d96b753495c8b312ecb5d1858116885ee55') Proof of work The proof of work for this case will be to generate hashes with five leading zeros (by incrementing the nonce). This is the \u0026ldquo;mining\u0026rdquo; part.\nBlock Blocks hold batches of valid transactions that are hashed and encoded into a Merkle tree Each block includes the cryptographic hash of the prior block in the blockchain, linking the two. The linked blocks form a chain. This iterative process confirms the integrity of the previous block, all the way back to the original genesis block.\nGenesis Block A genesis block or block0 is the first block of a block chain. The genesis block is almost always hardcoded into the software of the applications that utilize its block chain. It is a special case in that it does not reference a previous block\nimport datetime # Lets assume 5 person were given 100 coins each state = { \u0026#39;Person_1\u0026#39;: 100, \u0026#39;Person_2\u0026#39;: 100, \u0026#39;Person_3\u0026#39;: 100, \u0026#39;Person_4\u0026#39;: 100, \u0026#39;Person_5\u0026#39;: 100 } block0_data = { \u0026#39;timestamp\u0026#39;: datetime.datetime.now(), \u0026#39;index\u0026#39;: 0, \u0026#39;previous\u0026#39;: None, \u0026#39;transactions\u0026#39;: [state], \u0026#39;nonce\u0026#39;: 0 } raw, hashed = hash_sha256_nonce(block0_data) block0 = { \u0026#39;hash\u0026#39;: hashed, \u0026#39;data\u0026#39;: raw, } block0 {'hash': '0000044b11859efa71c555a87a68090f1f602cf8bcd35bb5446c3c5532f5ad5e', 'data': {'timestamp': datetime.datetime(2020, 9, 9, 7, 37, 44, 877080), 'index': 0, 'previous': None, 'transactions': [{'Person_1': 100, 'Person_2': 100, 'Person_3': 100, 'Person_4': 100, 'Person_5': 100}], 'nonce': 2700821}} This is the genesis block or block 0 here.\nTransactions Lets create some random transactions. The transactions for the demo purpose follow +x, -x semantic. See the examples below.\nimport random def random_transaction(state): temp_list = list(state.keys()) random.shuffle(temp_list) # randomly select two persons first_person = temp_list.pop() second_person = temp_list.pop() receive = random.randint(1, 10) give = -receive return { first_person:receive, second_person:give } test_transactions = [random_transaction(state) for x in range(5)] test_transactions [{'Person_3': 5, 'Person_5': -5}, {'Person_3': 7, 'Person_5': -7}, {'Person_4': 1, 'Person_1': -1}, {'Person_2': 4, 'Person_1': -4}, {'Person_4': 4, 'Person_5': -4}] Updating State def update_state(transaction, state): state = state.copy() for key in transaction: state[key] = state.get(key, 0) + transaction[key] return state for transaction in test_transactions: state = update_state(transaction, state) state {'Person_1': 95, 'Person_2': 104, 'Person_3': 112, 'Person_4': 105, 'Person_5': 84} Valid Transactions def check_transaction_validity(transaction, state): # check neg vs pos if sum(transaction.values()) is not 0: return False # check if amount in wallet to give for key in transaction.keys(): if state.get(key, 0) + transaction[key] \u0026lt; 0: return False return True for transaction in test_transactions: print (check_transaction_validity(transaction, state)) True True True True True # No balance print (check_transaction_validity({\u0026#39;A\u0026#39;: 5, \u0026#39;B\u0026#39;: -5}, {\u0026#39;A\u0026#39;: 0, \u0026#39;B\u0026#39;: 0})) False # Bad transaction print (check_transaction_validity({\u0026#39;A\u0026#39;: 5, \u0026#39;B\u0026#39;: 5}, {\u0026#39;A\u0026#39;: 50, \u0026#39;B\u0026#39;: 50})) False Initial State # Let us reset # Lets assume 5 person were given 100 coins each state = { \u0026#39;Person_1\u0026#39;: 100, \u0026#39;Person_2\u0026#39;: 100, \u0026#39;Person_3\u0026#39;: 100, \u0026#39;Person_4\u0026#39;: 100, \u0026#39;Person_5\u0026#39;: 100 } blockchain = [] # Adding the genesis block blockchain.append(block0) blockchain [{'hash': '0000044b11859efa71c555a87a68090f1f602cf8bcd35bb5446c3c5532f5ad5e', 'data': {'timestamp': datetime.datetime(2020, 9, 9, 7, 37, 44, 877080), 'index': 0, 'previous': None, 'transactions': [{'Person_1': 100, 'Person_2': 100, 'Person_3': 100, 'Person_4': 100, 'Person_5': 100}], 'nonce': 2700821}}] Non-genesis block / New block def new_block(transactions, blockchain): previous_block = blockchain[-1] data = { \u0026#39;timestamp\u0026#39;: datetime.datetime.now(), \u0026#39;index\u0026#39;: previous_block[\u0026#39;data\u0026#39;][\u0026#39;index\u0026#39;] + 1, \u0026#39;previous\u0026#39;: previous_block[\u0026#39;hash\u0026#39;], \u0026#39;transactions\u0026#39;: transactions, \u0026#39;nonce\u0026#39;: 0 } raw, hashed = hash_sha256_nonce(data) block = {\u0026#39;hash\u0026#39;: hashed, \u0026#39;data\u0026#39;: raw} return block sample_transactions = [random_transaction(state) for x in range(50)] sample_transactions [{'Person_5': 5, 'Person_1': -5}, {'Person_3': 10, 'Person_1': -10}, {'Person_2': 5, 'Person_4': -5}, {'Person_5': 9, 'Person_3': -9}, {'Person_3': 1, 'Person_2': -1}, {'Person_1': 9, 'Person_3': -9}, {'Person_1': 7, 'Person_3': -7}, {'Person_5': 4, 'Person_3': -4}, {'Person_5': 2, 'Person_4': -2}, {'Person_2': 4, 'Person_3': -4}, {'Person_3': 5, 'Person_5': -5}, {'Person_5': 1, 'Person_1': -1}, {'Person_1': 1, 'Person_2': -1}, {'Person_2': 7, 'Person_1': -7}, {'Person_2': 7, 'Person_5': -7}, {'Person_3': 2, 'Person_1': -2}, {'Person_3': 3, 'Person_1': -3}, {'Person_3': 3, 'Person_2': -3}, {'Person_3': 6, 'Person_1': -6}, {'Person_1': 5, 'Person_3': -5}, {'Person_2': 4, 'Person_3': -4}, {'Person_2': 1, 'Person_5': -1}, {'Person_1': 3, 'Person_2': -3}, {'Person_1': 10, 'Person_2': -10}, {'Person_3': 9, 'Person_5': -9}, {'Person_1': 3, 'Person_4': -3}, {'Person_4': 2, 'Person_3': -2}, {'Person_5': 6, 'Person_3': -6}, {'Person_2': 9, 'Person_1': -9}, {'Person_3': 3, 'Person_4': -3}, {'Person_3': 10, 'Person_4': -10}, {'Person_1': 9, 'Person_4': -9}, {'Person_2': 3, 'Person_1': -3}, {'Person_2': 6, 'Person_3': -6}, {'Person_4': 4, 'Person_1': -4}, {'Person_3': 7, 'Person_1': -7}, {'Person_3': 7, 'Person_1': -7}, {'Person_3': 5, 'Person_2': -5}, {'Person_3': 10, 'Person_2': -10}, {'Person_2': 1, 'Person_1': -1}, {'Person_1': 3, 'Person_5': -3}, {'Person_4': 4, 'Person_5': -4}, {'Person_1': 3, 'Person_2': -3}, {'Person_4': 1, 'Person_1': -1}, {'Person_5': 1, 'Person_4': -1}, {'Person_3': 5, 'Person_2': -5}, {'Person_1': 8, 'Person_4': -8}, {'Person_3': 8, 'Person_4': -8}, {'Person_3': 7, 'Person_4': -7}, {'Person_2': 1, 'Person_1': -1}] Transactions per block Bitcoin blocks used to contain fewer than 200 transactions and the largest number of transactions in a block was 1,976 at the time this answer was originally written (May 2013). In meanwhile (November 2017) the average number of transaction per block is well above 1500 with peaks above 2200.\n# Assume block size is 5 transactions_per_block = 5 transaction_block = [] for transaction in sample_transactions: if check_transaction_validity(transaction, state): state = update_state(transaction, state) transaction_block.append(transaction) if len(transaction_block) \u0026gt;= transactions_per_block: blockchain.append(new_block(transaction_block, blockchain)) transaction_block = [] import pprint pp = pprint.PrettyPrinter() for block in blockchain: pp.pprint(block) print(\u0026#39;\\n************************************************************************************\\n\u0026#39;) {'data': {'index': 0, 'nonce': 2700821, 'previous': None, 'timestamp': datetime.datetime(2020, 9, 9, 7, 37, 44, 877080), 'transactions': [{'Person_1': 100, 'Person_2': 100, 'Person_3': 100, 'Person_4': 100, 'Person_5': 100}]}, 'hash': '0000044b11859efa71c555a87a68090f1f602cf8bcd35bb5446c3c5532f5ad5e'} ************************************************************************************ {'data': {'index': 1, 'nonce': 2395688, 'previous': '0000044b11859efa71c555a87a68090f1f602cf8bcd35bb5446c3c5532f5ad5e', 'timestamp': datetime.datetime(2020, 9, 9, 7, 37, 58, 781195), 'transactions': [{'Person_1': -5, 'Person_5': 5}, {'Person_1': -10, 'Person_3': 10}, {'Person_2': 5, 'Person_4': -5}, {'Person_3': -9, 'Person_5': 9}, {'Person_2': -1, 'Person_3': 1}]}, 'hash': '00000303c468fe76fe73dfc089b856af02eda615c296ddde95c0c60999561048'} ************************************************************************************ {'data': {'index': 2, 'nonce': 2475862, 'previous': '00000303c468fe76fe73dfc089b856af02eda615c296ddde95c0c60999561048', 'timestamp': datetime.datetime(2020, 9, 9, 7, 38, 16, 454296), 'transactions': [{'Person_1': 9, 'Person_3': -9}, {'Person_1': 7, 'Person_3': -7}, {'Person_3': -4, 'Person_5': 4}, {'Person_4': -2, 'Person_5': 2}, {'Person_2': 4, 'Person_3': -4}]}, 'hash': '00000804a078686673c26bd3d391649da822c27a9eb87c2a86f07be5be7667c0'} ************************************************************************************ {'data': {'index': 3, 'nonce': 843595, 'previous': '00000804a078686673c26bd3d391649da822c27a9eb87c2a86f07be5be7667c0', 'timestamp': datetime.datetime(2020, 9, 9, 7, 38, 34, 360082), 'transactions': [{'Person_3': 5, 'Person_5': -5}, {'Person_1': -1, 'Person_5': 1}, {'Person_1': 1, 'Person_2': -1}, {'Person_1': -7, 'Person_2': 7}, {'Person_2': 7, 'Person_5': -7}]}, 'hash': '0000004fdf3bc704c17b3f38c8bcb0306443e24db0c5971c7494cea67e618fcd'} ************************************************************************************ {'data': {'index': 4, 'nonce': 456491, 'previous': '0000004fdf3bc704c17b3f38c8bcb0306443e24db0c5971c7494cea67e618fcd', 'timestamp': datetime.datetime(2020, 9, 9, 7, 38, 40, 443823), 'transactions': [{'Person_1': -2, 'Person_3': 2}, {'Person_1': -3, 'Person_3': 3}, {'Person_2': -3, 'Person_3': 3}, {'Person_1': -6, 'Person_3': 6}, {'Person_1': 5, 'Person_3': -5}]}, 'hash': '000001ea0d5ee3360f087e8fe25e643a8e749af6b42bf8c3045303a3e4dc80a5'} ************************************************************************************ {'data': {'index': 5, 'nonce': 1793595, 'previous': '000001ea0d5ee3360f087e8fe25e643a8e749af6b42bf8c3045303a3e4dc80a5', 'timestamp': datetime.datetime(2020, 9, 9, 7, 38, 43, 617133), 'transactions': [{'Person_2': 4, 'Person_3': -4}, {'Person_2': 1, 'Person_5': -1}, {'Person_1': 3, 'Person_2': -3}, {'Person_1': 10, 'Person_2': -10}, {'Person_3': 9, 'Person_5': -9}]}, 'hash': '00000101ed3a68b2340f553e1a373e7a590823fb34ee1e2f3c2e4ed44b647c2a'} ************************************************************************************ {'data': {'index': 6, 'nonce': 618433, 'previous': '00000101ed3a68b2340f553e1a373e7a590823fb34ee1e2f3c2e4ed44b647c2a', 'timestamp': datetime.datetime(2020, 9, 9, 7, 38, 56, 256050), 'transactions': [{'Person_1': 3, 'Person_4': -3}, {'Person_3': -2, 'Person_4': 2}, {'Person_3': -6, 'Person_5': 6}, {'Person_1': -9, 'Person_2': 9}, {'Person_3': 3, 'Person_4': -3}]}, 'hash': '00000b2395e1efb034610e89196aafe9407417bbac1cd60f300adccea3cd880d'} ************************************************************************************ {'data': {'index': 7, 'nonce': 4087255, 'previous': '00000b2395e1efb034610e89196aafe9407417bbac1cd60f300adccea3cd880d', 'timestamp': datetime.datetime(2020, 9, 9, 7, 39, 0, 705135), 'transactions': [{'Person_3': 10, 'Person_4': -10}, {'Person_1': 9, 'Person_4': -9}, {'Person_1': -3, 'Person_2': 3}, {'Person_2': 6, 'Person_3': -6}, {'Person_1': -4, 'Person_4': 4}]}, 'hash': '00000ef1b9a090a4b6636f86fa3d264dbcab13f5cf87d994637e656b6ec4abad'} ************************************************************************************ {'data': {'index': 8, 'nonce': 991443, 'previous': '00000ef1b9a090a4b6636f86fa3d264dbcab13f5cf87d994637e656b6ec4abad', 'timestamp': datetime.datetime(2020, 9, 9, 7, 39, 29, 983211), 'transactions': [{'Person_1': -7, 'Person_3': 7}, {'Person_1': -7, 'Person_3': 7}, {'Person_2': -5, 'Person_3': 5}, {'Person_2': -10, 'Person_3': 10}, {'Person_1': -1, 'Person_2': 1}]}, 'hash': '0000021d8da5dab8b274a0146db6ddd6b1c12f34d751f84e9755fae1ed6a42b3'} ************************************************************************************ {'data': {'index': 9, 'nonce': 24885, 'previous': '0000021d8da5dab8b274a0146db6ddd6b1c12f34d751f84e9755fae1ed6a42b3', 'timestamp': datetime.datetime(2020, 9, 9, 7, 39, 37, 55377), 'transactions': [{'Person_1': 3, 'Person_5': -3}, {'Person_4': 4, 'Person_5': -4}, {'Person_1': 3, 'Person_2': -3}, {'Person_1': -1, 'Person_4': 1}, {'Person_4': -1, 'Person_5': 1}]}, 'hash': '000002090602b338740f28ac66dfdc2f9949c8c40303574e0be08ad7033550bb'} ************************************************************************************ {'data': {'index': 10, 'nonce': 545615, 'previous': '000002090602b338740f28ac66dfdc2f9949c8c40303574e0be08ad7033550bb', 'timestamp': datetime.datetime(2020, 9, 9, 7, 39, 37, 232552), 'transactions': [{'Person_2': -5, 'Person_3': 5}, {'Person_1': 8, 'Person_4': -8}, {'Person_3': 8, 'Person_4': -8}, {'Person_3': 7, 'Person_4': -7}, {'Person_1': -1, 'Person_2': 1}]}, 'hash': '00000d59159d699830d454d68d2077ed40da8ae060ec67c19be0814d35a61e6f'} ************************************************************************************ The current state Syncing for the first time def validate_block(block, parent, state): error_msg = \u0026#39;Error in %d\u0026#39; % block[\u0026#39;data\u0026#39;][\u0026#39;index\u0026#39;] # check block hash assert block[\u0026#39;hash\u0026#39;] == hash_sha256(block[\u0026#39;data\u0026#39;]), error_msg # check block indices assert block[\u0026#39;data\u0026#39;][\u0026#39;index\u0026#39;] == parent[\u0026#39;data\u0026#39;][\u0026#39;index\u0026#39;] + 1, error_msg # check previous hash assert block[\u0026#39;data\u0026#39;][\u0026#39;previous\u0026#39;] == parent[\u0026#39;hash\u0026#39;], error_msg # validate all transactions for transaction in block[\u0026#39;data\u0026#39;][\u0026#39;transactions\u0026#39;]: assert check_transaction_validity(transaction, state), error_msg state = update_state(transaction, state) return state def check_chain(blockchain): state = {} for transaction in blockchain[0][\u0026#39;data\u0026#39;][\u0026#39;transactions\u0026#39;]: state = update_state(transaction, state) parent = blockchain[0] for block in blockchain[1:]: state = validate_block(block, parent, state) parent = block return state check_chain(blockchain) {'Person_1': 94, 'Person_2': 107, 'Person_3': 145, 'Person_4': 55, 'Person_5': 99} ","date":"2018-10-28T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/torpe/","section":"posts","tags":["blockchain","python"],"title":"Torpe Blockchain"},{"categories":["moonlit"],"contents":"Voronoi analysis and Sentiment Analysis in Business Feasibility study Apart from the conventional research methodology (convenience sampling, likert questionnaire, interview questions, swot analysis for competitions etc.) we tested two unconventional paradigms when a client approached us for feasibility of a business in Kathmandu.\nFirst of all, we looked into finding an ideal place for the business using voronoi diagrams. This was useful to avoid areas where competitions were prominent, and also to tap into neighbourhoods where demands were not met. Of course, factors such as population density, and similar business tending to sprout in close proximity needed to be considered, but voronoi diagrams was only used as a small piece in a very large puzzle.\nSecondly, we tried to understand people\u0026rsquo;s sentiment on the value the business was trying to sell, and also on current value providers. For this we used Twitter API to collect business relevant tweets from Kathmandu.\nApart from an aggregated view of the sentiments, we were able to gain insights on what was and was not working with services provided by current market players. This would allow our client to position their services by converting the weakness of their competition as their strength.\n","date":"2018-10-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/feasibility_study_vs/","section":"posts","tags":["data viz","voronoi","sentiment-analysis"],"title":"Feasibility study with sentiment analysis and voronoi"},{"categories":["pet projects","data viz","analytics"],"contents":"Concept Note on Inclusive Nepal (WIP) and Analysis of usage of Alt text in Nepali newspapers This is a work in progress\nThis notebook compiles statistics, resources and insights on:\n1. Status of Disability in Nepal 2. Nepali Mobile Apps and Web App penetration 3. Most used Nepali Mobile Apps 3. Most used Nepali Mobile Apps accessibility status 4. Comparitive analysis with other countries 5. Potential Solutions for few apps 6. Prototype and Tools suggestions for Accessibility 7. Published work regarding similar research Imports # Pandas import pandas as pd #Mapbox from mapboxgl.utils import * from mapboxgl.viz import * Status of Disability in Nepal df = pd.read_csv(\u0026#34;data_sources/disability_2011.csv\u0026#34;) df.head() District Male Female Population with Disability Percentage of PWDs 0 Kathmandu 9144 7978 17122 0.98 1 Morang 9494 7559 17053 1.77 2 Kailali 8143 7235 15378 1.98 3 Jhapa 8466 6910 15376 1.89 4 Chitwan 6973 5964 12937 2.23 df.head() District Male Female Population with Disability Percentage of PWDs 0 Kathmandu 9144 7978 17122 0.98 1 Morang 9494 7559 17053 1.77 2 Kailali 8143 7235 15378 1.98 3 Jhapa 8466 6910 15376 1.89 4 Chitwan 6973 5964 12937 2.23 df_geojson = pd.read_json(\u0026#39;data_sources/map.geojson\u0026#39;) df_geojson.head() type features 0 FeatureCollection {'properties': {'name': 'Humla', 'death': 1}, ... 1 FeatureCollection {'properties': {'name': 'Darchula', 'death': 2... 2 FeatureCollection {'properties': {'name': 'Bajhang', 'death': 3}... 3 FeatureCollection {'properties': {'name': 'Mugu'}, 'geometry': {... 4 FeatureCollection {'properties': {'name': 'Bajura'}, 'geometry':... df.set_index(\u0026#34;District\u0026#34;, inplace=True) def add_data(features): name = features.get(\u0026#39;properties\u0026#39;).get(\u0026#39;name\u0026#39;) if (name in df.index): percentage = df.loc[name][\u0026#39;Percentage of PWDs\u0026#39;] population = df.loc[name][\u0026#39;Population with Disability\u0026#39;] features[\u0026#39;properties\u0026#39;] = {\u0026#39;name\u0026#39;: name, \u0026#39;percentage\u0026#39;: percentage, \u0026#39;population\u0026#39;:population} return features df_geojson.features.apply(add_data) 0 {'properties': {'name': 'Humla', 'percentage':... 1 {'properties': {'name': 'Darchula', 'percentag... 2 {'properties': {'name': 'Bajhang', 'percentage... 3 {'properties': {'name': 'Mugu', 'percentage': ... 4 {'properties': {'name': 'Bajura', 'percentage'... ... 70 {'properties': {'name': 'Siraha', 'percentage'... 71 {'properties': {'name': 'Saptari', 'percentage... 72 {'properties': {'name': 'Morang', 'percentage'... 73 {'properties': {'name': 'Sunsari', 'percentage... 74 {'properties': {'name': 'Jhapa', 'percentage':... Name: features, Length: 75, dtype: object features = df_geojson[\u0026#39;features\u0026#39;].values.tolist() my_dict = {\u0026#34;type\u0026#34;:\u0026#34;FeatureCollection\u0026#34;, \u0026#34;features\u0026#34;:features} token = \u0026#34;pk................................................................\u0026#34; viz = ChoroplethViz(my_dict, access_token=token, color_property=\u0026#39;population\u0026#39;, color_stops=create_color_stops([0, 2500, 5000, 7500, 10000, 12500], colors=\u0026#39;YlOrRd\u0026#39;), color_function_type=\u0026#39;interpolate\u0026#39;, line_stroke=\u0026#39;--\u0026#39;, line_color=\u0026#39;rgb(128,0,38)\u0026#39;, line_width=1, opacity=0.8, center=(84, 28.5), zoom=6 ) viz.show() Percentage and Population of disabled in each district of Nepal viz = ChoroplethViz(my_dict, access_token=token, color_property=\u0026#39;percentage\u0026#39;, color_stops=create_color_stops([0, 1, 2, 3, 4], colors=\u0026#39;YlOrRd\u0026#39;), color_function_type=\u0026#39;interpolate\u0026#39;, line_stroke=\u0026#39;--\u0026#39;, line_color=\u0026#39;rgb(128,0,38)\u0026#39;, line_width=1, opacity=0.8, center=(84, 28.5), zoom=6 ) viz.show() Districts with most number of Disability (Sorted 5) df.sort_values([\u0026#39;Population with Disability\u0026#39;], ascending=False).head() Male Female Population with Disability Percentage of PWDs District Kathmandu 9144 7978 17122 0.98 Morang 9494 7559 17053 1.77 Kailali 8143 7235 15378 1.98 Jhapa 8466 6910 15376 1.89 Chitwan 6973 5964 12937 2.23 import pandas as pd from bs4 import BeautifulSoup from urllib.request import Request, urlopen import seaborn as sns site = \u0026#34;https://www.onlinekhabar.com/\u0026#34; hdr = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0\u0026#39;} req = Request(site,headers=hdr) page = urlopen(req) soup = BeautifulSoup(page, \u0026#34;lxml\u0026#34;) image_count = 0 alt_count = 0 collection = soup.findAll(\u0026#34;img\u0026#34;) for img in collection: image_count = image_count + 1 if \u0026#39;alt\u0026#39; in img.attrs: alt_count = alt_count + 1 print (\u0026#34;image_count\u0026#34;, image_count) print (\u0026#34;alt_count\u0026#34;, alt_count) print (\u0026#34;alt_count_percent\u0026#34;, alt_count/image_count*100) image_count 154 alt_count 1 alt_count_percent 0.6493506493506493 df = pd.read_csv(\u0026#39;data_sources/news_portals.csv\u0026#39;) df Portal Link 0 Online Khabar https://www.onlinekhabar.com/ 1 eKantipur http://www.ekantipur.com/ 2 Setopati https://www.setopati.com/ 3 The Himalayan Times https://thehimalayantimes.com/ 4 My Republica https://myrepublica.nagariknetwork.com/ 5 Nepal News https://www.nepalnews.com/ 6 Gorkhapatra http://www.gorkhapatraonline.com/ 7 Nepali Times https://www.nepalitimes.com/ def alt_counter(site): try: hdr = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0\u0026#39;} req = Request(site, headers=hdr) page = urlopen(req) soup = BeautifulSoup(page, \u0026#34;lxml\u0026#34;) image_count = 0 alt_count = 0 collection = soup.findAll(\u0026#34;img\u0026#34;) for img in collection: image_count = image_count + 1 if \u0026#39;alt\u0026#39; in img.attrs: alt_count = alt_count + 1 return (alt_count, image_count) except: return (None, None) df[\u0026#39;alt_count\u0026#39;],df[\u0026#39;image_count\u0026#39;]=zip(*df.Link.apply(alt_counter)) df Portal Link alt_count image_count 0 Online Khabar https://www.onlinekhabar.com/ 1 154 1 eKantipur http://www.ekantipur.com/ 129 147 2 Setopati https://www.setopati.com/ 121 122 3 The Himalayan Times https://thehimalayantimes.com/ 137 138 4 My Republica https://myrepublica.nagariknetwork.com/ 94 95 5 Nepal News https://www.nepalnews.com/ 198 212 6 Gorkhapatra http://www.gorkhapatraonline.com/ None None 7 Nepali Times https://www.nepalitimes.com/ 71 74 df[\u0026#39;percent\u0026#39;] = 100*df[\u0026#39;alt_count\u0026#39;]/df[\u0026#39;image_count\u0026#39;] df Portal Link alt_count image_count percent 0 Online Khabar https://www.onlinekhabar.com/ 1 154 0.649351 1 eKantipur http://www.ekantipur.com/ 129 147 87.7551 2 Setopati https://www.setopati.com/ 121 122 99.1803 3 The Himalayan Times https://thehimalayantimes.com/ 137 138 99.2754 4 My Republica https://myrepublica.nagariknetwork.com/ 94 95 98.9474 5 Nepal News https://www.nepalnews.com/ 198 212 93.3962 6 Gorkhapatra http://www.gorkhapatraonline.com/ None None NaN 7 Nepali Times https://www.nepalitimes.com/ 71 74 95.9459 sns.barplot(y=\u0026#39;Portal\u0026#39;, x=\u0026#39;percent\u0026#39;, data=df) ","date":"2018-08-08T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/inclusive_newspaper/","section":"posts","tags":["python","viz","maps"],"title":"Inclusive Nepal"},{"categories":["pet projects"],"contents":" Finding answers after Tootle In 2017, I went to Thailand to participate in the Seedstars summit. The platform provides funding to startups in emerging markets. The diversity of project ideas made me curious enough to scrape data from the Seedstars website. After leaving Tootle, I was interested in researching the NEXT product to invest my time in. The data scraped would allow me to create a one stop place for every startup that had applied to Seedstars (there were plenty).\nA small chunk of fintech startups that applied to Seedstars name problem solution stage Pymlo Pte. Ltd. Traditional accounting in SMEs are expensive, inefficient, and error-prone due to a lot of manual processes. Pymlo accounting platform helps non-accountant users in SMEs to manage accounting themselves easily at lower cost. Revenue Stage QuickCash Long \u0026amp; costly processing time, collateral requirement for working capital, no predictive data analytics platform for small business loans. QuickCash Platform analyzes business activity through innovative online scoring platform in minutes, providing working capital in one click Revenue Stage Afrikwity SME?s are playing a key role in africa positive trend (they represent 33% of GDP \u0026amp; 90% of private firms). 70% of SME are facing financing pb Afrikwity offers investors in the north opportunity to invest part of their savings in the capital of african innovative startups \u0026amp; SMEs. Beta Testing Stage S-Cash Payment 4 out of 5 people in developing countries are excluded from traditional financial services Offer the first digital financial service to savings, credit and payment that you can dedicate to your smartphone. Pre-Revenue Stage DIRECT SOLUTION SARL Trop de temps pour connaitre le prix de son assurance et souscrire. D\u0026rsquo;où un taux trop faible en Afrique. Mettre en place un chatbot qui permettre au client d\u0026rsquo;avoir son assurance en moins de 2mn. Beta Testing Stage Kibubu Limited Unplanned expenditures habits\\r\\r\\r\\r\\n - Withdraw money as one wishes not as needs.\\r\\r\\r\\r\\n - Involved in unplanned purchases. An application/A Platform to transform the traditional Kibubu (moneybox/piggy bank) into electronic one. Pre-Revenue Stage A-Trader Brokerage \u0026amp; Securities 82% of adults with savings but as of 2013, only 13.9% with access to formal banking, financial products, and savings and investment options. Web and app based platform providing investment options to any of 40 million mobile phone users in Tanzania, and 23 million internet users. Beta Testing Stage MaxiCash Financial Support in Africa through remittance agencies is inconvenient and costly; while Payment collection is complex and rudimentary. MaxiCash is a revolutionary way to enable Financial Support and Payment Collection in Africa with Cash Vouchers System using a Mobile App. Revenue Stage Riovic High capital costs in insurance are borne by consumers who end up paying high premiums and receive less value from their insurers. An online marketplace that provides companies in insurance with cost-efficient access to capital for various needs. Revenue Stage MamboPay Limited Most financially excluded people do not have access to a mobile phone and this puts them at a disadvantage and can not use mobile money. Our technology allows card holders (National ID card or MamboPay Card) to be able to make mobile money transactions using Card Numbers. Revenue Stage ytibcapital problem of growing cottage industry to a quality producing plant solution is to enable easy access to quality machinary andexpart knowladge through payment inkind to suppliers Beta Testing Stage DusuPay Ltd Lack of a proper payment gateway to Africa Creating an ubiquitous platform that works effectively with local modes of payment in various parts of the world to make payments possible Expansion Stage Fintech 60% of Nigerians still unbanked, \u0026lsquo;change\u0026rsquo; problem plaguing the street, low accountability and transparency on revenue Revenue assurance, getting the unbanked into the system,solving balance (\u0026lsquo;change\u0026rsquo;) problem, Revenue Stage Rafode ltd Rafode Addreses the problem of access to credit by rural poor to purchase solar lamps and energy savings cook stove and repay while using. Rafode provide solar lamps and energy savings cookstoves to rural poor on credit and repay the loan on instalments for a period of time Revenue Stage Flexitech Group Limited Tradition layaway method is tedious to manage and inconvenient for both the customer and the merchant. An automated layby system for both Merchants and their customers for the purchase of goods and services.\\r\\r\\r\\r\\n\\r\\r\\r\\r\\n Revenue Stage Bdrates Holdings Limited There are too many banks offering too many products. Offline comparison is tedious and inefficient. An online financial comparison and application can make the retail lending market more efficient for both borrowers and lenders. Revenue Stage FYPTO Leftover currencies: non exchangeable for small amount. Foreign travelers normally dump it at home or unwillingly spend it at airports. Deposit your leftover currencies easily to many FYPTO agents around you and use via FYPTO App in many ways that brings the value of it. Development Stage Imaginary Pay 1) Lack of Interaction between Financial Institutions\u0026rsquo; System. \\r\\r\\r\\r\\n2) There is no 24 by 7 model. \\r\\r\\r\\r\\n3) Data Access Restriction. Money Transfer and Payment Platform, which interact seamlessly within an ecosystem via API. Development Stage MyCash Online Currently, there is around 3.3 million* migrants, here in Malaysia. Most of them do not have access to any banks or credit cards. We are an online market place specially designed for the migrants, where they can purchase services online \u0026amp; pay using MyCash Coupons. Expansion Stage WHO\u0026rsquo;s GOOD Acquiring reliable environmental, social and governance (ESG) data to measure sustainability performance is time consuming and expensive WHO\u0026rsquo;S GOOD is an easy to use AI based platform that analyzes, compares and evaluates companies\u0026rsquo; ESG performance time and cost-effectively. Development Stage Smartly Getting started with investing in 2018 is still too complicated. Fully digital platform that teaches and allows everyday people to start investing and saving. Pre-Revenue Stage ZigWay Irregular incomes force poor families to take out expensive informal loans to meet daily needs like food, keeping them in a debt trap. We help people access quick automated Nano Loans ($5-50) to meet daily needs and smooth expenses in a cheaper, more flexible and safer way. Idea Stage Oncoinsurance Imperfect system of treating cancer in the country Getting better treatment in the case of insurance Revenue Stage Simple Invest Broker services are for traders and professionals. Retail investors has no suitable product to use for investing their money. Simple mobile app with brokerage account with AI roboadvisor Revenue Stage Mesfix The hard time that MSME´s face to find financial products with the traditional banking system Online platform that connects MSME´s in financial need with a community of people interested on investing from small amounts and high yield Revenue Stage Innovafunding lack of financing for small size companies An online platform that connects investors and small companies and help both to find finnancing and investment Beta Testing Stage Operadora SuSu SAPI de CV Companies offer employee\u0026rsquo;s savings fund, however these are a pain to manage and often lead to fraud when operated internally. After registering in the app employees can see their employee savings fund in real time and request a loan directly in the App. Revenue Stage Vest Wealth Management In Emerging Markets, the middle class doesn?t have access to the financial services like Investing because they are seen as \u0026ldquo;elite\u0026rdquo;. Vest is a digital investment advisor that target the middle class in emerging markets. Revenue Stage Payit Cash is expensive in time and money, and risky and that\u0026rsquo;s why we want to get rid of it! Mobile platform that makes it easy for users to pay everything digitally starting by:\\r\\r\\r\\r\\n- Our Friends\\r\\r\\r\\r\\n- Informal Market \\r\\r\\r\\r\\n- Services Pre-Revenue Stage Lefort More than 72% of SMEs in Mexico don\u0026rsquo;t have access to financing products. Current processes are manual and consume a lot time and resources. Replace a manual, complex and resource consuming process with a fully automatic service, 10 times cheaper 200 times faster. Revenue Stage Mutual People can\u0026rsquo;t do loans to another person or company legally in Brazil, just Banks. Mutual is the first Lending P2P Market Place enabling individuals to make and control their Loans each other with 100% autonomy 100% legal. Development Stage Facturedo Expensive and bad user experience (lack of transparency, pricing subject to bias) from existing traditional players. Platform with transparent and cheap pricing, plus great user experience. Revenue Stage RedCapital Small businesses have a little credit line on banks. So, they have higher rates in other financial institutions. On the RedCapital Website, SMEs request money and investors can choose the loans or the invoices that they would like to invest in. Expansion Stage Goodticket Tecnologia de Pagamentos Today the benefits market offers a bureaucratic and non malleable model, which ultimately makes the experience at the time of hiring a slow We offer a practical WEB platform for the company to manage simply by adding credits. For the beneficiaries, we are a quick app platform. Pre-Revenue Stage EASYCRÉDITO Credit is always the last eatapa buying process happens after the emotional experience and causes frustration when denied. Our solution enables you to transfer all the credit hire experience to a multi-channel online platform through applications or websites. Expansion Stage Neotic Large amount of info a stock trader has to deal with every day Providing a personnalisable robot that can crunch tons of data and learn from historical data. Revenue Stage 2nate Lack of money for fighting against poverty, cultural problems, social anomalies. Crowdfunding and micro-donations, What our fathers used to do. Pre-Revenue Stage Drupz Saving money is necessary for most of us, but we cannot do it effectively. It\u0026rsquo;s a complex, multi-faceted problem that is very hard to solve. Drupz\u0026rsquo;s novel machine learning algorithms analyze your financial behavior and automatically and seamlessly saves money for your goals. Beta Testing Stage AbantuCard ( Isogong pty ltd ) Its difficult to make better choice to make debit and credit card payments and to get cheaper and better interest on borrowings and savings Debit Card, Credit Card and Investment Account in ONE, automated, based on person to person network. Pre-Revenue Stage Democrance More than 350 million people in MENA do not have access to insurance- but are the ones that need insurance and protection the most. A FinTech platform that makes insurance affordable and accessible to the lower-income population and adds value to insurers and distributors Beta Testing Stage Finkee People usually know their income, but don\u0026rsquo;t know how to make their money work for them and to control where it is actually going. The best solution is to analyse your expenses and get personalized recommendations to achieve your goals with the help of our system. Development Stage Bloomzed (Rocket-A Lab) To make simple daily transactions a consumer needs to think about lots of stuff beforehand (withdraw money, change money, send money, etc.) Simplify, fasten and make more secure daily consumption and payment transactions via smartphone. Development Stage Expediente Azul Whenever a business customer asks for a loan, he has to send many documents, 5 to 20 on average, by email or on paper. Followup is a mess. A tool to easily capture, receive, analyze automatically and store the many documents sent by a particular customer that requests a loan. Revenue Stage Credytag The only credit card payment system in the market are card reader or a dongle. Both have allot of expensive and complicated requirements. Making the payment system work with just a QR Code and a phone that recibes SMS for the business and a mobile app for the consumer. Development Stage Kintos The limitation of income and credit cards access as well as financial education for young people. Loans and investments tuned just right for the necessities of young people, complemented with engaging financial education. Beta Testing Stage Companion Business Solutions There are more than 800k taxpayers in Azerbaijan and they have to use 3 web sites and softwares for sending reports and to the government. Using only one app we can simpler do all works in these portals, even in our phones and pads. Idea Stage FlutterPay Leverages a distributed ledger system to reduce costs associated with payments and increase speed. Leverages a distributed ledger system to reduce costs associated with payments and increase speed. Development Stage Fondify In Mexico there is a lack of option to get resources, traditional channels such as banks or government entities aren`t an option for creators We develop a Crowdfunding Platform inspired in the LATAM culture. With global funds for the creators from people who love their project Pre-Revenue Stage Dinerio People don´t save money due to a lack of knowledge and financial planning. There are no tools for people to track their expenses easily. Dinerio extracts transactions\u0026rsquo; information from online banking, categorizes everything and keeps track of budgets, automatically. Beta Testing Stage Distribution and Count by sectors Stages of startups Startup countries ","date":"2018-03-03T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/seedstars/","section":"posts","tags":["viz","python"],"title":"Startup analysis using Seedstars data"},{"categories":["open source","pet projects","proof of concept"],"contents":" Introduction\nI have been experimenting with Deep Learning models in PyTorch for a couple of weeks now. PyTorch is an open source python package that provides Tensor computation (similar to numpy) with GPU support. The dataset used for this particular blog post does no justice to the real-life usage of PyTorch for image classification. However, it serves as a general idea of how Transfer Learning can be used for more complicated image classification. Transfer learning, in a nutshell, is reusing a model developed for some other classification task, for your classification purposes. The dataset was created by scraping images from google image search.\nCreating the dataset\nFor our dataset, we need images of birds, planes, and Superman. We will be using the icrawler package to download the images from google image search.\nWe repeat the same for birds and Superman. Once all the files have been downloaded, we will restructure the folders to contain our training, testing and validating samples. I am allocating 70% for training, 20% for validating and 10% for testing.\nLoading the data\nPyTorch uses generators to read the data. Since datasets are usually large, it makes sense to not load everything in memory. Let\u0026rsquo;s import useful libraries that we will be using for classification.\nNow that we have imported useful libraries, we need to augment and normalize the images. Torchvision transforms is used to augment the training data with random scaling, rotations, mirroring and cropping. We do not need to rotate or flip our testing and validating sets. The data for each set will also be loaded with Torchivision\u0026rsquo;s DataLoader and ImageFolder.\nLet us visualize a few training images to understand the data augmentation.\nLoading a pre-trained model We will be using Densenet for our purposes.\nThe pre-trained model\u0026rsquo;s classifier takes 1920 features as input. We need to be consistent with that. However, the output feature for our case is 3 (bird, plane, and Superman).\nNow, let\u0026rsquo;s create our classifier and replace the model\u0026rsquo;s classifier.\nWe are using ReLU activation function with random dropouts with a probability of 20% in the hidden layers. For the output layer, we are using LogSoftmax.\nTraining Criterion, Optimizer, and Decay\nModel Training and Testing\nLet us calculate the accuracy of the model without training it first.\nThe accuracy is pretty low at this time, which is expected. The cuda parameter here is the boolean object passed for the availability of GPU hardware in the machine.\nLet us train the model.\nSince GPU is supported, the training took around 10 mins. The validation accuracy is almost 99%. Let us check the accuracy over training data again.\nImage Preprocessing We declare a few functions to preprocess images and pass on the trained model.\nPredicting by passing an image\nSince our model is ready and we have built functions that allows us to visualize, let us try it out on one of the sample images.\nSo, that is it.\n","date":"2018-02-02T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/bird_plane_superman/","section":"posts","tags":["ml","python","pytorch","classification"],"title":"Birds, Plane, Superman"},{"categories":["conceptual"],"contents":"Diffie-Hellman key exchange (The introductory content is from wikipedia)\nDiffie–Hellman key exchange is a method of securely exchanging cryptographic keys over a public channel and was one of the first public-key protocols as originally conceptualized by Ralph Merkle and named after Whitfield Diffie and Martin Hellman. DH is one of the earliest practical examples of public key exchange implemented within the field of cryptography.\nDiffie-Hellman is a way of generating a shared secret between two people in such a way that the secret can\u0026rsquo;t be seen by observing the communication. That\u0026rsquo;s an important distinction: You\u0026rsquo;re not sharing information during the key exchange, you\u0026rsquo;re creating a key together.\nThis is particularly useful because you can use this technique to create an encryption key with someone, and then start encrypting your traffic with that key. And even if the traffic is recorded and later analyzed, there\u0026rsquo;s absolutely no way to figure out what the key was, even though the exchanges that created it may have been visible. This is where perfect forward secrecy comes from. Nobody analyzing the traffic at a later date can break in because the key was never saved, never transmitted, and never made visible anywhere.\nThe way it works is reasonably simple. A lot of the math is the same as you see in public key crypto in that a trapdoor function is used. And while the discrete logarithm problem is traditionally used (the xy mod p business), the general process can be modified to use elliptic curve cryptography as well.\nBut even though it uses the same underlying principles as public key cryptography, this is not asymmetric cryptography because nothing is ever encrypted or decrypted during the exchange. It is, however, an essential building-block, and was in fact the base upon which asymmetric crypto was later built.\nCryptographic explanation The simplest and the original implementation of the protocol uses the multiplicative group of integers modulo p, where p is prime, and g is a primitive root modulo p. These two values are chosen in this way to ensure that the resulting shared secret can take on any value from 1 to p–1. Here is an example of the protocol, with non-secret values in blue, and secret values in red.\nImports import base64 from primesieve import nth_prime from random import randint from Crypto.Cipher import AES Key Sharing Public numbers # small prime number g = nth_prime(50) g 229 # large prime number p = nth_prime(1000) p 7919 Alice and Bob Private number a = nth_prime(randint(1, p-1)) b = nth_prime(randint(1, p-1)) a 49019 b 70639 Public Message Transfer Alice sends Bob publicly alice_sends = g**a % p Bob sends Alice publicly bob_sends = g**b % p Shared Secret key Alice shared_secret_key_alice = bob_sends**a % p shared_secret_key_alice 7065 Bob shared_secret_key_bob = alice_sends**b % p shared_secret_key_bob 7065 assert shared_secret_key_alice==shared_secret_key_bob shared_secret_key = shared_secret_key_alice Both Alica and Bob now have the secret key, without compromising their private keys.\nDH is public key/asymmetric crypto but not encryption. For the demo, AES 256 (takes 32 bytes)\nEncryption and Decryption Changing the key to 32 byte (a bad hacky way)\nkey = str(shared_secret_key) key_bytes = str.encode(key.zfill(32)) # The message should be a multiple of the byte size alice_to_bob_original = str.encode(\u0026#39;hello world how are you\u0026#39;.zfill(256)) cipher = AES.new(key_bytes, AES.MODE_ECB) cipher_msg = cipher.encrypt(alice_to_bob_original) cipher_msg is sent to the server, bob reads the cipher_msg and uses the secret key to decipher\ndecipher = AES.new(key_bytes, AES.MODE_ECB) print(decipher.decrypt(cipher_msg)) b'00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000hello world how are you' DH is prone to man-in-the-middle attacks The Diffie-Hellman Scheme does not provide authentication of any kind. It only allow 2 anonymous parties to share a common secret. But for all Alice knows, she could be shaking hands with the devil (instead of Bob). This is why we need at least one party to be authenticated.\nFor example: SSL (https), the webserver is authenticated using PKI (Public Key Infrastructure), and then a secure connection is established (D-H) between the website and the client. Since the website has been authenticated, the client can trust the website, but the website cannot trust the client. It is now safe for the client to provide his own authentication details on the webpage.\nFor a practical answer if you are configuring your SSL/TLS server: you should use a modulus of at least 2048-bit, and a generator g such that the order of g is a prime q of at least 256 bits; alternatively, you may use a modulus p which is a \u0026ldquo;safe prime\u0026rdquo; (the order of g will then be either a very big prime, or twice a very big prime, which is almost as good). Some people feel safer when they generate their DH parameters \u0026ldquo;themselves\u0026rdquo;(*) instead of reusing existing values; if that\u0026rsquo;s what it takes to allow you to sleep at night, then do it.\nElliptic Curve replaces primes with elliptic curve. The benefit is efficiency.\n","date":"2018-01-12T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/diffie_hellman/","section":"posts","tags":["encryption","python"],"title":"Diffie-Hellman key exchange (and AES-256)"},{"categories":["proof of concept"],"contents":"RSA RSA (Rivest–Shamir–Adleman) is one of the first public-key cryptosystems and is widely used for secure data transmission. In such a cryptosystem, the encryption key is public and it is different from the decryption key which is kept secret (private). In RSA, this asymmetry is based on the practical difficulty of the factorization of the product of two large prime numbers, the \u0026ldquo;factoring problem\u0026rdquo;. The acronym RSA is made of the initial letters of the surnames of Ron Rivest, Adi Shamir, and Leonard Adleman, who first publicly described the algorithm in 1978. Clifford Cocks, an English mathematician working for the British intelligence agency Government Communications Headquarters (GCHQ), had developed an equivalent system in 1973, but this was not declassified until 1997\nA user of RSA creates and then publishes a public key based on two large prime numbers, along with an auxiliary value. The prime numbers must be kept secret. Anyone can use the public key to encrypt a message, but with currently published methods, and if the public key is large enough, only someone with knowledge of the prime numbers can decode the message feasibly. Breaking RSA encryption is known as the RSA problem. Whether it is as difficult as the factoring problem remains an open question.\nRSA is a relatively slow algorithm, and because of this, it is less commonly used to directly encrypt user data. More often, RSA passes encrypted shared keys for symmetric key cryptography which in turn can perform bulk encryption-decryption operations at much higher speed.\nImports from primesieve import nth_prime RSA Algorithm Take two distinct, large primes p and q (Ideally these have a similar byte-length) Multiply p and q and store the result in n Find the totient for n using the formula φ(n)=(p−1)(q−1) Take an e coprime that is greater, than 1 and less than n Find d using the formula d⋅e≡1modφ(n) At this point, the pair (e, n) is the public key and the private key (d, n) is the private key.\np = nth_prime(10) q = nth_prime(15) n = p*q print (p,q,n) 29 47 1363 totient = (p-1)*(q-1) print (totient) 1288 Totient In number theory, Euler\u0026rsquo;s totient function counts the positive integers up to a given integer n that are relatively prime to n. It is written using the Greek letter phi as φ(n) or ϕ(n), and may also be called Euler\u0026rsquo;s phi function. It can be defined more formally as the number of integers k in the range 1 ≤ k ≤ n for which the greatest common divisor gcd(n, k) is equal to 1. The integers k of this form are sometimes referred to as totatives of n.\nThe line on the top represents distribution of prime numbers. The phi of a prime number is simply the (n-1)\nPhi function is multiplicative (for relatively prime numbers). Therefore, phi of A times B where A and B are prime is (A-1) times (B-1) Coprime In number theory, two integers a and b are said to be relatively prime, mutually prime, or coprime (also written co-prime) if the only positive integer (factor) that divides both of them is 1. Consequently, any prime number that divides one does not divide the other. This is equivalent to their greatest common divisor (gcd) being 1\nfrom math import gcd import random def modinv(a, m): for x in range(1, m): if (a * x) % m == 1: return x return None def coprimes(a): l = [] for x in range(2, a): if gcd(a, x) == 1 and modinv(x,a) != None: l.append(x) for x in l: if x == modinv(x,a): l.remove(x) return l coprime_list = coprimes(totient) secure_random = random.SystemRandom() e = secure_random.choice(coprime_list) d = modinv(e, totient) d 685 Private and Public key pairs print (\u0026#39;Public key pair:\u0026#39;, e, n) Public key pair: 1021 1363 print (\u0026#39;Private key pair:\u0026#39;, d, n) Private key pair: 685 1363 Test test = 2**e % n test 1029 test**d % n 2 Encryption and Decryption def encrypt(msg, pub, pri, mod): chars = [ord(x) for x in list(msg)] cipher = [] for char in chars: cipher.append(chr(char**pub%mod)) return \u0026#39;\u0026#39;.join(cipher) def decrypt(msg, pub, pri, mod): chars = [ord(x) for x in list(msg)] cipher = [] for char in chars: cipher.append(chr(char**pri%mod)) return \u0026#39;\u0026#39;.join(cipher) cipher = encrypt(msg = \u0026#39;hello\u0026#39;, pub=e, pri=d, mod=n) decrypt(cipher, pub=e, pri=d, mod=n) 'hello' Python library import rsa Bob generates a keypair, and gives the public key to Alice. This is done such that Alice knows for sure that the key is really Bob’s (for example by handing over a USB stick that contains the key).\n(bob_pub, bob_priv) = rsa.newkeys(512) (alice_pub, alice_priv) = rsa.newkeys(512) Alice writes a message, and encodes it in UTF-8. The RSA module only operates on bytes, and not on strings, so this step is necessary.\nmessage = \u0026#39;hello Bob!\u0026#39;.encode(\u0026#39;utf8\u0026#39;) Alice encrypts the message using Bob’s public key, and sends the encrypted message\ncipher = rsa.encrypt(message, bob_pub) Bob receives the message, and decrypts it with his private key.\ndecrypt_cipher = rsa.decrypt(cipher, bob_priv) print (decrypt_cipher) b'hello Bob!' Since Bob kept his private key private, Alice can be sure that he is the only one who can read the message. Bob does not know for sure that it was Alice that sent the message, since she didn’t sign it.\nSignature Suppose Alice uses Bob\u0026rsquo;s public key to send him an encrypted message. In the message, she can claim to be Alice but Bob has no way of verifying that the message was actually from Alice since anyone can use Bob\u0026rsquo;s public key to send him encrypted messages. In order to verify the origin of a message, RSA can also be used to sign a message.\nSuppose Alice wishes to send a signed message to Bob. She can use her own private key to do so. She produces a hash value of the message, raises it to the power of d (modulo n) (as she does when decrypting a message), and attaches it as a \u0026ldquo;signature\u0026rdquo; to the message. When Bob receives the signed message, he uses the same hash algorithm in conjunction with Alice\u0026rsquo;s public key. He raises the signature to the power of e (modulo n) (as he does when encrypting a message), and compares the resulting hash value with the message\u0026rsquo;s actual hash value. If the two agree, he knows that the author of the message was in possession of Alice\u0026rsquo;s private key, and that the message has not been tampered with since.\nThis works because multiplication is commutative so {\\displaystyle h=hash(m);(h^{e})^{d}=h^{ed}=h^{de}=(h^{d})^{e}\\equiv h{\\pmod {n}}} {\\displaystyle h=hash(m);(h^{e})^{d}=h^{ed}=h^{de}=(h^{d})^{e}\\equiv h{\\pmod {n}}} Thus, the keys may be swapped without loss of generality, that is a private key of a key pair may be used either to:\nsignature = rsa.sign(message, alice_priv, \u0026#39;SHA-1\u0026#39;) rsa.verify(message, signature, alice_pub) 'SHA-1' Complete Bob and Alice generate a keypair and share public keys.\n(bob_pub, bob_priv) = rsa.newkeys(512) (alice_pub, alice_priv) = rsa.newkeys(512) Alice writes a message.\nmessage = \u0026#39;hey Bob!\u0026#39;.encode(\u0026#39;utf8\u0026#39;) Alice signs the message with private key\nsignature = rsa.sign(message, alice_priv, \u0026#39;SHA-1\u0026#39;) Alice encrypts the message using Bob’s public key, and sends the encrypted message\ncipher = rsa.encrypt(message, bob_pub) Bob receives the message and checks for authenticity using alice\u0026rsquo;s public key\nrsa.verify(message, signature, alice_pub) 'SHA-1' Bob decrypts the message using his private key\ndecrypt_cipher = rsa.decrypt(cipher, bob_priv) print (decrypt_cipher) b'hey Bob!' signature b'\\x07Of\\xabW^\\x0b\\xaeFh\\x01L\\xf3\\x11\\xe0\\xe9\\\\\\x99r\\xc9\\x1c\\x044\\x11\\xfc{5\\xa3_ \\xeb\\xba\\xf8\\x84b\\x1e\\xb7\\xadK)\\xdf\\x9b\\x8f|\\xc4\u0026gt;\\x89\\x9d\\xdf\\x98OI\\xc1\\x87\\xc2\\xdd\\xf3\\xf6\\x16\\xae2`my' ","date":"2018-01-12T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/rsa_encryption/","section":"posts","tags":["encryption","python"],"title":"RSA encryption"},{"categories":["tootle"],"contents":"A brief history of Tootle Hello World, When Tootle started, there were four of us. At the time, we were working on EdCrayon (Three60’s Education and Classroom Management System). The development of EdCrayon was pretty much complete and one of the better schools in town had been implementing it for a year. We were busy with building fewer last minute requirements such as Nepalese school standard digital report cards and student ranking systems. After those were done and the academic session ended, we were preparing for the next academic session and we were also in talks with several schools for implementation of EdCrayon.\nhttps://princihere.files.wordpress.com/2016/10/androiddevices.png\nHowever, because of earthquake of 2015, the school that were lined up decided to back out and use their EdCrayon allocated resources on repairing the infrastructure damages. We would come to work and not feel productive at all. Due to the sheer boredom at work, we started researching on several ideas including location based services. One of the very first ideas was to create an app that would allow users to track location of Sajha buses on Google Maps so that they could plan on leaving offices/homes by referencing estimated time of arrival provided by the app. Even though Sajha buses were comparatively convenient, the problem was that people had to wait for the buses, generally up to half an hour, since bus stops arrival timings were more or less random. We developed the prototype for a Sajha bus route but since Sajha bus did not show any interest we decided to move on to other ideas.\nMeanwhile, I was researching on asynchronous API calls for a pet project of mine. I had been trying to figure out the best way to sync client’s on-device offline database and off-device master database. In this process, one of the first things that I did was to look into creating my own implementation by syncing Android SQLite database with MySQL database. Up until this point, when ever client sync was required, I would simply clear out the content adapter’s list, remove all elements, get all elements from master server and refill the adapter. This allowed for easy implementation. However, even if nothing was changed in the client database, whenever the client hit the sync button or pulled to refresh, the process would get repeated. Basically the question I was trying to answer was, “Is it faster and less expensive to individually assess for updates by comparing updated_date and id or to simply truncate and refill?” During the research for this, I stumbled upon newer technologies that would allow setting up changes listeners on client’s devices to refresh the changed list without having to trigger an action to sync the data. Using this finding and our research from location based services, we felt comfortable tackling the idea for a ride sharing application from a technological perspective, and Tootle was born. Tootle was not Tootle from the very beginning. We continued our research on appropriate Business Models, Brand Positioning, Business Strategy, and Marketing and Delivery. Based on the changes in how we were going to position our brand in the market, we changed the product name from CabIO (digital cabs), KAR.ma (share your ride for karma), Bzuli (environmentally conscious ride sharing) and finally to Tootle (a fun way to travel without restricting the service to electric vehicles and only four wheeled vehicles). In the meantime, based on the Business Models and Brand Positioning, several elements of the app were also changed.\nAlthough the concept of ride sharing is not new, and several companies such as Uber, Lyft, Ola and Go-Jek have implemented it tremendously well, Tootle is different mainly due to three core elements. First of all, we let our partners decide if they want to take a ride, i.e, we introduced the idea of casual Tootle partners. For example, a partner A can decide to give rides throughout the day and make this his/her full time job or simply give rides that matches his/her travel itinerary. The technology is adjusted in such a way that a tootle ride requests are sent to several partners within a vicinity rather than just one partner. Unlike aforementioned companies, there are no penalties involved for not taking a ride. Secondly, albeit not completely by choice, we have realized that frugality invites creativity. At every step of technology development, we have had to strategize inexpensive yet effective ways to solve problems. Although we are still purely in development phase as of today, I believe we can really take pride in what we have accomplished given the resources. This is also reflective in the product. We were forced to think about minimizing data consumption (given the high data cost in Nepal) and poor internet infrastructure. Currently, on average, a particular ride for a partner in terms of data exhausts 1 MB while him/her being able to log in, select appropriate ride, complete the ride and get paid, while the backend collects ride information such as timestamps for actions and exact route followed. This costs him/her 50 Paisa which is approximately $0.005. Finally, last but not the least, the major difference comes in the form of technology adaptation, contextualization and more importantly, communication. Kathmandu is traditionally more or less a close-knit community where people prefer talking to people to garner a sense of confirmation and safely. Therefore, it was crucial to build a simple and clean UI that gets the job done and focus massively on developing technologies that provided real time ride statistics to our team at call center so that they could assist people giving and taking Tootle rides. Similarly, since digitization of payment was essential but Nepal is not ready for secure and realtime credit card transaction, apart from local third party digital wallet integration we also have QR top-ups. To summarize, although from the surface, Tootle is a ride sharing application like Uber, it has established its own identity via contextualization of the requirements of Nepali needs. This is apparent throughout the technology.\nTootle Today From our experience, we have realized that market drives technology and not the other way around. We have also realized that although technology is merely a facilitator, it can do wonders to solve problems and invoke habit changes if done correctly. Now, we have a multidisciplinary team of 15 striving to make Tootle technologies and services better each passing day with the goal of doing it correctly.\n","date":"2017-10-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/tootle_history/","section":"posts","tags":["product"],"title":"A brief history of Tootle"},{"categories":["tootle"],"contents":"\nLink to Tootle Website\nTootle is a ride sharing service that matches bike riders and commuters. While the concept is not new, there are several contextualization factors that makes it unique. As research and development engineer and lead software developer, this was my biggest challenge to date and I learnt a lot of things. However, since we worked in a very small team, I occasionally came across problems related to branding, client relationship, marketing and overall B2C business model. This blog post will dive into those learnings rather than focus on technological implementations. To summarize the entire post, it is about my journey from a single dimensional software developer to a software developer who also understands project management, human resources, market, competition, branding and plethora of other factors that a software developer whose long term goal is to become a tech entrepreneur needs in their arsenal.\nStart small, www.rome.com was not built in a day This is trivial for most people, but not for me. There were times I was so focused on completing a task that I lost the bigger picture. Breaking down a task into smaller subtasks and solving them as soundly as possible is the key. Also, when you push yourself too hard, you miss small details that might have huge consequences in the future. Think scalability while solving little problems. Like Will Smith said, \u0026ldquo;You don\u0026rsquo;t set out to build a wall. You don\u0026rsquo;t say \u0026lsquo;I\u0026rsquo;m going to build the biggest, baddest, greatest wall that\u0026rsquo;s ever been built.\u0026rsquo; You don\u0026rsquo;t start there. You say, \u0026lsquo;I\u0026rsquo;m going to lay this brick as perfectly as a brick can be laid. You do that every single day. And soon you have a wall.\u0026rdquo; Also, Hofstadter\u0026rsquo;s law will come along to shatter all your to-do plans for the day. It\u0026rsquo;s just the way it is. Deal with it.\nDevelopment and design is an iterative, never ending process. It is not a sprint but a marathon. Seeking perfectionism hurts. First of all, there is no perfect code or design. There might be something that looks, feels and works great today, but it is never going to be perfect. You will always have to come back to that line of code, redesign that slider that looked great once upon a time and continuously update based on user feedback. Coding is a little of writing code and a lot of maintaining it. You are never really done. Project preplanning and risk management might help with not having to go back to drawing board again and again, but development and design is very much an agile process.\nHire people who love what they do and love learning Coworkers become your second family. During the course of Tootle development, I actually spent more time with my coworkers than my family. Fortunately, everyone involved with Tootle are very passionate, hard-working and motivated people who never shy from learning. Also, certain characteristics such as being ardent, an embodiment and advocate of the product, and hardworking are more important than being skillful. I was also on the hiring team and I made sure to look for these characteristics. After all, Leicester City won 2015-2016 Barclays Premier League primarily because of their team spirit.\nPrioritize what is important There will be a lot of bugs. Code related stuff aside, there will also be several things that need attention. However, time is limited. At times you could be in front of your computer debugging for hours and hours, but still the issue tracker list and crash reporting list will be full. Therefore it is necessary to prioritize based on impact level. Assessing impact of a particular task can be difficult. However, if you have an awesome team, it is just a matter of discussing from a business and technical point of view. Also distinguishing between bugs, enhancement, improvement proposal and task is very important.\nCommunication solves 95% of all problems Communication is the key. Often during the day, talk to coworkers about the problems that you are having. First of all, when you explain the problem to someone, you understand the problem better yourself. Secondly, you are also in sync with what problems everyone else is facing. Similarly, simply talking about work related problems, expressing how you feel about deadlines, doing risk analysis together, discussing business strategy together, communicating dissatisfactions etc. will help solve a lot of problems earlier. So, constantly take a break, look outside the window, get a glass of water and simply talk.\nStress can be a catalyst if applied properly This project was very much stressful. Retrospectively, it was supposed to be too because of its complexity. One of the things I really struggled with is handling stress. I have gotten better over the course of this project. However, I am still working on learning to change stress to a catalyst. I hope to become Arsene Wenger someday (giggles). However, I have learnt that if things don\u0026rsquo;t work tonight, you simply have to sleep over it. The freshness of morning solves a lot of problems.\nYou could also just watch SRK dance in Hosh na khud kahi josh\u0026hellip;.\nGo running in the morning, be healthy This one is self-explanatory. Run to remain stress free and healthy. I stopped running for a while because I was lazy. But, I was also lazy because I was not running.\nCompetition is good While reading The Personal MBA by Josh Kaufman, I realized that having market competition is actually advantageous. This concept was really counterintuitive to me previously. Competition is good because it implies higher probability of fulfillment of something known as the Iron Law of Market. It basically means that like you, other people have also seen the availability of the market you are targeting. Similarly, there is a lot to learn from your competition. Learning and applying the principles of game theory helps a lot too. Awesome tech implementation is second to client\u0026rsquo;s requirement You could spend weeks and months on awesome, flawless and amazing features that your clients don\u0026rsquo;t really care about or half an hour on simple features that makes all the difference to them. For example, adding a bitcoin as payment system would be cool, but allowing a female client to select a female driver makes the product secure and also adds marketing and advertising weight.\nBeta tests are really really really important You could sit hours upon hours, days upon days trying to find and fix all the problems. But, finding problems is more difficult once you know your product inside out. You are in a controlled office environment with fast internet, limited real test devices and a clear understanding of product workings. There is no way for you to find all the problems. The solution is to find real users to test, use and provide feedbacks. Canary tests and beta tests are therefore very important.\nJust because you like it, does not mean users are going to like it also You might absolutely love your product. However, it does not mean every user will like it also. Be prepared to get bad ratings. Also, they are not wrong to dislike something you adore so much.\nPrevious projects and keeping up to date with latest technologies help a lot I really believe this one is a biggie. Regardless of whether or not a product succeeds, it is always a stepping stone for future products. This product would not have been possible or would have been terribly difficult without Edcrayon, Edquake, and dozens of other location related prototype. When prototyping, research on newer stacks and practices.\nIn the end, revenue matters the most In the words of Josh Kaufman, Do not be a mercenary since dedication in craft, patience to find right market to be dedicated towards and consistency is required to eventually make money.\nAlso, do not be a crusader since you need money to pay the bills.\nWish us good luck with our tootle journey.\n","date":"2017-01-28T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/tootle_learnings/","section":"posts","tags":["product"],"title":"Learnings from Tootle"},{"categories":["three60"],"contents":" The objective of the game was to use texts, audios, videos, badges, quizzes, points, stories and characters to educate users on preparation for earthquake and decision making during and after earthquake.\nApplication Description\nBefore the game starts, a teaser video is played where an event of earthquake is shown inside a home scenario. The video is shown in order to reflect the importance of earthquake preparedness. The video can be skipped the next time as per the choice of the player.\nApplication Home Screen\nThe application home screen allows users to start a new game, resume from where they left off previously, change game settings such as sound and language settings and submit their score to leaderboards, and quit the application. It also displays current user name and character selection along with all the badges that the user has won.\nStarting a new game\nUpon starting a new game, a dialog appears where the user enter his/her name and selects a character. The user can choose two characters that of a boy or a girl in order to play the game. These characters will also act as protagonists in animated stories.\nBadges\nBadges are provided to users upon completion of a milestone or a level. The badges provides an incentive for the players and makes the game more engaging and interesting.\nPSA and Video Activities\nEngines have been setup to support audio and video activities.\nThe radio can be used as an informative element in the game. For example:\nA PSA for earthquake preparation informing about vital items one needs in their emergency backpack can be played in the radio.\nThe video platform can support a series of storytelling addressing to different type of circumstances and safety tips regarding the effects of earthquake. Nepali subtitles also appear on the bottom of each videos.\nDrag and Drop Activity\nEngines have been setup to support drag and drop activities.\nBased on this drag and drop engine we have developed relevant games. For example:\nPreparing a GoBag\nFollowing the PSA, the users are required to drag and drop items from a set of vital items (such as first aid kid) and useless items (such as toys) to their backpack. Since the backpack can only hold a certain number of items, the users will have to optimize for best preparation. Upon completion, the app lets the users know if their selection is correct.\nSimilarly, the app will let the users know if their selection was wise. In case of wrong selection, the app will also let the users know why their selection was incorrect. Unlike other quiz activities, the difference here is that the users will still be allowed to go back to change their answer. The rationale was to follow a formative teaching approach.\nQuiz Engines\nEngines have been setup to support quiz platform.\nQuizzes produces better organization of knowledge by helping the brain organize material in clusters to allow better retrieval. It also identifies gap in knowledge and lets people know what is learned and what is not.\nThe quiz engines supports quiz games which can be very informative. The answers chosen can also lead to some consequent information or videos later.\nThere are four different types of quiz questions.\nGrid Select True and False Multiple Choice Questions (3 Options) Multiple Choice Questions (4 Options) The first two types of questions carry a marking rubric of 200 points. The user receives complete points if he/she answers it correctly in the first go. The point a user can collect from each quiz is reduced depending on the number of mistakes.The third question carry a marking rubric of 300 points and the fourth question carry a marking rubric of 400 points. Similarly, depending on the mistakes, the point is reduced. The result is displayed with additional tip information and graphics.\nExample 1: Grid Quiz Activity\nExample 2: Multiple Choice Quiz Activity (3 options)\nLeaderboards and Facebook sharing\nThe application will allow users to submit their points to a global leaderboards system. This will allow users to compete with other players.\n","date":"2016-10-22T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/edquake/","section":"posts","tags":["android","product"],"title":"Edquake"},{"categories":["three60"],"contents":"Edcrayon’s Universal Deployment Model, converts tablets or phones to a smart, self-paced, predictive learning environment. It packages the following features:\nThousands of qualitative educational content that students can browse from and download for offline use. Gamified and engaging UI to make sure students love learning from the application. Serves as supplement to topics being taught in school or simply for self-study. Available in different languages to facilitate mother tongue learning. Learning materials is localized and contextualized too. Provides practice lessons and quizzes auto-graded by the application so that the students can work their way through the assessments as in self-paced learning. Features quiz engines that adapts to students previous knowledge on a particular subject material. If a student struggles with a specific type of problem the application will generate more questions of similar type. Provides performance charts and allows students to track their own performance. Facilitates discussion forums to allow students from different places to discuss and share ideas. The performance data will also be sent to regional or central data repository for analysis purposes. The data will help decision makers to facilitate better or different features and contents to ensure more productivity and performance. Current challenges in education\nDelivery of published books to every part of the country has been a significant problem due to the expenses involved in publishing and also due to the lack of road infrastructure. Even in cases where books are delivered on a timely basis, it only facilitates traditional teacher-centered teaching pedagogy. Twenty first century education calls for individualized learning which cannot be achieved since published books are not tailored to learners specific needs. Not all students get the optimum learning value from text based materials alone. Several schools have a large teacher to student ratio. Teachers also invest a huge portion of their time on things like keeping comprehensive performance records of every student, preparing and keeping track of lesson plans, and various other administrative tasks. If a system aided the teachers with these mundane tasks, the time saved can instead be utilized for student teaching and classroom engagement. Individualized attention can be provided to the students even in classes where the ratio is high. Decision makers in schools do not have all the performance data of all students and/or teachers at all time. A quantitative analytical report of performance with historical data assists in decision making processes. Schools still follow standardized and summative assessment system. This is partially because the system is inherent in us. However, we believe that the main reason is simply logistical. Large student size, lack of formative assessment tools, lack of engines to create individualized assessment etc. are some contributors. Use of technology in education can facilitate myriad of tools that help in solving the challenges. Edcrayon facilitates easy delivery of qualitative media contents, smart algorithms and/or modules that allow for individualized learning via tailored content pushing and group creation, self-paced learning, smart performance charts, and formative and summative assessment models.\nCurrently, private schools have benefitted from Edcrayon’s Classroom Deployment Model. This deployment model converts a traditional classroom environment to technology based, student-centered model. It packages the following features:\nConstitute all the elements of Universal Deployment Model with an addition to a guided learning and teaching environment. Replaces the traditional learning and teaching classroom environment to a student centered, formatively assessed, skill based, interactive learning and teaching environment. Rather than learning at their own pace, which has its own merits, this approach allows students to communicate and learn from teachers and other students in the classroom. Facilitates teachers with student performance charts. Allows teachers to assign tailored materials for more individualized learning approach. Facilitates formative and Inquiry based assessment through continuous feedbacks from teacher and other students. Performance data of students and teacher are available to decision makers. Government proposed Continuous Assessment System is implemented. Makes it very easy to perform day to day tasks such as attendance, register keeping, etc.\nSome Screenshots of Android Application:\nSome Screenshots of Web Application:\nMy role of R\u0026amp;D engineer and Project Lead incorporated architecting overall logic and process flow of Administrative and TeacherUI consisting User, Content, Classroom, Health and Pedagogy Management System. I also designed and co-developed EdCrayon Android application incorporating offline services, interactive charts, formative pedagogy, quiz engines among other features. Additionally, I developed several database administrative tools, wrote several crons and selenium test scripts, and managed multiple school servers. Similarly, I also trained and mentored junior programmers in programming methodologies and best practices. Additionally, I was also involved in training teachers, school-coordinators and school principal to use EdCrayon. Apart from my primary focus on technology, I also served as a core group member in defining and prioritizing technology investments and business objectives.\nPrototype samples: Sample 1 Sample 2 Sample 3\n","date":"2016-02-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/edcrayon/","section":"posts","tags":["android","product"],"title":"Edcrayon"},{"categories":["pet projects"],"contents":"\nIt uses MPAndroidChart which allows users to scroll within the graphs too.\n","date":"2016-01-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/etymology/","section":"posts","tags":["android","etymology"],"title":"Etymology and Ngram"},{"categories":["three60"],"contents":"\nImproving Literacy has been one of the biggest challenges faced by the developing world.\nUnited Nations underscored the importance of combating inequalities in education in its Sustainable Development Goals (Post 2015 Agenda) as the proposed SDG 4 suggests the international community to “Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all by 2030”.\nIn Nepal, the literacy rate for the total population is 57.4 % which shows that the inequalities still persist (CIA World Factbook). Significant portion of the population have been deprived of learning opportunities due to various reasons including early marriage, gender inequalities, social and family pressure to engage into economic activities at an early age etc. Research also suggests that in developing countries, there is very low self esteem among people who are illiterates or semi literates.\nIn this digital world, they are even at greater disadvantage. Those with the least amount of schooling will find it increasingly more difficult to participate in the evolving knowledge-based societies, deepening the social divide (Reimers, 2000). There needs to be an innovative approach to match both the education and technological gap. Recent development in the mobile technology provides us with great opportunities to fill this gap.\nThe mobile devices offer both affordability and storage capacity which makes it possible to equip with different types of educational content. There are over 5 billion mobile subscribers worldwide today –an astounding number considering the world’s current population which is roughly 6.8 billion (ITU, 2010). The rapid proliferation of mobile technologies throughout the world has brought substantial attention to the potential to leverage the power of these new technologies to address decades old problems, including educational inequalities (see Keen and Mackintosh, 2001; Ling, 2004).\nProduct / Technology Concept\nThe ICT tool (Android application) for the training will have different components for learning, assessments, data-collection and data-analysis. The idea is to make the application interactive and to reflect a game-environment where users will be motivated by game elements such as unlocking levels (lessons) as they complete certain tasks. The lessons will be based on the UNESCO’s newly-literate book (Mathani) for Awadhi language (Part-2). However, certain levels from Part-1 will also be added as a means to bridge their previous knowledge.\nSome of the lessons that will be included for the pilot are as follows:\nWord formation Basic math (addition and subtraction) Paragraph reading Paragraph listening Word math Time/Calendar Filling forms, writing letters etc. Some forms of assessments/practice lessons that will be included in the pilot are as follows:\nAndroid drawing canvas Multiple choice questions True/False questions Filling in the blanks Writing lessons The following user data will be collected within the application with the assumption that every learner will have a personal device to work on:\nUser information Level progression data Demographic data Application usage data Performance data Location data The data can be synced in real-time if Internet connectivity is not an issue. However, in places where connectivity can be a problem, or it is expensive, an alternative approach where data is synced periodically can be utilized.\n","date":"2015-10-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/neoliteracy/","section":"posts","tags":["Android"],"title":"Neo-literacy app concept"},{"categories":["pet projects"],"contents":"\nI recently stumbled upon a customer loyalty measurement scale termed Net Promoter Scale (NPS in short) via a conversation with a businessy friend of mine who was researching on it. I was truly amazed by the simplicity yet brilliance of NPS. Moreover, this is ideal for someone like me who wants to get honest customer feedback without harassing them with poorly designed questionnaires regarding a product.\nThe pith of NPS lies in the question:\n“How likely is it that you would recommend our company/product/service to a friend or colleague?”\nThose who respond with a score of 9-10 are called Promoters, and are considered likely to show positive behaviors like repeat purchase and positive referrals. Those who respond with a score of 0-6 are labeled Detractors, and they are believed to exhibit the negative behaviors like driving away from the brand, negative referrals. Responses of 7 and 8 are labeled Passives, and their behavior falls in the middle of Promoters and Detractors.\nThe NPS Calculation formula\n(Number of Promoters — Number of Detractors) / (Number of Respondents) x 100\nNPS ranges from -100 to 100 (inclusive).\nWith me doing the charting part and my friend doing the result implication and significance part we developed a simple NPS calculator app that takes total detractors, passives and promotors to output NPS. Refer to the screenshots below to identify what different NPS score signify.\n","date":"2015-04-04T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/nps/","section":"posts","tags":["Android"],"title":"Net Promoter Scale (NPS)"},{"categories":["three60"],"contents":" I had contributed as a debugger on Seeds of Empowerment\u0026rsquo;s Swahili Phonics application a while back. This blog post lists various test cases for Swahili Phonics Learning Application for identification of completed requirements and current issues. Thereafter, it also lists the problems that were solved.\n1. User interface and interaction\nThe application has great user interface. Since the majority of targeted users are young children, it is important to make learning fun. This application successfully does that. The only issue with the user interface is that it does not adapt to the full screen width.\nExpected screen:\nCurrent screen:\n2. Data collection\nThe application collects all sorts of data from the players and stores it locally in the user\u0026rsquo;s device. The players can re-login using their user-name and password and the application allows continuation from where they left before. The application also stores every hits and misses of a user. Although not currently implemented, the data can be used for analysis purposes to identify things like what set of phonetic sound players mostly struggle with.\n3. Algorithm\nAlthough the application provides a great framework for scalability, localization in different languages, and analytics there are few issues that we feel need some attention. We have been able to identify the following problems. We were not really sure if these problems were known since the realization came from analysis of the code rather than from the use of application. Therefore, we decided to list these problems with screen-shots so that it becomes more easier to understand.\nI) random number generation problem\nIn the current implementation, random numbers help generation of correct and incorrect answer. However, correct answer is always selected from the current level the player is at but incorrect answer set is generated from level 1 through current level. Because of this, although the options will have certain phoneme that are from previous level, they will never be the correct answer.\nSome examples:\nIf a user is at level 2, the correct option will never be from level 1. To generalize, if the user is at level n, the possible set of answers will only be from the phonemes allocated for that particular level n and not from any levels less than n.\nIn this case, the answer only be “bo”, “be” or “bi”. With progression of levels, it becomes more noticeable:\nHere the possible answer is only one (ie. 4). Also, with the progression of levels, the answers will most likely be the option that the user has not seen before. The game will not chose any phonemes as correct that were part of the option set in previous levels.\nII) the phonemes are always chosen from a statically defined list\nSince the game always refers to a single static list of phoneme for selecting correct and incorrect options, it does not adapt to re-test previous misses of a user.\nIII) chances of missing some phonetics because of dynamic array creation\nThe application generates a list during application runtime to check if all phonemes for a particular level have been tested. If a phoneme is tested twice in a level and the player chose the correct answer again, the stars are not increased because the generated list already has the phoneme. It was added to the list when the player answered correctly the first time around. This is a great implementation. However, there is also a small issue with this. Since the list is generated during runtime but the number of stars a player has in a particular level is stored and retrieved when the player re-logins, chances are, some phonemes are never tested.\nAn example, If Player A starts the game, and in Level 1 he/she successfully recognizes phonemes for {a, e, o, u}, the screen-shot would look something like this:\nWhen the player is playing the game, the game initializes an array and stores all the successful hits exactly once. The list has “a”, “e”, “o”, and “u” at this point. The player is promoted to another level when he/she has five stars.\nThe stars are increased if a phoneme can be added to a list (the condition being it does not already exist in the list). In this case, the stars will only be increased if the phonetic for “i” is tested and the player successfully answers that.\nBut, if the player decides to leave the game at this point to continue at another time, the array that stored all previous successful hits will be reset. Although he/she will still have 4 stars, now all it takes to move to another level is to identify the first phoneme put by the game. Because of this issue, the might progress to next level without having to identify phoneme for “i”.\n4. Adaptability\nThis feature is currently missing because it depends on algorithm design. Once the issues are resolved in that part, the next step would be to work on adapting the levels to test users on previously missed phonetics.\n5. Report Generation\nAlthough the application successfully collects useful data, the data is currently not being utilized for analysis.\nIdentified solutions for aforementioned bugs:\nThe solutions have already been implemented in the application. Currently, only algorithm specific problems have been solved. The application still needs additional design and game elements to make it more interactive.\n","date":"2015-02-02T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/swahili/","section":"posts","tags":["android","stanford"],"title":"Swahili Phonics App with Stanford University"},{"categories":["pet projects"],"contents":" The Zorganian Republic has some very strange customs. Couples only wish to have female children as only females can inherit the family\u0026rsquo;s wealth, so if they have a male child they keep having more children until they have a girl. If they have a girl, they stop having children. What is the ratio of girls to boys in Zorgania?\nThe ratio of girls to boys in Zorgania is 1:1. This might be a little counter-intuitive at first. Here are some ways of tackling this problem. 1. Monte Carlo Simulation: Although, Monte Carlo simulation does not necessarily show why the result is 1:1, it is appropriate because of the very counter-intuitive nature of the problem. At the very least, it helps us see that the result is indeed 1:1. Therefore, this is a good start.\nThe following R code estimates the probability of a child being a boy in Zorgania.\ncouples \u0026lt;- 100000 boycount \u0026lt;- 0 for (i in 1:couples){ # 0: boy while (sample(c(0,1),1) == 0) { boycount=boycount+1 } } probability \u0026lt;- boycount/(couples+boycount) Result:\n2. Understanding the question better: Here is another question: What is the probability of getting a tail in a fair coin toss, if all seven previous tosses resulted in heads? Since coin flips are independent events, the probability is still going to be 0.5. Similarly in this case, the child births are independent. It does not matter if the couples stop giving birth after they have a baby-girl. The expected value is unchanged.\nFor example, consider five couples: C1, C2, C3, C4 and C5. If B-\u0026gt; Boy and G-\u0026gt; Girl. Using R\u0026rsquo;s sample(). For C1: {B, G} For C2: {G} For C3: {B, B, G} For C4: {B, G} For C5: {G}\nNow, ignore the couples and only consider the children. The children are {B, G, G, B, B, G, B, G, G}. The only thing happening here is simply the generation of either a B or a G with equal probability for each generation. At this point, it is quite obvious that the part that has to do with \u0026ldquo;couple\u0026hellip;.\u0026rdquo; in the question is to mislead and confuse similar to the \u0026ldquo;previous seven tosses..\u0026rdquo; example that I mentioned in the beginning of 2.\n3. Counting:\nIf we start with 512 couples (hence 512 first borns), half of them are going to have a girl as their first. Those couples will stop having children. Among, the other half couples who had a son as their first child, half of them are going to have a girl as their second child and so on. At every step there is an equal numbers of boys and girls. Therefore, the expected ratio is 1:1.\n","date":"2014-04-18T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/zorgania/","section":"posts","tags":["R","monte carlo","probability"],"title":"Republic of Zorgania"},{"categories":["pet projects"],"contents":"The Du Compiler: This is the naive/brute-force implementation of the Mandelbrot Set plotting. I just followed the algorithm.\n# Plotting the Mandelbrot Set # length of sequence for real and imaginary parts of complex numbers length \u0026lt;- 1000 # sequences for real and imaginary parts real = seq(-1.8,0.6, len=length) imaginary = seq(-1.2,1.2, len=length) result \u0026lt;- matrix(nrow = length, ncol = length) for (i in 1:length) { for (j in 1:length) { result[i,j]=inmandelbrotset(complex(real = real[i], imaginary = imaginary[j])) } } image(result, axes=FALSE) # function that checks if a point E mandelbrot set inmandelbrotset \u0026lt;- function(c) { dwell.limit \u0026lt;- 2048 z \u0026lt;- 0 for (i in 1:dwell.limit) { z \u0026lt;- z ** 2 + c if (Mod(z) \u0026gt; 2) { return(FALSE) } } return(TRUE) } Adding colors: We now have a Boolean matrix that records if a point is in the Mandelbrot Set. Since the matrix can only have two values : true or false, thus far, we have only been able to plot read and white images. The next step is to add colors such that we get more information on when a particular point escapes the radius of 2. Again, this is the naive/brute force way of doing it.\n# Mandelbrot Plotting with colors length \u0026lt;- 1000 real = seq(-2.0,2.0, len=length) imaginary = seq(-2.0,2.0, len=length) result \u0026lt;- matrix(nrow = length, ncol = length) dwell.limit \u0026lt;- 512 for (i in 1:length) { for (j in 1:length) { z \u0026lt;- 0 c \u0026lt;-complex(real = real[i], imaginary = imaginary[j]) for (k in 1:dwell.limit) { z \u0026lt;- z ** 2 + c if (Mod(z) \u0026gt; 2) { result[i,j]=k break } } } } set.seed(2) image(result,breaks=0:dwell.limit ,col=c(1,sample(terrain.colors (dwell.limit-1,alpha = .8))),asp=1,ax=F) and, just for the heck of it..\nASCII Mandelbrot Set using R (naive)\ns \u0026lt;- seq(-1.7,1.2, by =.1) a \u0026lt;- \u0026quot;\u0026quot; for (i in 1:length(s)) { for (j in 1:length(s)) { a\u0026lt;-cat(a,inmandelbrotset(complex(r = s[j], i = s[i]))) } a \u0026lt;- cat(a,\u0026quot;\\n\u0026quot;) } Achieved by returning a \u0026quot; \u0026quot; or \u0026ldquo;#\u0026rdquo; instead of FALSE or TRUE from function \u0026ldquo;inmandelbrotset\u0026rdquo;. A better algorithm Utilizing R\u0026rsquo;s easy to use lists in implementation:\n# more efficient algorithm to plot the Mandelbrot set sequence \u0026lt;- seq(-2,2,len=1000) dwell.limit \u0026lt;- 200 # matrix of points to be iterated complex.matrix \u0026lt;- t((sapply(sequence,function(x)x+1i*sequence))) in.mandelbrot.index \u0026lt;- 1:length(complex.matrix) iter=z=array(0,dim(complex.matrix)) for(i in 1:dwell.limit){ # complex quadratic polynomial function for all points z[in.mandelbrot.index]=complex.matrix[in.mandelbrot.index]+z[in.mandelbrot.index]^2 # boolean matrix result=Mod(z[in.mandelbrot.index])\u0026lt;=2 # if result is false, store the iteration iter[in.mandelbrot.index[!result]]=i # save all the index where points are still in the mandelbrot in.mandelbrot.index=in.mandelbrot.index[result] } set.seed(19) image(iter,main=paste(\u0026quot;Iterations: \u0026quot;, i, sep=\u0026quot; \u0026quot;), breaks=0:dwell.limit ,col=c(1,sample(rainbow (dwell.limit-1,alpha = .8))),ax=F, asp=1) Plotting the Julia set A little modification to the code above (red and white Mandelbrot) produces Julia Sets. The idea here is to set a constant C and send Z to the function instead of C.\nc \u0026lt;- complex(real=-0.1,imaginary=0.651) label \u0026lt;- toString(c) injulia \u0026lt;- function(z) { dwell.limit \u0026lt;- 128 for (i in 1:dwell.limit) { z \u0026lt;- z ** 2 + c if (Mod(z) \u0026gt; 2) { return(FALSE) } } return(TRUE) } Adding colors: This is achieved by following the same process as above.\nSierpinski Gasket using Chaos game\n#### Chaos game for generation of Sierpinski Gasket # 1. Take 3 points in a plane to form a triangle, you need not draw it. # 2. Randomly select any point inside the triangle and consider that your current position. # 3. Randomly select any one of the 3 vertex points. # 4. Move half the distance from your current position to the selected vertex. # 5. Plot the current position. # 6. Repeat from step 3 plot.new() iterations \u0026lt;- 2000 vertices \u0026lt;- matrix(c(0,0,0.5,1,1,0),3,2, byrow=T) current.point \u0026lt;- c(0.5,0.5) random.vertex \u0026lt;- sample(1:3,iterations,replace=T) plot.result = matrix(nrow=iterations,ncol=2) for (i in 1:iterations){ current.point \u0026lt;- (current.point+vertices[random.vertex[i],])/2 plot.result[i,] \u0026lt;- current.point } points(plot.result,pch = 46) Adding colors:\npoints(plot.result,pch = 46,col=c(13,3,41)[random.vertex]) ","date":"2014-04-15T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/fractal_plots/","section":"posts","tags":["mandelbrot","R","data viz"],"title":"Fractal Plots"},{"categories":["pet projects"],"contents":"\nFor quite some time now, my father and my uncle have been debating over the coordinates of their houses with respect to each others. Albeit barely 750 meters from each other, there is not much visible reference points or landmarks to figure it out accurately. The blame is on the molasses thick concrete jungle of Kathmandu valley. (Side-note: Kathmandu will soon be synonymous to the word asphyxiation). Although, I sincerely appreciate their curiosity, I think it is time to end this for once and for all.\nI have used photos of my sister (Ashma) and my cousin (Samip) as labels to the directions.\nThe app implements Canvas to \u0026ldquo;draw\u0026rdquo; the direction. The two GPS coordinates (obtained from Google Earth) were hard-coded into the program and Azimuth from orientation sensor was used to calculate the direction. Basically, it is a compass that points the direction from one house to the other instead of pointing North. Apart from this rather trivial implementation, the code can be modified to achieve some fun/interesting/useful developments. For instance, the direction of Mecca for Muslim prayers is one that comes to mind. Or, it could be modified into a bearing pointer app by using GPS data and some input EditTexts.\nCode Snippet onCreate\nprotected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); mCustomDrawableView = new CustomDrawableView(this); setContentView(mCustomDrawableView); // Register the sensor listeners Resources res = getResources(); samip = BitmapFactory.decodeResource(res, R.drawable.samip); ashma = BitmapFactory.decodeResource(res, R.drawable.ashma); // Fill in correct latitude and longitude currentLoc.setLatitude(0.000000); currentLoc.setLongitude(0.00000); currentLoc.setAltitude(00); destinationLoc.setLatitude(0.00000); destinationLoc.setLongitude(0.0000); mSensorManager = (SensorManager) getSystemService(SENSOR_SERVICE); accelerometer = mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER); magnetometer = mSensorManager.getDefaultSensor(Sensor.TYPE_MAGNETIC_FIELD); } Canvas\nFloat azimuth; public Bitmap samip; public Bitmap ashma; public class CustomDrawableView extends View { Paint compassAxis = new Paint(); Paint compassCircle = new Paint(); Paint compasAarrow = new Paint(); public CustomDrawableView(Context context) { super(context); compassAxis.setColor(0xff00ff00); compassAxis.setStyle(Style.STROKE); compassAxis.setStrokeWidth(2); compassAxis.setAntiAlias(true); compassCircle.setColor(0xff000000); compassCircle.setStyle(Style.STROKE); compassCircle.setStrokeWidth(10); compassCircle.setAntiAlias(true); compasAarrow.setColor(0xff0000ff); compasAarrow.setStyle(Style.STROKE); compasAarrow.setStrokeWidth(3); compasAarrow.setAntiAlias(true); }; protected void onDraw(Canvas canvas) { int width = getWidth(), height = getHeight(); int centerx = width / 2, centery = height / 2; /* * Drawing the axis and circle Being symmetrical, these don't need * to be rotated */ // Axis canvas.drawLine(centerx, 0, centerx, height, compassAxis); canvas.drawLine(0, centery, width, centery, compassAxis); // Circle canvas.drawCircle(centerx, centery, 200, compassCircle); /* * since this was a pretty small scope app, magnetic north was not * changed to real north. See: \u0026quot;GeomagneticField\u0026quot; */ // Used Float instead of float for this check if (azimuth != null) { // Converting radians to degrees float temp = (float) Math.toDegrees(azimuth); float bearing = currentLoc.bearingTo(destinationLoc); float direction = temp - bearing; canvas.rotate(-direction, centerx, centery); } canvas.drawLine(centerx, centery - 200, centerx, centery, compasAarrow); canvas.drawBitmap(samip, centerx + 5, centery - 200, compassAxis); canvas.drawBitmap(ashma, centerx + 5, centery - 15, compassAxis); } } Screenshots:\nAnd after some time wasting:\n","date":"2014-03-31T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/thelostbrothers/","section":"posts","tags":["android"],"title":"The Lost Brothers"},{"categories":["pet projects"],"contents":"Android+Arduino RC Car The previous attempt was in shambles due to the lack of proper products. This time, I got myself a Sainsmart L293D motor driver (actually a cloned version of the ever so popular Adafruit L293D driver) and a handy four wheel drive chassis. Hereupon, the only adjustment required was the use of analog pins as digital pins. This is the consequence of the motor driver using up all the digital pins and leaving no pins for the Bluetooth shield. I also hooked up the Arduino to a USB power bank.\nTools used: ~ Arduino Uno ~ 4WD Chassis ~ SainSmart motor driver (L293D) ~ Sunkee 30ft Bluetooth Module ~ Anker 15000 mAh power bank\nArduino\n//Project: Android RC Car //Author: Ayush Subedi #include \u0026lt;AFMotor.h\u0026gt; //import Adafruit Motor library #include \u0026lt;SoftwareSerial.h\u0026gt;// import the serial library SoftwareSerial newPorts(15, 17); // RX =15= A1, TX=17=A3 AF_DCMotor motor1(1, MOTOR12_1KHZ); // create motor #1, 1KHz pwm AF_DCMotor motor2(2, MOTOR12_1KHZ); // create motor #2, 1KHz pwm AF_DCMotor motor3(3, MOTOR34_1KHZ); // create motor #3, 1KHz pwm AF_DCMotor motor4(4, MOTOR34_1KHZ); // create motor #4, 1KHz pwm void setup() { newPorts.begin(9600); motor1.setSpeed(255); // set the speed to 200/255 motor2.setSpeed(255); // set the speed to 200/255 motor3.setSpeed(255); // set the speed to 200/255 motor4.setSpeed(255); // set the speed to 200/255 } void loop() { while (newPorts.available() \u0026gt; 0) { char ch = newPorts.read(); newPorts.println(newPorts.read()); executeReceivedCommand(ch); } } void executeReceivedCommand(char command) { switch (command) { //Forward case '0': motor1.run(FORWARD); motor2.run(FORWARD); motor3.run(FORWARD); motor4.run(FORWARD); break; //Reverse case '1': motor1.run(BACKWARD); motor2.run(BACKWARD); motor3.run(BACKWARD); motor4.run(BACKWARD); break; //Left : skid steering case '3': motor1.run(FORWARD); motor4.run(FORWARD); motor2.run(RELEASE); motor3.run(RELEASE); break; //Right : skid steering case '4': motor2.run(FORWARD); motor3.run(FORWARD); motor1.run(RELEASE); motor4.run(RELEASE); break; //Stall case '2': motor1.run(RELEASE); motor2.run(RELEASE); motor3.run(RELEASE); motor4.run(RELEASE); break; } } In a nutshell, the Android device sends Char type to Arduino which is used to rotate the motors to maneuver towards a desired direction.\nForward: 0 Reverse: 1 Stop: 2 Left: 3 Right: 4\nThe car turns left and right by implementing skid steering.\nPictures and Video:\n","date":"2014-01-31T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/rc_car/","section":"posts","tags":["android","arduino"],"title":"Android+Arduino RC Car"},{"categories":["pet projects"],"contents":"Snakes and Ladders This game needs no introduction. Although there are several variations of this game (mainly on position of the snakes and ladders), the rules are generally the same. Some of the rules, (which are part of my algorithm) are mentioned below. Also, this game requires no skill as it solely depends on luck/probability. This allows several mathematical questions relating to this game be answered using Monte Carlo simulation. However, for this post, I will only be investigating from an analytic/subjective stand point. That is, I will be using Markov Chain to answer those questions. Markov Chain Wikipedia link\nThe game\u0026rsquo;s Markov property or memorylessness allows us to use Markov Chain. That is, the probability of occurrence of next event only depends on current event and not on any other events that occurred before. An example from our game: It does not matter if the player reached square 31 using the ladder from square 9, or by traversing the hard way around. Once the player is in 31, the probability of getting to square 32 does not depend on the \u0026ldquo;past\u0026rdquo;. Transition matrix Wikipedia link Transition matrix is a 2 dimensional array that encapsulates state transitional probabilities. For example, a transition matrix P, given the following information (Mathematical model of one dimensional random walk),\nwould be,\nTransition matrix for Snakes and Ladders Trivial Transition Matrix\nFirst of all, consider a case where there are no snakes or ladders. Let us call this our Trivial (for the lack of words) scenario.\nThis definitely makes for a boring game. However, it helps with the math. First of all, the square 0 is the position the player is before the game starts. Now, since we are using a cubic die, on the first roll, the probability of going from 0 to 1, 0 to 2, 0 to 3, 0 to 4, 0 to 5, and 0 to six is 1/6. Since we are not accounting for snakes or ladders, the probability of going from box i to boxes i+1, i+2, i+3, i+4, i+5, and i+6 are all going to be 1/6, unless we run out of space. That is, if we are at 97, the person can only move to next step if the die rolls 1, 2 or 3. For anything greater, the person will not go to the next step. Therefore, in this case, the probabilities are: 97 to 98 = 1/6, 97 to 99 = 1/6, 97 to 100 = 1/6 and 97 to 97 = 3/6.\nBuilding the Trivial transition matrix based on the aforementioned rule.\nimport Jama.Matrix; /** * @return 101X101 Transition Matrix for case: Trivial */ public Matrix trivialMatrix() { int difference, playerPosition = 0, matrixSize=101; double transitionM[][] = new double\\[matrixSize\\][matrixSize]; double probability = 1.0 / 6; for (playerPosition = 0; playerPosition \u0026lt; transitionM.length; playerPosition++) { for (int i = 1; i \u0026lt;= 6; i++) { if ((difference = matrixSize - playerPosition) \u0026lt;= 6) { for (int k = 1; k \u0026lt; difference; k++) { transitionM\\[playerPosition\\][playerPosition + k] = probability; } transitionM\\[playerPosition\\][playerPosition] = (6 - difference + 1) * probability; } else { transitionM\\[playerPosition\\][playerPosition + i] = probability; } } } return new Matrix(transitionM); } Top-Left\nBottom-Right\nNon-Trivial Transition Matrix\nSnakes:\n98 ~ 78, 95 ~ 75, 93 ~ 73, 87 ~ 24, 64 ~ 60, 62 ~ 19, 56 ~ 53, 49 ~ 11, 48 ~ 26, 16 ~ 6\nLadders:\n1 ~ 38, 4 ~ 14, 9 ~ 31, 21 ~ 42, 28 ~ 84, 36 ~ 44, 51 ~ 67, 71 ~ 91, 80 ~ 100\nI decided to use a simple List implementation for this. This might most likely be the Brute-Force implementation (I know several ways to make it better but none to make it worse). One way to make it more efficient would be to use 82 by 82 matrix instead of 101 by 101.\nThe advantage of using this implementation over the 82 by 82 matrix (apart from easy implementation) is that this method can be used for any snakes and ladders board variation. It also allows us to check for some hypothetical cases or answer more important questions. Eg: What is the best way to position snakes and ladders for maximum thrill to a player?\nBuilding the non-Trivial transition matrix\n/** * @return 101X101 Transition Matrix for case: non-Trivial */ public Matrix nonTrivialMatrix() { int playerPosition,matrixSize=101,difference; List\u0026lt;Integer\u0026gt; from = Arrays.asList(1, 4, 9, 21, 28, 36, 51, 71, 80, 98, 95, 93, 87, 64, 62, 56, 49, 48, 16); List\u0026lt;Integer\u0026gt; to = Arrays.asList(38, 14, 31, 42, 84, 44, 67, 91, 100, 78, 75, 73, 24, 60, 19, 53, 11, 26, 6); double probability = 1.0 / 6; double transitionM[][] = new double\\[matrixSize\\][matrixSize]; for (playerPosition = 0; playerPosition \u0026lt; transitionM.length; playerPosition++) { if (!from.contains(playerPosition)) { for (int i = 1; i \u0026lt;= 6; i++) { if ((difference = 6 - playerPosition) \u0026lt;= 6) { for (int k = 1; k \u0026lt; difference; k++) { if (from.contains(playerPosition + k)) { transitionM\\[playerPosition\\][to.get(from.indexOf(playerPosition + k))] = probability; } else { transitionM\\[playerPosition\\][playerPosition + k] = probability; } } if (from.contains(playerPosition)) { transitionM\\[playerPosition\\][to.get(from.indexOf(playerPosition))] = (6 - difference + 1) * probability; } else { transitionM\\[playerPosition\\][playerPosition] = (6 - difference + 1) * probability; } } else { if (from.contains(playerPosition + i)) { transitionM\\[playerPosition\\][to.get(from.indexOf(playerPosition + i))] = transitionM\\[playerPosition\\][to.get(from.indexOf(playerPosition + i))] + probability; } else { transitionM\\[playerPosition\\][playerPosition + i] = transitionM\\[playerPosition\\][playerPosition + i] + probability; } } } } } return new Matrix(transitionM); } Top-Left\nBottom-Right\nProbability Vector Wikipedia link\nProbability vectors\nrepresents the probability of being on a certain square after n dice rolls. It is a vector with non-negative entries that add up to one.\nimplies that the probability of being on square 0 is 1. This is our input.\n\u0026hellip;\u0026hellip;\nIf P is the Trivial transition matrix,\nIf P is the non-Trivial transition matrix.\nBuilding the Probability Vector\n/** * @return Probability Vector with 1 being the first element */ public Matrix probabilityVector() { double probabilityV[] = new double[101]; probabilityV[0] = 1; return new Matrix(probabilityV, 1); } Question 1: Probability of being on square s after n dice rolls: Using Vn-1 * P = Vn\n/** * @param transitionMatrix * @param probabilityVector * @param diceRolls */ public static void squareProbability(Matrix transitionMatrix, Matrix probabilityVector, int diceRolls) { NumberFormat nf = NumberFormat.getInstance(); nf.setMinimumFractionDigits(20); System.out.println(\u0026quot;Dice rolls: \u0026quot;+diceRolls); for (int i = 1; i \u0026lt;= diceRolls; i++) { probabilityVector = probabilityVector.times(transitionMatrix); } probabilityVector.print(nf, 3); } Some outputs for the Trivial matrix: I used html tables to simulate the board (basically printed the html tags within java code).\nSome outputs for the non-Trivial matrix:\nQuestion 2: Minimum length of a game and occurrence probability That is, after how many n, is the probability at square 100 greater than 0 for the first time?\nFor the trivial case, the answer is, ceiling of 100/6 = 17.\nFor the non-trivial case,\n/** * @param transitionMatrix * @param probabilityVector */ public static void gameCompletion(Matrix transitionMatrix, Matrix probabilityVector) { NumberFormat nf = NumberFormat.getInstance(); nf.setMinimumFractionDigits(30); int box = 100; int diceRolls = 0; while (probabilityVector.get(0, box) == 0) { diceRolls++; probabilityVector = probabilityVector.times(transitionMatrix); } System.out.println(\u0026quot;The game can be completed in min of \u0026quot; + diceRolls + \u0026quot; dice rolls.\u0026quot;); System.out.println(\u0026quot;Probability of it happening: \u0026quot; + nf.format(probabilityVector.get(0, box))); } Results:\nNon-Trivial Matrix\nThe game can be completed in min of 7 dice rolls. Probability of it happening: 0.001564643347050754000000000000 Rolls of {4,6,6,2,6,4,6} is one shortest solution. However, in theory, the game could last forever. Therefore, there is no longest game.\nTrivial Matrix\nThe game can be completed in min of 17 dice rolls. Probability of it happening: 0.000000000009038995585604526000 Rolls of {6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6} is the only shortest solution. Longest game in this case is 100 dice rolls. The player will have to roll 1, 100 consecutive times from start. Question 3: Expected length of a game: Let subStochasticMatrix be the 100 by 100 matrix obtained by deleting the last row and column of the transition matrix. Also, let I be 100 by 100 identity matrix. Let inverse be the inverse of the difference of I and subStochasticMatrix. The expected number of rolls is given by the sum of entries in top row of the matrix inverse.\n/** * @param transitionMatrix * @param probabilityVector */ public static void expectedLength(Matrix transitionMatrix, Matrix probabilityVector){ Matrix subStochasticMatrix=transitionMatrix.getMatrix(0, 99, 0 ,99); Matrix I = Matrix.identity(100,100); Matrix inverse = (I.minus(subStochasticMatrix)).inverse(); double sum=0; for (int i =0;i\u0026lt;=99;i++){ sum=sum+inverse.get(0, i); } System.out.println(\u0026quot;Expected game length: \u0026quot;+sum); } Results:\nNon-Trivial Matrix\nExpected game length: 39.59836564020812 Trivial Matrix\nExpected game length: 33.33333333333334 ","date":"2013-12-02T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/snakesandladders/","section":"posts","tags":["markov chains","probability","java"],"title":"Snakes and Ladders"},{"categories":["capstone project"],"contents":"The Du Compiler: For our Computer Science capstone project, we take on the beast - building a compiler from scratch. We had about three months to do it but we really had no clue of what we were doing for the first month and a half. Although we knew plenty of what there was to know about compilers in theory, we soon realized that building one from scratch was not going to be easy. We started with baby steps, and by the time the project was due, we were taking giant leaps (we had no other option). In the end, it turned out to be a reasonably fine compiler and an excellent experience. During these three months, I had the best and the worst experiences of my academic life. Therefore, this project is really special.\nSince this project belongs to two other people also, I won\u0026rsquo;t be posting any source code. This post will just have some example code for Du-Compiler.\nFinally, for anyone looking for a reasonably challenging senior projects, I would strongly encourage building a compiler. I say this for couple of reasons. First of all, it changes the way we look at code. It gives a better understanding of what exactly happens when the compile button is hit. In another words, it makes you aware of what is going on internally. Secondly, it involves learning/relearning several Computer Science topics such as regular expressions, hash table, data-structures, tree traversals, assembly level programming etc. It also involves A LOT of coding. When it all pans out, you are going to love what you have in your skill set.\nExamples //Hello World duhawk helloworld{ duPrint(%Hello World%); } //Simple Addition duhawk simpleAdd{ int a; a=5; int b; b = a + 5; duPrint b; } //Result: 10 //Simple Pattern duhawk test{ int i; i=1; int j; j=1; while (i\u0026lt;=10){ j=1; while (j\u0026lt;=i){ duPrint(%*%); j=j + 1; } duPrintln(%%); i= i + 1; } } /* Result * ** *** **** ***** ****** ******* ******** ********* ********** */ duhawk test{ int c; int d; duPrint (%Multiplication table of: %); duInput a; duPrint (%upto: %); duInput b; for (c=1;c\u0026lt;=b;c=c + 1){ duPrint a; duPrint (% X %); duPrint c; duPrint (% = %); d = a*c; duPrintln d; } } /* Input for a = 19 Input for b = 15 Result: Multiplication table of: 19 upto: 15 19 X 1 = 19 19 X 2 = 38 19 X 3 = 57 19 X 4 = 76 19 X 5 = 95 19 X 6 = 114 19 X 7 = 133 19 X 8 = 152 19 X 9 = 171 19 X 10 = 190 19 X 11 = 209 19 X 12 = 228 19 X 13 = 247 19 X 14 = 266 19 X 15 = 285 */ ","date":"2013-09-05T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/ducompiler/","section":"posts","tags":["compiler","java","capstone"],"title":"Compiler from Scratch"},{"categories":["capstone project"],"contents":"Area of the Mandelbrot Set: The area of the Mandelbrot Set The Mandelbrot set is a fractal (illustrates self-similarity). The set is obtained from the quadratic recurrence equation,\nwith\n, where points\nin the complex plane for which the orbit of\ndoes not tend to infinity are in the set. The Mandelbrot set is a compact set, contained in the closed disk of radius 2 around the origin. Since it is contained in a disk of known area, it is possible to approximate the area of the Mandelbrot Set\u0026rsquo;s using Monte Carlo method.\nJava: Since Java does not inherently understand complex numbers, a \u0026ldquo;real\u0026rdquo; approach will be applied to perform the quadratic recurrence equation,\nFirst, as shown in the figure above, inscribe the disk in a square of length 4 units. Let\nrepresent the coordinate along x-axis (real) and\nrepresent the coordinate along y axis. Now set\nand\n, where\nand\nare randomly generated real numbers from [-2, 2]. Basically, the\nand\ncoordinates are being duplicated at this step to preserve the point\n. Next, iteratively compute the following from\n(for programming purposes, choose a large Dwell Limit). Simultaneously, check if\n. If yes, increase count (not in the set) by 1 and get out of the loop (since all points should be contained in the disk).\nCompute the ratio of number of points that are in the set to total number of points used. Then multiply the area of square (16 units square) to get the approximate are of the Mandelbrot set.\nJava Code:\nimport java.util.Date; import java.util.Random; public class MandelbrotArea { public static int mcRep = 5000; public static int dwellLimit = 2048; /** * @return random double in [-2,2] */ public static double random() { return (new Random().nextDouble() * 4) - 2; } /** * @param r: real part of the complex number * @param s: imaginary part of the complex number * @return */ public static boolean isMandelbrotSet(double r, double s) { double a = r, b = s, temp; // Iterative function for (int j = 1; j \u0026lt;= dwellLimit; j++) { temp = a; a = Math.pow(a, 2) - Math.pow(b, 2) + r; b = (2 * temp * b) + s; if (Math.pow(a, 2) + Math.pow(b, 2) \u0026gt; 4) { return false; } } return true; } public static void main(String[] args) { long startTime = new Date().getTime(); long count = 0; for (int i = 0; i \u0026lt;= mcRep; i++) { if (isMandelbrotSet(random(), random())) { count++; } } System.out.println(\u0026quot;Input -\u0026gt; DwellLimit: \u0026quot; + dwellLimit + \u0026quot;, McRep: \u0026quot; + mcRep); System.out.println(\u0026quot;Area: \u0026quot; + ((double) (count * 16)) / mcRep); System.out.println(\u0026quot;Execution time: \u0026quot; + (new Date().getTime() - startTime) + \u0026quot; ms\u0026quot;); } } Result:\nInput -\u0026gt; DwellLimit: 2048, McRep: 5000 Area: 1.5136 Execution time: 389 ms R\nmonte.Carlo \u0026lt;- 5000 x \u0026lt;- runif(monte.Carlo, -2, 2) y \u0026lt;- runif(monte.Carlo, -2, 2) list \u0026lt;- numeric(monte.Carlo) for (j in 1:monte.Carlo){ list[j] \u0026lt;- if (inmandelbrotset(complex(real = x[j], imaginary = y[j]))) 1 else 0 } area\u0026lt;-mean(list)*16 # function that checks if a point E mandelbrot set inmandelbrotset \u0026lt;- function(c) { dwell.limit \u0026lt;- 2048 z \u0026lt;- 0 for (i in 1:dwell.limit) { z \u0026lt;- z ** 2 + c if (Mod(z) \u0026gt; 2) { return(FALSE) } } return(TRUE) } ","date":"2013-01-02T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/mandelbrot_area/","section":"posts","tags":["mandelbrot","java","monte carlo"],"title":"Area of the Mandelbrot Set"},{"categories":["capstone project"],"contents":"The Faro Shuffle: A Faro shuffle is probably one of the most intricate skills in an elite magician\u0026rsquo;s arsenal. Although it\u0026rsquo;s simple in concept, it is extremely difficult to perform and it typically needs years and years of practice. The idea of a shuffle is to randomize the cards, and since a typical shuffle is sloppy, that is usually true. However, a Faro shuffle is not random at all. In a perfect shuffle or a Faro shuffle the cards are divided exactly in half (top and bottom) and then interspersed alternately. Twenty one consecutive (out) Faro shuffles for a 50-card deck. However, it only takes eight consecutive Faro shuffles to bring a 52-card deck back to its original order. Interesting. Java: If the number of cards in the deck is 12,\npublic class outFaro { public static void main(String[] args) { // the number of card in the deck int numberofcardindeck = 12; // creating an array of cards int[] deck = new int[numberofcardindeck]; // printing the order of cards in original deck System.out.println(\u0026quot;Original deck\u0026quot;); for (int i = 0; i \u0026lt; numberofcardindeck; i++) { deck[i] = i + 1; System.out.print(deck[i] + \u0026quot; \u0026quot;); } System.out.println(); // dividing the cards in half int[] firsthalfdeck = new int[numberofcardindeck / 2]; int[] secondhalfdeck = new int[numberofcardindeck / 2]; // performing a (out) Faro shuffle for (int Faro = 1; Faro \u0026lt;= 50; Faro++) { for (int i = 0; i \u0026lt; numberofcardindeck / 2; i++) { firsthalfdeck[i] = deck[i]; secondhalfdeck[i] = deck[i + (numberofcardindeck / 2)]; } for (int i = 0; i \u0026lt; numberofcardindeck / 2; i++) { deck[2 * i] = firsthalfdeck[i]; deck[2 * i + 1] = secondhalfdeck[i]; } System.out.println(); System.out.println(\u0026quot;Order under Faro Shuffle: \u0026quot; + Faro); for (int i = 0; i \u0026lt; numberofcardindeck; i++) { System.out.print(deck[i] + \u0026quot; \u0026quot;); } // done when the second card comes back to its original position if (deck[1] == 2) { break; } } } } Result:\nOriginal deck 1 2 3 4 5 6 7 8 9 10 11 12 Order under Faro Shuffle: 1 1 7 2 8 3 9 4 10 5 11 6 12 Order under Faro Shuffle: 2 1 4 7 10 2 5 8 11 3 6 9 12 Order under Faro Shuffle: 3 1 8 4 11 7 3 10 6 2 9 5 12 Order under Faro Shuffle: 4 1 10 8 6 4 2 11 9 7 5 3 12 Order under Faro Shuffle: 5 1 11 10 9 8 7 6 5 4 3 2 12 Order under Faro Shuffle: 6 1 6 11 5 10 4 9 3 8 2 7 12 Order under Faro Shuffle: 7 1 9 6 3 11 8 5 2 10 7 4 12 Order under Faro Shuffle: 8 1 5 9 2 6 10 3 7 11 4 8 12 Order under Faro Shuffle: 9 1 3 5 7 9 11 2 4 6 8 10 12 Order under Faro Shuffle: 10 1 2 3 4 5 6 7 8 9 10 11 12 After 10 out-Faro shuffles, the card returns to its original permutation. Therefore, 10 is the order of the permutation.\nThe table below shows number of cards in a deck and the number of out-Faro shuffles required to bring it back to its original permutation.\nThe definition of permutation and permutation group:\n“A permutation of a set A is a function from A to A that is both one-to-one and onto. A permutation group of a set A is a set of permutations of A that forms a group under function composition.”\nA Faro Shuffle is one-to-one and onto. At any frequency of the shuffle, every index (1-n) will have a unique card. Also, for every card, there is a unique index regardless of the frequency of the shuffle. The permutation for every “Number of cards” can be thought as a group under the operation - FaroShuffle. Also, since “Number of Faro Shuffles” returns us back to the original permutation, we can establish it as the order of the permutation group.\nTheorem: The order of a permutation of a finite set written in disjoint cycle form is the least common multiple of the lengths of the cycles.The order of a permutation of a finite set written in disjoint cycle form is the least common multiple of the lengths of the cycles.\nThis is one of the most important Group Theory Theorems. Let us consider a 52-card deck to demonstrate this. From the Java code, the order is 8 when n is 52. Instead of looking at disjoint cycle form of the permutation to figure out the order of each element, we can also look at every permutation the original permutation has been to before returning to the original permutation. For n = 52, the table below shows all the permutations.\nFrom the table above, order of 1 and 52 = 1, order of 18 and 35 = 2 and order of rest of the elements = 8 The L.C.M (1,2,8) = 8 is the order of the group according to the Theorem. This is also what we got from the Java code.\nConjectures / Proofs:\nIn-Faro and Out-Faro In an out-Faro shuffle, the top card from first half of the deck always remains on top. In an in-Faro shuffle, the top card from second half will be the new top card of the new shuffled deck.\nSince I did not have a deck of card with me, and I was bored, I wrote a very simple Android app to demonstrate in and out shuffle.\nGenerating a random deck with 4 cards. Notice that the Eight of Hearts is on top.\nOut-Faro 1: Eight of Hearts is on top.\nOut-Faro 2: Eight of Hearts is still on top and the deck is back in its original permutation. Therefore, order = 2.\nIn-Faro 1: Using the same deck, Eight of Hearts is no longer on top.\nIn-Faro 2: Again\u0026hellip;\nIn-Faro 3:\nIn-Faro 4: Back to its original order. Order = 4.\nMore screenshots:\nAnother reason for writing the app was to see the connection between Binary Number System and Faro Shuffle. I read somewhere that magicians and gamblers use this for their advantage.\nConsider a random eight card deck.\nNow, the trick is, to send the Queen of Clubs to say for example, 7th position in the deck (6 cards on top of it), the gambler would perform two in-Faros and one out-Faro. 6 in binary is 110; so, for digit 1, the magician would perform an in-Faro and for 0, an out-Faro.\n1st in-Faro:\n2nd in-Faro:\nout-Faro:\nQueen of Clubs is now in the 7th position. Also, its really interesting that this works irrespective of the number of cards in the deck.\nI also found that the order of (2n+2) out-Faro shuffle is equal to the order of (2n) in-Faro shuffles.\nI find that pretty interesting and I do not really see that to be obvious. It also implies that for no 2n, In-Faro=Out-Faro (I think). I will be investigating on these more later.\n","date":"2012-12-05T00:00:00Z","permalink":"https://ayushsubedi.github.io/posts/faro_shuffle/","section":"posts","tags":["faro","java","group theory"],"title":"The Faro Shuffle"},{"categories":["analytics","machine-learning"],"contents":"Machine Learning Glossary Basic Machine Learning Confusion Matrix Data Design of Experiments Game Theory Model Quality Non-Parametric Tests Optimization Probability based models Regression Variable Selection Time series models Misc Basic Machine Learning Algorithm In the context of machine learning, an algorithm is a set of instructions that a computer follows in order to learn from data. Machine learning algorithms take input data and use statistical analysis to predict an output value within an acceptable range. The goal of a machine learning algorithm is to improve its prediction accuracy over time by adjusting the parameters of the model based on the input data. Change detection Change detection is a process in which a system is able to identify changes in a given environment over time. In the context of machine learning, change detection involves using algorithms to analyze data from a given environment in order to identify any changes that have occurred. This can be useful in a variety of different applications, including monitoring changes in financial markets, detecting changes in customer behavior, or identifying changes in the physical environment. Classification Classification is a supervised learning problem in which the model is trained to predict a discrete label or class for a given input data. The goal is to predict the class or category that a new instance belongs to, based on the training data. For example, a classifier could be trained to predict whether an email is spam or not spam, based on the contents of the email. The input data would be the contents of the email, and the output class would be either \u0026ldquo;spam\u0026rdquo; or \u0026ldquo;not spam\u0026rdquo;. There are many different algorithms that can be used for classification, including logistic regression, support vector machines (SVMs), and decision trees. The choice of algorithm depends on the characteristics of the data and the desired complexity of the model. Classifier A classifier is a machine learning model that is trained to predict a discrete class or category for a given input data. Classifiers are used in a variety of applications, including spam filtering, image classification, and natural language processing. There are many different types of classifiers, including logistic regression, support vector machines (SVMs), and decision trees. The choice of classifier depends on the characteristics of the data and the desired complexity of the model. To train a classifier, the model is presented with a labeled dataset that includes input data and the corresponding correct class or category. The model then \u0026ldquo;learns\u0026rdquo; to predict the correct class by finding patterns in the training data. Once trained, the classifier can then be used to predict the class for new, unseen data. Cluster In the context of machine learning, a cluster refers to a group of data points that are similar to one another. Clustering is an unsupervised learning problem in which the goal is to divide the data into distinct groups, or clusters, such that the data points within each cluster are more similar to one another than they are to data points in other clusters. There are many different algorithms that can be used for clustering, including k-means clustering and hierarchical clustering. The choice of algorithm depends on the characteristics of the data and the desired properties of the clusters. Clustering can be used for a variety of purposes, including data compression, anomaly detection, and generating hypotheses for further testing. It is a useful tool for exploring and understanding the structure of a dataset. Cluster center In the context of clustering, a cluster center is a representative data point for a cluster. It is typically the mean or median of the points in the cluster, depending on the specific clustering algorithm being used. In k-means clustering, for example, the cluster center is the mean of all the data points in the cluster. The k-means algorithm works by iteratively assigning each data point to the cluster with the closest cluster center and then updating the cluster center to be the mean of the points in the cluster. In hierarchical clustering, the cluster center can be thought of as the point at the center of the cluster, which is determined by the specific linkage criterion being used. The cluster center is used to represent the \u0026ldquo;typical\u0026rdquo; data point in a cluster, and can be useful for understanding the characteristics of the cluster and for visualization purposes. Clustering Clustering is an unsupervised learning problem in which the goal is to divide a dataset into distinct groups, or clusters, such that the data points within each cluster are more similar to one another than they are to data points in other clusters. Clustering is a useful tool for exploring and understanding the structure of a dataset, and can be used for a variety of purposes, including data compression, anomaly detection, and generating hypotheses for further testing. There are many different algorithms that can be used for clustering, including k-means clustering, hierarchical clustering, and density-based clustering. The choice of algorithm depends on the characteristics of the data and the desired properties of the clusters. In k-means clustering, for example, the goal is to partition the data into a specified number (k) of clusters by iteratively assigning each data point to the cluster with the closest cluster center and then updating the cluster center to be the mean of the points in the cluster. Hierarchical clustering, on the other hand, involves creating a hierarchy of clusters, where at each step, the two closest clusters are merged together. Density-based clustering algorithms, such as DBSCAN, identify clusters as areas of higher density surrounded by areas of lower density. CUSUM CUSUM is an acronym for \u0026ldquo;Cumulative Sum.\u0026rdquo; It is a statistical algorithm that is used to detect small shifts in the mean of a process over time. It is often used in quality control and reliability engineering to monitor processes and detect changes that may indicate a problem or deviation from the norm. The CUSUM algorithm works by keeping track of a running total of the difference between the observed values and the expected or target value. When the running total exceeds a pre-determined threshold, it indicates that the process has shifted and may need to be corrected or investigated. CUSUM charts are often used to visualize the performance of the CUSUM algorithm, with the running total being plotted on the y-axis and the time steps on the x-axis. The chart can then be used to identify when the running total exceeds the threshold and to identify any trends or patterns in the data. Deep learning Deep learning is a subfield of machine learning that is inspired by the structure and function of the brain, specifically the neural networks that make up the brain. It involves the use of artificial neural networks, which are computational models inspired by the structure and function of the brain, to learn from data and make decisions. Deep learning algorithms learn by example, just like humans do. They learn by being presented with a large amount of labeled data and adjusting the internal parameters of the network to optimize performance on a specific task. The \u0026ldquo;deep\u0026rdquo; in deep learning refers to the fact that these algorithms typically have multiple layers of artificial neurons, with each layer learning to extract higher-level features of the data. Deep learning has been successful in a wide range of applications, including image and speech recognition, natural language processing, and autonomous driving. It has revolutionized the field of machine learning and has enabled the development of many practical applications that were previously thought to be impossible. Dimension In the context of machine learning, a dimension refers to a particular feature or attribute of a dataset. For example, if you are working with a dataset that includes information about houses (such as price, number of bedrooms, square footage, and location), each of these features would be considered a separate dimension. The number of dimensions in a dataset is often referred to as the \u0026ldquo;dimensionality\u0026rdquo; of the dataset. High-dimensional datasets, which have a large number of dimensions, can be difficult to work with and visualize, as it can be challenging to represent the relationships between all of the dimensions in a meaningful way. In machine learning, techniques such as dimensionality reduction can be used to reduce the number of dimensions in a dataset, while still preserving the important information. This can be useful for tasks such as visualization and training machine learning models, which may be more efficient and effective on lower-dimensional data. EM algorithm (Expectation-Maximization algorithm) The EM algorithm (Expectation-Maximization algorithm) is a widely used method for estimating the parameters of a statistical model when there is missing or incomplete data. It is an iterative algorithm that alternates between two steps: the expectation (E) step and the maximization (M) step. In the E step, the algorithm estimates the expected value of the complete data likelihood function (a measure of the probability of the data given the model parameters) based on the current parameter values. In the M step, the algorithm updates the parameter values to maximize the expected complete data likelihood. The process is then repeated until convergence, at which point the parameter estimates are considered to be optimal. The EM algorithm is widely used in a variety of applications, including machine learning, natural language processing, and bioinformatics. It is particularly useful when the data are incomplete or when the model is a mixture model (i.e., a model that consists of a mixture of different underlying distributions). Heuristic In machine learning, a heuristic is a simplified, approximate solution to a problem that is used to quickly find a satisfactory answer. It is often used in situations where finding the optimal solution is computationally infeasible or impractical. Heuristics are often used in machine learning as a way to quickly search through a large space of possible solutions and find a good, but not necessarily optimal, solution. They can be useful for tasks such as optimization, feature selection, and model selection. Heuristics are often designed to be domain-specific and are based on the specific characteristics of the problem at hand. They can be useful for providing a rough estimate or approximation of the solution, but they may not always be reliable or accurate. In general, heuristics should be used with caution and should be validated against more rigorous methods where possible. 𝑘-means algorithm The k-means algorithm is a method for clustering data into a specified number (k) of distinct clusters. It is an iterative algorithm that works by first randomly initializing k cluster centers, and then iteratively assigning each data point to the cluster with the closest cluster center and updating the cluster center to be the mean of the points in the cluster. The k-means algorithm has the following steps: Initialize k cluster centers randomly. Assign each data point to the cluster with the closest cluster center. Update the cluster centers to be the mean of the points in the cluster. Repeat steps 2 and 3 until the cluster assignments stop changing or a maximum number of iterations is reached. The k-means algorithm is sensitive to the initial cluster assignments, so it is common to run the algorithm multiple times with different random initializations to ensure that the final clusters are stable. The algorithm is also sensitive to outliers and may produce suboptimal clusters if the data contain outliers. 𝑘-Nearest-Neighbor (KNN) The k-nearest neighbor (KNN) algorithm is a method for classifying objects based on the closest training examples in the feature space. It is a non-parametric method, which means that it does not make any assumptions about the underlying distribution of the data. The KNN algorithm works by calculating the distance between the new data point and all the training data, and then selecting the k training points that are closest to the new data point. The class label of the new data point is then determined by majority vote among the k nearest neighbors. The value of k is a hyperparameter of the KNN algorithm and must be chosen by the practitioner. A larger value of k will make the model more robust to noise, but a smaller value may be more sensitive to the underlying structure of the data. KNN is a simple and effective method for classification, but it can be computationally expensive for large datasets, as it requires calculating the distance between the new data point and all the training examples. Kernel In the context of machine learning, a kernel is a function that takes in two inputs and returns a scalar value. Kernels are used in a variety of machine learning algorithms, including support vector machines (SVMs) and kernel principal component analysis (PCA). In SVMs, kernels are used to define a similarity measure between two data points. The kernel function is applied to the data points to transform them into a higher-dimensional space, where it is then possible to find a linear separation between the classes. By using a kernel function, it is possible to learn a non-linear decision boundary in the original feature space using a linear classifier in the transformed space. In kernel PCA, kernels are used to define a similarity measure between data points in the original space, and the resulting kernel matrix is used to perform PCA in the feature space. This allows for non-linear dimensionality reduction, which can be useful for data that is not linearly separable. There are many different kernel functions that can be used, including linear kernels, polynomial kernels, and radial basis function (RBF) kernels. The choice of kernel depends on the characteristics of the data and the desired properties of the model. Margin In the context of machine learning, the margin is the distance between the decision boundary (i.e., the line or hyperplane that separates the classes) and the nearest training data points. The margin is an important concept in certain types of algorithms, such as support vector machines (SVMs), where the goal is to find the decision boundary that has the largest margin. In SVMs, the margin is the distance between the decision boundary and the closest data points from each class. The margin is maximized when the decision boundary is as far as possible from the closest data points from each class, which leads to a model that is more robust and generalizable to new data. The margin can also be thought of as a measure of the confidence of the classifier. A larger margin indicates that the classifier is more confident in its predictions, as it is based on a wider separation between the classes. The margin is an important consideration when training a machine learning model, as a model with a large margin is often preferred to a model with a small margin, as it is likely to be more robust and generalizable to new data. Machine learning Machine learning is a field of artificial intelligence that involves the use of computational models to learn from data and make predictions or decisions without being explicitly programmed. It involves the development of algorithms that can automatically improve their performance through experience. There are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning. In supervised learning, the goal is to learn a function that maps input data to output labels, based on a labeled training dataset. The model is trained on the training data and then evaluated on a separate test dataset to evaluate its performance. Examples of supervised learning tasks include classification and regression. In unsupervised learning, the goal is to discover patterns or relationships in the data without any prior knowledge or labeled training data. Examples of unsupervised learning tasks include clustering and dimensionality reduction. In reinforcement learning, the goal is to learn a policy that maximizes a reward signal. The model is trained by interacting with its environment and receiving feedback in the form of rewards or punishments. Reinforcement learning is used in a variety of applications, including robotics and control systems. Machine learning has been successful in a wide range of applications, including image and speech recognition, natural language processing, and autonomous driving. It has revolutionized many fields and has enabled the development of practical applications that were previously thought to be impossible. Neural network A neural network is a type of machine learning model inspired by the structure and function of the brain. It is composed of layers of interconnected \u0026ldquo;neurons,\u0026rdquo; which process and transmit information. Neural networks are able to learn and adapt to new data by adjusting the strengths of the connections between neurons. The basic building block of a neural network is the neuron, which is a simple computational unit that receives input, processes it, and produces an output. The input is passed through multiple layers of neurons, with each layer learning to extract higher-level features of the data. The output of the final layer is the prediction or decision made by the neural network. There are many different types of neural networks, including feedforward neural networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs). The choice of neural network architecture depends on the characteristics of the data and the desired properties of the model. Neural networks have been successful in a wide range of applications, including image and speech recognition, natural language processing, and autonomous driving. They have revolutionized the field of machine learning and have enabled the development of many practical applications that were previously thought to be impossible. Supervised learning Supervised learning is a type of machine learning in which the model is trained on a labeled dataset, where the correct output is provided for each example in the training set. The goal of supervised learning is to learn a function that can map input data to the correct output labels. Supervised learning algorithms can be divided into two main categories: regression and classification. In regression, the goal is to predict a continuous value, such as the price of a house or the likelihood of a customer churning. Examples of regression algorithms include linear regression and support vector regression. In classification, the goal is to predict a discrete label or class, such as whether an email is spam or not spam. Examples of classification algorithms include logistic regression, k-nearest neighbors, and decision trees. Supervised learning is the most widely used type of machine learning and has been successful in a wide range of applications, including image and speech recognition, natural language processing, and fraud detection. It requires a labeled dataset to train the model, which can be expensive and time-consuming to obtain. Support vector machine (SVM) Support vector machine (SVM) is a type of supervised learning algorithm that can be used for classification or regression. It is based on the idea of finding a hyperplane in a high-dimensional space that maximally separates the classes. In the case of classification, the goal is to find a hyperplane that separates the data points into different classes as well as possible. The SVM algorithm finds the hyperplane that has the largest margin, or distance, between the closest data points of each class. This maximizes the separation between the classes and leads to a more robust and generalizable model. In the case of regression, the goal is to find a hyperplane that predicts the output value for a given input value. The SVM algorithm finds the hyperplane that minimizes the error between the predicted and actual values. SVMs are effective in high-dimensional spaces and are widely used in a variety of applications, including image and speech recognition, natural language processing, and bioinformatics. They are also robust to noise and can handle datasets with a large number of features. However, they can be computationally expensive to train and are not well-suited for very large datasets. Unsupervised learning Unsupervised learning is a type of machine learning in which the model is not given any labeled training data and must find patterns or relationships in the data on its own. The goal of unsupervised learning is to discover the underlying structure of the data, without any prior knowledge or assumptions. Unsupervised learning algorithms can be divided into two main categories: clustering and dimensionality reduction. In clustering, the goal is to group the data points into distinct clusters such that the points within each cluster are more similar to one another than they are to points in other clusters. Examples of clustering algorithms include k-means clustering and hierarchical clustering. In dimensionality reduction, the goal is to reduce the number of dimensions (features) in the data while preserving as much of the information as possible. This can be useful for tasks such as visualization and feature selection. Examples of dimensionality reduction algorithms include principal component analysis (PCA) and t-SNE (t-distributed stochastic neighbor embedding). Unsupervised learning is useful for exploring and understanding the structure of a dataset, and can be used for tasks such as anomaly detection and data compression. It does not require labeled data and can be used with data that has not been labeled or has incomplete labels. However, it can be more difficult to evaluate the performance of unsupervised learning algorithms, as there is no ground truth to compare the results to. Voronoi diagram A Voronoi diagram is a graphical representation of the partitioning of a plane into regions based on the distance to a set of points. It is named after Russian mathematician Georgy Voronoi, who developed the concept in 1908. In a Voronoi diagram, the plane is divided into a set of cells, with each cell corresponding to one of the input points. The points are called the \u0026ldquo;generators\u0026rdquo; of the Voronoi diagram. Each cell consists of all points that are closer to its generator than to any other generator. The boundary between cells is called a Voronoi edge, and the points where Voronoi edges intersect are called Voronoi vertices. Voronoi diagrams have a wide range of applications, including computer graphics, image processing, and spatial analysis. They are used to model the spatial distribution of points and can be used to optimize the placement of facilities, such as warehouses or cell phone towers, to minimize the distance to the nearest facility. They are also used in computer games to determine the visibility of objects on the screen and in the design of efficient algorithms for solving problems in computational geometry. Confusion Matrix Accuracy Accuracy is a measure of how well a model correctly predicts the outcome of a given data sample. It is commonly used in classification problems, where the model is trying to predict a label for a given input. The accuracy score is calculated by dividing the number of correct predictions made by the model by the total number of predictions made. This value is then expressed as a percentage. For example, if a model made 100 predictions and 75 of them were correct, the accuracy score would be 75%. To calculate the accuracy score, you need a set of predictions made by the model and the corresponding true labels for those predictions. You can then compare the predictions to the true labels to see how many were correct. Here is an example of how to calculate the accuracy score in Python: def accuracy_score(y_true, y_pred): # Calculate the number of correct predictions correct = sum(y_true == y_pred) # Calculate the total number of predictions total = len(y_true) # Calculate the accuracy score as a percentage return correct / total * 100 Here, y_true is a list of the true labels and y_pred is a list of the predictions made by the model. The function first calculates the number of correct predictions and then divides that by the total number of predictions to get the accuracy as a decimal. It then multiplies that value by 100 to express the accuracy as a percentage. Confusion matrix A confusion matrix is a table that is used to evaluate the performance of a classification algorithm. It helps to visualize the correct and incorrect predictions made by the model and allows you to see which classes are being predicted accurately and which are not.\nThe rows of the matrix represent the actual classes of the samples and the columns represent the predicted classes. The diagonal elements of the matrix represent the number of samples that have been correctly classified, while the off-diagonal elements represent the number of misclassified samples.\nHere is an example of a confusion matrix:\nPredicted Positive Predicted Negative Actual Positive TP FP Actual Negative FN TN In this example, TP (true positive) is the number of samples that are actually positive and have been correctly predicted as positive. TN (true negative) is the number of samples that are actually negative and have been correctly predicted as negative. FP (false positive) is the number of samples that are actually negative but have been predicted as positive. FN (false negative) is the number of samples that are actually positive but have been predicted as negative. To calculate the values for the confusion matrix, you need a set of predictions made by the model and the corresponding true labels for those predictions. You can then compare the predictions to the true labels to see how many were correct and how many were incorrect. Here is an example of how to calculate a confusion matrix in Python: from sklearn.metrics import confusion_matrix y_true = [1, 0, 1, 1, 0, 1] y_pred = [1, 1, 1, 1, 0, 0] confusion_matrix(y_true, y_pred) This will output the following confusion matrix: array([[2, 1], [1, 2]]) Diagnostic odds ratio The diagnostic odds ratio (DOR) is a measure of the accuracy of a diagnostic test. It is used to compare the accuracy of two or more diagnostic tests or to compare the accuracy of a diagnostic test to a reference standard. The DOR is calculated as the ratio of the odds of a positive test result in patients with the condition being tested for to the odds of a positive test result in patients without the condition. Here is the formula for calculating the DOR: DOR = (TP / FP) / (FN / TN) Where TP (true positive) is the number of samples that are actually positive and have been correctly predicted as positive, TN (true negative) is the number of samples that are actually negative and have been correctly predicted as negative, FP (false positive) is the number of samples that are actually negative but have been predicted as positive, and FN (false negative) is the number of samples that are actually positive but have been predicted as negative. The DOR can range from 0 to infinity, with higher values indicating a more accurate diagnostic test. A DOR of 1 indicates that the test is no better than a coin flip, while a DOR of infinity indicates perfect accuracy. To calculate the DOR, you need a set of predictions made by the diagnostic test and the corresponding true labels for those predictions. You can then use the formula above to calculate the DOR. Here is an example of how to calculate the DOR in Python: def diagnostic_odds_ratio(y_true, y_pred): tp = sum((y_true == 1) \u0026amp; (y_pred == 1)) tn = sum((y_true == 0) \u0026amp; (y_pred == 0)) fp = sum((y_true == 0) \u0026amp; (y_pred == 1)) fn = sum((y_true == 1) \u0026amp; (y_pred == 0)) dor = (tp / fp) / (fn / tn) return dor Here, y_true is a list of the true labels and y_pred is a list of the predictions made by the diagnostic test. The function calculates the values for TP, TN, FP, and FN using boolean masks and then uses these values to calculate the DOR using the formula above. Fall out Fallout (also known as false positive rate or type I error) is a measure of the performance of a diagnostic test or classification algorithm. It is the percentage of negative samples that are incorrectly classified as positive. In the context of a diagnostic test, fallout represents the probability that a person without the condition being tested for will receive a positive test result. In the context of a classification algorithm, fallout represents the percentage of negative samples that are incorrectly classified as positive. Here is the formula for calculating fallout: Fallout = FP / (FP + TN) Where FP (false positive) is the number of samples that are actually negative but have been predicted as positive, and TN (true negative) is the number of samples that are actually negative and have been correctly predicted as negative. To calculate fallout, you need a set of predictions made by the diagnostic test or classification algorithm and the corresponding true labels for those predictions. You can then use the formula above to calculate the fallout. Here is an example of how to calculate fallout in Python: def fallout(y_true, y_pred): fp = sum((y_true == 0) \u0026amp; (y_pred == 1)) tn = sum((y_true == 0) \u0026amp; (y_pred == 0)) fallout = fp / (fp + tn) return fallout Here, y_true is a list of the true labels and y_pred is a list of the predictions made by the diagnostic test or classification algorithm. The function calculates the values for FP and TN using boolean masks and then uses these values to calculate the fallout using the formula above. False negative (FN) A false negative (FN) is a prediction made by a diagnostic test or classification algorithm that is incorrect. It refers to a situation where the test or algorithm predicts a negative result for a sample that is actually positive. In the context of a diagnostic test, a false negative means that the test failed to detect the presence of a condition in a person who actually has the condition. In the context of a classification algorithm, a false negative means that the algorithm failed to correctly classify a positive sample. False negatives are often more serious than false positives, as they can have more serious consequences. For example, if a diagnostic test for a disease returns a false negative result, the person may not receive the necessary treatment and their condition may worsen. To calculate the number of false negatives, you need a set of predictions made by the diagnostic test or classification algorithm and the corresponding true labels for those predictions. You can then compare the predictions to the true labels to see how many were incorrect. Here is an example of how to calculate the number of false negatives in Python: def false_negatives(y_true, y_pred): fn = sum((y_true == 1) \u0026amp; (y_pred == 0)) return fn Here, y_true is a list of the true labels and y_pred is a list of the predictions made by the diagnostic test or classification algorithm. The function calculates the number of false negatives using a boolean mask that compares the true labels to the predictions False negative rate The false negative rate (FNR) is a measure of the performance of a diagnostic test or classification algorithm. It is the percentage of positive samples that are incorrectly classified as negative. In the context of a diagnostic test, the false negative rate represents the probability that a person with the condition being tested for will receive a negative test result. In the context of a classification algorithm, the false negative rate represents the percentage of positive samples that are incorrectly classified as negative. Here is the formula for calculating the false negative rate: FNR = FN / (FN + TP) Where FN (false negative) is the number of samples that are actually positive but have been predicted as negative, and TP (true positive) is the number of samples that are actually positive and have been correctly predicted as positive. To calculate the false negative rate, you need a set of predictions made by the diagnostic test or classification algorithm and the corresponding true labels for those predictions. You can then use the formula above to calculate the false negative rate. Here is an example of how to calculate the false negative rate in Python: def false_negative_rate(y_true, y_pred): fn = sum((y_true == 1) \u0026amp; (y_pred == 0)) tp = sum((y_true == 1) \u0026amp; (y_pred == 1)) fnr = fn / (fn + tp) return fnr Here, y_true is a list of the true labels and y_pred is a list of the predictions made by the diagnostic test or classification algorithm. The function calculates the values for FN and TP using boolean masks and then uses these values to calculate the false negative rate using the formula above. False positive (FP) A false positive (FP) is a prediction made by a diagnostic test or classification algorithm that is incorrect. It refers to a situation where the test or algorithm predicts a positive result for a sample that is actually negative. In the context of a diagnostic test, a false positive means that the test detected the presence of a condition in a person who actually does not have the condition. In the context of a classification algorithm, a false positive means that the algorithm incorrectly classified a negative sample. False positives can sometimes be less serious than false negatives, as they may lead to unnecessary follow-up tests or treatment. However, they can also be costly and cause anxiety for the person being tested. To calculate the number of false positives, you need a set of predictions made by the diagnostic test or classification algorithm and the corresponding true labels for those predictions. You can then compare the predictions to the true labels to see how many were incorrect. Here is an example of how to calculate the number of false positives in Python: def false_positives(y_true, y_pred): fp = sum((y_true == 0) \u0026amp; (y_pred == 1)) return fp Here, y_true is a list of the true labels and y_pred is a list of the predictions made by the diagnostic test or classification algorithm. The function calculates the number of false positives using a boolean mask that compares the true labels to the predictions. False positive rate In the context of diagnostic tests, the false positive rate is the probability that a patient with a negative disease status will receive a positive test result. In other words, it is the probability of a false alarm. A high false positive rate means that there is a high probability of a patient being told they have a disease when they actually do not. This can lead to unnecessary anxiety and further testing, and can also reduce the overall credibility of the diagnostic test. The false positive rate is often considered in conjunction with the sensitivity and specificity of a diagnostic test. Sensitivity is the probability of a positive test result given that the patient actually has the disease, and specificity is the probability of a negative test result given that the patient does not have the disease. Together, these measures can give a more complete picture of the performance of a diagnostic test. False omission rate In the context of diagnostic tests, the false omission rate, also known as the false negative rate, is the probability that a patient with a positive disease status will receive a negative test result. A high false negative rate means that there is a high probability of a patient being told they do not have a disease when they actually do. This can have serious consequences, as the patient may not receive the necessary treatment. The false negative rate is often considered in conjunction with the sensitivity and specificity of a diagnostic test. Sensitivity is the probability of a positive test result given that the patient actually has the disease, and specificity is the probability of a negative test result given that the patient does not have the disease. Together, these measures can give a more complete picture of the performance of a diagnostic test. For example, consider a diagnostic test for a particular disease. The test has a sensitivity of 90%, meaning that it correctly identifies 90% of patients with the disease. It also has a specificity of 95%, meaning that it correctly identifies 95% of patients who do not have the disease. However, if the disease is relatively rare, the false negative rate may still be unacceptably high. For example, if the prevalence of the disease is 1%, and the test has a false negative rate of 10%, then out of 100 patients with the disease, the test will correctly identify only 81 of them (90% sensitivity), while 19 will be misdiagnosed as not having the disease (10% false negative rate). This could lead to a significant number of missed diagnoses. Hit rate Hit rate, also known as the hit ratio, is a measure of the accuracy of a classifier, predictor, or other machine learning model. It is the number of times the model correctly predicts the outcome (a \u0026ldquo;hit\u0026rdquo;) divided by the total number of predictions made. For example, if a model makes 100 predictions and is correct 70 times, the hit rate is 70%. Hit rate is often used as a measure of performance for models that make binary predictions (e.g., \u0026ldquo;positive\u0026rdquo; or \u0026ldquo;negative\u0026rdquo;). In this case, a hit is a correct prediction of the positive or negative class, and the hit rate is the proportion of positive or negative predictions that are correct. Hit rate is related to the true positive rate and the false positive rate, which are measures of the performance of a binary classifier. The true positive rate is the proportion of positive cases that are correctly classified as positive, while the false positive rate is the proportion of negative cases that are incorrectly classified as positive. Together, these measures can give a more complete picture of the performance of a classifier. Miss rate Miss rate, also known as the miss ratio or false negative rate, is a measure of the accuracy of a classifier, predictor, or other machine learning model. It is the number of times the model incorrectly predicts the outcome (a \u0026ldquo;miss\u0026rdquo;) divided by the total number of predictions made. For example, if a model makes 100 predictions and is incorrect 30 times, the miss rate is 30%. Miss rate is often used as a measure of performance for models that make binary predictions (e.g., \u0026ldquo;positive\u0026rdquo; or \u0026ldquo;negative\u0026rdquo;). In this case, a miss is an incorrect prediction of the positive or negative class, and the miss rate is the proportion of positive or negative predictions that are incorrect. Miss rate is related to the true positive rate and the false positive rate, which are measures of the performance of a binary classifier. The true positive rate is the proportion of positive cases that are correctly classified as positive, while the false positive rate is the proportion of negative cases that are incorrectly classified as positive. Together, these measures can give a more complete picture of the performance of a classifier. Negative likelihood ratio The negative likelihood ratio (NLR) is a measure of the performance of a diagnostic test or other classifier. It is the ratio of the probability of a negative test result given that the patient does not have the disease (specificity) to the probability of a negative test result given that the patient does have the disease (1 - sensitivity). The NLR is used to assess the ability of a test to rule out the presence of a disease. The NLR can be calculated using the following formula: NLR = (1 - sensitivity) / specificity A diagnostic test with a high NLR (greater than 1) is said to have a high negative predictive value, meaning that it is good at ruling out the presence of a disease. A test with a low NLR (less than 1) has a low negative predictive value, meaning that it is not good at ruling out the presence of a disease. The NLR is often used in conjunction with the positive likelihood ratio (PLR), which is the ratio of the probability of a positive test result given that the patient has the disease (sensitivity) to the probability of a positive test result given that the patient does not have the disease (1 - specificity). The PLR is used to assess the ability of a test to detect the presence of a disease. Together, the NLR and PLR can give a more complete picture of the performance of a diagnostic test. Negative predictive value The negative predictive value (NPV) is a measure of the performance of a diagnostic test or other classifier. It is the probability that a patient with a negative test result does not have the disease. The NPV is used to assess the ability of a test to rule out the presence of a disease. The NPV can be calculated using the following formula: NPV = TN / (TN + FN) where TN is the number of true negatives (patients with a negative test result who do not have the disease) and FN is the number of false negatives (patients with a negative test result who do have the disease). A diagnostic test with a high NPV (close to 1) is said to have a high negative predictive value, meaning that it is good at ruling out the presence of a disease. A test with a low NPV (close to 0) has a low negative predictive value, meaning that it is not good at ruling out the presence of a disease. The NPV is often used in conjunction with the positive predictive value (PPV), which is the probability that a patient with a positive test result does have the disease. The PPV is used to assess the ability of a test to detect the presence of a disease. Together, the NPV and PPV can give a more complete picture of the performance of a diagnostic test. Positive likelihood ratio The positive likelihood ratio (PLR) is a measure of the performance of a diagnostic test or other classifier. It is the ratio of the probability of a positive test result given that the patient has the disease (sensitivity) to the probability of a positive test result given that the patient does not have the disease (1 - specificity). The PLR is used to assess the ability of a test to detect the presence of a disease. The PLR can be calculated using the following formula: PLR = sensitivity / (1 - specificity) A diagnostic test with a high PLR (greater than 1) is said to have a high positive predictive value, meaning that it is good at detecting the presence of a disease. A test with a low PLR (less than 1) has a low positive predictive value, meaning that it is not good at detecting the presence of a disease. The PLR is often used in conjunction with the negative likelihood ratio (NLR), which is the ratio of the probability of a negative test result given that the patient does not have the disease (specificity) to the probability of a negative test result given that the patient does have the disease (1 - sensitivity). The NLR is used to assess the ability of a test to rule out the presence of a disease. Together, the PLR and NLR can give a more complete picture of the performance of a diagnostic test. Positive predictive value The positive predictive value (PPV) is a measure of the performance of a diagnostic test or other classifier. It is the probability that a patient with a positive test result does have the disease. The PPV is used to assess the ability of a test to detect the presence of a disease. The PPV can be calculated using the following formula: PPV = TP / (TP + FP) where TP is the number of true positives (patients with a positive test result who do have the disease) and FP is the number of false positives (patients with a positive test result who do not have the disease). A diagnostic test with a high PPV (close to 1) is said to have a high positive predictive value, meaning that it is good at detecting the presence of a disease. A test with a low PPV (close to 0) has a low positive predictive value, meaning that it is not good at detecting the presence of a disease. The PPV is often used in conjunction with the negative predictive value (NPV), which is the probability that a patient with a negative test result does not have the disease. The NPV is used to assess the ability of a test to rule out the presence of a disease. Together, the PPV and NPV can give a more complete picture of the performance of a diagnostic test. Precision In the context of statistical hypothesis testing and machine learning, precision is a measure of the accuracy of a classifier, predictor, or other model. It is the number of true positive predictions made by the model divided by the total number of positive predictions made by the model. Precision is used to evaluate the performance of a model that makes binary predictions (e.g., \u0026ldquo;positive\u0026rdquo; or \u0026ldquo;negative\u0026rdquo;). For example, consider a model that makes 100 predictions, of which 70 are positive and 30 are negative. If the model is correct in 60 of the positive predictions and all of the negative predictions, the precision of the model is 60/70 = 0.86. This means that of all the positive predictions made by the model, 86% are correct. Precision is often used in conjunction with the recall, which is the number of true positive predictions made by the model divided by the total number of actual positive cases. Precision and recall are both used to evaluate the performance of a binary classifier, and can be balanced against each other to achieve the desired trade-off in a particular application. Recall In the context of statistical hypothesis testing and machine learning, recall is a measure of the accuracy of a classifier, predictor, or other model. It is the number of true positive predictions made by the model divided by the total number of actual positive cases. Recall is used to evaluate the performance of a model that makes binary predictions (e.g., \u0026ldquo;positive\u0026rdquo; or \u0026ldquo;negative\u0026rdquo;). For example, consider a model that makes 100 predictions, of which 70 are positive and 30 are negative. If the model is correct in 60 of the positive predictions and all of the negative predictions, and there are 80 actual positive cases, the recall of the model is 60/80 = 0.75. This means that of all the actual positive cases, 75% are correctly predicted by the model. Recall is often used in conjunction with the precision, which is the number of true positive predictions made by the model divided by the total number of positive predictions made by the model. Precision and recall are both used to evaluate the performance of a binary classifier, and can be balanced against each other to achieve the desired trade-off in a particular application. Sensitivity Sensitivity, also known as the true positive rate or the recall, is a measure of the performance of a diagnostic test or other classifier. It is the probability of a positive test result given that the patient actually has the disease. Sensitivity is used to evaluate the ability of a test to detect the presence of a disease. The sensitivity of a diagnostic test can be calculated using the following formula: sensitivity = TP / (TP + FN) where TP is the number of true positives (patients with a positive test result who do have the disease) and FN is the number of false negatives (patients with a negative test result who do have the disease). A diagnostic test with a high sensitivity (close to 1) is said to have a high true positive rate, meaning that it is good at detecting the presence of a disease. A test with a low sensitivity (close to 0) has a low true positive rate, meaning that it is not good at detecting the presence of a disease. Sensitivity is often used in conjunction with the specificity of a diagnostic test, which is the probability of a negative test result given that the patient does not have the disease. Together, sensitivity and specificity can give a more complete picture of the performance of a diagnostic test. Specificity Specificity, also known as the true negative rate, is a measure of the performance of a diagnostic test or other classifier. It is the probability of a negative test result given that the patient does not have the disease. Specificity is used to evaluate the ability of a test to rule out the presence of a disease. The specificity of a diagnostic test can be calculated using the following formula: specificity = TN / (TN + FP) where TN is the number of true negatives (patients with a negative test result who do not have the disease) and FP is the number of false positives (patients with a positive test result who do not have the disease). A diagnostic test with a high specificity (close to 1) is said to have a high true negative rate, meaning that it is good at ruling out the presence of a disease. A test with a low specificity (close to 0) has a low true negative rate, meaning that it is not good at ruling out the presence of a disease. Specificity is often used in conjunction with the sensitivity of a diagnostic test, which is the probability of a positive test result given that the patient does have the disease. Together, sensitivity and specificity can give a more complete picture of the performance of a diagnostic test. True negative (TN) A true negative is a prediction made by a diagnostic test or other classifier that an event or condition is absent, and the event or condition is indeed absent. In the context of statistical hypothesis testing and machine learning, a true negative is a prediction made by a model that an instance belongs to the negative class, and the instance does indeed belong to the negative class. True negatives are typically represented by the letter TN in performance metrics such as sensitivity, specificity, and the positive and negative predictive values. These metrics are used to evaluate the accuracy of a diagnostic test or other classifier. For example, the sensitivity of a test is the proportion of true positive predictions made by the test to the total number of actual positive cases, while the specificity of a test is the proportion of true negative predictions made by the test to the total number of actual negative cases. True positive (TP) A true positive is a prediction made by a diagnostic test or other classifier that an event or condition is present, and the event or condition is indeed present. In the context of statistical hypothesis testing and machine learning, a true positive is a prediction made by a model that an instance belongs to the positive class, and the instance does indeed belong to the positive class. True positives are typically represented by the letter TP in performance metrics such as sensitivity, specificity, and the positive and negative predictive values. These metrics are used to evaluate the accuracy of a diagnostic test or other classifier. For example, the sensitivity of a test is the proportion of true positive predictions made by the test to the total number of actual positive cases, while the specificity of a test is the proportion of true negative predictions made by the test to the total number of actual negative cases. Data Attribute In the context of data modeling and database design, an attribute is a property or characteristic of an entity, typically represented as a column in a database table. An attribute can be a simple data value (e.g., a string, integer, or date) or a complex data structure (e.g., an array or object). For example, consider a database table that represents a collection of users. Each user in the table might have attributes such as name, email, and date of birth. These attributes can be used to describe the characteristics of each user in the table. In the context of machine learning, an attribute is a feature or characteristic of a data instance that can be used for prediction or classification. For example, in a dataset of customer data, each customer might have attributes such as age, income, and location, which could be used to predict their purchasing behavior. In both cases, the attributes of an entity or data instance are used to describe and differentiate it from other entities or instances in the same data set. Box and whisker plot A box and whisker plot (also known as a box plot) is a graphical representation of a set of numerical data that summarizes several important features of the data using a simple and visually effective display. It is typically used to visualize the distribution of the data and to identify any outliers or unusual observations. To create a box and whisker plot, the data is first sorted into numerical order. The middle 50% of the data is then represented by a box, which extends from the lower quartile (the 25th percentile) to the upper quartile (the 75th percentile). The lower and upper quartiles are the points that divide the data into four equal parts. The median (the 50th percentile) is represented by a line inside the box. The median is the middle value of the data, such that half of the data is above it and half is below it. The \u0026ldquo;whiskers\u0026rdquo; of the plot extend from the box to the minimum and maximum values of the data, unless there are outliers present, in which case the whiskers extend only to the most extreme data points that are not outliers. Outliers are data points that are significantly farther from the main body of the data than the rest of the data. They are typically plotted separately as individual points on the plot. Box and whisker plots are useful for comparing the distributions of different sets of data, or for identifying patterns and trends in a single set of data. Categorical data Categorical data is data that can be divided into categories or groups. These categories are usually based on some shared characteristics or qualities. Categorical data can be either nominal, meaning the categories do not have any specific order or ranking, or ordinal, meaning the categories are ranked or ordered in some way. Examples of categorical data include: Nominal data: Gender (male, female) Eye color (brown, blue, green) Type of animal (cat, dog, bird) Ordinal data: Educational degree (high school, bachelor\u0026rsquo;s degree, master\u0026rsquo;s degree) Customer satisfaction ratings (very satisfied, satisfied, neutral, dissatisfied, very frustrated) Military rank (private, sergeant, lieutenant, captain) Categorical data is often used in statistical analysis, and it is important to understand the type of data you are working with in order to choose the appropriate statistical techniques and analysis tools. Collective outlier A collective outlier is a group of data points that are significantly different from the rest of the data. Collective outliers can occur when there is a group of data points that have a different distribution or pattern from the rest of the data. These data points may be the result of a measurement error, an unusual event, or a different process or population. Collective outliers can be difficult to identify, as they may not stand out as clearly as individual outliers. It is important to carefully examine the data and consider the context in which it was collected to determine if a group of data points may be collective outliers. There are several methods for detecting collective outliers, including visual inspection of the data, statistical tests, and machine learning algorithms. Once identified, it is important to determine the cause of the collective outliers and consider whether they should be included in the analysis or removed from the data. Contextual outlier A contextual outlier is a data point that is unusual or unexpected in the context in which it occurs, but may not be unusual if considered in a different context. Contextual outliers can occur when there are differences in the populations, processes, or environments being studied, or when the data is being collected for different purposes or using different methods. For example, if you are studying the height of adult men and women, a data point representing the height of a 6-foot-tall woman might be considered a contextual outlier, as it is unusual compared to the rest of the data on women\u0026rsquo;s height, but not necessarily unusual compared to the overall distribution of heights in the population. It is important to consider the context in which the data was collected when identifying and analyzing contextual outliers. This can help to identify any underlying causes of the outlier and determine whether it is appropriate to include the outlier in the analysis or exclude it from the data. Covariate A covariate is a variable that is correlated with another variable and is included in a statistical model to control for its effect. Covariates are often used in statistical analysis to adjust for differences between groups or to better understand the relationship between two variables. For example, in a study of the relationship between age and blood pressure, age might be included as a covariate to control for its effect on blood pressure. This is because age is known to be related to blood pressure, and including it as a covariate in the statistical model can help to isolate the relationship between blood pressure and other factors being studied. In general, covariates are used to improve the accuracy and validity of statistical models by accounting for the influence of other variables that might confound the relationship being studied. Data point A data point is a single piece of data or a single observation in a dataset. Data points can represent a wide variety of things, depending on the context in which the data was collected. For example, a data point might represent a person\u0026rsquo;s age, the number of sales made by a company in a given month, the temperature at a specific location on a given day, or the result of a laboratory experiment. Data points are usually organized and stored in a dataset, which can be a table, spreadsheet, or other structured format. A dataset typically contains multiple data points, and each data point is often represented by a row in the dataset. Data points are used in statistical analysis to understand patterns, trends, and relationships within the data. By examining individual data points and the relationships between them, it is possible to draw conclusions and make predictions about the population or system being studied. Detrending Detrending is the process of removing trends or long-term patterns from data in order to better understand short-term fluctuations or changes. Detrending is often used in time series analysis, where the goal is to identify and analyze patterns in data that occur over time. There are several methods for detrending data, including: Subtracting the mean: This method involves calculating the mean value of the data over a certain period of time, and then subtracting that value from each data point. Fitting a trend line: This method involves fitting a line to the data using a statistical model, such as a linear or polynomial model, and then subtracting the predicted values from the actual data. Differencing: This method involves subtracting each data point from the previous data point, which removes any trend that is present in the data. Detrending can help to identify and analyze shorter-term patterns or cycles in the data, and can be useful for forecasting or predicting future values. However, it is important to carefully consider the appropriateness of detrending for a particular dataset, as removing trends can also remove important information about the underlying process or system being studied. Eigenvalue An eigenvalue is a special number that is associated with a linear transformation or matrix. In mathematics, a linear transformation is a function that maps one set of numbers (called vectors) to another set of numbers, in such a way that the transformation preserves certain properties of the original vectors. Matrices are used to represent linear transformations, and the eigenvalues of a matrix are a measure of its overall behavior or characteristics. The eigenvalues of a matrix are the values that satisfy a particular equation involving the matrix and a vector. These values can be real numbers or complex numbers, and each matrix has a set of eigenvalues that are unique to that matrix. Eigenvalues are used in a variety of mathematical and statistical contexts, including image processing, machine learning, and data analysis. They are often used to understand the behavior or characteristics of a matrix or linear transformation, and can be used to identify patterns or trends in data. Eigenvector An eigenvector is a special type of vector that is associated with a linear transformation or matrix. In mathematics, a vector is a set of numbers that can be used to represent quantities such as position, velocity, or force. A linear transformation is a function that maps one set of vectors to another set of vectors, in such a way that the transformation preserves certain properties of the original vectors. Matrices are used to represent linear transformations, and the eigenvectors of a matrix are vectors that are unchanged (up to a scale factor) by the matrix. The eigenvectors of a matrix are the vectors that satisfy a particular equation involving the matrix and the vector. These vectors can have any number of dimensions, and each matrix has a set of eigenvectors that are unique to that matrix. Eigenvectors are used in a variety of mathematical and statistical contexts, including image processing, machine learning, and data analysis. They are often used to understand the behavior or characteristics of a matrix or linear transformation, and can be used to identify patterns or trends in data. Feature In the context of machine learning, features are pieces of data or characteristics that are used as inputs for a model. A machine learning model is a mathematical model that is trained to perform a specific task, such as classifying objects, predicting a numerical value, or generating text. In order to train a model, it is necessary to provide a set of input data, called features, along with the corresponding output data, called labels. The choice of features can have a significant impact on the performance of a machine learning model. Good features should be relevant to the task being performed and should contain enough information to allow the model to make accurate predictions or decisions. In some cases, it may be necessary to transform or engineer the features in order to extract the relevant information or to improve the model\u0026rsquo;s performance. For example, in a machine learning model that is used to classify images of animals, the features might include the pixel values of the images, or characteristics such as the shape or color of the objects in the images. In a model that is used to predict the price of a house, the features might include characteristics of the house, such as the size, location, and age, as well as external factors such as the local housing market. Imputation Imputation is the process of estimating or replacing missing or incomplete data in a dataset. Missing data can occur for a variety of reasons, such as errors in data collection, missing values in a database, or respondents who do not answer certain questions in a survey. Imputation is often necessary in order to use the available data for statistical analysis or machine learning tasks. There are several methods for imputing missing data, including: Mean imputation: This method involves replacing missing values with the mean or average value of the data. Median imputation: This method involves replacing missing values with the median value of the data. Mode imputation: This method involves replacing missing values with the most frequent or common value in the data. Regression imputation: This method involves using a statistical model, such as linear regression, to predict the missing values based on the other variables in the data. It is important to carefully consider the appropriate method for imputing missing data, as the choice of method can affect the accuracy and validity of the results. Observation An observation is a single piece of data or a single measure of a variable. Observations can be collected in a variety of ways, depending on the context and the purpose of the study. For example, observations might be collected through experiments, surveys, or measurements. Observations are used to collect and analyze data in order to understand patterns, trends, and relationships within the data. By examining individual observations and the relationships between them, it is possible to draw conclusions and make predictions about the population or system being studied. Observations can be either qualitative, meaning they describe a characteristic or attribute of an object or phenomenon, or quantitative, meaning they represent a numerical measurement. Observations are usually organized and stored in a dataset, which can be a table, spreadsheet, or other structured format. A dataset typically contains multiple observations, and each observation is often represented by a row in the dataset. Principal component analysis (PCA) Principal component analysis (PCA) is a statistical technique that is used to reduce the dimensionality of a dataset by identifying and projecting the data onto a smaller set of orthogonal (uncorrelated) dimensions, called principal components. PCA is often used as a preprocessing step for machine learning algorithms, as it can help to remove noise and redundancy from the data, and make the data easier to visualize and analyze. It can also help to identify patterns and trends in the data, and to identify the most important variables or features in the dataset. To perform PCA, the data is first standardized, so that all of the variables have a mean of zero and a standard deviation of one. The data is then decomposed into a set of orthogonal principal components, which are ranked in order of their importance or variability in the data. The first principal component represents the direction in the data that has the highest variance, and the subsequent principal components represent directions that have decreasing variance. PCA is a powerful tool for analyzing and understanding complex datasets, and it has a wide range of applications in fields such as machine learning, data mining, and image processing. Point outlier A point outlier is a data point that is significantly different from the rest of the data. Point outliers can occur when there is an unusual or unexpected measurement, an error in data collection, or a different process or population being studied. Point outliers can be identified by visual inspection of the data, or by using statistical tests or machine learning algorithms. It is important to carefully consider the cause of the outlier and determine whether it is appropriate to include the outlier in the analysis or exclude it from the data. In some cases, point outliers may be the result of errors or mistakes in data collection, and it may be appropriate to remove them from the data. In other cases, point outliers may represent unusual or unexpected events or observations, and it may be important to include them in the analysis in order to better understand the underlying process or system being studied. Predictor A predictor is a variable that is used to predict or estimate the value of another variable, called the response variable. In statistical analysis, predictor variables are often used to build models that can be used to make predictions or estimations about the response variable. For example, in a study of the relationship between age and blood pressure, age might be used as a predictor variable to predict blood pressure. In this case, age would be considered a predictor because it is believed to have an effect on blood pressure, and the goal is to use it to predict or estimate blood pressure in a given population. Predictor variables can be either continuous, meaning they can take on any value within a certain range, or categorical, meaning they belong to a specific category or group. The type of predictor variables and the relationship between them and the response variable can influence the choice of statistical techniques and models that are used to analyze the data. Quantitative data Quantitative data is data that is numerical and can be measured or counted. Quantitative data is often used in statistical analysis to understand patterns, trends, and relationships within the data. There are two main types of quantitative data: continuous data and discrete data. Continuous data can take on any value within a certain range, such as weight, height, or temperature. Discrete data can only take on specific values, such as the number of students in a class or the number of emails a person receives in a day. Examples of quantitative data include: Age Income Height Weight Temperature Distance Time Sales revenue Quantitative data is often used in statistical analysis to understand patterns, trends, and relationships within the data. It can be analyzed using statistical techniques such as mean, median, mode, standard deviation, and correlation. Response In statistical analysis, the response (also known as the dependent variable) is the variable that is being predicted or estimated based on the values of one or more predictor variables (also known as independent variables). For example, in a study of the relationship between age and blood pressure, blood pressure might be the response variable, and age might be a predictor variable. In this case, the goal might be to use age to predict or estimate blood pressure in a given population. The response variable is often the main focus of statistical analysis, and the goal is usually to understand how the predictor variables influence the response variable. The choice of predictor variables and the relationship between them and the response variable can influence the choice of statistical techniques and models that are used to analyze the data. Scaling Scaling is the process of transforming data so that it is on the same scale or within the same range. Scaling is often necessary when comparing data from different sources or when the data has a wide range of values. There are several methods for scaling data, including: Min-Max scaling: This method scales the data to a specific range, such as 0 to 1, by subtracting the minimum value from each data point and dividing by the range of the data. Standardization: This method scales the data so that it has a mean of zero and standard deviation of one. Z-score normalization: This method scales the data so that it has a mean of zero and a standard deviation of one, and transforms it into a standard normal distribution. Scaling can be useful for improving the performance of machine learning algorithms, as it can help to prevent certain features from dominating the model due to their large scale. Scaling can also be useful for visualizing the data and comparing different variables or datasets. Standardization In the context of machine learning, standardization refers to the process of transforming data features so that they have zero mean and unit variance. This is often done to ensure that all features are on the same scale, which can be important for some machine learning algorithms to function properly. For example, suppose that you have a dataset with two features, one that ranges from 0 to 100 and another that ranges from 0 to 1. Without standardization, the feature with a larger range will dominate the model. By standardizing the data, both features will be transformed to have the same scale, which can lead to better performance from the machine learning model. Standardization is typically done by subtracting the mean of each feature from the feature values and dividing by the standard deviation of the feature. This ensures that the resulting feature values have zero mean and unit variance. Structured data Structured data is data that is organized in a specific way and follows a clear set of rules. It is typically stored in a tabular form, with rows representing individual instances or observations and columns representing the attributes or features of the data. Structured data can be easily processed and analyzed by machines because it follows a well-defined format. Examples of structured data include databases, spreadsheets, and tables in a relational database management system (RDBMS). Structured data is often contrasted with unstructured data, which does not follow a fixed format and is more difficult for machines to process and analyze. In the context of machine learning, structured data refers to data that is organized in a way that can be easily fed into a machine learning model. This often involves formatting the data into a tabular form with rows representing individual observations and columns representing the features or attributes of the data. Machine learning algorithms are typically designed to work with structured data, so it is important to ensure that the data is properly structured before using it for training or testing a model. Time series data Time series data is a type of data that is collected over time at regular intervals. It is typically used to analyze trends and patterns in data over time. Time series data can be represented as a sequence of data points, where each data point represents the value of a particular variable at a specific time. Examples of time series data include stock prices, weather data, and traffic data. Time series data can be used in a variety of applications, including financial forecasting, demand forecasting, and anomaly detection. In the context of machine learning, time series data can be used to train models to make predictions about future values of a particular variable based on its past values. This can be done using techniques such as time series forecasting, which involves using machine learning algorithms to model the temporal dependencies in the data and make predictions about future values. Time series data is often analyzed using specialized tools and techniques, such as autoregressive integrated moving average (ARIMA) models and long short-term memory (LSTM) neural networks. Unstructured data Unstructured data is data that does not follow a specific format or structure. It is often unorganized and does not fit neatly into a traditional database or spreadsheet. Examples of unstructured data include natural language text, images, audio and video files, and social media posts. Unstructured data is difficult for machines to process and analyze because it does not follow a fixed format. This makes it more challenging to extract insights and information from unstructured data compared to structured data, which is organized in a well-defined format and can be easily processed by machines. In the context of machine learning, unstructured data can be used as input to train models, but it often requires preprocessing and feature engineering to extract relevant features that can be used by the model. This can involve techniques such as natural language processing (NLP) for text data, image processing for image data, and audio processing for audio data. The extracted features can then be used to train machine learning models, which can be used to make predictions or classify the data in some way. Design of Experiments A/B testing A/B testing, also known as split testing or bucket testing, is a statistical hypothesis testing procedure used to compare the results of two versions of a product or service. It is commonly used in the fields of marketing and user experience to determine which version is more effective. In A/B testing, a random sample of users is selected and divided into two groups, referred to as the control group and the treatment group. The control group is exposed to the current version of the product or service, while the treatment group is exposed to the new version. The results of the two groups are then compared to determine if the new version is an improvement over the current version. A/B testing is often used to test changes to websites, apps, and other products or services to determine their impact on user behavior. It is a powerful tool for making data-driven decisions because it allows you to measure the impact of a change in a controlled and statistically rigorous way. Analysis of Variance Analysis of variance (ANOVA) is a statistical test used to compare the mean of a continuous variable between two or more groups. It is used to determine whether there is a significant difference between the means of the groups, and if so, where the difference lies. ANOVA is based on the idea of partitioning the total variance in a dataset into different components, such as the variance within each group and the variance between groups. By comparing the size of these components, ANOVA can determine whether the differences between the group means are statistically significant or if they are likely due to random chance. ANOVA can be used with both categorical and continuous independent variables, and it is a widely used tool in a variety of fields, including psychology, sociology, and economics. There are several different types of ANOVA tests, including one-way ANOVA, two-way ANOVA, and repeated measures ANOVA, which are used in different situations depending on the design of the study. Balanced design A balanced design is a type of experimental design in which the number of observations in each group is equal. Balanced designs are often used in experiments to ensure that the groups are comparable and that any differences between the groups can be attributed to the independent variable being tested. For example, suppose that you are conducting an experiment to test the effectiveness of a new drug. You might use a balanced design by dividing the study participants into two groups: one group that receives the drug and another group that receives a placebo. By ensuring that the two groups are equal in size and composition, you can control for other factors that might influence the results and increase the reliability of your findings. Balanced designs can be contrasted with unbalanced designs, in which the number of observations in each group is unequal. Unbalanced designs can be more prone to bias and may not be as reliable as balanced designs. Blocking In the context of experimental design, blocking refers to the process of dividing the study subjects into groups, or \u0026ldquo;blocks,\u0026rdquo; based on certain factors that could potentially affect the outcome of the experiment. The goal of blocking is to control for these factors and reduce the potential for extraneous variability in the results. For example, suppose that you are conducting an experiment to test the effectiveness of a new teaching method. You might use blocking by dividing the students into groups based on their prior knowledge of the subject matter, in order to control for differences in their initial understanding. By ensuring that the groups are balanced with respect to this factor, you can increase the reliability of your findings and reduce the risk of confounding variables influencing the results. Blocking is often used in conjunction with randomization, in which the subjects within each block are randomly assigned to the different treatment groups. This helps to further control for extraneous variables and increase the internal validity of the experiment. Control In the context of experimental design, a control group is a group of subjects that does not receive the treatment being tested. The control group is used for comparison with the experimental group, which does receive the treatment. By comparing the results of the two groups, researchers can determine the effect of the treatment on the outcome of interest. The control group is an important element of experimental design because it helps to control for extraneous variables that might influence the results. For example, suppose that you are conducting an experiment to test the effectiveness of a new drug. By including a control group that does not receive the drug, you can control for other factors that might affect the outcome, such as the placebo effect or the natural course of the disease. In order to be effective, the control group should be similar to the experimental group in all aspects except for the treatment being tested. This helps to ensure that any differences between the two groups can be attributed to the treatment, rather than other factors. Design of experiments The design of experiments (DOE) refers to the systematic and scientific approach to planning, conducting, analyzing, and interpreting experiments. It is a powerful tool for understanding the relationships between variables and for making informed decisions based on data. The goal of DOE is to identify the key factors that affect the outcome of an experiment and to determine the optimal combination of these factors. This is typically done by manipulating the levels of the different variables and observing the resulting changes in the outcome. There are many different types of experimental designs, including randomized controlled trials, cross-over designs, and factorial designs. The choice of design depends on the specific research question being addressed and the resources available for the experiment. DOE is widely used in a variety of fields, including medicine, engineering, and the social sciences. It is an important tool for scientific research and for making data-driven decisions in a variety of settings. Exploitation Exploitation refers to the act of using something or someone to achieve a benefit or gain, often in a way that is unfair or unethical. In the context of machine learning, exploitation can refer to the use of data or algorithms in ways that unfairly advantage certain individuals or groups, or that violate the privacy or autonomy of those whose data is being used. For example, exploitation in machine learning could involve using sensitive personal data for purposes that were not disclosed to the individual when the data was collected, or using algorithms that are biased against certain groups. Such practices can lead to negative consequences for those affected by the exploitation, including loss of privacy, discrimination, or loss of opportunities. It is important to be aware of the potential for exploitation in machine learning and to take steps to ensure that data and algorithms are used ethically and responsibly. This can involve adopting ethical principles and guidelines, such as those put forth by organizations like the Association for Computing Machinery (ACM) and the International Association for AI and Ethics (IAAIE). Exploration In the context of design of experiments (DOE), exploration refers to the process of systematically varying the levels of the input factors in order to better understand the response of the system being studied. Exploration is an important aspect of DOE because it helps to identify the important factors that influence the response, as well as the relationships between these factors and the response. This information can be used to optimize the system by identifying the optimal levels of the input factors for a desired response. Exploration can be carried out using a variety of DOE techniques, such as factorial designs, response surface methodology, and DOE software tools. Factorial design A factorial design is a type of experimental design in which multiple levels of multiple input factors are tested simultaneously. This allows researchers to study the combined effect of multiple factors on a response, as well as the interaction between the factors. Factorial designs are commonly used in DOE because they are efficient and can provide a lot of information about the system being studied. For example, if there are two factors being studied, each at two levels, a 2x2 factorial design would involve testing all four possible combinations of the factor levels. This allows researchers to see how the response changes as each factor is varied independently, as well as how the response changes when the factors are combined. Factorial designs can have more than two factors and more than two levels per factor. The number of treatment combinations in a factorial design increases quickly as the number of factors and levels increases, so it is important to carefully plan the design to ensure that it is both practical and efficient. Fractional factorial design A fractional factorial design is a type of experimental design that is similar to a full factorial design, but involves testing only a fraction of the possible combinations of factor levels. This allows researchers to study the effects of multiple factors with a smaller number of experimental runs. Fractional factorial designs are useful when there are a large number of factors that need to be studied, or when it is not practical or cost-effective to test all possible combinations of factor levels. However, because not all combinations of factor levels are tested, a fractional factorial design may not be as accurate as a full factorial design. There are several types of fractional factorial designs, including two-level fractional factorial designs, which involve testing only a fraction of the possible combinations of two levels of each factor, and Plackett-Burman designs, which are a type of fractional factorial design that is commonly used to identify the important factors in a system. Full factorial design A full factorial design is a type of experimental design in which all possible combinations of the levels of multiple input factors are tested. This allows researchers to study the combined effect of multiple factors on a response, as well as the interaction between the factors. Full factorial designs are commonly used in design of experiments (DOE) because they provide a lot of information about the system being studied. For example, if there are two factors being studied, each at two levels, a full factorial design would involve testing all four possible combinations of the factor levels. This allows researchers to see how the response changes as each factor is varied independently, as well as how the response changes when the factors are combined. Full factorial designs can have more than two factors and more than two levels per factor. The number of treatment combinations in a full factorial design increases quickly as the number of factors and levels increases, so it is important to carefully plan the design to ensure that it is both practical and efficient. Multi-armed bandit A multi-armed bandit is a type of optimization problem that involves balancing the exploration of different options (the \u0026ldquo;arms\u0026rdquo; of the bandit) with the exploitation of the best option known so far. The goal is to maximize the reward over time by choosing the arm that is most likely to provide the highest reward at each step. The multi-armed bandit problem is often used to model situations in which there is a trade-off between exploration and exploitation. For example, in online advertising, a website owner may need to choose which ads to display to a user. The website owner may not know which ad will be the most effective at converting the user into a customer, so they must balance the need to explore different ads with the need to exploit the most effective ad. There are various algorithms that can be used to solve the multi-armed bandit problem, such as the epsilon-greedy algorithm and the upper confidence bound (UCB) algorithm. These algorithms use different approaches to balance exploration and exploitation, and can be modified to suit the specific needs of a given application. Response surface Response surface methodology (RSM) is a statistical technique used to model and optimize the relationship between one or more input variables (also known as factors or independent variables) and an output variable (also known as the response). RSM involves designing experiments to study the response of a system to different levels of the input variables, and then fitting a mathematical model to the data to represent the relationship between the variables. The response surface is the graphical representation of the response of the system as a function of the input variables. It is usually a two-dimensional plot showing the response as a function of two input variables, although it can also be a three-dimensional plot for systems with three or more input variables. The response surface can be used to identify the optimal combination of input variables that produce the desired response, as well as to understand the nature of the relationship between the variables. RSM is commonly used in engineering and scientific research to optimize processes and products, and it can be applied to a wide range of systems and industries. Game Theory Cooperative game theory Cooperative game theory is a branch of game theory that studies situations in which multiple players can form coalitions and make binding agreements in order to achieve a common goal. In cooperative game theory, the players are assumed to be rational and to act in their own self-interest, but they are also able to communicate and make agreements with each other. One important concept in cooperative game theory is the concept of the \u0026ldquo;value\u0026rdquo; of a game, which is the maximum payoff that can be achieved by the players if they cooperate. The value of a game can be determined using various solution concepts, such as the Shapley value, the nucleolus, and the core. These solution concepts provide a way to divide the value of the game among the players in a fair and stable way. Cooperative game theory is used in a variety of fields, including economics, political science, and computer science. It is particularly useful for studying situations in which the players have conflicting interests, but may still be able to cooperate in order to achieve a mutually beneficial outcome. Game theory Game theory is the study of mathematical models of strategic interactions between rational decision-makers. It has applications in a wide range of disciplines, including economics, political science, and psychology, as well as in biology and computer science. In game theory, a \u0026ldquo;game\u0026rdquo; is defined as a situation in which multiple players, called \u0026ldquo;players,\u0026rdquo; have to make decisions that will affect the outcome of the game. Each player has a set of possible actions they can take, called a \u0026ldquo;strategy,\u0026rdquo; and a corresponding payoff that depends on the strategies chosen by all the players. The players are assumed to be rational and to act in their own self-interest, trying to maximize their payoff. There are two main types of games in game theory: cooperative games and non-cooperative games. In cooperative games, the players can form coalitions and make binding agreements, while in non-cooperative games, the players act independently and cannot make agreements. Game theory has been used to study a wide range of real-world situations, including auctions, negotiation, and voting systems. It has also been used to analyze strategic interactions in biology, such as predator-prey relationships and the evolution of social behavior. Mixed strategy/randomized strategy In game theory, a mixed strategy is a strategy in which a player randomly selects one of several pure strategies with a specified probability. A pure strategy is a strategy in which the player always chooses a particular action, while a mixed strategy allows the player to choose among several different actions with some probability. Mixed strategies are often used to model situations in which a player has incomplete information about the other player\u0026rsquo;s strategies or preferences, or in which the payoffs for each action are not fixed and may vary from one round of the game to the next. In non-cooperative games, mixed strategies can be used to find a Nash equilibrium, which is a situation in which no player has an incentive to deviate from their current strategy given the strategies of the other players. In a Nash equilibrium, each player\u0026rsquo;s mixed strategy is a best response to the mixed strategies of the other players. Mixed strategies can also be used in cooperative games, although they are not always necessary to find a solution. In cooperative games, mixed strategies can be used to divide the value of the game among the players in a fair and stable way. Prisoner\u0026rsquo;s dilemma The prisoner\u0026rsquo;s dilemma is a classic example of a game used to illustrate the concept of game theory. It is a non-cooperative game that involves two players who must decide whether to cooperate with each other or to defect (i.e., not cooperate). In the prisoner\u0026rsquo;s dilemma, the players are assumed to be two prisoners who are being held in separate cells and are offered the following deal: if both prisoners defect, each one will serve a two-year prison sentence; if one defects and the other cooperates, the defector will go free while the cooperator will serve a three-year prison sentence; and if both cooperate, each one will serve a one-year prison sentence. The prisoner\u0026rsquo;s dilemma is interesting because the rational choice for each player, given the other player\u0026rsquo;s choice, is to defect. However, if both players defect, they both end up with a worse outcome than if they had cooperated. This illustrates the concept of the \u0026ldquo;prisoner\u0026rsquo;s dilemma,\u0026rdquo; in which individual rationality leads to a suboptimal outcome for both players. The prisoner\u0026rsquo;s dilemma has been used to model a wide range of real-world situations, including negotiations, international relations, and the evolution of social behavior. Pure strategy In game theory, a pure strategy is a strategy in which a player always chooses a particular action. A pure strategy is contrasted with a mixed strategy, in which a player randomly selects one of several actions with a specified probability. Pure strategies are often used to model situations in which a player has complete information about the other player\u0026rsquo;s strategies or preferences, or in which the payoffs for each action are fixed and do not vary from one round of the game to the next. In non-cooperative games, pure strategies can be used to find a Nash equilibrium, which is a situation in which no player has an incentive to deviate from their current strategy given the strategies of the other players. In a Nash equilibrium, each player\u0026rsquo;s pure strategy is a best response to the pure strategies of the other players. Pure strategies can also be used in cooperative games, although they are not always necessary to find a solution. In cooperative games, pure strategies can be used to divide the value of the game among the players in a fair and stable way. Sequential game A sequential game is a type of game in which the players take turns making decisions, and the actions of each player depend on the actions of the previous players. In a sequential game, the players have the opportunity to observe the actions of the other players before making their own decisions, which allows them to adjust their strategies based on the actions of the other players. Sequential games can be either cooperative or non-cooperative. In cooperative sequential games, the players can communicate and make binding agreements with each other, while in non-cooperative sequential games, the players act independently and cannot make agreements. There are several solution concepts that can be used to analyze sequential games, including the subgame perfect equilibrium, the backward induction solution, and the trembling hand perfect equilibrium. These solution concepts provide a way to predict the outcomes of sequential games and to understand the strategic interactions between the players. Sequential games are often used to model real-world situations in which the players have the opportunity to observe and learn from each other\u0026rsquo;s actions, such as in auctions and negotiations. Simultaneous game A simultaneous game is a type of game in which all of the players make their decisions at the same time, without knowing the decisions of the other players. In a simultaneous game, the players have to make their decisions based on their beliefs about the other players\u0026rsquo; strategies or preferences, rather than on the actual actions of the other players. Simultaneous games can be either cooperative or non-cooperative. In cooperative simultaneous games, the players can communicate and make binding agreements with each other, while in non-cooperative simultaneous games, the players act independently and cannot make agreements. There are several solution concepts that can be used to analyze simultaneous games, including the Nash equilibrium, the correlated equilibrium, and the rationalizability concept. These solution concepts provide a way to predict the outcomes of simultaneous games and to understand the strategic interactions between the players. Simultaneous games are often used to model real-world situations in which the players make their decisions simultaneously and do not have the opportunity to observe each other\u0026rsquo;s actions, such as in auctions and political elections. Stable equilibrium An equilibrium is a state in which no player has an incentive to change their behavior given the behavior of the other players. In game theory, an equilibrium is considered \u0026ldquo;stable\u0026rdquo; if it is the unique outcome of the game and if all the players are satisfied with the outcome. There are several types of stable equilibria in game theory, including the Nash equilibrium, the correlated equilibrium, and the rationalizability concept. These solution concepts provide a way to predict the outcomes of games and to understand the strategic interactions between the players. Stable equilibria are important because they provide a way to predict the behavior of players in strategic situations. They are often used to model real-world situations in which the players have conflicting interests and must make decisions that will affect the outcome of the game. In order for an equilibrium to be stable, it must be the unique outcome of the game and all the players must be satisfied with the outcome. This means that if any player has an incentive to deviate from the equilibrium, the equilibrium is not stable. Zero-sum game A zero-sum game is a type of game in which the total gain or loss of the players is always zero. This means that the gain of one player is exactly balanced by the loss of the other player(s). In a zero-sum game, the players are in direct competition with each other, and the outcome of the game depends on the relative skill of the players. If one player wins, the other player(s) must lose an equal amount. Examples of zero-sum games include poker, chess, and the prisoner\u0026rsquo;s dilemma. In these games, one player\u0026rsquo;s gain is exactly offset by the other player\u0026rsquo;s loss, so the total gain or loss of the players is always zero. Zero-sum games are important in game theory because they provide a simple and well-defined framework for analyzing strategic interactions between players. They are also important in economics, where they are used to model situations in which the total resources available to the players are fixed and cannot be increased or decreased. Model Quality Akaike information criterion (AIC) AIC stands for \u0026ldquo;Akaike\u0026rsquo;s Information Criterion.\u0026rdquo; It is a statistical measure that is used to evaluate the quality of a statistical model. The AIC is based on the idea that the best model is the one that strikes the right balance between fit to the data and parsimony (i.e., simplicity). The AIC is calculated as follows: AIC = 2k - 2ln(L) where k is the number of parameters in the model and L is the maximum likelihood of the model. The AIC is a measure of the relative quality of a model, with lower values indicating a better model.\nThe AIC is often used in model selection, where it is used to compare the relative quality of different models. It can also be used to compare the quality of nested models, where one model is a special case of another model. The AIC is widely used in statistics and is particularly useful for comparing models with different numbers of parameters. It has been applied in a wide range of fields, including economics, engineering, and the natural sciences. Bayesian Information criterion (BIC) The Bayesian Information Criterion (BIC) is a statistical measure that is used to evaluate the quality of a statistical model. It is based on the idea that the best model is the one that strikes the right balance between fit to the data and parsimony (i.e., simplicity). The BIC is calculated as follows: BIC = kln(n) - 2ln(L) where k is the number of parameters in the model, n is the number of data points, and L is the maximum likelihood of the model. The BIC is a measure of the relative quality of a model, with lower values indicating a better model.\nThe BIC is often used in model selection, where it is used to compare the relative quality of different models. It can also be used to compare the quality of nested models, where one model is a special case of another model. The BIC is widely used in statistics and is particularly useful for comparing models with different numbers of parameters. It has been applied in a wide range of fields, including economics, engineering, and the natural sciences. Causation Causation refers to the relationship between an event (the cause) and a second event (the effect), where the second event is the result of the first. In order for an event to be considered the cause of another event, it must be shown that there is a clear link between the two events and that the first event directly led to the second event. There are several factors that are often used to establish causation, including the following: Temporal precedence: The cause must occur before the effect. Covariation: The cause and effect must vary together. Control: When other variables are controlled for, the cause and effect should still be related. Plausibility: The proposed cause must be scientifically plausible. Establishing causation can be challenging, particularly in complex systems where there may be multiple potential causes and it is difficult to control for all other variables. In these cases, it is often necessary to use statistical methods to assess the strength of the relationship between the cause and effect. Corrected AIC Corrected AIC, also known as AICc, is a variant of Akaike\u0026rsquo;s Information Criterion (AIC) that is used to evaluate the quality of a statistical model. Like the AIC, the AICc is based on the idea that the best model is the one that strikes the right balance between fit to the data and parsimony (i.e., simplicity). The AICc is calculated as follows: AICc = AIC + (2k(k + 1)) / (n - k - 1) where k is the number of parameters in the model, n is the number of data points, and AIC is Akaike\u0026rsquo;s Information Criterion. The AICc is a measure of the relative quality of a model, with lower values indicating a better model.\nThe AICc is often used in model selection, where it is used to compare the relative quality of different models. It is particularly useful for comparing models with small sample sizes, as it adjusts for the bias that can occur when using the AIC with small sample sizes. The AICc is widely used in statistics and has been applied in a wide range of fields, including economics, engineering, and the natural sciences. Correlation Correlation is a statistical measure of the relationship between two variables. It is a way to describe the degree to which two variables are related to each other. The correlation between two variables is usually represented by the correlation coefficient, which can range from -1 to 1. A correlation coefficient of -1 indicates a perfect negative correlation, meaning that as one variable increases, the other decreases. A correlation coefficient of 1 indicates a perfect positive correlation, meaning that as one variable increases, the other also increases. A correlation coefficient of 0 indicates no correlation. Correlation does not imply causation, meaning that the presence of a correlation between two variables does not necessarily mean that one variable is causing the other. It is possible for two variables to be correlated without there being a causal relationship between them. Correlation is an important statistical concept that is used in a wide range of fields, including economics, psychology, and the natural sciences. It is often used to understand the relationship between different variables and to predict future outcomes. Cross-validation Cross-validation is a method used to evaluate the performance of a statistical model. It involves dividing the data into a training set, which is used to train the model, and a test set, which is used to evaluate the model. There are several types of cross-validation, including the following: K-fold cross-validation: The data is divided into k folds, and the model is trained on k-1 folds and tested on the remaining fold. This process is repeated k times, with a different fold being used as the test set each time. Leave-one-out cross-validation: The model is trained on all but one data point, and then tested on the left-out data point. This process is repeated for each data point, resulting in a model being trained and tested n times, where n is the number of data points. Stratified cross-validation: The data is divided into folds such that the proportions of different classes in the folds are similar to the proportions in the entire dataset. This is useful when the classes are imbalanced. Cross-validation is a useful tool for evaluating the performance of a statistical model and for selecting the best model for a given dataset. It helps to ensure that the model is not overfitted to the training data and that it generalizes well to unseen data. Hypothesis test A hypothesis test is a statistical procedure used to test whether a hypothesis about a population parameter is true or false. It involves collecting data from a sample and using it to make a decision about the hypothesis. The process of conducting a hypothesis test usually involves the following steps: State the null hypothesis and the alternative hypothesis. The null hypothesis is the assumption that there is no relationship between the variables being tested, while the alternative hypothesis is the assumption that there is a relationship. Select a sample and collect data. The sample should be representative of the population being studied. Choose a test statistic and a critical value. The test statistic is a measure of the difference between the sample and the null hypothesis, while the critical value is a predetermined threshold that is used to decide whether to reject or accept the null hypothesis. Calculate the p-value. The p-value is the probability of obtaining a test statistic as extreme as the one observed, given that the null hypothesis is true. Make a decision. If the p-value is less than the critical value, the null hypothesis is rejected in favor of the alternative hypothesis. If the p-value is greater than the critical value, the null hypothesis is not rejected. Hypothesis tests are an important tool for making decisions about statistical relationships and are widely used in a variety of fields, including psychology, economics, and the natural sciences. k-fold cross-validation K-fold cross-validation is a method used to evaluate the performance of a statistical model. It involves dividing the data into k folds (also known as \u0026ldquo;subsets\u0026rdquo;) and training the model k times, each time using a different fold as the test set and the remaining folds as the training set. The performance of the model is then averaged across the k iterations. For example, in 5-fold cross-validation, the data is divided into 5 folds, and the model is trained and tested 5 times. Each time, a different fold is used as the test set, and the model is trained on the other 4 folds. The performance of the model is then averaged across the 5 iterations. K-fold cross-validation is a useful tool for evaluating the performance of a model and for selecting the best model for a given dataset. It helps to ensure that the model is not overfitted to the training data and that it generalizes well to unseen data. K-fold cross-validation is a widely used method in machine learning and is particularly useful for small datasets, where it can provide a more reliable estimate of model performance than other methods. Likelihood In statistics, the likelihood of a model is a measure of how well the model fits the data. It is defined as the probability of observing the data given the model and a set of parameters. The likelihood is often used to compare the fit of different models to the same data. A higher likelihood indicates a better fit, while a lower likelihood indicates a poorer fit. The likelihood is often used in maximum likelihood estimation, a method used to estimate the parameters of a statistical model. In maximum likelihood estimation, the parameters of the model are chosen to maximize the likelihood of the model given the data. The likelihood is an important concept in statistics that is used in a wide range of applications, including hypothesis testing, model selection, and statistical inference. It provides a way to evaluate the fit of a model to the data and to compare the fit of different models to the same data. Maximum likelihood Maximum likelihood is a method used to estimate the parameters of a statistical model. It is based on the idea of finding the set of parameters that maximize the likelihood of the model given the data. The likelihood of a model is a measure of how well the model fits the data. It is defined as the probability of observing the data given the model and a set of parameters. In maximum likelihood estimation, the parameters of the model are chosen to maximize the likelihood of the model given the data. Maximum likelihood estimation has several desirable properties, including being asymptotically efficient (i.e., the estimators converge to the true values as the sample size increases) and being relatively easy to implement. It is widely used in a variety of fields, including economics, psychology, and the natural sciences. Maximum likelihood estimation is often used in conjunction with other statistical methods, such as hypothesis testing and model selection, to make inferences about the underlying population from which the data were collected. Missing data Missing data refers to data that is not available or that has not been collected. It is a common problem in statistical analysis and can occur for a variety of reasons, including errors in data collection, missing values in the data, and data that is not recorded. Missing data can be a problem because it can bias the results of statistical analyses. For example, if the missing data is not randomly distributed, it can lead to sampling bias and affect the validity of the conclusions. There are several approaches for dealing with missing data, including the following: Complete case analysis: This involves removing any cases with missing data from the analysis. This is the simplest approach, but it can lead to biased results if the missing data is not missing at random. Imputation: This involves replacing the missing values with estimates based on the available data. There are several methods for imputing missing data, including mean imputation, regression imputation, and multiple imputation. Maximum likelihood: This involves using a statistical model to estimate the missing data based on the observed data. The best approach for dealing with missing data depends on the nature of the missing data and the goals of the analysis. It is important to carefully consider the implications of missing data and choose an appropriate approach to ensure the validity of the results. Random effects In statistics, a random effect is a variable that is included in a statistical model to account for the fact that the data is a sample from a larger population. Random effects are used to model the variability between different groups or individuals in the population. For example, consider a study that aims to investigate the relationship between diet and blood pressure. In this study, the researchers might collect data from several different groups of people, such as men and women, or people from different countries. If the researchers want to account for the fact that the data is a sample from a larger population, they might include a random effect for group in their statistical model. This would allow them to estimate the average effect of diet on blood pressure within each group, as well as the overall effect across all groups. Random effects are often used in mixed-effects models, which are used to analyze data that has both fixed and random effects. They are an important tool for understanding the sources of variability in data and for making inferences about the population from which the data were collected. Real effects In statistics, a real effect is a variable that is included in a statistical model to represent an underlying relationship or effect that is believed to exist in the population. Real effects are often used to test hypotheses about the relationships between variables and to estimate the strength and direction of those relationships. For example, consider a study that aims to investigate the relationship between diet and blood pressure. In this study, the researchers might collect data from a sample of people and include a real effect for diet in their statistical model. This would allow them to estimate the average effect of diet on blood pressure in the population and to test whether this effect is statistically significant. Real effects are often contrasted with random effects, which are used to account for the fact that the data is a sample from a larger population. While real effects represent underlying relationships in the population, random effects represent the variability between different groups or individuals in the population. Sum-of-squared errors The sum of squared errors (SSE) is a measure of the deviation of a set of values from a predicted value. It is often used in statistical analysis to evaluate the fit of a model to a set of data. The SSE is calculated as follows: SSE = ∑(observed value - predicted value)^2 where the sum is taken over all the data points.\nThe SSE is a measure of the sum of the squared differences between the observed values and the predicted values. It is a common measure of the error or deviation of a set of values from a predicted value, and it is often used to compare the fit of different models to the same data. In general, a smaller SSE indicates a better fit of the model to the data, while a larger SSE indicates a poorer fit. The SSE is often used in conjunction with other measures of fit, such as the coefficient of determination (R^2), to evaluate the quality of a statistical model. Test data/test set A test set is a set of data that is used to evaluate the performance of a statistical model. It is separate from the training set, which is used to fit the model, and is used to assess how well the model generalizes to new, unseen data. The test set is often used to estimate the accuracy of the model, as well as other performance metrics such as precision, recall, and F1 score. It is a crucial step in the model development process, as it allows the model to be evaluated on data that it has not seen before and provides a way to assess the generalizability of the model. The test set is usually chosen to be representative of the data that the model will encounter in real-world use. It is important to ensure that the test set is independent of the training set and that it is not used in any way to fit the model. The test set is an important tool for evaluating the performance of a statistical model and for comparing the performance of different models. It is widely used in a variety of fields, including machine learning, data mining, and statistical analysis. Training data/training set The training data or training set is a set of data that is used to fit a statistical model. It is used to learn the parameters of the model and to improve the model\u0026rsquo;s ability to make predictions on new, unseen data. The training set is usually a subset of the total dataset and is chosen to be representative of the data that the model will encounter in real-world use. It is important to ensure that the training set is representative of the data that the model will encounter in order to improve the model\u0026rsquo;s ability to generalize to new data. The training set is used to fit the model by adjusting the model\u0026rsquo;s parameters to minimize the error between the predicted values and the observed values. Once the model has been trained on the training set, it can be evaluated on a separate test set to assess its performance on new data. The training set is an important tool for building and evaluating statistical models and is widely used in a variety of fields, including machine learning, data mining, and statistical analysis. Validation data/validation set The validation data or validation set is a set of data that is used to evaluate the performance of a statistical model. It is used to tune the model\u0026rsquo;s hyperparameters and to select the best model among a set of candidates. The validation set is usually a subset of the total dataset and is used to assess the model\u0026rsquo;s ability to generalize to new, unseen data. It is important to ensure that the validation set is independent of the training set and is not used to fit the model in any way. The validation set is used to compare the performance of different models and to select the best model based on a predetermined criterion, such as the accuracy of the model or the Akaike Information Criterion (AIC). Once the best model has been selected, it can be evaluated on a separate test set to assess its performance on new data. The validation set is an important tool for building and evaluating statistical models and is widely used in a variety of fields, including machine learning, data mining, and statistical analysis. Non-Parametric Tests Mann-Whitney test The Mann-Whitney test is a nonparametric statistical test used to compare the means of two independent samples. It is used when the data is not normally distributed or when the variances of the two samples are not equal. The Mann-Whitney test is based on the ranks of the data rather than the raw data values. It involves ranking the data from the two samples and comparing the ranks of the observations from the two samples. The Mann-Whitney test is used to test the hypothesis that the two samples come from the same population. If the null hypothesis is rejected, it indicates that there is a statistically significant difference between the means of the two samples. The Mann-Whitney test is a widely used statistical test and is particularly useful when the assumptions of other tests, such as the t-test, are not met. It is an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. McNemar\u0026rsquo;s test McNemar\u0026rsquo;s test is a statistical test used to compare the proportions of two dependent samples. It is used when the data is in the form of pairs, such as before and after measurements on the same group of individuals. The McNemar\u0026rsquo;s test is used to test the hypothesis that the proportions of the two samples are equal. It is based on the difference between the two proportions and is used to determine whether the difference is statistically significant. The McNemar\u0026rsquo;s test is a nonparametric test, which means that it does not assume that the data follows a specific distribution. It is often used when the assumptions of other tests, such as the chi-squared test, are not met. The McNemar\u0026rsquo;s test is an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. It is widely used in a variety of fields, including psychology, medicine, and the social sciences. Nonparametric test A nonparametric test is a statistical test that does not assume that the data follows a specific distribution. Nonparametric tests are often used when the assumptions of parametric tests, such as the t-test or the ANOVA test, are not met or when the sample size is too small to make such assumptions. Nonparametric tests are based on the ranks or the frequencies of the data rather than the raw data values. They are often used to compare the means or proportions of two or more groups or to test for associations between variables. Some examples of nonparametric tests include the Mann-Whitney test, the Wilcoxon signed-rank test, the Kruskal-Wallis test, the chi-squared test, and the McNemar\u0026rsquo;s test. Nonparametric tests are an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. They are widely used in a variety of fields, including psychology, medicine, and the social sciences. Paired samples Paired samples are two sets of measurements that are taken on the same group of individuals or units. Paired samples are often used in statistical analysis to compare the means or proportions of the two samples and to test for statistical significance. Paired samples are often used when the two samples are dependent, meaning that the measurements in one sample are related to the measurements in the other sample. For example, paired samples might be used to compare the scores of the same group of individuals on two different tests, or to compare the blood pressure of the same group of individuals before and after a treatment. Paired samples can be analyzed using parametric or nonparametric statistical tests, depending on the assumptions of the data. Some examples of statistical tests for paired samples include the paired t-test, the Wilcoxon signed-rank test, and the McNemar\u0026rsquo;s test. Paired samples are an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. They are widely used in a variety of fields, including psychology, medicine, and the social sciences. Parametric test A parametric test is a statistical test that assumes that the data follows a specific distribution, such as the normal distribution. Parametric tests are based on the parameters of the distribution and are used to test hypotheses about the population means or proportions. Parametric tests are often more powerful than nonparametric tests, which means that they can detect smaller differences between the samples. However, they are also more sensitive to violations of the assumptions of the test, such as normality and homoscedasticity. Some examples of parametric tests include the t-test, the ANOVA test, and the linear regression model. Parametric tests are an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. They are widely used in a variety of fields, including psychology, medicine, and the social sciences. Wilcoxon signed rank test (one sample) The Wilcoxon signed-rank test is a nonparametric statistical test used to compare the median of a single sample to a hypothesized value. It is used when the data are not normally distributed or when the sample size is small. The Wilcoxon signed-rank test is based on the ranks of the differences between the observations and the hypothesized value. It involves ranking the differences and testing the hypothesis that the median of the ranked differences is equal to zero. The Wilcoxon signed-rank test is used to test the hypothesis that the median of the sample is equal to the hypothesized value. If the null hypothesis is rejected, it indicates that there is a statistically significant difference between the median of the sample and the hypothesized value. The Wilcoxon signed-rank test is an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. It is widely used in a variety of fields, including psychology, medicine, and the social sciences. Wilcoxon signed rank test The Wilcoxon signed-rank test is a nonparametric statistical test used to compare the means of two related or dependent samples. It is used when the data are not normally distributed or when the variances of the two samples are not equal. The Wilcoxon signed-rank test is based on the ranks of the differences between the observations in the two samples. It involves ranking the differences and testing the hypothesis that the median of the ranked differences is equal to zero. The Wilcoxon signed-rank test is used to test the hypothesis that the means of the two samples are equal. If the null hypothesis is rejected, it indicates that there is a statistically significant difference between the means of the two samples. The Wilcoxon signed-rank test is an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. It is widely used in a variety of fields, including psychology, medicine, and the social sciences. Optimization Approximate dynamic program Approximate dynamic programming is a method for solving optimization problems that involves iteratively improving approximate solutions to a problem. It is often used when the exact solution to the problem is computationally intractable, but it is possible to compute approximate solutions that are good enough for a particular application. In approximate dynamic programming, a sequence of approximate solutions is generated, with each successive solution being an improvement upon the previous one. This process is often done using techniques from machine learning, such as supervised learning, reinforcement learning, or unsupervised learning, to learn a function that can be used to generate the approximate solutions. Approximate dynamic programming can be used in a wide variety of applications, including resource allocation, scheduling, control systems, and decision making. It is a powerful tool for solving optimization problems in real-time, and has been applied in a number of different fields, including economics, engineering, and computer science. Arc In the context of optimization, an arc is a continuous path between two points in a graph or network. Arcs are often used to represent connections or relationships between variables or points in a problem. In mathematical optimization, arcs can be used to represent constraints or limitations on the solution to a problem. For example, in a transportation optimization problem, arcs might represent the routes that can be taken between different locations, and the cost of traveling along each route. The optimization problem would then involve finding the lowest cost path that satisfies all of the constraints represented by the arcs. Arcs can also be used to represent relationships between variables in a problem. For example, in a linear programming problem, arcs might represent the flow of a resource between different locations or activities. The optimization problem would then involve finding the values for the variables (such as the amount of the resource to be allocated to each location or activity) that maximize or minimize some objective function, subject to the constraints represented by the arcs. Assignment problem The assignment problem is a type of optimization problem that involves finding the optimal way to assign a set of resources to a set of tasks. The goal of the assignment problem is to minimize the total cost of the assignment, where the cost of an assignment is the sum of the cost of assigning each resource to its corresponding task. -The assignment problem can be represented as a bipartite graph, with one set of vertices representing the resources and the other set representing the tasks. Arcs are then drawn between the vertices, with the cost of assigning a resource to a task being represented by the weight of the corresponding arc. The assignment problem then involves finding a complete matching (a set of arcs such that every vertex is incident to exactly one arc) in the graph that minimizes the total arc weight. The assignment problem can be solved using a number of different algorithms, including the Hungarian algorithm, the auction algorithm, and the primal-dual algorithm. It has a wide range of applications, including resource allocation, scheduling, and transportation optimization. Bellman\u0026rsquo;s equation Bellman\u0026rsquo;s equation is a mathematical equation that is used in dynamic programming to compute the value of a given state in a Markov decision process. It is named after Richard Bellman, who introduced the concept of dynamic programming in the 1950s. In dynamic programming, a Markov decision process is represented as a sequence of states, transitions, and rewards. At each time step, the decision maker can choose from a set of actions that will transition the system to a new state. The value of a state is defined as the expected sum of future rewards that can be obtained by starting in that state and following an optimal policy. Bellman\u0026rsquo;s equation is used to compute the value of a given state by considering all of the possible actions that can be taken from that state and the resulting rewards and next states. It is typically written as: V(s) = max[R(s,a) + γV(s\u0026#39;)] where V(s) is the value of state s, R(s,a) is the reward for taking action a in state s, s\u0026rsquo; is the next state resulting from taking action a in state s, and γ is a discount factor that determines the importance of future rewards relative to immediate rewards.\nBellman\u0026rsquo;s equation is used to solve many different types of optimization problems, including problems in economics, engineering, and computer science. Binary integer program A binary integer program (BIP) is a type of mathematical optimization problem in which the variables are restricted to be binary (i.e., either 0 or 1) and the objective function and constraints are linear. BIPs are often used to model decision-making problems in which the variables represent the selection or assignment of resources, and the objective function and constraints represent the costs and limitations of the problem. BIP problems can be expressed in the following standard form: maximize c^T x subject to Ax \u0026lt;= b x is binary where x is a vector of binary variables, c is a vector of coefficients representing the objective function, A is a matrix of coefficients representing the constraints, and b is a vector of constants representing the right-hand side of the constraints.\nBIP problems can be solved using a variety of algorithms, including branch and bound, cutting plane, and branch and cut. They have a wide range of applications, including resource allocation, scheduling, and transportation optimization. Binary variable In optimization, a binary variable is a type of decision variable that can take on only two values: 0 or 1. Binary variables are often used to represent choices or assignments in optimization problems, where a value of 0 indicates that the corresponding choice or assignment is not made, and a value of 1 indicates that it is made. Binary variables are commonly used in mathematical optimization to model problems in which the variables represent the selection or assignment of resources. For example, in a scheduling problem, a binary variable might be used to represent whether or not a particular machine is assigned to a particular task. In this case, a value of 0 would indicate that the machine is not assigned to the task, and a value of 1 would indicate that it is. Binary variables can be included in optimization problems using a number of different modeling languages and software packages, such as AMPL, GAMS, and CPLEX. They are often used in conjunction with other types of variables, such as continuous or integer variables, to model more complex optimization problems. Chance constraint A chance constraint is a type of constraint that is used in optimization problems to ensure that a certain probability is achieved. In a chance constraint, the constraint is expressed in terms of a probability, and the solution to the optimization problem must satisfy the constraint with a certain probability, which is usually specified in advance. Chance constraints are often used in optimization problems to model uncertainty or risk. For example, in a transportation optimization problem, a chance constraint might be used to ensure that a certain percentage of shipments arrive at their destination on time. In this case, the probability would represent the likelihood that a shipment will arrive on time, and the constraint would specify the minimum acceptable probability. Chance constraints can be difficult to handle in optimization problems, because they introduce a probabilistic element that is not present in traditional constraints. As a result, special techniques are often needed to solve optimization problems with chance constraints, such as Monte Carlo simulation or approximation methods. Clique In graph theory, a clique is a subset of vertices in an undirected graph such that every two distinct vertices in the clique are adjacent, that is, they are connected by an edge. A clique is said to be maximal if it is not a subset of any other clique in the graph. Cliques have a number of interesting properties and have been studied extensively in the field of graph theory. For example, it is easy to determine whether a given set of vertices forms a clique, and it is also easy to find the maximum size of a clique in a given graph. Cliques have a wide range of applications in computer science and other fields. They are often used in network analysis to identify groups of nodes that are highly connected, and they have also been used in machine learning and data mining to identify patterns and trends in data. Concave function In mathematics, a concave function is a function that is always below its tangent lines. Equivalently, a concave function is a function for which the line segment connecting any two points on the graph of the function lies above the graph. Concave functions have a number of interesting properties and are often used in optimization problems. For example, the graph of a concave function is always curved downward, and it has a single global minimum. As a result, it is often relatively easy to find the global minimum of a concave function using optimization algorithms. Concave functions are used in a wide variety of applications, including economics, engineering, and computer science. They are often used to model cost and utility functions, and they are also used to model constraints in optimization problems. Constraint In the context of optimization, a constraint is a condition that must be satisfied by the solution to a problem. Constraints are used in optimization to specify the limits and requirements of a problem, and they help to define the feasible region of the problem, which is the set of all possible solutions that satisfy the constraints. Constraints can be expressed in a variety of ways, depending on the type of optimization problem being solved. In linear programming, for example, constraints are typically expressed as linear inequalities or equations. In nonlinear programming, constraints can be expressed as nonlinear functions. Constraints play a central role in optimization problems, as they help to define the space of possible solutions and the objective that the optimization algorithm is trying to maximize or minimize. Constraints can be used to represent a wide range of requirements and limitations, including capacity limits, resource availability, and physical laws. Convex function In mathematics, a convex function is a function that is always above its tangent lines. Equivalently, a convex function is a function for which the line segment connecting any two points on the graph of the function lies below the graph. Convex functions have a number of interesting properties and are often used in optimization problems. For example, the graph of a convex function is always curved upwards, and it has a single global minimum. As a result, it is often relatively easy to find the global minimum of a convex function using optimization algorithms. Convex functions are used in a wide variety of applications, including economics, engineering, and computer science. They are often used to model cost and utility functions, and they are also used to model constraints in optimization problems. Convex optimization is a field of optimization that focuses specifically on optimization problems with convex objective functions and convex constraints. Convex optimization model Convex optimization is a subfield of optimization that studies optimization problems for which the objective function and the feasible region are both convex. Convex optimization problems can be formulated and solved in a variety of ways. They can be expressed as linear programming problems, quadratic programming problems, second-order cone programming problems, and semidefinite programming problems, among others. One of the key features of convex optimization problems is that they have a unique global minimum, which can be found efficiently using algorithms such as gradient descent or interior point methods. Additionally, convex optimization problems satisfy strong duality, which means that the solution to the primal problem (the original optimization problem) can be obtained from the solution to the dual problem (a related optimization problem). Convex optimization has a wide range of applications in fields such as machine learning, control engineering, and economics. It is used to solve problems such as training neural networks, designing control systems, and finding equilibrium in market models. Convex quadratic function A convex quadratic function is a function of the form: f(x) = x^T Q x + q^T x + c where Q is a symmetric matrix, q is a vector, and c is a scalar.\nThe function f is convex if and only if Q is positive semidefinite, i.e., all of its eigenvalues are nonnegative. If Q is positive definite, then f is strictly convex, meaning that it has a unique global minimum. Convex quadratic functions can be minimized using a variety of algorithms, such as gradient descent, Newton\u0026rsquo;s method, and interior point methods. They are often used in convex optimization problems as a simple and efficient way to model objective functions or constraints. Examples of convex quadratic functions include the negative log likelihood of a Gaussian distribution, the objective function of a least squares regression problem, and the objective function of a support vector machine. Convex quadratic program A convex quadratic program (CQP) is an optimization problem of the form: minimize x^T Q x + q^T x subject to Ax \u0026lt;= b l \u0026lt;= x \u0026lt;= u where x is the optimization variable, Q is a symmetric matrix, q is a vector, A is a matrix, b is a vector, l is a vector of lower bounds on x, and u is a vector of upper bounds on x.\nThe objective function f(x) = x^T Q x + q^T x is a convex quadratic function, and the feasible region defined by Ax \u0026lt;= b and l \u0026lt;= x \u0026lt;= u is a convex set. Therefore, the problem is a convex optimization problem. CQPs can be solved using a variety of algorithms, such as gradient descent, Newton\u0026rsquo;s method, and interior point methods. They are often used to model problems in fields such as machine learning, control engineering, and economics. Examples of CQPs include the problem of training a support vector machine, the problem of designing a linear controller, and the problem of finding an equilibrium in a market model. Convex set A convex set is a subset of a vector space that contains all the points on the line segments connecting any two of its points. Equivalently, a set is convex if for any two points x and y in the set and for any scalar t in the interval [0, 1], the point (1 - t)x + ty is also in the set. Convex sets have several useful properties. For example, if a function is defined on a convex set and is minimized over that set, then it has a unique global minimum. Additionally, the intersection of any two convex sets is convex, and the convex hull of any set is convex. Convex sets are important in optimization because many optimization problems can be formulated as minimizing a function over a convex set. Such problems are called convex optimization problems, and they can be solved efficiently using algorithms such as gradient descent or interior point methods. Examples of convex sets include the set of all points in a plane that are contained in a circle, the set of all positive semidefinite matrices, and the set of all points in a Euclidean space that satisfy a system of linear inequalities. Diet problem The diet problem is a classic example of a linear programming problem, which is a type of optimization problem. It involves finding the optimal combination of foods to consume in order to meet certain nutritional requirements at the lowest cost. The diet problem can be formulated as follows: minimize c^T x subject to Ax \u0026lt;= b x \u0026gt;= 0 where x is a vector of decision variables representing the amounts of each food to consume, c is a vector of costs per unit of each food, A is a matrix representing the nutritional content of each food, and b is a vector representing the required intake of each nutrient.\nThe objective is to minimize the total cost of the diet, subject to the constraints that the nutritional requirements are met. The diet problem can be solved using linear programming techniques, such as the simplex algorithm or the interior point method. It has applications in fields such as nutrition, public health, and economics. Dynamic programming Dynamic programming is a method for solving optimization problems by breaking them down into smaller subproblems and storing the solutions to these subproblems in a table or array. The solutions to the subproblems are then combined to obtain the solution to the original problem. Dynamic programming is particularly useful for problems that exhibit the following two properties: Optimal substructure: The optimal solution to a problem can be obtained by combining the optimal solutions to its subproblems. Overlapping subproblems: Many of the subproblems in the problem are identical, or \u0026ldquo;overlap,\u0026rdquo; meaning that they can be solved just once and the solution can be reused many times. Dynamic programming algorithms are typically implemented using recursion, and they can be either top-down (starting with the original problem and breaking it down into subproblems) or bottom-up (starting with the subproblems and combining them to solve the original problem). Examples of problems that can be solved using dynamic programming include the knapsack problem, the shortest path problem, and the longest common subsequence problem. Edge In the context of machine learning, an edge refers to the boundary between different classes or clusters in a dataset. For example, in a classification problem, an edge may represent the boundary between different categories of data points, such as between points that belong to the \u0026ldquo;positive\u0026rdquo; class and points that belong to the \u0026ldquo;negative\u0026rdquo; class. Edges can be used to inform the design of machine learning models, particularly in the context of supervised learning. For example, a model that is designed to classify data points into different categories might use edges to define the decision boundaries between classes. In this case, the model would aim to find a line or curve that maximally separates the points in one class from those in another class. Edges can also be used as features in machine learning models. For example, in a computer vision problem, edges in an image might be used as input to a model that is designed to classify objects in the image. Overall, the concept of an edge is important in machine learning because it represents the separation between different classes or clusters of data, and this separation can be used to inform the design and behavior of machine learning models. Feasible solution A feasible solution to an optimization problem is a solution that satisfies all of the constraints of the problem. In other words, it is a solution that lies within the feasible region defined by the constraints. For example, consider the following linear programming problem: minimize c^T x subject to Ax \u0026lt;= b x \u0026gt;= 0 In this problem, x is the optimization variable, and the constraints Ax \u0026lt;= b and x \u0026gt;= 0 define the feasible region. A feasible solution is a vector x that lies within this region, i.e., it satisfies the constraints Ax \u0026lt;= b and x \u0026gt;= 0. Feasible solutions are important in optimization because they represent the set of possible solutions to the problem. The goal of an optimization algorithm is to find the optimal solution, which is the feasible solution that minimizes (or maximizes) the objective function. Fixed charge In the context of optimization, a fixed charge is a cost that is independent of the decision variables and is not included in the objective function. Instead, it is treated as a constraint on the problem. For example, consider the following linear programming problem: minimize c^T x subject to Ax \u0026lt;= b x \u0026gt;= 0 F(x) \u0026lt;= f In this problem, x is the optimization variable, c is a vector of costs per unit of each decision variable, and Ax \u0026lt;= b and x \u0026gt;= 0 define the feasible region. The constraint F(x) \u0026lt;= f represents a fixed charge on the problem. Fixed charges are important in optimization because they can represent costs or constraints that are not captured by the objective function or the constraints on the decision variables. For example, a fixed charge might represent a budget constraint, a regulatory requirement, or a capacity constraint. Optimization algorithms can be used to find the optimal solution to a problem with fixed charges by taking these constraints into account. The optimal solution is the feasible solution that minimizes (or maximizes) the objective function subject to all of the constraints on the problem. Flow In the context of optimization, flow typically refers to the movement of goods, resources, or people from one location to another. Optimization problems that involve flow often involve finding the optimal allocation of resources or the optimal path for goods or people to follow. Examples of optimization problems that involve flow include network flow problems, transportation problems, and logistics problems. These problems can be formulated as linear programming problems, integer programming problems, or network flow problems, depending on the specifics of the problem. In a network flow problem, the goal is to find the optimal flow of goods or resources through a network of nodes and edges, subject to capacity constraints on the edges and demand or supply constraints at the nodes. In a transportation problem, the goal is to find the optimal allocation of goods from a set of sources to a set of destinations, subject to capacity constraints on the transportation vehicles and demand constraints at the destinations. In a logistics problem, the goal is to find the optimal route or schedule for moving goods from one location to another, subject to time and resource constraints. Overall, the concept of flow is important in optimization because it represents the movement of goods, resources, or people, and the optimization of this flow can lead to improved efficiency and cost savings. Global optimum/maximum/minimum The global optimum (or global maximum or minimum, depending on the context) of a function is the point at which the function achieves its highest (or lowest) value. For a function defined over a continuous domain, the global optimum is the point at which the function has a local minimum (or maximum) and there are no other points with a lower (or higher) value. In optimization, the goal is often to find the global optimum of an objective function subject to certain constraints. For example, in a linear programming problem, the goal is to find the point at which the objective function is minimized (or maximized) subject to a set of linear constraints. In this case, the global optimum is the point at which the objective function has the lowest (or highest) value among all points that satisfy the constraints. The global optimum is important because it represents the best possible solution to an optimization problem. In contrast, a local optimum is a point at which the objective function has a local minimum (or maximum) but may not be the global optimum. The global optimum can be found using a variety of optimization algorithms, such as gradient descent, Newton\u0026rsquo;s method, and interior point methods. These algorithms can be used to find the global optimum of a wide range of optimization problems, including linear programming problems, nonlinear programming problems, and convex optimization problems. Greedy algorithm A graph is a mathematical structure used to represent relationships between objects. It consists of a set of vertices (also called nodes) and a set of edges connecting the vertices. The vertices in a graph represent the objects, and the edges represent the relationships between the objects. The edges can be directed (meaning that they have a specific starting and ending vertex) or undirected (meaning that they do not have a specific direction). Graphs are commonly used to represent networks, such as social networks, transportation networks, and communication networks. They are also used to represent data structures, such as trees and maps, and to model optimization problems, such as the shortest path problem and the traveling salesman problem. There are many different types of graphs, including directed graphs, undirected graphs, weighted graphs (where the edges have weights or costs associated with them), and bipartite graphs (where the vertices can be divided into two disjoint sets and the edges only connect vertices in different sets). Overall, the concept of a graph is a fundamental one in mathematics and computer science, and it has many applications in fields such as data analysis, machine learning, and operations research. Improving direction A graph is a mathematical structure used to represent relationships between objects. It consists of a set of vertices (also called nodes) and a set of edges connecting the vertices. The vertices in a graph represent the objects, and the edges represent the relationships between the objects. The edges can be directed (meaning that they have a specific starting and ending vertex) or undirected (meaning that they do not have a specific direction). Graphs are commonly used to represent networks, such as social networks, transportation networks, and communication networks. They are also used to represent data structures, such as trees and maps, and to model optimization problems, such as the shortest path problem and the traveling salesman problem. There are many different types of graphs, including directed graphs, undirected graphs, weighted graphs (where the edges have weights or costs associated with them), and bipartite graphs (where the vertices can be divided into two disjoint sets and the edges only connect vertices in different sets). Overall, the concept of a graph is a fundamental one in mathematics and computer science, and it has many applications in fields such as data analysis, machine learning, and operations research. Initialization In the context of optimization, initialization refers to the process of setting the initial values of the decision variables or other parameters of the optimization algorithm. These initial values are used to begin the optimization process, and they can have a significant impact on the convergence and performance of the algorithm. The choice of initial values can depend on the specific optimization problem being solved and the optimization algorithm being used. For example, in a gradient descent algorithm, the initial values of the decision variables might be set to random values or to the solution of a related optimization problem. In an interior point algorithm, the initial values of the decision variables and the algorithm parameters might be chosen based on the properties of the problem, such as the condition number of the constraint matrix. Initialization is important in optimization because it can affect the convergence and performance of the algorithm. Careful initialization can help to ensure that the algorithm converges to a good solution and does not get stuck in a local minimum (or maximum). On the other hand, poor initialization can lead to slow convergence or failure to find the optimal solution. Integer program An integer program is an optimization problem in which some or all of the decision variables are required to be integers. Integer programming problems are often used to model problems that involve discrete choices or decisions, such as the selection of a set of products to manufacture or the allocation of resources to different projects. Integer programming problems can be formulated in a variety of ways, depending on the specific constraints and objective of the problem. For example, a common form of integer programming problem is the linear integer programming problem, which has the following form: minimize c^T x subject to Ax \u0026lt;= b x \u0026gt;= 0 x is integer In this problem, x is the optimization variable, and c is a vector of costs per unit of each decision variable. The constraints Ax \u0026lt;= b and x \u0026gt;= 0 define the feasible region, and the constraint x is integer requires that the decision variables must be integers. Integer programming problems can be difficult to solve, because the feasible region is typically discrete and may not be smooth or continuous. Specialized algorithms, such as branch and bound and cutting plane algorithms, can be used to solve integer programming problems. Linear equation A linear equation is an equation in which the highest power of the variable(s) is 1. For example, the equation y = 2x + 1 is a linear equation because the highest power of x is 1. Linear equations can take many forms, but they all have the property that the highest power of the variable(s) is 1. Linear equations can be written in the standard form: ax + by = c where a and b are constants, and x and y are variables. The standard form of a linear equation is useful because it allows us to easily identify the slope and y-intercept of the line described by the equation. The slope of the line is represented by the coefficient of x (a), and the y-intercept is the point where the line crosses the y-axis (the value of y when x = 0), which is represented by the constant term (c).\nLinear equations can also be written in slope-intercept form: y = mx + b where m is the slope of the line and b is the y-intercept. This form of the equation is useful when we want to find the equation of a line given its slope and y-intercept.\nLinear equations can have one or more variables, and they can have any number of terms. For example, the equation 2x + 3y - 4z = 5 is a linear equation because the highest power of any of the variables (x, y, and z) is 1. Linear function A linear function is a function of the form f(x) = mx + b, where x is the input variable and f(x) is the output variable. The constants m and b are called the slope and y-intercept of the function, respectively. The slope is a measure of how steep the line described by the function is, and the y-intercept is the point where the line crosses the y-axis (the value of f(x) when x = 0). Linear functions have the property that the graph of the function is a straight line. The slope of the line is determined by the value of m, and the y-intercept is determined by the value of b. For example, the function f(x) = 2x + 1 has a slope of 2 and a y-intercept of (0, 1). Linear functions are useful in many applications because they are easy to work with and understand. They are also widely used in mathematics and science because they often provide a good approximation to real-world phenomena that exhibit linear behavior. Linear inequality A linear inequality is an inequality that involves a linear function. A linear function is a function of the form f(x) = mx + b, where m and b are constants and x is a variable. The graph of a linear inequality is a region of the coordinate plane that satisfies the inequality. Linear inequalities can be represented in one of two ways: in standard form or in slope-intercept form. In standard form, a linear inequality is written as: ax + by \u0026gt; c or ax + by \u0026lt; c or ax + by ≥ c or ax + by ≤ c where a, b, and c are constants and x and y are variables. The standard form of a linear inequality is useful because it allows us to easily identify the slope and y-intercept of the line described by the inequality.\nIn slope-intercept form, a linear inequality is written as: y \u0026gt; mx + b or y \u0026lt; mx + b where m is the slope of the line and b is the y-intercept. This form of the inequality is useful when we want to find the inequality that defines a particular region of the coordinate plane.\nThe solution to a linear inequality is the set of all points that satisfy the inequality. The graph of a linear inequality is a region of the coordinate plane that includes all of the points that satisfy the inequality. The graph of a linear inequality is often represented by shading the region of the coordinate plane that satisfies the inequality. Linear program A linear program (LP) is a mathematical optimization problem in which the objective function and the constraints are all linear. Linear programs are used to find the maximum or minimum value of a linear objective function subject to a set of linear inequality or equality constraints. Linear programs have the following general form: maximize c1x1 + c2x2 + ... + cnxn subject to: a11x1 + a12x2 + ... + a1nxn ≤ b1 a21x1 + a22x2 + ... + a2nxn ≤ b2 ... am1x1 + am2x2 + ... + amnxn ≤ bm where x1, x2, \u0026hellip;, xn are the decision variables, c1, c2, \u0026hellip;, cn are the objective coefficients, aij are the constraint coefficients, and b1, b2, \u0026hellip;, bm are the right-hand side values.\nLinear programs can be solved using a variety of techniques, including simplex method, interior point method, and duality. These techniques are used to find the values of the decision variables that maximize or minimize the objective function subject to the constraints. Linear programs are widely used in a variety of fields, including economics, engineering, and operations research, to model and solve real-world problems involving optimization. Local optimum/maximum/minimum A local optimum, maximum, or minimum is a point in a function where the function has a locally best value. In other words, it is a point where the function has a value that is better than the values of the function in the immediate vicinity of the point. For example, consider a function f(x) defined on the real numbers. If there exists a value x0 such that f(x0) is greater than or equal to f(x) for all x in a certain interval around x0, then x0 is a local maximum of the function. Similarly, if there exists a value x0 such that f(x0) is less than or equal to f(x) for all x in a certain interval around x0, then x0 is a local minimum of the function. It\u0026rsquo;s important to note that a local optimum, maximum, or minimum is not necessarily the global optimum, maximum, or minimum of the function. The global optimum, maximum, or minimum is the point where the function has the best value over its entire domain. For example, if f(x) has a local maximum at x0, it does not necessarily mean that f(x0) is the highest possible value that the function can take on. It could be that there exists another point x1 where the function has an even higher value. In this case, x1 would be the global maximum of the function. Louvain algorithm The Louvain algorithm is a fast and efficient method for community detection in large networks. It is a heuristic algorithm that is used to find the community structure of a network by optimizing a measure called modularity. Modularity is a measure of the quality of a partition of a network into communities, and it is defined as the fraction of the edges that fall within the communities minus the expected fraction of edges that fall within the communities in a random network with the same degree distribution as the original network. The Louvain algorithm operates in two phases. In the first phase, it starts with each node in its own community and iteratively merges pairs of communities based on the modularity gain of the merge. In the second phase, it aggregates the nodes in the same community into a supernode and repeats the process on the reduced network until the modularity cannot be improved further. The Louvain algorithm is fast and scalable, making it well-suited for large networks. It has been applied to a wide variety of networks, including social networks, biological networks, and transportation networks. Markov decision process A Markov decision process (MDP) is a mathematical framework for modeling decision-making problems in which an agent must choose actions in a sequence of steps in order to maximize some reward. MDPs are used in many areas of artificial intelligence, including reinforcement learning, to solve problems involving optimization under uncertainty. An MDP is defined by a set of states, a set of actions, a transition model, and a reward function. The states represent the possible situations that the agent can be in. The actions represent the choices available to the agent at each step. The transition model specifies the probability of transitioning from one state to another as a result of taking a particular action. The reward function specifies the rewards that the agent receives for being in a particular state or taking a particular action. The goal of an MDP is to find a policy, which is a function that specifies the action to take in each state. The optimal policy is the policy that maximizes the expected cumulative reward over time. MDPs can be solved using various algorithms, such as value iteration, policy iteration, and Q-learning. Mathematical programming Mathematical programming is a branch of applied mathematics that deals with the optimization of systems described by mathematical models. It is a broad field that encompasses a variety of optimization techniques, including linear programming, nonlinear programming, integer programming, and mixed-integer programming. The goal of mathematical programming is to find the values of the decision variables that optimize an objective function subject to a set of constraints. The objective function and the constraints are typically represented as a system of equations or inequalities that must be satisfied. Mathematical programming is used in a wide range of applications, including economics, engineering, and operations research. It is used to model and solve real-world problems involving the optimization of resources, such as time, money, and materials. There are many algorithms and software packages available for solving mathematical programming problems. These algorithms and software packages use a variety of techniques, such as linear algebra, gradient descent, and optimization algorithms, to find the optimal solution to the problem. Maximization problem A maximization problem is a type of optimization problem in which the goal is to find the maximum value of a function. Maximization problems are commonly encountered in a variety of fields, including economics, engineering, and operations research. A maximization problem is typically written in the form: maximize f(x) subject to: g1(x) ≤ b1 g2(x) ≤ b2 ... gn(x) ≤ bn where f(x) is the objective function to be maximized, g1(x), g2(x), \u0026hellip;, gn(x) are the constraint functions, and b1, b2, \u0026hellip;, bn are the right-hand side values. The variables x1, x2, \u0026hellip;, xn are the decision variables, and the values of these variables that maximize the objective function subject to the constraints are called the optimal solutions.\nThere are many algorithms and software packages available for solving maximization problems. These algorithms and software packages use a variety of techniques, such as linear algebra, gradient descent, and optimization algorithms, to find the optimal solution to the problem. Maximum flow problem The maximum flow problem is a problem in graph theory that involves finding the maximum flow that can be sent through a network from a source to a sink. The problem can be formalized as follows: given a weighted directed graph with a source vertex s and a sink vertex t, find the maximum flow from s to t such that the flow on any edge does not exceed its capacity. The maximum flow problem is a fundamental problem in computer science and has numerous applications, including network design, transportation planning, and resource allocation. It is also closely related to the minimum cut problem, which involves finding the minimum-capacity cut that separates the source from the sink in the network. There are many algorithms for solving the maximum flow problem, including the Ford-Fulkerson algorithm and the Dinic algorithm. These algorithms are used to find the maximum flow through a network by iteratively increasing the flow along paths from the source to the sink until no additional flow is possible. Minimization problem A minimization problem is a type of optimization problem in which the goal is to find the minimum value of a function. Minimization problems are commonly encountered in a variety of fields, including economics, engineering, and operations research. A minimization problem is typically written in the form: minimize f(x) subject to: g1(x) ≤ b1 g2(x) ≤ b2 ... gn(x) ≤ bn where f(x) is the objective function to be minimized, g1(x), g2(x), \u0026hellip;, gn(x) are the constraint functions, and b1, b2, \u0026hellip;, bn are the right-hand side values. The variables x1, x2, \u0026hellip;, xn are the decision variables, and the values of these variables that minimize the objective function subject to the constraints are called the optimal solutions.\nThere are many algorithms and software packages available for solving minimization problems. These algorithms and software packages use a variety of techniques, such as linear algebra, gradient descent, and optimization algorithms, to find the optimal solution to the problem. Modularity Modularity is a measure of the quality of a partition of a network into communities. It is defined as the fraction of the edges that fall within the communities minus the expected fraction of edges that fall within the communities in a random network with the same degree distribution as the original network. Modularity is often used as a metric for evaluating the quality of community detection algorithms. It is a widely used measure in the field of network science and has been applied to a variety of real-world networks, including social networks, biological networks, and technological networks. Modularity is a useful measure because it captures the intuition that a good partition of a network into communities should have a higher density of edges within the communities than between the communities. A high value of modularity indicates that the communities in the partition are well-defined and distinct. There are many algorithms for optimizing modularity, including the Louvain algorithm and the spectral clustering algorithm. These algorithms are used to find the partition of a network into communities that maximizes the modularity. Network A network is a group of interconnected entities or nodes. Networks can be found in many different contexts, such as social networks, transportation networks, and computer networks. In the context of social networks, a network is a group of people who are connected to each other by some type of relationship, such as friendship, kinship, or professional association. In transportation networks, a network is a group of locations connected by transportation links, such as roads, railways, or air routes. In computer networks, a network is a group of computers and other devices connected by communication channels, such as cables or wireless connections, for the purpose of exchanging data. Networks can be represented using a graph, which is a mathematical structure consisting of vertices (also called nodes) and edges. The nodes represent the entities in the network, and the edges represent the relationships or connections between the entities. Networks are often analyzed using techniques from graph theory and network science. These techniques are used to study the structure and properties of networks, such as the number of connections per node, the connectivity of the network, and the centrality of the nodes. Network optimization problem A network is a group of interconnected entities or nodes. Networks can be found in many different contexts, such as social networks, transportation networks, and computer networks. In the context of social networks, a network is a group of people who are connected to each other by some type of relationship, such as friendship, kinship, or professional association. In transportation networks, a network is a group of locations connected by transportation links, such as roads, railways, or air routes. In computer networks, a network is a group of computers and other devices connected by communication channels, such as cables or wireless connections, for the purpose of exchanging data. Networks can be represented using a graph, which is a mathematical structure consisting of vertices (also called nodes) and edges. The nodes represent the entities in the network, and the edges represent the relationships or connections between the entities. Networks are often analyzed using techniques from graph theory and network science. These techniques are used to study the structure and properties of networks, such as the number of connections per node, the connectivity of the network, and the centrality of the nodes. Node In the context of a graph, a node is a vertex or a point that represents an entity or an object in the graph. Nodes are typically represented by circles or points in a graph, and they are connected to other nodes by edges. In the context of a tree, a node is a point at which one or more branches originate. The top node in a tree is called the root, and the nodes that do not have any children are called leaf nodes. In the context of a network, a node is a device or a point in the network that is connected to other nodes by communication links. Nodes in a network can be computers, routers, switches, or any other device that is capable of sending and receiving data. In the context of a graph or network, nodes can have attributes, such as a label or a weight, which describe the properties of the node. The structure and properties of nodes in a graph or network are often studied using techniques from graph theory and network science. Non-convex program A non-convex program is a type of optimization problem in which the objective function or the constraint functions are not convex. A convex function is a function that satisfies the property of convexity, which means that the line connecting any two points on the graph of the function lies above the graph. A non-convex function is a function that does not satisfy this property. Non-convex programs are more difficult to solve than convex programs because they may have multiple local optima, rather than a single global optimum. This means that there may be multiple points that are locally optimal, but the globally optimal solution may not be attainable by starting from any of these local optima. Non-convex programs can be solved using a variety of techniques, including local search algorithms, global search algorithms, and gradient-based algorithms. These algorithms are used to find the optimal solution to the problem by exploring the solution space and searching for points that improve the objective function. Examples of non-convex programs include nonlinear programming, integer programming, and mixed-integer programming. Non-convex programs are useful for modeling and solving real-world problems involving the optimization of resources, such as time, money, and materials. Non-negativity constraints Non-negativity constraints are constraints that require the decision variables in an optimization problem to be non-negative. In other words, the decision variables are required to be greater than or equal to zero. Non-negativity constraints are common in optimization problems because many real-world problems involve quantities that cannot be negative, such as the number of items produced, the amount of money spent, or the volume of a fluid. Non-negativity constraints are typically written in the form: x1 ≥ 0 x2 ≥ 0 ... xn ≥ 0 where x1, x2, ..., xn are the decision variables. Non-negativity constraints can be incorporated into an optimization problem by adding them as inequality constraints to the problem. For example, consider the following linear programming problem: maximize c1x1 + c2x2 + ... + cnxn subject to: a11x1 + a12x2 + ... + a1nxn ≤ b1 a21x1 + a22x2 + ... + a2nxn ≤ b2 ... am1x1 + am2x2 + ... + amnxn ≤ bm x1 ≥ 0 x2 ≥ 0 ... xn ≥ 0 In this problem, the non-negativity constraints x1 ≥ 0, x2 ≥ 0, \u0026hellip;, xn ≥ 0 ensure that the decision variables are non-negative. Objective function In optimization, an objective function is a function that represents the goal of the optimization. The goal of the optimization is to find the values of the decision variables that either maximize or minimize the objective function. The objective function is typically written as a mathematical expression that depends on the decision variables. For example, in a linear programming problem, the objective function is a linear function of the decision variables. In a nonlinear programming problem, the objective function may be a nonlinear function of the decision variables. The objective function is typically written in the form: f(x1, x2, ..., xn) where x1, x2, \u0026hellip;, xn are the decision variables. -`The objective function is an important component of an optimization problem because it determines the goal of the optimization. The values of the decision variables that optimize the objective function are called the optimal solutions.\nOptimal solution In optimization, an optimal solution is a set of values for the decision variables that either maximizes or minimizes the objective function. The objective function is a mathematical expression that represents the goal of the optimization, and the decision variables are the variables that are being optimized. The optimal solution to an optimization problem is the solution that satisfies all of the constraints of the problem and either maximizes or minimizes the objective function, depending on the type of optimization problem. There may be multiple optimal solutions to an optimization problem, or there may be none. If there are multiple optimal solutions, the problem is said to have multiple optima. If there are no optimal solutions, the problem is said to be infeasible. The optimal solution to an optimization problem can be found using a variety of algorithms and techniques, depending on the specific problem and the structure of the objective function and constraints. These techniques may include linear programming, nonlinear programming, and heuristics. Optimization Optimization is the process of finding the best solution to a problem among a set of possible solutions. Optimization problems are common in many fields, including economics, engineering, and operations research. Optimization problems can be classified into several categories based on the type of objective function and the type of constraints. For example, linear programming involves optimizing a linear objective function subject to linear constraints, while nonlinear programming involves optimizing a nonlinear objective function subject to nonlinear constraints. The goal of optimization is to find the values of the decision variables that either maximize or minimize the objective function subject to the constraints of the problem. The values of the decision variables that optimize the objective function are called the optimal solutions. There are many algorithms and techniques for solving optimization problems, including linear programming, nonlinear programming, integer programming, and heuristics. These algorithms and techniques use a variety of approaches, such as linear algebra, gradient descent, and optimization algorithms, to find the optimal solution to the problem. Robust solution A robust solution is a solution that is resistant to changes in the input data or assumptions of the problem. In other words, a robust solution is a solution that performs well under a wide range of conditions or scenarios. Robust solutions are often desired in optimization problems because real-world problems often involve uncertainty or variability in the input data or assumptions. A robust solution is able to withstand such uncertainty or variability and still produce good results. There are several ways to design robust solutions in optimization. One approach is to use robust optimization, which is a methodology that seeks to find solutions that are robust to uncertainty in the input data. Robust optimization involves optimizing an objective function that is a function of both the decision variables and the uncertain parameters, subject to constraints on both the decision variables and the uncertain parameters. Another approach is to use sensitivity analysis to identify the key parameters or assumptions that have the greatest impact on the solution, and to design the solution in a way that is insensitive to variations in these parameters. This can be done by using techniques such as scenario analysis, which involves analyzing the solution for a range of different scenarios. Robust solutions are useful for modeling and solving real-world problems because they are able to perform well under a wide range of conditions and uncertainties. Shortest path problem The shortest path problem is a problem in graph theory that involves finding the shortest path between two nodes in a graph. The shortest path is the path with the minimum number of edges or the minimum distance between the two nodes. The shortest path problem is a fundamental problem in computer science and has numerous applications, including network design, transportation planning, and resource allocation. It is closely related to the minimum spanning tree problem, which involves finding the minimum set of edges that connects all of the nodes in a graph. There are many algorithms for solving the shortest path problem, including Dijkstra\u0026rsquo;s algorithm and the A* algorithm. These algorithms are used to find the shortest path through a graph by exploring the edges of the graph and updating the shortest known distance to each node as the algorithm progresses. The shortest path problem can be generalized to include additional constraints or objectives, such as finding the shortest path with the minimum number of edges, the minimum cost, or the minimum time. These variations of the shortest path problem are known as the single-source shortest path problem, the single-pair shortest path problem, and the all-pairs shortest path problem, respectively. Solution (in the optimization sense) In optimization, a solution is a set of values for the decision variables that satisfies the constraints of the problem. The decision variables are the variables that are being optimized, and the constraints are the limitations or requirements that must be satisfied in the solution. The solution to an optimization problem is the set of values for the decision variables that either maximizes or minimizes the objective function, depending on the type of optimization problem. The objective function is a mathematical expression that represents the goal of the optimization. There may be multiple solutions to an optimization problem, or there may be none. If there are multiple solutions, the problem is said to have multiple optima. If there are no solutions, the problem is said to be infeasible. The solution to an optimization problem can be found using a variety of algorithms and techniques, depending on the specific problem and the structure of the objective function and constraints. These techniques may include linear programming, nonlinear programming, and heuristics. State In the context of optimization, a state is a set of values for the variables that defines the current configuration of the system being optimized. The variables in the state may include the decision variables, which are the variables that are being optimized, as well as other variables that describe the system, such as the state variables and the parameters. In the context of a dynamic optimization problem, the state at a given time represents the configuration of the system at that time. The state of the system at each time point is typically represented by a vector of variables, and the evolution of the state over time is described by a system of differential equations or a difference equation. In the context of a discrete optimization problem, the state at a given time represents the configuration of the system at that time, and the state at each time point is typically represented by a vector of variables. The state of the system evolves over time as the decision variables are updated according to the optimization algorithm. The state of the system plays an important role in optimization because it determines the objective function and the constraints of the problem. The optimal solution to the optimization problem is the set of values for the decision variables that either maximizes or minimizes the objective function subject to the constraints of the problem, given the current state of the system. Step size In optimization, the step size is a parameter that determines the size of the steps taken by an optimization algorithm as it searches for the optimal solution to a problem. The step size is often used in gradient-based optimization algorithms, such as gradient descent and stochastic gradient descent, which use the gradient of the objective function to guide the search for the optimal solution. The step size plays a crucial role in the convergence of the optimization algorithm. If the step size is too small, the algorithm may take a long time to converge to the optimal solution. If the step size is too large, the algorithm may overshoot the optimal solution or even diverge. There are several approaches to setting the step size in an optimization algorithm. One approach is to use a fixed step size, which is a constant value that is chosen manually or based on some heuristics. Another approach is to use a variable step size, which is a step size that changes over the course of the optimization. Variable step sizes can be determined using techniques such as line search or trust region methods. The step size is an important hyperparameter in optimization algorithms and can have a significant impact on the performance of the algorithm. It is important to choose an appropriate step size for the specific optimization problem and the specific optimization algorithm being used. Stochastic dynamic program A stochastic dynamic program (SDP) is a type of optimization problem that involves finding the optimal decision rule for a system that evolves over time in the presence of uncertainty. SDPs are used to model and solve problems in which the future states of the system are uncertain and depend on both the current state of the system and the actions taken by the decision-maker. In an SDP, the decision variables are the actions that are taken at each time point, and the objective is to maximize or minimize a function of the actions and the future states of the system. The constraints of the problem may include both state constraints, which limit the possible values of the future states, and action constraints, which limit the possible values of the actions. SDPs are solved using dynamic programming algorithms, which involve breaking the optimization problem into smaller subproblems and solving these subproblems recursively. The solution to the SDP is the optimal decision rule, which is a function that maps the current state of the system to the optimal action. SDPs are useful for modeling and solving real-world problems involving the optimization of resources over time in the presence of uncertainty, such as resource allocation problems and risk management problems. Stochastic optimization Stochastic optimization is a type of optimization that involves finding the optimal solution to a problem in the presence of uncertainty. Stochastic optimization problems are characterized by randomness or uncertainty in the input data or assumptions of the problem. In stochastic optimization, the objective is to find the optimal solution that is robust to the uncertainty or variability in the input data. This is often achieved by minimizing the expected value of the objective function, which is the average value of the objective function over the distribution of the uncertain parameters. Stochastic optimization can be used to solve a variety of problems, including resource allocation problems, portfolio optimization problems, and risk management problems. There are many algorithms and techniques for solving stochastic optimization problems, including stochastic gradient descent, Monte Carlo simulation, and dynamic programming. These algorithms and techniques use a variety of approaches, such as sampling and statistical techniques, to find the optimal solution to the problem. Stochastic optimization is useful for modeling and solving real-world problems because it allows for the incorporation of uncertainty or variability into the optimization process. This is particularly important in situations where the input data or assumptions of the problem are uncertain or subject to change. Uncertainty Uncertainty in optimization refers to the presence of randomness or variability in the input data or assumptions of an optimization problem. Uncertainty can arise in many forms, such as random errors in the data, unknown parameters, or stochastic processes. Uncertainty can be incorporated into an optimization problem in several ways. One approach is to use stochastic optimization, which is a type of optimization that involves finding the optimal solution to a problem in the presence of uncertainty. In stochastic optimization, the objective is to find the optimal solution that is robust to the uncertainty or variability in the input data. This is often achieved by minimizing the expected value of the objective function, which is the average value of the objective function over the distribution of the uncertain parameters. Another approach is to use robust optimization, which is a methodology that seeks to find solutions that are robust to uncertainty in the input data. Robust optimization involves optimizing an objective function that is a function of both the decision variables and the uncertain parameters, subject to constraints on both the decision variables and the uncertain parameters. Uncertainty can be a challenging aspect of optimization because it can make it difficult to predict the behavior of the optimization problem and to determine the optimal solution. However, accounting for uncertainty in the optimization process can be important for modeling and solving real-world problems because it allows for the incorporation of variability and randomness into the optimization process. Vertex In optimization, a vertex is a point on the feasible region of an optimization problem that satisfies all of the constraints of the problem. The feasible region is the set of points that satisfy the constraints of the problem, and a vertex is a point on the boundary of the feasible region. In a linear programming problem, the feasible region is a polyhedron, and the vertices are the points where the constraints intersect. The optimal solution to the linear programming problem is either a vertex of the feasible region or a point on a constraint that is not a vertex. In a nonlinear programming problem, the feasible region is a more complex shape, and the vertices may or may not be part of the optimal solution. The optimal solution to a nonlinear programming problem is typically found using algorithms such as gradient descent or conjugate gradient, which search for the optimal solution by moving from one point to another along the feasible region. The vertices of the feasible region are important in optimization because they represent the extreme points of the region, and the optimal solution may be found at a vertex or along a constraint. Understanding the structure of the feasible region and the location of the vertices can be helpful for solving optimization problems. Probability based models Action In probability-based models, an action is a decision or course of action that is taken by a decision-maker in a given situation. The action is chosen based on the available information and the objectives of the decision-maker. In probability-based models, the action is typically represented by a random variable, which is a variable that represents the possible outcomes of the action. The probability of each outcome is determined by the information available to the decision-maker and the objectives of the decision. Probability-based models are used in many fields, including economics, finance, and operations research, to model and solve decision-making problems involving uncertainty or risk. These models are used to determine the optimal action in a given situation, given the available information and the objectives of the decision-maker. Examples of probability-based models include decision trees, Markov decision processes, and Bayesian networks. These models are used to represent the uncertain outcomes of the action and to determine the optimal action based on the probabilities of the outcomes. Arrival rate The arrival rate is a measure of the frequency at which events or customers arrive at a system or service. In the context of queueing theory, the arrival rate is the rate at which customers arrive at a service or queue, and is typically measured in units of time, such as customers per minute or customers per hour. The arrival rate is an important parameter in queueing models because it determines the number of customers that are waiting to be served at a given time. The arrival rate is often used in conjunction with other parameters, such as the service rate, to analyze the performance of a queueing system. The arrival rate can be constant or variable, depending on the nature of the system being analyzed. For example, in a service system with a fixed arrival rate, the number of customers arriving at the system is constant over time. In a system with a variable arrival rate, the number of customers arriving at the system may vary over time. The arrival rate can be estimated using historical data or by analyzing the characteristics of the system or the customers. The arrival rate is an important factor in the design and analysis of queueing systems and is used to determine the capacity and performance of the system. Balking In the context of queueing theory, balking refers to the behavior of customers who decide not to join a queue or wait for service when confronted with a long wait. Customers may balk for a variety of reasons, such as time constraints, impatience, or dissatisfaction with the service. Balking is an important consideration in the analysis and design of queueing systems because it can have a significant impact on the performance of the system. When customers balk, the system experiences a reduction in the number of customers being served, which can affect the utilization and efficiency of the system. Balking can be modeled using a balking function, which is a function that describes the probability that a customer will balk as a function of the waiting time or the number of customers in the queue. The balking function can be used to predict the impact of balking on the performance of the queueing system and to identify strategies for reducing the rate of balking. Balking is an important factor in the analysis of queueing systems and is often taken into account in the design of service systems to ensure that the system is efficient and effective in serving customers. Bayes\u0026rsquo; theorem/Bayes\u0026rsquo; rule Bayes\u0026rsquo; theorem is a fundamental principle in probability theory that describes the relationship between the probability of an event and the probability of other related events. It is used to calculate the probability of an event based on the probability of other events that are related to it. The theorem is named after Thomas Bayes, an 18th-century mathematician and statistician who developed the theorem to describe the probability of an event based on prior knowledge or evidence. Bayes\u0026rsquo; theorem is often expressed as follows: P(A|B) = (P(B|A) * P(A)) / P(B) where P(A|B) is the conditional probability of event A given event B, P(B|A) is the conditional probability of event B given event A, P(A) is the probability of event A, and P(B) is the probability of event B.\nBayes\u0026rsquo; theorem is used in many fields, including statistics, machine learning, and artificial intelligence, to update the probability of an event based on new evidence or information. It is a powerful tool for making decisions under uncertainty and is widely used in statistical analysis and data modeling. Continuous-time simulation Continuous-time simulation is a type of simulation in which the system being simulated is represented by a set of continuous variables that change over time. Continuous-time simulation is used to model and analyze systems in which the state of the system is continuously changing, such as physical systems, chemical processes, and biological systems. In continuous-time simulation, the evolution of the system over time is typically described using differential equations, which are mathematical equations that describe the rate of change of a variable with respect to time. The differential equations are used to compute the values of the variables at each time step, and the simulation is run for a specified period of time. Continuous-time simulation is useful for modeling and analyzing systems that involve continuous processes or phenomena, such as fluid flow, heat transfer, and chemical reactions. It is also useful for analyzing the behavior of systems over long periods of time, as it allows for the modeling of small changes in the system that may have significant impacts over time Continuous-time simulation is a powerful tool for understanding and predicting the behavior of complex systems and is used in a variety of fields, including engineering, science, and business. Decision point A decision point is a point in time at which a decision must be made. Decision points are often encountered in decision-making processes and can involve choices between different options or courses of action. Decision points can occur at various stages of a process or in different contexts, such as business, finance, or personal decision-making. In many cases, decision points involve a trade-off between different objectives or conflicting goals, and the decision must be made based on the available information and the desired outcomes. Decision points can be modeled and analyzed using decision analysis techniques, such as decision trees, utility analysis, and decision tables. These techniques are used to evaluate the different options and to identify the optimal decision based on the desired outcomes and the probabilities or consequences of each option. Decision points are an important aspect of decision-making and can have significant consequences on the outcomes of a process or the overall success of an endeavor. It is important to carefully consider the options and to make informed decisions at decision points in order to achieve the desired outcomes. Deterministic simulation Deterministic simulation is a type of simulation in which the system being simulated is represented by a set of fixed, known variables, and the evolution of the system over time is determined by the values of these variables. Deterministic simulation is used to model and analyze systems in which the behavior of the system is completely determined by the initial conditions and the underlying rules or laws governing the system. In deterministic simulation, the values of the variables are known with certainty, and the evolution of the system is determined by the values of these variables and the rules governing the system. The simulation is run for a specified period of time, and the values of the variables are computed at each time step based on the rules of the system. Deterministic simulation is useful for modeling and analyzing systems that are well-understood and can be accurately represented by a set of fixed variables, such as physical systems, chemical processes, and mathematical models. It is also useful for analyzing the behavior of systems over short periods of time, as it does not allow for the modeling of randomness or uncertainty. Deterministic simulation is a powerful tool for understanding and predicting the behavior of simple systems and is used in a variety of fields, including engineering, science, and business. Discrete-event simulation Discrete-event simulation is a type of simulation in which the system being simulated is represented by a set of discrete variables that change at specific points in time. Discrete-event simulation is used to model and analyze systems in which the state of the system changes in discrete steps or events, such as manufacturing systems, computer networks, and business processes. In discrete-event simulation, the evolution of the system over time is represented by a series of discrete events, such as the arrival of a customer at a service system or the completion of a manufacturing process. The simulation is run for a specified period of time, and the events are scheduled and executed at specific times based on the rules of the system. Discrete-event simulation is useful for modeling and analyzing systems that involve discrete events or processes, such as manufacturing systems, transportation systems, and communication networks. It is also useful for analyzing the behavior of systems over short periods of time, as it allows for the modeling of complex interactions between the events. Discrete-event simulation is a powerful tool for understanding and predicting the behavior of complex systems and is used in a variety of fields, including engineering, science, and business. Empirical Bayes model An empirical Bayes model is a statistical model that uses observed data to estimate the parameters of a Bayesian model. Bayesian models are a type of statistical model that involves the use of prior knowledge or assumptions to make inferences about the probability of an event. The parameters of a Bayesian model represent the degree of belief or uncertainty about the event. In an empirical Bayes model, the parameters of the model are estimated from observed data rather than being specified a priori. This allows the model to adapt to the data and to make more accurate predictions about the probability of the event. Empirical Bayes models are used in a variety of fields, including statistics, machine learning, and artificial intelligence, to estimate the parameters of Bayesian models and to make predictions about the probability of an event. They are particularly useful for making predictions in situations where there is limited prior knowledge or data about the event. Empirical Bayes models are a powerful tool for modeling and analyzing data and are used in a wide range of applications, including risk assessment, resource allocation, and decision-making under uncertainty. Entity In probability-based models, an entity refers to an object or concept that can be described or represented by a set of characteristics or variables. In statistical modeling, an entity can be a person, a group, an event, or any other thing that can be represented by data. The probability of an event or outcome is often calculated based on the characteristics or variables associated with the entity. For example, in a medical study, the entity might be a patient, and the probability of the patient experiencing a certain outcome might be calculated based on factors such as age, gender, and medical history. FIFO FIFO stands for \u0026ldquo;first-in, first-out.\u0026rdquo; In probability-based models, FIFO is a queueing discipline that refers to the way in which items or entities are processed or served. Under a FIFO system, the first item that enters the queue is the first one to be served or processed. This is in contrast to other queueing disciplines, such as LIFO (last-in, first-out) or priority-based systems, in which the order of service or processing is determined by some other criterion. In probability models, FIFO systems are often used to model real-world situations in which items are processed in the order in which they arrive. For example, a FIFO system might be used to model the way in which customers are served in a bank or a grocery store, where the first customer in line is the first one to be assisted. The probability of a customer being served within a certain time period might be calculated based on the number of customers already in the queue and the rate at which they are being served. Interarrival time In probability and statistics, interarrival time refers to the time that elapses between the arrival of successive entities at a particular location or system. For example, in a queueing system, the interarrival time is the time between the arrival of successive customers. In a communication network, the interarrival time is the time between the arrival of successive packets of data. Interarrival times are often modeled in probability-based systems in order to understand and predict the flow of entities through the system. For example, in a queueing system, the interarrival times of customers might be modeled in order to understand the workload on the system and predict how long customers will have to wait before being served. In a communication network, interarrival times might be modeled in order to understand the capacity of the network and predict how long it will take for data to be transmitted. Kendall notation Kendall notation is a formal way of describing the relationships between entities in a system. It is used to model the behavior of systems in fields such as computer science, engineering, and operations research. In Kendall notation, a system is represented by a graph, with the entities in the system represented as nodes, and the relationships between the entities represented as edges. The graph is then annotated with labels that describe the type of relationship that exists between the entities. There are several types of labels that can be used in Kendall notation, including: \u0026ldquo;m\u0026rdquo; for a mutual relationship between two entities, in which each entity has an effect on the other \u0026ldquo;o\u0026rdquo; for an one-way relationship, in which one entity has an effect on the other but not vice versa \u0026ldquo;r\u0026rdquo; for a reciprocal relationship, in which two entities have an effect on each other but the effect is not necessarily equal Kendall notation is a useful tool for understanding and analyzing the behavior of complex systems. It can be used to identify and model the relationships between different entities in a system, and to analyze how changes in one part of the system may affect other parts of the system. LIFO LIFO is an acronym for Last In, First Out, and is a method of organizing and manipulating data in a data structure such as a stack or queue. In a LIFO data structure, the most recent item added to the structure is the first one to be removed. This is in contrast to a FIFO (First In, First Out) data structure, in which the first item added is the first one to be removed. LIFO data structures are often used in computing and programming because they are simple to implement and can be manipulated quickly and efficiently. An example of a LIFO data structure is a stack, which is a list of items that are added and removed in a specific order. When an item is added to the top of a stack, it is said to be \u0026ldquo;pushed\u0026rdquo; onto the stack. When an item is removed from the top of the stack, it is said to be \u0026ldquo;popped\u0026rdquo; off the stack. LIFO data structures have a number of applications, including implementing undo/redo functions in software, evaluating mathematical expressions, and implementing memory allocators in operating systems. Markov chain A Markov chain is a mathematical system that undergoes transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the system arrived at its current state, the possible future states are fixed. In other words, the probability of transitioning to any particular state is dependent only on the current state and time elapsed. A Markov chain can be represented as a directed graph, with the edges representing the probability of transitioning from one state to another. The nodes of the graph represent the states of the system, and the edges are labeled with the probabilities of transitioning between the states. Markov chains are used to model a wide variety of systems in which the future state of the system is dependent only on the current state, including processes that involve randomness, such as the spread of disease, the movement of financial markets, and the analysis of computer algorithms. They are also used in the study of animal behavior, linguistics, and other fields. Queue In the context of a probability-based model, a queue is a system in which items arrive at a certain rate and are processed or served at a different rate. The items that arrive and are waiting to be processed form a queue. Queueing theory is a branch of mathematics that studies the behavior of queues and the systems that create them. It is used to model and analyze the performance of systems that involve waiting in line, such as call centers, computer networks, and manufacturing systems. In a queueing model, the arrival rate of items and the processing rate of the system are important factors that determine the behavior of the queue. If the arrival rate is higher than the processing rate, the queue will grow over time, leading to an increase in waiting time for items. If the processing rate is higher than the arrival rate, the queue will shrink over time. Queueing models can be used to analyze the performance of a system and to make predictions about the behavior of the queue under different conditions. They can also be used to identify bottlenecks in a system and to optimize the performance of the system by adjusting the arrival rate and the processing rate. Service rate In a probability model, the service rate refers to the rate at which a system is able to process or serve items. The service rate is an important factor that determines the behavior of a queue or other system in which items are waiting to be processed. In a queueing model, the service rate is typically represented as the average number of items that the system is able to process per unit of time. It is used to calculate the expected waiting time for items in the queue, as well as the probability of the queue being empty or full at a given time. The service rate is often influenced by factors such as the number of servers or processing units available, the efficiency of the processing units, and the complexity of the tasks being performed. By adjusting the service rate, it is possible to optimize the performance of a system and to reduce the waiting time for items in the queue. The service rate is typically contrasted with the arrival rate, which is the rate at which items arrive at the system and enter the queue. The relationship between the service rate and the arrival rate determines the behavior of the queue and the expected waiting time for items. Simulation Simulation is the process of creating a model of a real-world system or process, and using it to predict the behavior of the system over time. Simulations are used in a wide variety of fields, including engineering, computer science, economics, and the natural sciences, to study and analyze the behavior of complex systems. There are several types of simulations, including: Discrete event simulation: This type of simulation models the behavior of systems that change state at discrete points in time, such as a manufacturing system or a computer network. Continuous simulation: This type of simulation models the behavior of systems that change continuously over time, such as a chemical reaction or a mechanical system. Monte Carlo simulation: This type of simulation uses random numbers to model the behavior of systems that involve uncertainty, such as financial markets or weather patterns. Simulations can be used to study the behavior of a system under different conditions, to optimize the performance of a system, and to make predictions about the behavior of the system in the future. They are often used in conjunction with other analytical and mathematical techniques to study complex systems. Steady state In a system or process, the steady state is a condition in which the system is in a stable, equilibrium state and is not changing over time. In other words, the system has reached a state of balance and is no longer undergoing significant changes. The concept of steady state is used in a variety of fields, including physics, engineering, economics, and biology. In physics and engineering, the steady state is often used to describe systems that are in a state of equilibrium and are not undergoing any net change, such as a fluid flowing through a pipe at a constant rate. In economics, the steady state is often used to describe the long-term equilibrium of an economy, in which the growth rate of the economy is constant and there is no net increase in the capital stock. The concept of steady state is often contrasted with the concept of transient state, which refers to a temporary condition in which a system is changing and is not yet in a stable equilibrium. The process of reaching the steady state from a transient state is known as relaxation. Stochastic simulation Stochastic simulation is a type of simulation that involves the use of random numbers or probabilities to model the behavior of a system or process. It is used to study systems that involve uncertainty or randomness, such as financial markets, weather patterns, and biological systems. In a stochastic simulation, the system or process being studied is represented by a set of rules or equations that describe how the system changes over time. These rules may include probabilistic elements, such as the probability of a certain event occurring or the probability distribution of certain variables. The simulation is then run by randomly generating values for the variables and using them to update the state of the system at each time step. Stochastic simulation is often used in conjunction with other analytical and mathematical techniques, such as statistical analysis and optimization, to study complex systems. It can be used to study the behavior of a system under different conditions, to optimize the performance of a system, and to make predictions about the behavior of the system in the future. Transition matrix A transition matrix is a matrix that is used to describe the transitions between different states in a Markov process. In a Markov process, a system moves from one state to another according to certain probabilistic rules, and the transition matrix specifies the probability of transitioning from one state to another. The elements of a transition matrix are the probabilities of transitioning between states. The rows of the matrix represent the starting states, and the columns represent the ending states. The element in the i-th row and j-th column of the matrix represents the probability of transitioning from the i-th state to the j-th state. Transition matrices are used in a variety of fields, including engineering, computer science, and economics, to model and analyze the behavior of systems that change over time. They are commonly used in the analysis of Markov processes, which are systems in which the future state of the system is determined only by the current state and the elapsed time. Transition matrices can be used to calculate the probability of reaching a particular state at a given time, to analyze the long-term behavior of a system, and to make predictions about the future behavior of the system. They are also used to identify patterns and trends in the system and to optimize the performance of the system. Transition probability A transition matrix is a matrix that is used to describe the transitions between different states in a Markov process. In a Markov process, a system moves from one state to another according to certain probabilistic rules, and the transition matrix specifies the probability of transitioning from one state to another. The elements of a transition matrix are the probabilities of transitioning between states. The rows of the matrix represent the starting states, and the columns represent the ending states. The element in the i-th row and j-th column of the matrix represents the probability of transitioning from the i-th state to the j-th state. Transition matrices are used in a variety of fields, including engineering, computer science, and economics, to model and analyze the behavior of systems that change over time. They are commonly used in the analysis of Markov processes, which are systems in which the future state of the system is determined only by the current state and the elapsed time. Transition matrices can be used to calculate the probability of reaching a particular state at a given time, to analyze the long-term behavior of a system, and to make predictions about the future behavior of the system. They are also used to identify patterns and trends in the system and to optimize the performance of the system. Validation (of simulation) Validation is the process of evaluating a simulation model to determine whether it accurately represents the real-world system or process that it is intended to model. The goal of validation is to ensure that the simulation results are reliable and accurately reflect the behavior of the system being studied. There are several steps involved in validating a simulation model, including: Defining the scope and objectives of the simulation Developing the simulation model using a suitable modeling approach Verifying the internal consistency and logic of the model Calibrating the model using real-world data Testing the model using a variety of inputs and scenarios Comparing the results of the simulation to real-world data or other sources of information Validation is an important step in the development of a simulation model, as it helps to ensure that the model is accurate and can be trusted to make reliable predictions about the behavior of the system being studied. It is also an ongoing process, as the model may need to be revised and re-validated as new information becomes available or the system being studied changes over time. Probability distributions Bernoulli distribution The Bernoulli distribution is a probability distribution that describes the outcome of a binary event, such as the toss of a coin or the outcome of a yes/no question. It is a discrete distribution, meaning that the random variable can only take on a finite number of values. In the Bernoulli distribution, there are only two possible outcomes: success (denoted by a value of 1) or failure (denoted by a value of 0). The probability of success is denoted by p, and the probability of failure is denoted by (1-p). The Bernoulli distribution is defined by a single parameter, p, which represents the probability of success. The probability mass function (PMF) of the Bernoulli distribution is given by: PMF(x) = p^x * (1-p)^(1-x) where x is a value of 0 (failure) or 1 (success).\nThe Bernoulli distribution is a special case of the binomial distribution, which describes the outcome of a series of independent binary events. It is often used to model the probability of success in situations where there are only two possible outcomes, such as the probability of winning a game or the probability of a coin landing heads. Bias Bias refers to a systematic error or deviation from the true value of a measurement or estimate. It can occur in various forms, such as: Sampling bias: This occurs when the sample of data being analyzed is not representative of the population being studied. Measurement bias: This occurs when the measurement process is not accurate or reliable, leading to systematic errors in the measurements. Observational bias: This occurs when the observer or researcher\u0026rsquo;s expectations or preconceptions influence the results of an experiment or study. Confirmation bias: This occurs when the researcher is more likely to accept or seek out evidence that supports their hypothesis or preconceptions, and is less likely to consider evidence that contradicts their beliefs. Bias can have significant impacts on the accuracy and reliability of research and measurement, and it is important to try to minimize bias whenever possible. This can be done through careful design of experiments and studies, using random sampling and other techniques to ensure a representative sample, and using objective measurement techniques to minimize measurement bias. Binomial distribution The binomial distribution is a probability distribution that describes the outcome of a series of independent binary events, such as the toss of a coin or the outcome of a series of yes/no questions. It is a discrete distribution, meaning that the random variable can only take on a finite number of values. In the binomial distribution, there are only two possible outcomes for each event: success (denoted by a value of 1) or failure (denoted by a value of 0). The probability of success is denoted by p, and the probability of failure is denoted by (1-p). The number of events is denoted by n. The binomial distribution is defined by two parameters: n, the number of events, and p, the probability of success. The probability mass function (PMF) of the binomial distribution is given by: PMF(x) = (n choose x) * p^x * (1-p)^(n-x) where x is the number of successes and (n choose x) is the binomial coefficient.\nThe binomial distribution is often used to model the probability of a certain number of successes in a series of independent events, such as the probability of flipping heads a certain number of times in a series of coin flips. It is a useful distribution for modeling situations where there are only two possible outcomes and the probability of success is constant for each event. Distribution-fitting Distribution fitting is the process of selecting a statistical distribution that best represents the data being analyzed. It is a common step in statistical analysis and is used to describe the distribution of a set of data and to make predictions about future observations. There are several methods for fitting a distribution to a set of data, including: Visual inspection: This involves plotting the data and visually comparing it to the shape of known distributions to see which one is the best fit. Goodness-of-fit tests: These tests evaluate the fit of the data to a particular distribution by calculating a statistic, such as a p-value, which measures the probability of observing the data if the chosen distribution is true. Maximum likelihood estimation: This method estimates the parameters of a distribution that maximize the likelihood of observing the data. It is important to choose an appropriate distribution for the data being analyzed, as the choice of distribution can affect the accuracy of the results and the conclusions drawn from the data. In some cases, it may be necessary to use more than one distribution to adequately describe the data. Exponential distribution The exponential distribution is a probability distribution that describes the time between events in a Poisson process, which is a process in which events occur independently at a constant average rate. It is a continuous distribution, meaning that the random variable can take on any value within a given range. The exponential distribution is defined by a single parameter, lambda (λ), which represents the average rate at which events occur. The probability density function (PDF) of the exponential distribution is given by: PDF(x) = λ * e^(-λx) where x is the time between events and e is the base of the natural logarithm.\nThe exponential distribution has the property that the time between events is exponentially distributed, meaning that the probability of an event occurring decreases exponentially as the time since the last event increases. This makes it a useful distribution for modeling processes in which the rate of occurrence decreases over time, such as the time between arrivals at a busy airport or the time between failures of a piece of equipment. The exponential distribution is often used in reliability engineering and other fields to model the time between failures or other events of interest. It is also related to the Poisson distribution, which is a discrete distribution that describes the number of events in a given time period. Geometric distribution The geometric distribution is a probability distribution that describes the number of Bernoulli trials needed to get a single success. It is a discrete distribution, meaning that the random variable can only take on a finite number of values. In the geometric distribution, there are two possible outcomes for each trial: success (denoted by a value of 1) or failure (denoted by a value of 0). The probability of success is denoted by p, and the probability of failure is denoted by (1-p). The geometric distribution is defined by a single parameter, p, which represents the probability of success. The probability mass function (PMF) of the geometric distribution is given by: PMF(x) = (1-p)^(x-1) * p where x is the number of trials.\nThe geometric distribution is often used to model the number of trials needed to get a single success in a series of independent events, such as the number of coin flips needed to get a heads or the number of times a die must be rolled to get a particular number. It is a useful distribution for modeling situations where there are only two possible outcomes and the probability of success decreases with each trial. IID IID stands for \u0026ldquo;independent and identically distributed.\u0026rdquo; It is a term used to describe a sequence of random variables that are independent of each other and have the same probability distribution. In other words, IID random variables are uncorrelated and have the same statistical properties. This means that the value of one random variable in the sequence does not affect the value of any other random variable in the sequence, and that the probability of any given value occurring is the same for all variables in the sequence. IID random variables are often used in statistical analysis and probability theory, as they have a number of useful properties that make them easier to work with. For example, the expected value of the sum of IID random variables is equal to the sum of the expected values of the individual variables, which can simplify calculations. IID random variables are used in a variety of fields, including statistics, economics, engineering, and computer science. They are commonly used to model the behavior of systems that involve uncertainty or randomness, such as financial markets or communication networks. Lower tail In a probability distribution, the lower tail refers to the portion of the distribution that is below a certain value or threshold. The lower tail is often defined relative to a particular value, such as the mean or median of the distribution, and it represents the probability that a random variable will take on a value that is less than this threshold. The lower tail of a distribution can be important in understanding the behavior of a random variable and in making predictions about its value. For example, in a financial context, the lower tail of a distribution may represent the probability of experiencing a significant loss or downturn. The lower tail of a distribution can be characterized by various statistics, such as the lower quartile, which is the value below which 25% of the observations fall, or the lower decile, which is the value below which 10% of the observations fall. These statistics can provide insight into the skewness of the distribution and the relative frequency of lower values. Memoryless (distribution) A memoryless distribution is a type of probability distribution that has the property that the conditional probability of an event occurring at a future time is independent of the time that has elapsed since the last event. This means that the probability of an event occurring at a given time is the same as the probability of it occurring at any other time, regardless of how much time has passed since the last event. An example of a memoryless distribution is the exponential distribution, which is often used to model the time between events in a Poisson process. The exponential distribution has the property that the probability of an event occurring at a given time is the same as the probability of it occurring at any other time, regardless of how much time has passed since the last event. Memoryless distributions are often used to model processes in which the probability of an event occurring does not depend on the time that has elapsed since the last event, such as the time between failures of a piece of equipment or the time between arrivals at a busy airport. They are useful for modeling systems in which the probability of an event occurring is constant over time. Normal distribution The normal distribution, also known as the Gaussian distribution, is a continuous probability distribution that is widely used to model real-valued random variables. It is a symmetrical distribution with a bell-shaped curve and has a single peak, which is at the mean of the distribution. The normal distribution is defined by two parameters: the mean, which is the average value of the distribution, and the standard deviation, which is a measure of the spread of the distribution. The probability density function (PDF) of the normal distribution is given by: PDF(x) = (1 / (sqrt(2*π)σ)) * e^(-(x-μ)^2 / (2σ^2)) where x is the value of the random variable, μ is the mean of the distribution, σ is the standard deviation, and π is approximately 3.14.\nThe normal distribution is often used to model variables that are continuous and have a symmetrical distribution, such as height, weight, and IQ scores. It is a useful distribution because it has a number of desirable properties, such as being stable under linear transformations and having a simple analytical form. It is also commonly used in statistical analysis and probability theory. Poisson distribution The Poisson distribution is a discrete probability distribution that describes the number of events that occur in a given time period, such as the number of phone calls received by a call center or the number of defects in a manufactured product. It is often used to model the number of times an event occurs in a fixed interval of time, space, or volume. The Poisson distribution is defined by a single parameter, lambda (λ), which represents the average rate at which events occur. The probability mass function (PMF) of the Poisson distribution is given by: PMF(x) = (λ^x * e^(-λ)) / x! where x is the number of events and e is the base of the natural logarithm.\nThe Poisson distribution has the property that the probability of observing a given number of events decreases exponentially as the number of events increases. This makes it a useful distribution for modeling processes in which the rate of occurrence decreases as the number of events increases, such as the number of errors in a document or the number of vehicles arriving at a traffic light. The Poisson distribution is often used in fields such as engineering, economics, and operations research to model the occurrence of events over time or space. It is related to the exponential distribution, which is a continuous distribution that describes the time between events in a Poisson process. Q-Q plot A Q-Q (quantile-quantile) plot is a graphical tool used to assess whether two data sets come from the same distribution. It is a scatter plot that plots the quantiles of one data set against the quantiles of another data set, and it is used to visualize the similarity or dissimilarity between the two distributions. To create a Q-Q plot, the data is first sorted in ascending order and then divided into equal-sized groups, called quantiles. The quantiles of each data set are then plotted against each other on the graph. If the two data sets come from the same distribution, the points on the Q-Q plot will fall along a straight line. If the distributions are different, the points will deviate from the line. Q-Q plots are often used in statistical analysis to compare the distributions of two data sets or to assess whether a data set follows a particular distribution. They are also useful for identifying outliers and assessing the normality of a data set. Tail(s) In a probability distribution, the tails refer to the portion of the distribution that is above or below a certain value or threshold. The tails of a distribution can be characterized by various statistics, such as the upper and lower quartiles or deciles, which are values above or below which a certain percentage of the observations fall. The tails of a distribution can be important in understanding the behavior of a random variable and in making predictions about its value. For example, in a financial context, the tails of a distribution may represent the probability of experiencing a significant gain or loss. The tails of a distribution can also be used to characterize the skewness of the distribution. A distribution with a long left tail (i.e., a tail that extends to the left of the mean) is said to be left-skewed, while a distribution with a long right tail is said to be right-skewed. A symmetrical distribution, on the other hand, has equal tails on both sides of the mean. Upper tail In a probability distribution, the upper tail refers to the portion of the distribution that is above a certain value or threshold. The upper tail is often defined relative to a particular value, such as the mean or median of the distribution, and it represents the probability that a random variable will take on a value that is greater than this threshold. The upper tail of a distribution can be important in understanding the behavior of a random variable and in making predictions about its value. For example, in a financial context, the upper tail of a distribution may represent the probability of experiencing a significant gain or upturn. The upper tail of a distribution can be characterized by various statistics, such as the upper quartile, which is the value above which 25% of the observations fall, or the upper decile, which is the value above which 10% of the observations fall. These statistics can provide insight into the skewness of the distribution and the relative frequency of higher values. Weibull distribution The Weibull distribution is a continuous probability distribution that is often used to model the time it takes for a failure to occur in a system or the time between events in a process. It is a flexible distribution that can take on a variety of shapes, including a bell-shaped curve similar to the normal distribution, a curve with a long tail to the right (similar to the exponential distribution), or a curve with a long tail to the left (similar to the log-normal distribution). The Weibull distribution is defined by two parameters: alpha (α), which determines the shape of the distribution, and beta (β), which determines the scale of the distribution. The probability density function (PDF) of the Weibull distribution is given by: PDF(x) = (α / β) * (x / β)^(α-1) * e^(-(x/β)^α) where x is the value of the random variable.\nThe Weibull distribution is often used in reliability engineering and other fields to model the time to failure of a system or the time between events. It is a useful distribution because it can take on a variety of shapes, making it suitable for modeling a wide range of phenomena. It is also commonly used in statistical analysis and probability theory. Regression Adjusted R-squared/Adjusted R2 Adjusted R-squared is a statistic that attempts to adjust the R-squared value for the number of predictors in a regression model. It is often used to determine whether the addition of new predictors to a model significantly improves the model\u0026rsquo;s ability to predict the response variable. R-squared is a measure of how well a model fits the data. It is calculated as the proportion of the variance in the response variable that is explained by the model. However, R-squared can sometimes be artificially inflated when adding additional predictors to the model, even if those predictors do not significantly improve the model\u0026rsquo;s ability to predict the response. Adjusted R-squared is calculated by taking into account the number of predictors in the model and the sample size. It adjusts the R-squared value downward to account for the addition of predictors that do not significantly improve the model. In general, a higher adjusted R-squared value indicates a better fit for the model. However, it is important to consider other evaluation metrics in addition to adjusted R-squared when assessing the performance of a regression model. Area under curve/AUC The area under the curve (AUC) is a measure of the performance of a binary classifier, such as a diagnostic test. It represents the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example. The AUC can be calculated from the receiver operating characteristic (ROC) curve, which plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) at various classification thresholds. The AUC is the area under the ROC curve. AUC values range from 0 to 1, with a higher value indicating a better performing classifier. An AUC of 0.5 indicates that the classifier is no better than random guessing, while an AUC of 1 indicates perfect classification. The AUC is a useful evaluation metric because it is independent of the classification threshold and is not sensitive to the imbalance in the class distribution. It is often used in medical research to evaluate the performance of diagnostic tests. Bayesian regression Bayesian regression is a type of regression analysis that is based on Bayesian statistics. In Bayesian regression, model parameters are considered random variables, and a probability distribution is assigned to each parameter. In contrast to traditional regression, where the values of the model parameters are estimated using maximum likelihood estimation, Bayesian regression involves estimating the posterior distribution of the model parameters given the data and a prior distribution. This posterior distribution represents the updated belief about the model parameters after taking into account the observed data. One advantage of Bayesian regression is that it allows for the incorporation of prior knowledge or beliefs about the model parameters into the analysis. It also provides a full probability distribution for the model parameters, which can be useful for making predictions and for understanding the uncertainty in the estimates. Bayesian regression can be implemented using Markov Chain Monte Carlo (MCMC) techniques or variational inference methods. It is often used in situations where the number of predictors is large or when there is limited data available. Box-Cox transformation The Box-Cox transformation is a way to transform non-normal dependent variables into a normal shape. Normality is an assumption for many statistical techniques, so being able to transform a non-normal dependent variable into a normal shape can be useful for the purposes of analysis. The Box-Cox transformation is defined as: Y = (X^lambda - 1) / lambda where X is the variable to be transformed, Y is the transformed variable, and lambda is a parameter that you can choose to optimize the transformation. If lambda is equal to 0, the transformation becomes the natural logarithm. If lambda is equal to 1, the transformation is the identity transformation (i.e., the variable is left unchanged).\nThe Box-Cox transformation is often used in regression analysis, particularly when the dependent variable is not normal. It can be used to stabilize the variance of the dependent variable, improve the linearity of the model, and/or meet the assumptions of normality. It is important to note that the Box-Cox transformation is only appropriate for continuous variables. If you have a categorical variable, you should not use the Box-Cox transformation. Branching In the context of machine learning, branching can be used to create decision trees, which are a type of model used for classification and regression tasks. A decision tree is a flowchart-like tree structure where an internal node represents feature (an attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the input data by recursive partitioning.\nHere is an example of a decision tree in Python using the scikit-learn library:\nfrom sklearn import tree # features (X) and labels (y) X = [[0, 0], [1, 1]] y = [0, 1] # create the decision tree model model = tree.DecisionTreeClassifier() # train the model model.fit(X, y) # predict a label for a new sample print(model.predict([[2., 2.]])) In the context of machine learning, branching can be used to create decision trees, which are a type of model used for classification and regression tasks. A decision tree is a flowchart-like tree structure where an internal node represents feature (an attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the input data by recursive partitioning.\nHere is an example of a decision tree in Python using the scikit-learn library:\nfrom sklearn import tree # features (X) and labels (y) X = [[0, 0], [1, 1]] y = [0, 1] # create the decision tree model model = tree.DecisionTreeClassifier() # train the model model.fit(X, y) # predict a label for a new sample print(model.predict([[2., 2.]])) In this example, the decision tree will learn to predict a label (0 or 1) based on the input features (X). The tree will create branches based on the conditions specified in the decision rules, and the leaves of the tree will contain the predicted labels. Decision trees are a simple and powerful tool for many machine learning tasks, and they can be used in a variety of applications such as recommendation systems, fraud detection, and image classification. CART CART (Classification and Regression Trees) is a decision tree algorithm used for classification and regression tasks. It is a popular algorithm for building decision trees because it is simple to understand and implement, and it can handle both continuous and categorical variables. In a CART decision tree, the tree is built by selecting splits on the features that maximize the reduction in impurity. Impurity refers to the amount of uncertainty or randomness in the data. The goal of the algorithm is to create splits that reduce the impurity of the data as much as possible, resulting in purer leaves (i.e., leaves with a single class or value). Here is an example of a CART decision tree in Python using the scikit-learn library: from sklearn import tree # features (X) and labels (y) X = [[0, 0], [1, 1]] y = [0, 1] # create the CART model model = tree.DecisionTreeClassifier(criterion=\u0026#39;gini\u0026#39;) # train the model model.fit(X, y) # predict a label for a new sample print(model.predict([[2., 2.]])) In this example, the CART model will learn to predict a label (0 or 1) based on the input features (X). The tree will create splits based on the Gini impurity criterion, and the leaves of the tree will contain the predicted labels. CART is a widely used algorithm for building decision trees, and it is often used in a variety of applications such as recommendation systems, fraud detection, and image classification. Classification tree A classification tree is a type of decision tree that is used for classification tasks. It is a flowchart-like tree structure where an internal node represents feature (an attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a classification tree is known as the root node. It learns to partition on the input data by recursive partitioning. Concordance index A concordance index (also known as the c-index) is a measure of the predictive ability of a binary classifier. It is calculated as the fraction of all pairs of samples (one positive, one negative) where the positive sample has a higher predicted probability of being positive than the negative sample. The c-index ranges from 0 to 1, where a value of 1 indicates perfect prediction and a value of 0 indicates no predictive ability. The c-index is often used to evaluate the performance of a classification model, particularly in the field of medical statistics. Here is an example of how to calculate the c-index in Python: import numpy as np # true labels (y_true) and predicted probabilities (y_pred) y_true = [0, 1, 1, 0, 1] y_pred = [0.1, 0.8, 0.7, 0.2, 0.9] # calculate the c-index c_index = 0 for i in range(len(y_true)): for j in range(i+1, len(y_true)): if (y_true[i] == 0 and y_true[j] == 1) or (y_true[i] == 1 and y_true[j] == 0): c_index += (y_pred[i] \u0026gt; y_pred[j]) + (y_pred[i] == y_pred[j]) print(c_index / (len(y_true) * (len(y_true) - 1) / 2)) Decision tree A decision tree is a flowchart-like tree structure that is used to make decisions based on conditions specified in the decision rules. It is a popular tool in machine learning and is often used for classification and regression tasks. In a decision tree, the tree is built by selecting splits on the features that maximize the reduction in impurity. Impurity refers to the amount of uncertainty or randomness in the data. The goal of the algorithm is to create splits that reduce the impurity of the data as much as possible, resulting in purer leaves (i.e., leaves with a single class or value). Here is an example of a decision tree in Python using the scikit-learn library: from sklearn import tree # features (X) and labels (y) X = [[0, 0], [1, 1]] y = [0, 1] # create the decision tree model model = tree.DecisionTreeClassifier() # train the model model.fit(X, y) # predict a label for a new sample print(model.predict([[2., 2.]])) In this example, the decision tree will learn to predict a label (0 or 1) based on the input features (X). The tree will create splits based on the decision rules, and the leaves of the tree will contain the predicted labels. Decision trees are a simple and powerful tool for many machine learning tasks, and they can be used in a variety of applications such as recommendation systems, fraud detection, and image classification. Elastic net Elastic Net is a linear regression model that combines the penalties of both L1 (Lasso) and L2 (Ridge) regularization. It is trained with both L1 and L2 regularization, and the mixing parameter alpha determines the weighting between the two. When alpha=0, Elastic Net is equivalent to Ridge Regression, and when alpha=1, it is equivalent to Lasso Regression. The advantage of Elastic Net over Ridge Regression is that it is able to handle correlated features better, and the advantage over Lasso is that it does not require the features to be standardized. Forest In the context of regression, a decision tree forest is a type of ensemble model that is made up of a collection of decision trees trained on different subsets of the training data. The individual decision trees in the forest make predictions based on the features in the data, and the predictions of the trees are combined to make the final prediction for the forest. There are several ways to combine the predictions of the individual trees in the forest. One common method is to take the mean of the predictions of all the trees in the forest. Another method is to have each tree vote on the final prediction, and the most popular prediction is chosen as the output of the forest. Decision tree forests are often used for regression tasks because they can handle high-dimensional data and are resistant to overfitting. They are also able to handle missing values in the data, which is a common problem in real-world datasets. Interaction term An interaction term is a term in a statistical model that represents the effect of two variables on an outcome, rather than the effect of each variable individually. Interaction terms allow you to determine whether the relationship between two variables and the outcome is different from the individual relationships of each variable with the outcome. For example, let\u0026rsquo;s say you are studying the relationship between income, education, and happiness. You might include an interaction term in your model to test whether the relationship between income and happiness is different for people with different levels of education. If you find a significant interaction term, it suggests that the relationship between income and happiness is not the same for all levels of education. In a statistical model, interaction terms are included as additional predictors along with the main effects (individual variables). They are usually represented by the product of the two variables that are being interacted. For example, if you are including an interaction term between variables X and Y, it would be represented as X*Y in the model. 𝑘-Nearest-Neighbor regression k-Nearest Neighbor (k-NN) regression is a simple and easy-to-implement machine learning method used for regression tasks. In k-NN regression, the prediction for a given data point is based on the mean of the target values of the k nearest neighbors to that data point. To make a prediction for a new data point, the distance between that point and all the other points in the training set is calculated. The k points in the training set that are closest to the new point are then identified, and the mean of the target values of these k points is taken as the prediction for the new point. One of the main advantages of k-NN regression is that it is a non-parametric method, which means that it does not make any assumptions about the underlying functional form of the data. This makes it well-suited for working with complex, non-linear relationships in the data. However, k-NN regression can be computationally expensive and may not be suitable for very large datasets. Knot In the context of a spline, a knot is a point where the function changes curvature. A spline is a piecewise continuous curve that is used to approximate a set of data points, and knots are used to control the smoothness of the curve. For example, consider a set of data points that show the relationship between temperature and ice cream sales at a store. A spline with a single knot would be a simple curve that passes through all the data points, while a spline with multiple knots would have a more complex shape that is able to better capture the underlying pattern in the data. The position and number of knots in the spline can be chosen to trade off between smoothness and fit to the data. In general, splines are used to smooth noisy data or to fit a curve to a set of data points when a parametric form for the curve is not known. They are commonly used in regression and smoothing applications. Lasso regression Lasso regression is a linear regression method that uses L1 regularization to encourage sparsity in the model. L1 regularization is a form of regularization that adds a penalty term to the objective function of the model based on the absolute values of the model coefficients. The penalty term is controlled by a hyperparameter alpha, which determines the strength of the regularization. Lasso regression has the effect of driving some of the coefficients of the model to zero, effectively removing the corresponding features from the model. This can be useful for feature selection, as it allows you to identify the most important features in the data and remove the rest. Lasso regression is particularly well-suited for cases where there are a large number of features and only a few of them are truly relevant. It is also useful when the relationships between the features and the outcome are sparse, i.e., when most of the features have little or no effect on the outcome. However, Lasso regression can be sensitive to the scale of the features, and it is generally recommended to standardize the features before fitting a Lasso model. Leaf In the context of a decision tree, a leaf is a terminal node that does not have any children. In a decision tree for regression, the value stored at a leaf node is the mean of the target values of the training examples that reach that leaf. For example, consider a decision tree for predicting the price of a house based on features such as the size of the house, the number of bedrooms, and the location. The tree might have a leaf node for houses with three bedrooms that are located in a certain neighborhood. The value stored at this leaf node would be the mean of the prices of all the houses with three bedrooms in that neighborhood in the training set. When making a prediction for a new house using the decision tree, the tree is traversed from the root to a leaf node based on the feature values of the new house. The value stored at the leaf node is then taken as the prediction for the house. Decision trees are often used for regression tasks because they are able to handle high-dimensional data and can handle missing values in the data. Linear regression Linear regression is a statistical method used to model the linear relationship between a dependent variable and one or more independent variables. The goal of linear regression is to find the best-fitting line to a set of data points, where the best-fitting line is one that minimizes the sum of the squared differences between the predicted values and the true values. Linear regression models can be represented by the equation: y = b0 + b1x1 + b2x2 + ... + bn*xn where y is the dependent variable, x1, x2, \u0026hellip;, xn are the independent variables, and b0, b1, b2, \u0026hellip;, bn are the coefficients that represent the strength and direction of the relationship between each independent variable and the dependent variable. The coefficients are determined by fitting the model to the training data.\nLinear regression is a simple and widely-used method for modeling linear relationships in data. It is well-suited for cases where the relationship between the variables is well-approximated by a straight line. However, it is not capable of modeling more complex, non-linear relationships. Logistic regression Logistic regression is a statistical method used for classification tasks. It is a supervised learning algorithm that takes a set of input features and uses them to predict a binary outcome (0 or 1). The predictions of a logistic regression model are based on the probability of the positive class (class 1). The probability is computed using the logistic function, which maps any real-valued number to the range [0, 1]. The logistic function has the following form: p = 1 / (1 + e^(-z)) where p is the probability of the positive class and z is a linear combination of the input features and the model coefficients.\nTo make a prediction for a new data point, the model computes the probability of the positive class using the logistic function, and the predicted class is 1 if the probability is greater than or equal to 0.5 and 0 otherwise. Logistic regression is widely used in a variety of applications, including image classification, spam filtering, and predicting customer churn. It is simple to implement and efficient to train, and it can be extended to handle multi-class classification tasks by using one-vs-rest or one-vs-all approaches. Logit model A logit model is a type of statistical model that is used for binary classification tasks. It is a type of generalized linear model that uses the logit function as the link function and the binary outcome as the dependent variable. The logit function is a transformation of the probability of the positive class (class 1) into the real line. It is defined as the natural logarithm of the odds ratio: logit(p) = ln(p / (1 - p)) where p is the probability of the positive class. The logit function maps any probability value in the range [0, 1] to the range (-infinity, infinity).\nIn a logit model, the predicted probability of the positive class is computed using the logit function and a linear combination of the input features and the model coefficients. The predicted class is then obtained by thresholding the probability at 0.5: class 1 if the probability is greater than or equal to 0.5 and class 0 otherwise. Logit models are widely used in a variety of applications, including image classification, spam filtering, and predicting customer churn. They are simple to implement and efficient to train, and they can be extended to handle multi-class classification tasks by using one-vs-rest or one-vs-all approaches. Multi-adaptive regression splines (MARS) Multi-adaptive regression splines (MARS) is a non-parametric regression technique that uses a combination of linear and non-linear basis functions to model complex relationships between the predictor and response variables. MARS uses a forward-stepwise algorithm to add or remove basis functions, and adaptively adjust the model complexity to fit the data. The technique is particularly useful for handling non-linear and non-monotonic relationships, and can handle high-dimensional data with many predictor variables. MARS is an alternative to traditional linear regression and other non-parametric techniques such as decision trees and random forests. p-value A p-value is a probability value that is used in statistical hypothesis testing to determine the significance of a sample\u0026rsquo;s results. The p-value is the probability of obtaining a test statistic as extreme or more extreme than the one observed, assuming that the null hypothesis is true. The smaller the p-value, the more evidence there is against the null hypothesis and in favor of the alternative hypothesis. A common threshold for the p-value is 0.05, meaning that if the p-value is less than 0.05, the results are considered statistically significant and the null hypothesis is rejected. This means that there is less than a 5% chance that the results are due to chance. However, it is important to note that a p-value of less than 0.05 does not necessarily mean that the results are true or that the alternative hypothesis is correct, it just means that the results are unlikely to have occurred by chance. p-value fishing P-value fishing, also known as \u0026ldquo;data dredging\u0026rdquo; or \u0026ldquo;p-hacking,\u0026rdquo; refers to the practice of manipulating data, or selectively reporting results, in order to achieve a desired p-value. This can be done by, for example, selecting a subset of data, changing the parameters of a test, or repeating a test multiple times until a \u0026ldquo;significant\u0026rdquo; result is obtained. P-value fishing can lead to false positive results, and increase the risk of type I errors (i.e. rejecting the null hypothesis when it is actually true). It can also inflate the false positive rate and decrease the statistical power of a study. It is considered a serious violation of scientific integrity and can lead to unreliable or misleading conclusions. To avoid p-value fishing, it is recommended to pre-register study hypotheses, designs, and analysis plans before collecting any data, and to use appropriate multiple testing correction methods, such as the Bonferroni correction, to account for the number of tests performed and control the false positive rate. Poisson regression Poisson regression is a statistical method used to model count data, such as the number of occurrences of an event over a period of time. Poisson regression is a type of generalized linear model (GLM) that assumes that the response variable follows a Poisson distribution, which is a discrete probability distribution used to model the number of times an event occurs in a fixed interval of time or space. In Poisson regression, the response variable is modeled as a function of one or more predictor variables, using a log-linear relationship. The model is specified by a probability density function (pdf) of the form: λ = e^(β0 + β1x1 + β2x2 + ... + βk*xk) Where λ is the expected value of the response variable, x1, x2, \u0026hellip;, xk are the predictor variables, and β0, β1, β2, \u0026hellip;, βk are the parameters of the model.\nPoisson regression can be used to analyze count data with one or more predictor variables, and can handle over-dispersion, which is a common feature of count data, and can estimate the relative risk ratio and the incidence rate ratio. Poisson regression can also be extended to handle more complex data structures, such as clustered data, and can be used to model both cross-sectional and longitudinal data. Pruning Pruning is a technique used in decision tree learning and other machine learning algorithms, to reduce the size of the tree and prevent overfitting. Overfitting occurs when a model is too complex and captures noise in the training data, which leads to poor generalization performance on unseen data. Pruning is a method to improve the generalization of decision tree by removing branches that do not contribute much to the classification or regression task. There are two main types of pruning techniques: Reduced Error Pruning and Cost Complexity Pruning. Reduced Error Pruning, also known as \u0026ldquo;Reduced Error Pruning\u0026rdquo; or \u0026ldquo;Minimum Description Length Pruning\u0026rdquo; (MDL) starts from the bottom of the tree and moves up, removing a node if it does not improve the classification accuracy. Cost Complexity Pruning, also known as \u0026ldquo;Weakest Link Pruning\u0026rdquo; or \u0026ldquo;Minimum Description Length Pruning\u0026rdquo; (MDL) is based on a trade-off between the complexity of the tree and the accuracy of the tree. It involves adding a complexity parameter to the tree, which penalizes the number of nodes in the tree, and then prunes the tree by minimizing the sum of the accuracy and the complexity. Both methods are used to improve the generalization of decision tree by removing branches that do not contribute much to the classification or regression task. Pseudo-R-squared/Pseudo-R2 Pseudo-R-squared is a statistical measure that is used to assess the goodness of fit of a model, similar to the R-squared statistic used in traditional linear regression. However, unlike R-squared, pseudo-R-squared is not a true measure of the proportion of variance explained by the model, and cannot be directly compared across different models or even different dependent variables. Some examples of pseudo-R-squared are McFadden\u0026rsquo;s R-squared, Cox and Snell R-squared, and Nagelkerke R-squared. R-squared/R2 R-squared, also known as the coefficient of determination, is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in a linear regression model. It ranges from 0 to 1, with higher values indicating a better fit of the model to the data. R-squared is a value between 0 and 1, with higher values indicating a better fit of the model to the data. An R-squared of 1 indicates that all variation in the dependent variable is completely explained by the independent variables, while an R-squared of 0 indicates that the model explains none of the variation in the dependent variable. It\u0026rsquo;s important to remember that a high R-squared value doesn\u0026rsquo;t necessarily mean that the model is a good fit for the data, as it doesn\u0026rsquo;t account for other important factors such as model complexity, outliers, or lack of independence of errors. Additionally, R-squared is not a model-independent measure, meaning that it is not comparable across different models or even different dependent variables. Random forest Random Forest is an ensemble learning method for classification and regression. It is a type of decision tree algorithm that creates multiple decision trees and combines their predictions to make a final decision. The algorithm works by creating multiple decision trees, or \u0026ldquo;forest,\u0026rdquo; and each tree is created using a random subset of the data. These trees are then used to make predictions, and the final prediction is made by averaging or voting among the predictions of all the trees in the forest. This process is designed to reduce the overfitting that can occur when using a single decision tree by averaging out the errors made by individual trees. Random Forest is considered to be one of the most accurate and robust machine learning algorithms available and it can handle both categorical and numerical features, as well as missing data. It is also relatively easy to interpret, and it can be used for feature selection, which is the process of identifying the most important features in the data. It\u0026rsquo;s widely used in various industries and domains such as finance, healthcare, marketing, computer vision and natural language processing. Receiver operating characteristic curve (ROC curve) A Receiver Operating Characteristic (ROC) curve is a graphical representation of the performance of a binary classifier system as the discrimination threshold is varied. It is a plot of the true positive rate (sensitivity) against the false positive rate (1-specificity) for different threshold settings. The ROC curve allows for the visualization of the trade-off between the true positive rate and false positive rate for every possible threshold setting. A good classifier will have a large area under the ROC curve (AUC), which means that it will have a good balance between the true positive rate and false positive rate. An AUC of 1 represents a perfect classifier, while an AUC of 0.5 represents a random classifier. ROC curves are commonly used to evaluate the performance of diagnostic tests, but it can also be used for evaluating machine learning models. It\u0026rsquo;s widely used in areas such as medicine, biometrics, natural language processing and computer vision. It is important to note that ROC curves are used when the outcome variable is binary. In case of multi-class classification, one vs all ROC or micro and macro averaged ROC can be used. Regression Regression is a statistical method used to analyze the relationship between a dependent variable and one or more independent variables. The goal of regression is to find the best-fitting line or model that describes the relationship between the variables. There are several types of regression, including linear regression, logistic regression, and polynomial regression. Linear regression is used to model the relationship between a continuous dependent variable and one or more independent variables by fitting a linear equation to the observed data. The equation takes the form of Y = a + bX, where Y is the dependent variable, X is the independent variable, a is the y-intercept, and b is the slope of the line. Logistic regression is used when the dependent variable is binary (i.e., it only takes on two possible values). It models the probability that a given input belongs to a particular category. Polynomial regression is a generalization of linear regression in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial. In addition to these basic types, there are many variations and extensions of regression, such as multiple regression, non-linear regression, and regularized regression. Regression is widely used in various industries and domains such as finance, healthcare, marketing, and engineering. Regression splines Regression splines are a technique used to fit a smooth curve to a set of data points. They are a type of non-parametric regression method that can be used to model complex, non-linear relationships between a dependent variable and one or more independent variables. A spline is a piecewise polynomial function that is used to approximate a smooth curve. The basic idea behind regression splines is to divide the independent variable into a set of intervals or knots and then fit a separate polynomial function to each interval. The polynomials are then \u0026ldquo;stitched\u0026rdquo; together to form a smooth curve that can be used to model the relationship between the independent and dependent variables. There are several different types of regression splines, including natural cubic splines, thin plate splines, and radial basis function splines. Regression splines are particularly useful when the relationship between the independent and dependent variables is not well understood, or when the data points are non-linearly distributed. They are widely used in various fields such as economics, engineering, and bio-statistics. It\u0026rsquo;s important to note that, unlike linear regression, the interpretability of the results in regression splines can be more challenging and require more expertise to understand the results. Regression tree A regression tree is a type of decision tree used for regression problems. It is a tree-based model where each internal node represents a feature, each branch represents a decision based on that feature, and each leaf node represents an outcome or predicted value. The algorithm works by recursively splitting the data into subsets based on the values of the input features. At each node, the algorithm selects the feature and the threshold that results in the most homogeneous subsets of the target variable. The process continues until a stopping criterion is met, such as a minimum number of samples per leaf or a maximum tree depth is reached. Regression trees are simple to understand and interpret, they can handle both categorical and numerical features and missing values. They are also relatively insensitive to outliers, and they can handle non-linear relationships between the independent and dependent variables. Regression trees are used for both linear and non-linear regression problems. They are widely used in various industries such as finance, healthcare, and engineering. It\u0026rsquo;s important to note that, like other decision tree-based models, regression trees are prone to overfitting if the tree is grown too deep, therefore it\u0026rsquo;s important to use techniques such as pruning to prevent overfitting. Ridge regression Ridge regression is a type of linear regression that adds a L2 regularization term to the objective function. Regularization is a technique used to prevent overfitting by adding a penalty term to the objective function that discourages large weights. The L2 regularization term is the sum of the squares of the weights. The objective function in Ridge regression is defined as: J(w) = 1/N * ∑(y - Xw)^2 + λ * ∑w^2 Where w is the weight vector, X is the input data, y is the target variable, N is the number of samples, and λ is the regularization term.\nThe regularization term λ is a scalar that controls the strength of the regularization. A higher value of λ will result in smaller weights and a simpler model, while a lower value of λ will result in larger weights and a more complex model. Ridge regression is particularly useful when there are a large number of correlated input features, as it tends to shrink the coefficients of correlated features towards each other. It\u0026rsquo;s important to note that Ridge regression is similar to Lasso regression, which uses L1 regularization instead of L2 regularization. Lasso tends to produce sparse models, setting some of the weights to zero, while Ridge regression keeps all the weights non-zero but smaller. ROC curve A Receiver Operating Characteristic (ROC) curve is a graphical representation of the performance of a binary classifier system as the discrimination threshold is varied. It is a plot of the true positive rate (sensitivity) against the false positive rate (1-specificity) for different threshold settings. The ROC curve allows for the visualization of the trade-off between the true positive rate and false positive rate for every possible threshold setting. A good classifier will have a large area under the ROC curve (AUC), which means that it will have a good balance between the true positive rate and false positive rate. An AUC of 1 represents a perfect classifier, while an AUC of 0.5 represents a random classifier. ROC curves are commonly used to evaluate the performance of diagnostic tests, but it can also be used for evaluating machine learning models. It\u0026rsquo;s widely used in areas such as medicine, biometrics, natural language processing and computer vision. It is important to note that ROC curves are used when the outcome variable is binary. In case of multi-class classification, one vs all ROC or micro and macro averaged ROC can be used. Additionally, ROC curve is a powerful tool to evaluate the performance of a classifier, it\u0026rsquo;s also important to consider other evaluation metrics such as precision, recall, and F1-score, for a more comprehensive evaluation of the classifier performance. Root In the context of regression, the \u0026ldquo;root\u0026rdquo; typically refers to the root node of a decision tree or a regression tree. A decision tree is a tree-based model where each internal node represents a feature, each branch represents a decision based on that feature, and each leaf node represents an outcome or predicted value. In a regression tree, the root node represents the entire dataset, and the tree is built by recursively splitting the data into subsets based on the values of the input features. At each internal node, the algorithm selects the feature and the threshold that results in the most homogeneous subsets of the target variable. The process continues until a stopping criterion is met, such as a minimum number of samples per leaf or a maximum tree depth is reached. The root node of a regression tree represents the starting point of the tree, and the predictions are made by traversing the tree from the root to a leaf node. The value at the leaf node is the predicted value for the given input. It\u0026rsquo;s important to note that, like other decision tree-based models, regression trees are prone to overfitting if the tree is grown too deep, therefore it\u0026rsquo;s important to use techniques such as pruning to prevent overfitting. Spline regression Spline regression is a technique used to fit a smooth curve to a set of data points. It is a type of non-parametric regression method that can be used to model complex, non-linear relationships between a dependent variable and one or more independent variables. A spline is a piecewise polynomial function that is used to approximate a smooth curve. The basic idea behind spline regression is to divide the independent variable into a set of intervals or knots and then fit a separate polynomial function to each interval. The polynomials are then \u0026ldquo;stitched\u0026rdquo; together to form a smooth curve that can be used to model the relationship between the independent and dependent variables. There are several different types of spline regression, including natural cubic splines, thin plate splines, and radial basis function splines. Spline regression is particularly useful when the relationship between the independent and dependent variables is not well understood, or when the data points are non-linearly distributed. It\u0026rsquo;s widely used in various fields such as economics, engineering, and bio-statistics. It\u0026rsquo;s important to note that, unlike linear regression, the interpretability of the results in spline regression can be more challenging and require more expertise to understand the results. It\u0026rsquo;s also important to select the appropriate number and location of knots to achieve a good balance between fit and smoothness. Transformation In regression, data transformation refers to the process of applying a mathematical function to a variable in order to change its distribution or relationship with other variables. There are several reasons why data transformation may be necessary in regression: Linearity: Linear regression assumes that the relationship between the independent and dependent variables is linear. If the data violates this assumption, then transforming the variables can help to linearize the relationship. Normality: Linear regression assumes that the errors are normally distributed. If the residuals are not normally distributed, then transforming the variables can help to improve the normality of the residuals. Outliers: Outliers can have a large impact on the regression coefficients and can lead to poor predictions. Transforming the variables can help to reduce the influence of outliers. Collinearity: Collinearity occurs when two or more independent variables are highly correlated. Transforming the variables can help to reduce collinearity and improve the interpretability of the regression coefficients. -Common transformations used in regression include: Log transformation: It can be used to transform variables that have a positive skew, it can help to linearize the relationship between variables and reduce the influence of outliers. Square root transformation: It can be used to transform variables that have a positive skew and is useful when the variable is strictly positive. Box-Cox transformation: It is a more general transformation that can be used to handle a wide range of skewness, it can also be used to handle non-normality of the residuals. It\u0026rsquo;s important to note that, the choice of transformation depends on the specific characteristics of the data and the goals of the analysis. Additionally, the interpretation of the coefficients can be more challenging when using transformed variables. Tree A Regression Tree is a type of decision tree used for regression problems. It is a tree-based model where each internal node represents a feature, each branch represents a decision based on that feature, and each leaf node represents an outcome or predicted value. The algorithm works by recursively splitting the data into subsets based on the values of the input features. At each node, the algorithm selects the feature and the threshold that results in the most homogeneous subsets of the target variable, using a criterion such as mean squared error, mean absolute error or others. The process continues until a stopping criterion is met, such as a minimum number of samples per leaf or a maximum tree depth is reached. Regression trees are simple to understand and interpret, they can handle both categorical and numerical features and missing values. They can model non-linear relationships as well as interactions between features. They are also relatively insensitive to outliers, and they can handle non-linear relationships between the independent and dependent variables. Regression trees are used for both linear and non-linear regression problems. They are widely used in various industries such as finance, healthcare, and engineering. It\u0026rsquo;s important to note that, like other decision tree-based models, regression trees are prone to overfitting if the tree is grown too deep, therefore it\u0026rsquo;s important to use techniques such as pruning or early stopping to prevent overfitting. Additionally, when dealing with large datasets, decision tree algorithms like random forest and gradient boosting can be used which tend to perform better than a single decision tree. Time series models Additive seasonality Additive seasonality refers to a pattern in which the seasonal component of a time series is modeled as a separate, additive component that is added to the overall trend. This means that the seasonal component is not considered to be related to the trend or the level of the time series, but is instead treated as an independent factor that affects the overall value of the time series. In additive seasonality, the time series can be modeled as Y = T + S + E where Y is the original time series, T is the trend component, S is the seasonal component, and E is the error or residual component. This type of seasonality is useful when the seasonal pattern is relatively constant over time and does not change with the level of the time series. It is often used in time series with a relatively small amplitude of seasonal fluctuations, such as temperature data. Additive seasonality can be modeled using various methods such as moving averages, exponential smoothing, and seasonal decomposition of time series (STL), and it can be removed from the time series to better understand the underlying trend and forecast future values. ARIMA ARIMA (AutoRegressive Integrated Moving Average) is a statistical model that is used to analyze and forecast time series data. It is a combination of three components: the autoregression (AR) component, the difference component (I for integrated), and the moving average (MA) component. The autoregression (AR) component models the dependence between an observation and a number of lagged observations. The difference component (I for integrated) models the dependence between an observation and the differences between consecutive observations (i.e., the dependence between an observation and the previous observation). The moving average (MA) component models the dependence between the observation and a residual error from a moving average model applied to lagged observations. The parameters of the model, (p,d,q) are determined by analyzing the characteristics of the time series such as trend, seasonality and autocorrelation. The order of differencing (d) is used to make the time series stationary, this means that the mean and variance are constant over time. ARIMA models are widely used in various industries such as finance, economics, and engineering to forecast future values of time series data. It\u0026rsquo;s a powerful tool to model and forecast time series data, however, it can be challenging to determine the appropriate values for the parameters of the model, and the model assumptions must be met for accurate forecasting. Autoregression Autoregression (AR) is a statistical model that describes the relationship between a variable and its own lagged values. It is a type of time series model that is used to analyze and forecast time series data. In an autoregressive (AR) model, the current value of a variable is assumed to be a linear combination of its past values. Mathematically, an autoregressive model of order p is represented as: Yt = c + ϕ1Yt-1 + ϕ2Yt-2 + … + ϕpYt-p + εt Where Yt is the value of the variable at time t, c is a constant, ϕ1, ϕ2, …, ϕp are the autoregressive coefficients, Yt-1, Yt-2, …, Yt-p are the lagged values of the variable, and εt is the error term.\nAutoregression is used to model the dependence between an observation and a number of lagged observations. It is widely used to model time series data that exhibit a predictable pattern such as a trend, seasonal pattern or cyclic behavior. The order of the model (p) determines the number of lagged values that are used in the model. A higher order autoregression model will use more lagged values, which makes the model more complex but also more accurate. It\u0026rsquo;s important to note that Autoregression is often used in combination with other models such as moving average and differencing to create a more complete time series model such as ARIMA or ARMA. Differencing Differencing in time series refers to the process of taking the difference between consecutive observations in a time series data. It is used to remove the trend and/or seasonality in the data, and it is a common step in preparing time series data for analysis and forecasting. There are two types of differencing: first difference and second difference. First difference is calculated by subtracting the value of each observation from the value of the previous observation. This can be used to remove the trend in the data, making it stationary. Second difference is calculated by taking the first difference and then subtracting the value of the first difference from the value of the previous first difference. This can be used to remove the seasonality in the data. -Differencing can be used to remove both trend and seasonality in a time series by taking the first difference of the data to remove the trend, then taking the first difference of the difference data to remove the seasonality. This is known as seasonal differencing or second differencing, and it is represented as d=1, D=1 in the notation of ARIMA models, where d is the order of differencing and D is the seasonal order of differencing. It\u0026rsquo;s important to note that differencing can make the data more difficult to interpret, and it can also make it more challenging to forecast future values. Additionally, it is important to check the stationarity of the data before and after differencing to make sure that the differencing process has been done correctly. Double exponential Double Exponential Smoothing, also known as Holt-Winters method, is a technique used for forecasting time series data that takes into account both the level and trend of the data. It is an extension of exponential smoothing that adds a term for the trend component to the model. In Double Exponential smoothing, the forecast for time t+1 is a weighted average of the level and the trend at time t, with weights determined by the smoothing parameters. The level component is similar to the simple exponential smoothing, while the trend component is a linear function of time. The forecast equation can be represented as: F(t+1) = αy(t) + (1-α)(F(t) + T(t)) Where F(t) is the forecast for time t, y(t) is the observed value at time t, T(t) is the trend at time t, and α is the smoothing parameter for the level component.\nDouble Exponential Smoothing is suitable for time series that exhibit both a trend and seasonality. The method can be extended to include the seasonal component, called Holt-Winters seasonal method, by adding a term for the seasonal component to the model. It\u0026rsquo;s widely used in various industries such as finance, economics, and engineering to forecast future values of time series data. smoothing Smoothing refers to the process of making a set of data points less variable and more predictable. In time series analysis, smoothing is used to remove the noise or random variation in the data and to reveal the underlying trend or pattern. There are several types of smoothing techniques used in time series analysis: Moving average: This technique involves calculating the average value of a set of data points over a certain period of time (e.g., a rolling window of 3 or 7 days). This technique can be used to smooth out the noise in the data, but it can also smooth out the underlying pattern. Exponential smoothing: This technique involves giving more weight to the more recent data points, and less weight to the older data points. This technique can be used to smooth out the noise in the data and to reveal the underlying trend. Loess smoothing: This technique uses a local regression model that fits a polynomial function to the data points in a neighborhood around a target data point. It is a non-parametric method that can handle non-linear trends. Savitzky-Golay filtering: This technique uses a polynomial least-squares method to smooth the data by fitting a polynomial to a set of data points. It can be used to smooth out noise and preserve the underlying pattern. Smoothing techniques can be used to improve the accuracy of predictions, but it\u0026rsquo;s important to choose the appropriate smoothing technique depending on the characteristics of the data and the goals of the analysis. Additionally, over-smoothing can lead to loss of important information in the data. Exponential smoothing Exponential smoothing is a time series forecasting method that uses a weighted average of past observations to predict future values. It is used to smooth out the noise or random variation in the data and to reveal the underlying trend or pattern. In exponential smoothing, the forecast for time t+1 is a weighted average of the past observations, with more weight given to the more recent observations. The weights decrease exponentially as the observations get older. The forecast equation can be represented as: F(t+1) = α*y(t) + (1-α)*F(t) Where F(t) is the forecast for time t, y(t) is the observed value at time t, and α is the smoothing parameter, a value between 0 and 1 that determines the weight given to the most recent observation.\nSimple Exponential smoothing is suitable for time series that do not have a trend or seasonality. However, Double Exponential Smoothing, also known as Holt-Winters method, is a more powerful technique that takes into account both the level and trend of the data and can be used for time series that exhibit both a trend and seasonality. Exponential smoothing is a simple yet powerful method that can be used to forecast future values of time series data. It\u0026rsquo;s widely used in various industries such as finance, economics, and engineering. It\u0026rsquo;s important to note that the choice of the smoothing parameter α is important for the accuracy of the forecasts, and it can be chosen using methods such as grid search or optimization algorithms. Generalized autoregressive conditional heteroscedasticity (GARCH) GARCH (Generalized Autoregressive Conditional Heteroskedasticity) is a statistical model that is used to model and forecast volatility in financial time series data. It is an extension of the ARCH (Autoregressive Conditional Heteroskedasticity) model, which was developed to model time series with varying variances, also known as volatility clustering. The GARCH model is a combination of two components: the autoregressive component, which models the dependence between the current volatility and the past volatility, and the moving average component, which models the dependence between the current volatility and the past errors or residuals. The GARCH model can be represented mathematically as: σ^2(t) = ω + α* ε^2(t-1) + β*σ^2(t-1) Where σ^2(t) is the conditional variance at time t, ε(t) is the error or residual at time t, ω is the constant term, α is the weight given to the past error, and β is the weight given to the past volatility.\nGARCH models are widely used in finance and economics to model and forecast volatility in financial time series data such as stock prices and exchange rates. It\u0026rsquo;s an important tool for risk management, portfolio optimization and option pricing. GARCH models are typically estimated using maximum likelihood estimation and the order of the model (p,q) is determined by analyzing the characteristics of the time series such as autocorrelation and partial autocorrelation. Holt-Winters method The Holt-Winters method, also known as the triple exponential smoothing, is a technique used for forecasting time series data that takes into account both the level and trend of the data, as well as the seasonality of the data. It is an extension of the exponential smoothing method that adds a term for the trend component and a term for the seasonal component to the model. The Holt-Winters method can be used to forecast future values in time series data that exhibit both a trend and a seasonality. It can be used in cases where the seasonal pattern is relatively stable over time, and the amplitude of the seasonal fluctuations is relatively constant. The forecast equation can be represented as: F(t+1) = αy(t) + (1-α)(F(t) + T(t)) + γ*(y(t) - y(t-m)) Where F(t) is the forecast for time t, y(t) is the observed value at time t, T(t) is the trend at time t, α is the smoothing parameter for the level component, γ is the smoothing parameter for the seasonal component, and m is the number of seasons.\nThe Holt-Winters method can be used in various industries such as finance, economics, and engineering to forecast future values of time series data. It\u0026rsquo;s important to note that the choice of the smoothing parameters α and γ is important for the accuracy of the forecasts, and it can be chosen using methods such as grid search or optimization algorithms. Additionally, when dealing with large datasets, advanced methods like ARIMA, ETS and SARIMA can be used which tend to perform better than Holt Winters method. Moving average A Moving Average (MA) is a statistical method used to smooth out fluctuations in time series data and to reveal the underlying trend. It is used to remove the noise or random variation in the data and to make the data more predictable. In a moving average, a set of consecutive data points is used to calculate the average value, which is then used as a forecast for the next time period. The forecast for time t+1 is calculated by taking the average of a fixed number of past observations, called the window size. The moving average can be represented mathematically as: F(t+1) = (1/n) * (y(t) + y(t-1) + … + y(t-n+1)) Where F(t+1) is the forecast for time t+1, y(t) is the observed value at time t, y(t-1) is the observed value at time t-1, and so on, n is the window size.\nMoving average is a simple and widely used technique for time series analysis, it is computationally efficient, easy to understand and interpret. However, it can be sensitive to outliers and it can also smooth out the underlying pattern, especially if the window size is too large. It\u0026rsquo;s important to choose the appropriate window size for the moving average depending on the characteristics of the data and the goals of the analysis. Additionally, it is important to check the stationarity of the data before and after the moving average is applied to make sure that the process has been done correctly. Multiplicative seasonality Multiplicative seasonality refers to a pattern in which the seasonal component of a time series is modeled as a separate, multiplicative component that is multiplied by the overall trend. This means that the seasonal component is considered to be related to the trend or the level of the time series, and it affects the overall value of the time series in proportion to the level. In multiplicative seasonality, the time series can be modeled as Y = T * S * E where Y is the original time series, T is the trend component, S is the seasonal component, and E is the error or residual component. This type of seasonality is useful when the seasonal pattern changes with the level of the time series, such as sales data or temperature data. Multiplicative seasonality can be modeled using various methods such as moving averages, exponential smoothing, and seasonal decomposition of time series (STL), and it can be removed from the time series to better understand the underlying trend and forecast future values. It\u0026rsquo;s important to note that when working with data that has multiplicative seasonality, it is necessary to log transform the data in order to make it additive, this way it\u0026rsquo;s easier to forecast with traditional methods. Seasonality/cycles Seasonality refers to patterns in time series data that occur at regular intervals, such as daily, weekly, or yearly. These patterns are often predictable and can be used to forecast future values. Seasonality can be caused by natural or man-made factors such as weather, holidays, or business cycles. Cycles refer to patterns in time series data that occur at irregular intervals, but are still predictable. These patterns can be caused by economic or political factors, and they can be used to forecast future values. Both seasonality and cycles can have an impact on the forecast of a time series. Identifying and modeling seasonality and cycles can improve the accuracy of forecasts by taking into account these predictable patterns. There are several methods used to identify and model seasonality and cycles in time series data, such as: Visual inspection of the time series plot, Autocorrelation and partial autocorrelation plots, Seasonal decomposition of time series (STL), Spectral analysis, and others. It\u0026rsquo;s important to note that for some time series, the patterns could be complex, and a combination of methods may be necessary to fully capture the seasonality and cycles. Additionally, once identified, those patterns can be removed from the time series to better understand the underlying trend and forecast future values. Seasonality length/cycle length Seasonality length refers to the number of time periods in a seasonal pattern. For example, in a daily time series, a seasonality length of 7 would indicate a weekly pattern, while a seasonality length of 365 would indicate a yearly pattern. The length of the seasonality can be determined by analyzing the time series data, and it is an important factor when choosing an appropriate model for the data. Cycle length refers to the number of time periods in a cyclical pattern. It is similar to seasonality length, but it is used to describe patterns that occur at irregular intervals. The length of a cycle can vary depending on the nature of the data and the underlying causes of the pattern. When working with time series data, it\u0026rsquo;s important to determine the correct seasonality length and cycle length, as it will help in choosing the appropriate model for the data and in forecasting future values. The length of seasonality can be determined by visual inspection of the time series plot, by analyzing autocorrelation and partial autocorrelation plots, or by using decomposition methods such as seasonal decomposition of time series (STL). For cycles, spectral analysis or other advanced methods can be used to identify the length of the cycle. It\u0026rsquo;s important to note that the length of seasonality and cycle can change over time, and it\u0026rsquo;s important to monitor and update the model if necessary. Single exponential smoothing Single Exponential Smoothing (SES) is a time series forecasting method that uses a weighted average of past observations to predict future values. It is used to smooth out the noise or random variation in the data and to reveal the underlying trend or pattern. In single exponential smoothing, the forecast for time t+1 is a weighted average of the past observations, with more weight given to the most recent observation. The weights decrease exponentially as the observations get older. The forecast equation can be represented as: F(t+1) = α*y(t) + (1-α)*F(t) Where F(t) is the forecast for time t, y(t) is the observed value at time t, and α is the smoothing parameter, a value between 0 and 1 that determines the weight given to the most recent observation.\nSingle Exponential Smoothing is suitable for time series that do not have a trend or seasonality. It is a simple and easy to understand method that can be used to forecast future values of time series data, it\u0026rsquo;s widely used in various industries such as finance, economics, and engineering. However, it may not be the best method to be used when the data has a trend or seasonality. It\u0026rsquo;s important to note that the choice of the smoothing parameter α is important for the accuracy of the forecasts, and it can be chosen using methods such as grid search or optimization algorithms. It\u0026rsquo;s also important to check the stationarity of the data before and after the single exponential smoothing is applied to make sure that the process has been done correctly. Smoothing Smoothing refers to the process of making a set of data points less variable and more predictable. In time series analysis, smoothing is used to remove the noise or random variation in the data and to reveal the underlying trend or pattern. There are several types of smoothing techniques used in time series analysis: Moving average: This technique involves calculating the average value of a set of data points over a certain period of time (e.g., a rolling window of 3 or 7 days). This technique can be used to smooth out the noise in the data, but it can also smooth out the underlying pattern. Exponential smoothing: This technique involves giving more weight to the more recent data points, and less weight to the older data points. This technique can be used to smooth out the noise in the data and to reveal the underlying trend. Loess smoothing: This technique uses a local regression model that fits a polynomial function to the data points in a neighborhood around a target data point. It is a non-parametric method that can handle non-linear trends. Savitzky-Golay filtering: This technique uses a polynomial least-squares method to smooth the data by fitting a polynomial to a set of data points. It can be used to smooth out noise and preserve the underlying pattern. Smoothing techniques can be used to improve the accuracy of predictions, but it\u0026rsquo;s important to choose the appropriate smoothing technique depending on the characteristics of the data and the goals of the analysis. Additionally, over-smoothing can lead to loss of important information in the data. It\u0026rsquo;s worth noting that some of these methods are more suited for certain types of data and it\u0026rsquo;s important to select the right one to achieve the best results. Smoothing constant A smoothing constant is a parameter used in smoothing techniques, such as exponential smoothing, that determines the weight given to the most recent observations. It is used to control the amount of smoothing applied to the data. In exponential smoothing, the smoothing constant (also known as the smoothing parameter) is denoted by α and it is a value between 0 and 1. A value of α close to 1 gives more weight to the most recent observations and less weight to the older observations, which results in less smoothing. A value of α close to 0 gives more weight to the older observations and less weight to the most recent observations, which results in more smoothing. The choice of the smoothing constant is important for the accuracy of the forecasts. A high value of α will give more weight to recent observations, which is useful when the data is highly variable and the underlying trend is changing rapidly. A low value of α will give more weight to older observations, which is useful when the data is less variable and the underlying trend is relatively stable. The smoothing constant can be chosen using methods such as grid search or optimization algorithms, where different values of α are tried and the one that results in the best forecast accuracy is selected. It\u0026rsquo;s also important to note that the smoothing constant is different for each type of smoothing technique, for example, in moving average, it\u0026rsquo;s the window size that acts as a smoothing constant. Stationary process A stationary process is a time series in which the statistical properties (such as mean, variance, and autocovariance) are constant over time. In other words, a stationary time series has a constant mean, a constant variance, and a constant autocovariance that does not change over time. A process is said to be stationary if the following conditions are met: The mean is constant over time The variance is constant over time The covariance between observations at different times is constant over time A stationary process is useful for time series forecasting because the future behavior of a stationary process can be predicted using its past behavior. For example, if a time series has a constant mean and a constant variance, then it is possible to predict the future values of the series using the past values. There are two types of stationary process: Weakly stationary: The mean, variance, and autocovariance function do not depend on time. Strictly stationary: The joint probability distribution of any two or more random variables is the same for all time pairs or groups. Stationarity is an important assumption in many time series models such as ARIMA and GARCH. To check if a time series is stationary, one can use visual inspection of the time series plot, the Augmented Dickey-Fuller test and the Kwiatkowski-Phillips-Schmidt-Shin test. Additionally, when the data is not stationary, it can be made stationary using techniques such as differencing, logarithmic transformation and detrending. Trend A trend refers to a pattern in time series data that shows a gradual increase or decrease in the value of the data over time. Trends can be upward, downward, or flat. An upward trend indicates that the value of the data is increasing over time, a downward trend indicates that the value of the data is decreasing over time, and a flat trend indicates that the value of the data is staying relatively constant over time. Trends can be caused by various factors such as economic growth, population growth, technological advancements, and more. Identifying and modeling trends in time series data can improve the accuracy of forecasts by taking into account the long-term patterns in the data. There are several methods used to identify and model trends in time series data, such as: Visual inspection of the time series plot Linear regression Exponential smoothing Trend decomposition and others It\u0026rsquo;s important to note that trends can change over time, and it\u0026rsquo;s important to monitor and update the model if necessary. Additionally, it\u0026rsquo;s important to identify the right type of trend, linear or non-linear, as this will help in choosing the appropriate model for the data. Triple exponential smoothing Triple Exponential Smoothing, also known as the Holt-Winters method, is a time series forecasting method that uses a weighted average of past observations, past trends, and past seasonal patterns to predict future values. It is used to smooth out the noise or random variation in the data and to reveal the underlying trend, pattern, and seasonality of the data. In triple exponential smoothing, the forecast for time t+1 is a weighted average of the past observations, past trends, and past seasonal patterns, with more weight given to the more recent observations, trends, and seasonal patterns. The weights decrease exponentially as the observations, trends, and seasonal patterns get older. The forecast equation can be represented as: F(t+1) = αy(t) + (1-α)(F(t) + T(t)) + γ*(y(t) - y(t-m)) Where F(t) is the forecast for time t, y(t) is the observed value at time t, T(t) is the trend at time t, α is the smoothing parameter for the level component, γ is the smoothing parameter for the seasonal component, and m is the number of seasons.\nTriple Exponential Smoothing is suitable for time series that have trend and seasonality. It is a more advanced and sophisticated method than single and double exponential smoothing. It can be used in various industries such as finance, economics, and engineering to forecast future values of time series data. It\u0026rsquo;s important to note that the choice of the smoothing parameters α and γ is important for the accuracy of the forecasts, and it can be chosen using methods such as grid search or optimization algorithms. Additionally, it\u0026rsquo;s important to check the stationarity of the data before and after the triple exponential smoothing is applied to make sure that the process has been done correctly. Winters\u0026rsquo; method Winters\u0026rsquo; method, also known as Exponential smoothing with additive damped trend and additive seasonality, is a forecasting method for time series data that includes trends and seasonality. It\u0026rsquo;s a variation of the Holt-Winters method, which is a generalization of exponential smoothing method that adds a trend and a seasonality component to the forecast equation. The Winters\u0026rsquo; method uses two smoothing parameters, α and β, to control the level and trend components, respectively, and another parameter, γ, to control the seasonal component. The method is designed to forecast future values based on the past observations, past trends and past seasonal patterns. The forecast equation for Winters\u0026rsquo; method can be represented as: F(t+h|t) = l_t + h*b_t + s_{t-m+1+h(mod m)} Where F(t+h|t) is the forecast for time t+h, l_t is the level at time t, b_t is the trend at time t, s_i is the seasonal component at time i, h is the forecast horizon, m is the number of seasons, and the mod operator denotes the modulo operation.\nWinters\u0026rsquo; method can be used to forecast future values of time series data with additive seasonality and trends and it\u0026rsquo;s suitable for data that has a stable pattern. It\u0026rsquo;s widely used in various industries such as finance, economics, and engineering, just like Holt-Winters method. It\u0026rsquo;s important to note that the choice of the smoothing parameters α, β and γ is important for the accuracy of the forecasts, and it can be chosen using methods such as grid search or optimization algorithms. Additionally, it\u0026rsquo;s important to check the stationarity of the data before and after the Winters\u0026rsquo; method is applied to make sure that the process has been done correctly. Variable Selection Backward elimination Backward elimination is a feature selection method used in regression analysis to identify the most important predictor variables that contribute to the response variable. The method starts with all the predictor variables in the model and then iteratively removes the variable that has the least statistical significance until all remaining variables are considered important. The basic steps of backward elimination are: Start with a full model that includes all predictor variables. Fit the model and calculate the p-value for each predictor variable. Select the predictor variable with the highest p-value and remove it from the model. Fit the model again with the remaining variables and calculate the p-value for each variable. Repeat steps 3 and 4 until all remaining variables have p-values lower than a given threshold. The threshold for the p-value is usually set to 0.05, which means that a predictor variable will be removed from the model if its p-value is greater than 0.05. This threshold can be adjusted depending on the specific application and the desired level of significance. Backward elimination is a simple and easy to understand method, and it\u0026rsquo;s widely used in various industries such as finance, economics, and engineering. However, it has some limitations, such as it can be computationally expensive and it could lead to overfitting if the number of observations is not large enough. It\u0026rsquo;s important to note that Backward elimination should be used in combination with other feature selection methods, such as forward selection and recursive feature elimination, to get a more robust model. Elastic net Elastic net is a regularization method used in linear regression to prevent overfitting by combining the L1 regularization (also known as Lasso) and L2 regularization (also known as Ridge) techniques. The L1 regularization adds a penalty term to the cost function that is proportional to the absolute value of the coefficients, and the L2 regularization adds a penalty term that is proportional to the square of the coefficients. The elastic net method is controlled by two parameters: α, the mixing parameter that controls the balance between L1 and L2 regularization, and λ, the regularization parameter that controls the overall strength of the regularization. When α = 0, the elastic net is equivalent to the Ridge regularization. When α = 1, the elastic net is equivalent to the Lasso regularization. For 0 \u0026lt; α \u0026lt; 1, the elastic net is a combination of Ridge and Lasso and it can select some variables while shrinking others. The elastic net method can be used to handle collinearity and when the number of predictors is greater than the number of observations. It also can handle correlated features and can be used to select relevant features. It\u0026rsquo;s important to note that the selection of the α and λ parameters is important for the performance of the elastic net method. It\u0026rsquo;s usually done using cross-validation or other optimization techniques. Forward selection Forward selection is a feature selection method used in regression analysis to identify the most important predictor variables that contribute to the response variable. The method starts with an empty model and then iteratively adds the variable that has the highest statistical significance until all remaining variables are considered important. The basic steps of forward selection are: Start with an empty model that includes no predictor variables. Fit the model and calculate the p-value for each predictor variable. Select the predictor variable with the lowest p-value and add it to the model. Fit the model again with the added variable and calculate the p-value for each variable. Repeat steps 3 and 4 until all remaining variables have p-values lower than a given threshold. The threshold for the p-value is usually set to 0.05, which means that a predictor variable will be added to the model if its p-value is lower than 0.05. This threshold can be adjusted depending on the specific application and the desired level of significance. Forward selection is a simple and easy to understand method, and it\u0026rsquo;s widely used in various industries such as finance, economics, and engineering. However, it has some limitations, such as it can be computationally expensive and it could lead to overfitting if the number of observations is not large enough. It\u0026rsquo;s important to note that Forward selection should be used in combination with other feature selection methods, such as backward elimination and recursive feature elimination, to get a more robust model. Lasso/Lasso regression Lasso regression (Least Absolute Shrinkage and Selection Operator) is a linear regression method that uses L1 regularization to shrink the coefficients of the predictor variables towards zero. L1 regularization adds a penalty term to the cost function that is proportional to the absolute value of the coefficients. The L1 penalty term causes some coefficients to be exactly equal to zero, which results in some predictor variables being completely excluded from the model. The Lasso method solves the following optimization problem: minimize (1/n) * ||y - Xw||^2 + λ * ||w||_1 Where w is the vector of coefficients, X is the design matrix, y is the response variable, λ is the regularization parameter, and ||w||_1 is the L1-norm of the coefficients.\nLasso regression can be used to handle high-dimensional data with many predictor variables, where some of the variables may be irrelevant. It can also be used to handle correlated features and can be used to select relevant features. It\u0026rsquo;s important to note that the choice of the regularization parameter λ is important for the performance of the Lasso regression, as it controls the trade-off between the goodness of fit and the complexity of the model. It\u0026rsquo;s usually done using cross-validation or other optimization techniques, and it\u0026rsquo;s important to keep in mind that Lasso will always produce sparse models, i.e. models with few non-zero coefficients. Overfitting Overfitting is a phenomenon that occurs when a machine learning model is trained to fit the training data too closely, resulting in poor generalization performance on new, unseen data. It occurs when a model is too complex or has too many parameters relative to the amount of training data available. Overfitting is a common problem in machine learning and can be caused by various factors, such as: Having too many features or variables relative to the number of observations Using a complex model with a large number of parameters Using a model that is not well-suited for the data When a model is overfitting, it will perform well on the training data but poorly on the testing data. This is because the model has learned the noise in the training data and not the underlying pattern. The model becomes too specialized to the training data, rather than generalizing to new, unseen data. To prevent overfitting, several techniques can be used, such as: Simplifying the model by reducing the number of features or parameters Regularization techniques, such as L1 and L2 regularization Early stopping Using cross-validation to estimate the generalization error Ensemble methods such as random forests and gradient boosting It\u0026rsquo;s important to keep in mind that a balance between underfitting and overfitting should be sought, and the best model is the one that generalizes well on unseen data while keeping the complexity at bay. Regularization Regularization is a technique used in machine learning and statistics to prevent overfitting by adding a penalty term to the cost function of the model. The purpose of regularization is to shrink the coefficients of the predictor variables towards zero, which reduces the complexity of the model and the variance of the predictions. There are two main types of regularization: L1 and L2 regularization. L1 regularization, also known as Lasso regularization, adds a penalty term to the cost function that is proportional to the absolute value of the coefficients. This type of regularization tends to shrink the coefficients of the less important features to zero, effectively removing them from the model. L2 regularization, also known as Ridge regularization, adds a penalty term to the cost function that is proportional to the square of the coefficients. This type of regularization tends to shrink the coefficients of all features, but it doesn\u0026rsquo;t remove any features from the model. The regularization term is controlled by a regularization parameter, which determines the strength of the regularization. A smaller regularization parameter results in stronger regularization and a larger parameter results in weaker regularization. The regularization parameter can be chosen using cross-validation or other optimization techniques. Regularization is used to prevent overfitting by reducing the complexity of the model, it\u0026rsquo;s used in various types of models such as linear regression, logistic regression, and neural networks. Regularization can be used alone or in combination with other techniques such as early stopping, dropout, and data augmentation. Ridge regression Ridge regression is a linear regression method that uses L2 regularization to shrink the coefficients of the predictor variables towards zero. L2 regularization adds a penalty term to the cost function that is proportional to the square of the coefficients. The L2 penalty term causes the coefficients to be close to zero, but not exactly zero, which results in a model that is less complex than the unregularized model. The Ridge method solves the following optimization problem: minimize (1/n) * ||y - Xw||^2 + λ * ||w||^2_2 Where w is the vector of coefficients, X is the design matrix, y is the response variable, λ is the regularization parameter, and ||w||^2_2 is the L2-norm of the coefficients.\nRidge regression can be used to handle high-dimensional data with many predictor variables, where some of the variables may be irrelevant. It can also be used to handle correlated features and can handle multicollinearity. It\u0026rsquo;s important to note that Ridge regression shrinks the coefficient towards zero, but it doesn\u0026rsquo;t eliminate any feature from the model. Also, the choice of the regularization parameter λ is important for the performance of the Ridge regression, as it controls the trade-off between the goodness of fit and the complexity of the model. It\u0026rsquo;s usually done using cross-validation or other optimization techniques. Simplicity (of a model) Simplicity of a model refers to how easy it is to understand and interpret the model, and how few parameters it has relative to the amount of data available. A simple model has a small number of parameters and is easy to understand, while a complex model has a large number of parameters and is difficult to understand. Simplicity of a model is important because it helps to reduce overfitting, as a simple model is less likely to fit the noise in the data. Simple models are also easier to interpret and explain, which is important in many real-world applications. However, it\u0026rsquo;s important to keep in mind that a model that is too simple might not capture the underlying patterns in the data and might underfit the data, leading to poor predictions. Therefore, a balance between simplicity and complexity should be sought in building a model. There are several techniques that can be used to make a model simpler, such as: Feature selection: removing irrelevant or redundant features from the data Dimensionality reduction: reducing the number of features in the data Regularization: adding a penalty term to the cost function to shrink the coefficients of the model Ensemble methods: combining multiple simpler models to make a more robust model It\u0026rsquo;s important to keep in mind that the choice of a model depends on the specific problem and the data available, so it\u0026rsquo;s important to evaluate different models and select the one that strikes the right balance between simplicity and complexity. Stepwise regression Stepwise regression is a feature selection method used in regression analysis to identify the most important predictor variables that contribute to the response variable. It is a type of automated feature selection method that combines both forward selection and backward elimination by iteratively adding or removing variables based on their statistical significance. The basic steps of stepwise regression are: Start with an empty model or a full model that includes all predictor variables Fit the model and calculate the p-value for each predictor variable. Select the predictor variable with the lowest p-value (forward selection) or the highest p-value (backward elimination) and add or remove it from the model. Fit the model again with the updated variables and calculate the p-value for each variable. Repeat steps 3 and 4 until no more variables can be added or removed from the model. Stepwise regression is a simple and easy to understand method, and it\u0026rsquo;s widely used in various industries such as finance, economics, and engineering. However, it has some limitations, such as it can be computationally expensive and it could lead to overfitting if the number of observations is not large enough. Additionally, it\u0026rsquo;s a greedy method and it may not find the optimal solution. It\u0026rsquo;s important to note that Stepwise regression should be used with caution, it\u0026rsquo;s not recommended to rely solely on this method, and it\u0026rsquo;s important to use it in combination with other feature selection methods, such as forward selection, backward elimination, and recursive feature elimination, to get a more robust model. Variable selection Variable selection is the process of identifying a subset of relevant variables from a larger set of predictor variables for a given problem. It\u0026rsquo;s an important step in building a machine learning model as it can help to improve the model\u0026rsquo;s performance, reduce overfitting, and make the model more interpretable. There are several variable selection methods that can be used, such as: Filter methods: These methods use a pre-defined criterion, such as correlation or mutual information, to select a subset of variables. They are generally fast, but they may not select the best subset of variables for the problem. Wrapper methods: These methods use the performance of a given model to select a subset of variables. They are more computationally expensive than filter methods, but they generally select a better subset of variables for the problem. Embedded methods: These methods use the optimization of the model\u0026rsquo;s parameters as part of the variable selection process. Examples include Lasso and Ridge regression. Hybrid methods: These methods combine the strengths of different variable selection methods to select the best subset of variables for the problem. It\u0026rsquo;s important to note that the choice of a variable selection method depends on the specific problem and the data available, and it\u0026rsquo;s important to evaluate different methods and select the one that strikes the right balance between model\u0026rsquo;s performance and interpretability. Additionally, it\u0026rsquo;s important to use variable selection in conjunction with other techniques such as regularization, feature engineering, and model evaluation to get a more robust model. Misc 1-norm The 1-norm, also known as the L1-norm, is a measure of the size or magnitude of a vector, and it\u0026rsquo;s calculated as the sum of the absolute values of the vector\u0026rsquo;s elements. It\u0026rsquo;s also called the \u0026ldquo;Manhattan norm\u0026rdquo; or \u0026ldquo;taxi-cab norm\u0026rdquo; because it\u0026rsquo;s the distance between two points in a grid if you can only move horizontally or vertically, like a taxi driving on the streets of Manhattan. The L1-norm of a vector x is defined as: ||x||1 = ∑|x_i| Where x_i is the i-th element of the vector x.\nThe L1-norm has several properties, such as: It\u0026rsquo;s not differentiable at x_i = 0 It\u0026rsquo;s not a Euclidean norm, meaning that it doesn\u0026rsquo;t satisfy the triangle inequality It\u0026rsquo;s not a norm in the mathematical sense, since it doesn\u0026rsquo;t satisfy the homogeneity and subadditivity properties The L1-norm is used in various areas of machine learning and optimization, such as in Lasso regression, and in feature selection, where it\u0026rsquo;s used as a measure of feature importance. In Lasso regression, the L1-norm is used as a regularization term to shrink the coefficients of the predictor variables towards zero. This results in some variables being completely excluded from the model, effectively performing feature selection. It\u0026rsquo;s also used in the field of computer vision, particularly in problems such as image denoising, where the L1-norm is used to minimize the difference between the original image and the denoised image. 2-norm The 2-norm, also known as the L2-norm or Euclidean norm, is a measure of the size or magnitude of a vector, and it\u0026rsquo;s calculated as the square root of the sum of the squares of the vector\u0026rsquo;s elements. It\u0026rsquo;s the most commonly used norm in machine learning and optimization, and it\u0026rsquo;s the standard Euclidean distance between two points in a space. The L2-norm of a vector x is defined as: ||x||2 = √( ∑x_i^2 ) Where x_i is the i-th element of the vector x.\nThe L2-norm has several properties, such as: It\u0026rsquo;s a Euclidean norm, meaning that it satisfies the triangle inequality It\u0026rsquo;s differentiable everywhere It\u0026rsquo;s a norm in the mathematical sense, since it satisfies the homogeneity and subadditivity properties The L2-norm is used in various areas of machine learning and optimization, such as in Ridge regression, and in feature selection, where it\u0026rsquo;s used as a measure of feature importance. In Ridge regression, the L2-norm is used as a regularization term to shrink the coefficients of the predictor variables towards zero. This results in a model that is less complex than the unregularized model. It\u0026rsquo;s also used in various other areas such as in control theory, where it\u0026rsquo;s used to measure the stability of a system, and in image processing, where it\u0026rsquo;s used to measure the quality of image reconstructions. Convex hull (of a set of points) The convex hull of a set of points is the smallest convex polygon that contains all the points in the set. A convex polygon is a shape where, for any two points inside the shape, the entire line segment between them is also contained within the shape. In other words, all interior angles are less than 180 degrees. There are different algorithms to compute the convex hull of a set of points, such as: Graham’s scan algorithm Jarvis march (or gift wrapping) algorithm QuickHull algorithm Chan\u0026rsquo;s algorithm The convex hull is a fundamental concept in computational geometry and it\u0026rsquo;s used in various areas such as computer graphics, image processing, and pattern recognition. It can be used, for example, to find the boundaries of a shape, to compute the area of a shape, or to find the shortest path between two points that lies within a shape. It\u0026rsquo;s also used in machine learning and data analysis, such as in clustering, where it\u0026rsquo;s used to define the boundaries of clusters and in outlier detection, where it\u0026rsquo;s used to define the boundaries of the data set. It\u0026rsquo;s important to note that, a set of points that are all collinear, the Convex Hull will be a line segment, and in the case where there are only two points in the set, the Convex Hull will be the two points themselves. Descriptive analytics Descriptive analytics is a branch of data analytics that is used to summarize, describe, and understand data. It involves the use of various techniques such as statistics, data visualization, and data mining to extract insights and information from data. The goal of descriptive analytics is to understand the characteristics of the data, such as patterns, trends, and relationships, and to communicate those insights effectively to stakeholders. Descriptive analytics can be applied to various types of data, such as transactional data, log data, sensor data, and social media data. It can be used to answer questions such as: What are the most common patterns in the data? What are the key trends in the data? How is the data distributed? Are there any outliers or anomalies in the data? Some of the common techniques used in descriptive analytics include: Summarizing data using measures of central tendency (mean, median, mode) and measures of dispersion (standard deviation, variance, range) Creating data visualizations such as histograms, bar charts, and scatter plots to help understand the data Identifying patterns and relationships in the data using techniques such as correlation analysis, cluster analysis, and association rule mining Descriptive analytics is a fundamental step in the data analytics process and it\u0026rsquo;s essential to understand the data before performing more advanced analytics such as predictive or prescriptive analytics. It\u0026rsquo;s used in various industries such as finance, retail, healthcare, and manufacturing. Elbow diagram An elbow diagram is a graphical representation of the performance of a clustering algorithm, typically used to determine the optimal number of clusters for a given dataset. The elbow method is a heuristic used to determine this optimal number of clusters. The process of creating an elbow diagram involves running the clustering algorithm multiple times with different values of the number of clusters (k) and calculating the sum of squared distances between each point and its nearest centroid (also called Within-cluster-sum-of-squares or WCSS). The elbow diagram is a plot of the WCSS against the number of clusters (k) and the idea is that, as the number of clusters increases, the WCSS will decrease. However, as the number of clusters increases, the decrease in WCSS will become less pronounced. The point at which the decrease in WCSS begins to level off is considered to be the optimal number of clusters, and it\u0026rsquo;s typically represented by an \u0026ldquo;elbow\u0026rdquo; shape on the plot. It\u0026rsquo;s important to note that the elbow method is a heuristic and it doesn\u0026rsquo;t guarantee to find the optimal number of clusters, and it\u0026rsquo;s not suitable for all types of data. It\u0026rsquo;s recommended to use it in combination with other techniques such as the silhouette method and the gap statistic to get a better understanding of the data and make a more informed decision about the optimal number of clusters. The Elbow method is widely used in the field of unsupervised learning and it\u0026rsquo;s used to determine the optimal number of clusters in various types of data such as image data, text data, and time series data. Euclidian distance/straight- line distance Euclidean distance is a measure of the distance between two points in a multi-dimensional space. It is calculated as the square root of the sum of the squares of the differences in the coordinates of the two points. In other words, the Euclidean distance between two points, P and Q, in n-dimensional space is the square root of the sum of the squares of the differences of their coordinates. It is also known as L2 norm or L2 distance. It is widely used in various applications such as image processing, clustering, and pattern recognition. Heteroscedasticity Heteroscedasticity is a statistical term used to describe a situation in which the variance of a variable is non-constant across the range of values of a predictor variable. In other words, it refers to a situation in which the spread of the dependent variable is not the same across all levels of the independent variable. Heteroscedasticity can occur in linear regression models and can lead to unreliable parameter estimates and inaccurate hypothesis tests. It can be detected by visual inspection of a residual plot or by formal tests such as the Breusch-Pagan test or the White test. To address heteroscedasticity, one can use techniques such as weighted least squares, or use of heteroscedasticity-consistent standard errors. Infinity-norm The infinity norm, also known as the maximum norm, is a type of vector norm that calculates the largest absolute value of the elements in a vector. Given a vector x, the infinity norm is defined as: ||x||∞ = max(|x1|,|x2|, …, |xn|) This norm is particularly useful when dealing with large or infinite dimensional vectors, such as sequences or functions, as it provides a way to measure the \u0026ldquo;size\u0026rdquo; or \u0026ldquo;magnitude\u0026rdquo; of a vector. It is also known as Chebyshev norm or L∞ norm. It is widely used in various fields such as optimization, control theory and numerical analysis. It\u0026rsquo;s different from the Euclidean distance, which is calculated as the square root of the sum of the squares of the differences in the coordinates of the two points. Linear combination A linear combination is an expression of the form c1x1 + c2x2 + … + cnxn, where x1, x2, …, xn are variables and c1, c2, …, cn are constants. It is a linear combination of variables, because the exponents of the variables are 1 and the coefficients are constants. Linear combinations are used in many areas of mathematics and science, such as linear algebra, physics, and economics. They are also used to express a vector as a linear combination of other vectors, known as a basis. Linear combinations are used to express a solution of a linear system of equations in terms of the coefficients of the variables. In linear algebra, a linear combination of a set of vectors is a vector that can be obtained by multiplying each vector by a scalar (a constant) and then adding the results. In summary, a linear combination is a mathematical expression that is composed of variables multiplied by scalars, and added together. Manhattan distance Manhattan distance, also known as L1 norm or taxicab distance, is a measure of the distance between two points in a multi-dimensional space. It is calculated as the sum of the absolute differences of their coordinates. In other words, given two points P = (p1, p2, …, pn) and Q = (q1, q2, …, qn) in an n-dimensional space, the Manhattan distance between them is: ||P - Q||1 = |p1 - q1| + |p2 - q2| + … + |pn - qn| The Manhattan distance is named after the grid layout of the streets in Manhattan, where one can only travel on the grid horizontally or vertically, not diagonally. It is less affected by outliers than the Euclidean distance and thus often used in clustering and image processing. It is also used in other fields such as natural language processing, recommendation systems and in machine learning algorithms such as k-nearest neighbors. Minkowski distance (of order 𝑝) Minkowski distance is a generalization of both the Euclidean distance and the Manhattan distance. It is a measure of the distance between two points in a multi-dimensional space, and it is defined as the pth root of the sum of the absolute differences in their coordinates, raised to the power of p. Given two points P = (p1, p2, …, pn) and Q = (q1, q2, …, qn) in an n-dimensional space, the Minkowski distance between them is: ||P - Q||p = (|p1 - q1|^p + |p2 - q2|^p + … + |pn - qn|^p)^(1/p) When p = 2, the Minkowski distance becomes the Euclidean distance. When p = 1, the Minkowski distance becomes the Manhattan distance.\nThe Minkowski distance is widely used in various applications such as image processing, pattern recognition, and machine learning. It has a wide range of use cases, including in clustering, outlier detection, and computer vision. The Minkowski distance can also be used as a similarity measure in recommendation systems, where it is used to compute the similarity between users or items based on their ratings. Model (mathematical) In mathematics, a model is a simplified representation of a real-world system or phenomenon. It is a set of mathematical equations and/or algorithms that can be used to make predictions, simulate the behavior of the system, or understand the underlying mechanisms of the phenomenon. Models can be used in various fields such as physics, engineering, economics, computer science, and many more. They can be as simple as a linear equation or as complex as a neural network. Depending on the complexity and accuracy of the model, it can be used for different purposes such as forecasting, prediction, optimization, control, or understanding. There are many types of models, such as: Deterministic models: the output of the model is completely determined by the initial conditions and the model\u0026rsquo;s parameters. Stochastic models: the output of the model is determined by both the initial conditions and a random variable. Static models: the model represents a snapshot of the system at a certain point in time. Dynamic models: the model represents the evolution of the system over time. Multiplier In mathematics, a multiplier is a scalar or a vector that is used to scale or change the magnitude of another vector or a scalar. In a linear equation, a multiplier is a coefficient that is multiplied by a variable to increase or decrease its value. In other words, it is a factor by which a value is multiplied. For example, in the equation y = 2x, 2 is the multiplier of x. In vector algebra, a multiplier is a scalar that is used to scale a vector. For example, multiplying a vector by 2 will double its magnitude. Similarly, multiplying a vector by -1 will change its direction. In calculus, a multiplier is used to represent the change in a function\u0026rsquo;s output as a result of a change in its input. In optimization, the multiplier is used to represent the sensitivity of the objective function to changes in the constraints. In economics, the multiplier is used to represent the effect of an initial change in investment or government spending on the overall level of economic activity. In summary, a multiplier is a scalar or a vector that is used to scale or change the magnitude of another vector or scalar. It is a factor by which a value is multiplied. Norm/distance norm In mathematics, a norm, also known as a distance norm, is a function that assigns a non-negative value to each vector in a vector space, with the following properties: Positivity: The norm of any vector is greater than or equal to zero, and is equal to zero if and only if the vector is the zero vector. Homogeneity: The norm of a vector multiplied by a scalar is equal to the absolute value of the scalar multiplied by the norm of the vector. Triangle inequality: The norm of the sum of two vectors is less than or equal to the sum of the norms of the vectors. There are different types of norms, each of which measures the \u0026ldquo;size\u0026rdquo; or \u0026ldquo;magnitude\u0026rdquo; of a vector in a different way. Some examples include: Euclidean norm (also known as L2 norm): The square root of the sum of the squares of the elements of a vector. Manhattan norm (also known as L1 norm): The sum of the absolute values of the elements of a vector. Infinity norm (also known as L∞ norm): The maximum absolute value of the elements of a vector. Minkowski norm: A generalization of the Euclidean and Manhattan norms that uses a parameter p to control the degree of homogeneity. These norms are used in various fields such as optimization, control theory, machine learning, image processing and many more. Order of magnitude The order of magnitude of a number is a measure of the size of the number relative to some reference value. It is typically represented as a power of 10. For example, the order of magnitude of 1,000 is 3 (1,000 = 10^3), and the order of magnitude of 0.01 is -2 (0.01 = 10^-2). The concept of order of magnitude is used to simplify and approximate large or small numbers by reducing them to a single digit followed by a power of 10. This can be useful when comparing numbers that are vastly different in size, or when working with scientific or engineering data. Order of magnitude can also be used to estimate the relative uncertainty of a measurement. For example, a measurement that is accurate to within one order of magnitude is considered to be roughly accurate, while a measurement that is accurate to within three orders of magnitude is considered to be less precise. In summary, the order of magnitude of a number is a measure of the size of the number relative to some reference value and it is typically represented as a power of 10. It is a way of simplifying and approximating large or small numbers, and it is also used to estimate the relative uncertainty of a measurement. Orthogonal In mathematics, orthogonality refers to the concept of two or more objects being perpendicular to each other. The objects in question can be vectors, matrices, subspaces, functions and more. Orthogonal vectors: Two vectors are said to be orthogonal if the angle between them is 90 degrees. This means that the dot product of the vectors is equal to 0. Orthonormal vectors: Two vectors are said to be orthonormal if they are orthogonal and have a norm of 1. This means that they are not only perpendicular to each other, but also have a unit length. Orthogonal matrices: A matrix is said to be orthogonal if its inverse is equal to its transpose. This means that the matrix preserves the angle between any two vectors when it operates on them. Orthonormal basis: A set of vectors is said to be an orthonormal basis if they are mutually orthogonal and have a norm of 1. Orthogonality is a fundamental concept in many areas of mathematics and physics, such as linear algebra, geometry, and quantum mechanics. In particular, it plays an important role in the study of orthogonal projections and orthogonal complements in vector spaces, and in the study of eigenvectors and eigenvalues in linear algebra. In summary, orthogonality is the concept of two or more objects being perpendicular to each other. It can be used to describe vectors, matrices, subspaces, functions, and more and is important in many areas of mathematics and physics. Outlier In statistics and data analysis, an outlier is an observation that is significantly different from the other observations in a dataset. It can be caused by measurement error, data entry errors, or by the presence of rare and unusual events. Outliers can have a significant impact on the results of statistical analyses, and can lead to misleading conclusions if they are not identified and dealt with appropriately. There are several ways to identify outliers, including: Visual inspection of data, such as scatter plots and box plots Using descriptive statistics, such as the mean and standard deviation, to identify observations that are significantly different from the majority of the data. Using statistical tests, such as the Grubbs\u0026rsquo; test or the Mahalanobis distance, to identify observations that are unlikely to have been generated by the same population as the majority of the data. Once outliers have been identified, there are several ways to deal with them, including: Removing them from the dataset, if they are believed to be caused by measurement error or data entry errors. Keeping them in the dataset, but treating them as special cases in the analysis, if they are believed to be caused by rare and unusual events that are important to the research. Transforming the data, such as taking the logarithm of the values, to make the outliers less extreme. In summary, outliers are observations that are significantly different from the other observations in a dataset. They can be caused by measurement error, data entry errors, or by the presence of rare and unusual events. There are several ways to identify and deal with outliers, depending on the context and the purpose of the analysis. Overfitting Overfitting is a common problem in machine learning and statistical modeling, where a model is trained too well on the training data and performs poorly on new, unseen data. It occurs when a model is too complex and has too many parameters relative to the amount of training data, and it learns the noise in the data rather than the underlying relationship. Overfitting can be identified by comparing the performance of the model on the training data and the validation data. If the model performs well on the training data but poorly on the validation data, it is overfitting. There are several techniques to prevent overfitting, such as: Using simpler models with fewer parameters Using techniques such as regularization, which adds a penalty term to the model\u0026rsquo;s objective function to discourage large values of the parameters Using techniques such as early stopping, which stops the training process before the model becomes too complex Using techniques such as cross-validation, which divides the data into multiple subsets and trains and evaluates the model multiple times. In summary, overfitting is a common problem in machine learning and statistical modeling where a model is trained too well on the training data and performs poorly on new unseen data. It occurs when a model is too complex and has too many parameters relative to the amount of training data. There are several techniques to prevent overfitting such as using simpler models, regularization, early stopping and cross-validation. 𝑝-norm In mathematics, a p-norm, is a generalization of the concept of vector norm. It is a measure of the size or magnitude of a vector in a vector space, and it is defined as the pth root of the sum of the absolute values of the elements of the vector, raised to the power of p. Given a vector x = (x1, x2, …, xn), the p-norm of x is: ||x||p = (|x1|^p + |x2|^p + … + |xn|^p)^(1/p) p = 1 is the Manhattan norm (also known as L1 norm), p = 2 is the Euclidean norm (also known as L2 norm), and p = infinity is the infinity norm (also known as L∞ norm or Chebyshev norm).\np-norms are used in various areas of mathematics and science such as optimization, control theory, machine learning and image processing. They are also used in other fields such as natural language processing, recommendation systems, and in machine learning algorithms such as k-nearest neighbors. In summary, p-norm is a generalization of the concept of vector norm. It is a measure of the size or magnitude of a vector in a vector space, and it is defined as the pth root of the sum of the absolute values of the elements of the vector, raised to the power of p. The p-norms are used in various areas of mathematics and science. Parameter In mathematics and statistics, a parameter is a value that describes a characteristic of a population or a probability distribution. Parameters are typically unknown and must be estimated from sample data using statistical methods. There are two types of parameters: Fixed parameters: These are parameters that are constant and do not change with the sample size. Random parameters: These are parameters that vary with the sample size, they are treated as random variables. There are different types of parameters depending on the model or the analysis: In probability distributions, parameters are used to describe the shape or the behavior of the distribution, such as the mean, standard deviation, and probability of success in a binomial distribution. In statistical models, parameters are used to describe the relationship between the variables, such as the slope and intercept in a linear regression model. In machine learning, parameters are the values that are learned from the data during the training process, such as the weights and biases in a neural network. In summary, parameters are values that describe a characteristic of a population or a probability distribution. They are typically unknown and must be estimated from sample data using statistical methods. There are two types of parameters: fixed and random, depending on whether they change with the sample size or not. Parameters are used in different types of models and analyses such as probability distributions, statistical models and machine learning. Perturbation In mathematics, physics and engineering, perturbation theory is a method used to analyze and make approximations for systems that are similar to but slightly different from a known system. It is used to study the behavior of a system when small changes are made to its parameters, such as the strength of a force or the value of a constant. The goal of perturbation theory is to find an approximate solution to a problem that is easier to solve than the original problem, while still retaining enough accuracy to be useful. There are two main types of perturbation methods: Regular perturbation: This is used when the small parameter is in the problem\u0026rsquo;s equation, and it is used to find an approximate solution in a series of terms of increasing powers of the small parameter. Singular perturbation: This is used when the small parameter is in the problem\u0026rsquo;s boundary conditions, and it is used to find an approximate solution in a \u0026ldquo;fast\u0026rdquo; variable and a \u0026ldquo;slow\u0026rdquo; variable. Perturbation theory has many applications in physics, engineering and mathematics. It is used in Quantum mechanics, Celestial mechanics, fluid dynamics and control theory, among other fields. In summary, Perturbation theory is a method used to analyze and make approximations for systems that are similar to but slightly different from a known system. It is used to study the behavior of a system when small changes are made to its parameters. It has two main types of perturbation methods: regular and singular perturbation and it has many applications in physics, engineering and mathematics. Prediction Prediction is the process of using data, models, and knowledge to make forecasts or estimates about future events or outcomes. In the context of machine learning, prediction refers to the task of using a trained model to make predictions about new, unseen data. The goal of prediction is to use the information from the past to make informed decisions about the future. There are different types of prediction, including: Classification: This type of prediction is used when the outcome variable is categorical, such as predicting whether an email is spam or not. Regression: This type of prediction is used when the outcome variable is continuous, such as predicting the price of a stock or the temperature tomorrow. Time series forecasting: This type of prediction is used when the outcome variable is a function of time, such as predicting the number of sales in the next quarter or the weather forecast for tomorrow. Prediction is used in many fields such as finance, medicine, weather forecasting, transportation and many more. The quality of a prediction is often measured using metrics such as accuracy, precision, recall, and the area under the ROC curve. In summary, Prediction is the process of using data, models, and knowledge to make forecasts or estimates about future events or outcomes. In machine learning, prediction refers to the task of using a trained model to make predictions about new, unseen data. There are different types of prediction such as classification, regression and time series forecasting, used in many fields such as finance, medicine, weather forecasting, transportation and many more. Predictive analytics Predictive analytics is a branch of data analytics that uses statistical models, machine learning algorithms, and other techniques to analyze historical data and make predictions about future events or outcomes. It combines techniques from statistics, computer science, and domain expertise to extract insights from data and make data-driven decisions. Predictive analytics is used in many industries such as finance, healthcare, marketing, and transportation to identify patterns and trends in data and make predictions about future customer behavior, market trends, and more. There are several steps involved in the predictive analytics process: Data collection: This step involves gathering and cleaning data from various sources Data exploration and visualization: This step involves exploring the data to identify patterns and trends Modeling: This step involves building and testing statistical models or machine learning algorithms to make predictions Evaluation: This step involves evaluating the performance of the model and fine-tuning it if necessary Deployment: This step involves putting the model into production, so it can be used to make predictions on new data. Predictive analytics can be used for a wide range of applications such as fraud detection, customer churn prediction, predictive maintenance, and inventory forecasting. In summary, Predictive analytics is a branch of data analytics that uses statistical models, machine learning algorithms, and other techniques to analyze historical data and make predictions about future events or outcomes. It is used in many industries such as finance, healthcare, marketing, and transportation to identify patterns and trends in data and make predictions about future customer behavior, market trends, and more. The process of predictive analytics includes several steps such as data collection, data exploration, modeling, evaluation and deployment. Prescriptive analytics Prescriptive analytics is a branch of data analytics that goes beyond traditional descriptive and predictive analytics by using advanced mathematical models and algorithms to recommend actions or decisions that can optimize a specific outcome or objective. It combines techniques from operations research, decision theory, and machine learning to analyze data and suggest the best course of action. Prescriptive analytics can be used in many industries such as finance, healthcare, transportation, and manufacturing to optimize operations, improve efficiency, and make better decisions. There are several steps involved in the prescriptive analytics process: Data collection and preparation: This step involves gathering and cleaning data from various sources Modeling: This step involves building mathematical models or using machine learning algorithms to analyze the data and generate recommendations Simulation: This step involves testing different scenarios and evaluating the outcomes of different decisions Optimization: This step involves finding the best course of action that maximizes a specific objective or minimizes a specific risk Implementation: This step involves putting the recommended actions into practice Prescriptive analytics can be used for a wide range of applications such as supply chain optimization, workforce scheduling, and inventory management. In summary, Prescriptive analytics is a branch of data analytics that goes beyond traditional descriptive and predictive analytics by using advanced mathematical models and algorithms to recommend actions or decisions that can optimize a specific outcome or objective. It is used in many industries such as finance, healthcare, transportation, and manufacturing to optimize operations, improve efficiency, and make better decisions. The process of prescriptive analytics includes several steps such as data collection and preparation, modeling, simulation, optimization, and implementation. Rectilinear distance Rectilinear distance, also known as Manhattan distance or L1-norm, is a measure of the distance between two points in a Euclidean space. It is calculated as the sum of the absolute differences of the coordinates of the points, and it is often used in situations where the path taken to travel between the points is restricted to a grid, such as in navigation or image processing. Given two points A(x1, y1) and B(x2, y2), the rectilinear distance, d, between them is: d = |x1 - x2| + |y1 - y2| This distance metric is also known as Manhattan distance since it is the distance that a car would drive in a city laid out on a rectangular grid, like Manhattan. The rectilinear distance is a special case of the Minkowski distance, where the parameter p=1. In summary, Rectilinear distance, also known as Manhattan distance or L1-norm, is a measure of the distance between two points in a Euclidean space. It is calculated as the sum of the absolute differences of the coordinates of the points and it is often used in situations where the path taken to travel between the points is restricted to a grid, such as in navigation or image processing. It is also a special case of the Minkowski distance where the parameter p=1 ","date":"0001-01-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/ml_glossary/","section":"","tags":["analytics","machine-learning"],"title":"Machine Learning Glossary"},{"categories":null,"contents":"Title: Use of telemedicine and artificial intelligence in Eye and ENT: a boon for developing countries Published on: March 17th, 2023 Published by: IEEE\nTitle: Use of Data in the private sector of Nepal Published on: July 1st, 2020 Published by: The World Bank\nTitle: Use of ICT in Classrooms in Nepal; A study of Student-Centered Learning using ICT Published on: Dec 16th, 2015 Published by: IEEE\n","date":"0001-01-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/publications/","section":"","tags":null,"title":"Publications"},{"categories":null,"contents":"Actively seeking leadership roles in Analytics, Data Science or Machine Learning. ","date":"0001-01-01T00:00:00Z","permalink":"https://ayushsubedi.github.io/resume/","section":"","tags":null,"title":"Resume"}]