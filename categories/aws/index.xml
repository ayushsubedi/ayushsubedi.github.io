<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AWS on Ayush Subedi</title>
    <link>https://ayushsubedi.github.io/categories/aws/</link>
    <description>Recent content in AWS on Ayush Subedi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 01 Jan 2023 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ayushsubedi.github.io/categories/aws/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>AWS Certified ML - Specialty exam (MLS-C01) - 1. Data Engineering</title>
      <link>https://ayushsubedi.github.io/posts/aws_ml_speciality_data_engineering/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/aws_ml_speciality_data_engineering/</guid>
      <description>&lt;h1 id=&#34;data-engineering&#34;&gt;Data Engineering&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#s3&#34;&gt;S3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kinesis&#34;&gt;Kinesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#glue&#34;&gt;Glue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#redshift&#34;&gt;Redshift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rds&#34;&gt;RDS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dynamodb&#34;&gt;DynamoDB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#opensearch&#34;&gt;OpenSearch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aws-data-pipeline&#34;&gt;AWS Data Pipeline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aws-batch&#34;&gt;AWS Batch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aws-batch&#34;&gt;AWS DMS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-functions&#34;&gt;Step Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#efs&#34;&gt;EFS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ebs&#34;&gt;EBS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#emr&#34;&gt;EMR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#misc&#34;&gt;Misc&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This domain requires understanding of creating data repositories for machine learning, identification and implementation of data ingestion solution, and
identification and implementation of a data transformation solution.&lt;/p&gt;
&lt;p&gt;Data engineering is the process of building and maintaining the infrastructure and systems that are used to store, process, and analyze data. In the context of Amazon Web Services (AWS), data engineering involves the use of various AWS services and tools to build and operate data pipelines, data lakes, and other data processing systems.&lt;/p&gt;
&lt;p&gt;Some common AWS services and tools that are used in data engineering on AWS include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amazon S3: A fully managed object storage service that is used to store and retrieve data.&lt;/li&gt;
&lt;li&gt;Amazon EMR: A fully managed big data processing service that is used to process and analyze large datasets using Apache Hadoop and Apache Spark.&lt;/li&gt;
&lt;li&gt;AWS Glue: A fully managed extract, transform, and load (ETL) service that is used to move and transform data between data stores.&lt;/li&gt;
&lt;li&gt;Amazon Redshift: A fully managed data warehouse service that is used to store and analyze large amounts of data using SQL and business intelligence tools.&lt;/li&gt;
&lt;li&gt;Amazon RDS: A fully managed database service that is used to set up, operate, and scale relational databases in the cloud.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By using these and other AWS services, data engineers can build and maintain robust, scalable, and cost-effective data processing systems on the AWS Cloud.&lt;/p&gt;
&lt;h2 id=&#34;s3&#34;&gt;S3&lt;/h2&gt;
&lt;p&gt;Amazon S3 (Simple Storage Service) is a cloud storage service that allows you to store and retrieve data at any time, from anywhere on the web. It is designed to make web-scale computing easier for developers by providing a simple, highly scalable, and cost-effective way to store and retrieve any amount of data. With S3, you can store and retrieve any amount of data, at any time, from anywhere on the web. S3 is designed to provide 99.999999999% durability and scale past trillions of objects worldwide. It is used to store and retrieve any amount of data, at any time, from anywhere on the web. It is an object storage service that offers industry-leading scalability, data availability, security, and performance.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;foundational for machine learning projects since it is a cost effective solution for datasets storage&lt;/li&gt;
&lt;li&gt;object based storage, bucket name need to be globally unique, however the storage itself is unique to regions&lt;/li&gt;
&lt;li&gt;key is the full path of the file and even though it looks like there is a folder based heirarchy, that is not how it works&lt;/li&gt;
&lt;li&gt;you can have a very long file name, in the sense that the path (key) can be very long&lt;/li&gt;
&lt;li&gt;individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 TB. The largest object that can be uploaded in a single PUT is 5 GB.&lt;/li&gt;
&lt;li&gt;object tags can be added, helpful with classification and security lifecycle (these are key value pairs)&lt;/li&gt;
&lt;li&gt;decoupling of compute and storage side&lt;/li&gt;
&lt;li&gt;perfect use case of data lake, since it can store various formats of data (object storage)&lt;/li&gt;
&lt;li&gt;it is possible to partition the storage, which is helpful (speedy) when querying via athena. Kinesis partitions the data automatically.&lt;/li&gt;
&lt;li&gt;11 9&amp;rsquo;s durability (for all storage classes)&lt;/li&gt;
&lt;li&gt;availability differs between availability classes&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;storage-classes&#34;&gt;Storage classes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;S3, Standard / General Purpose: for frequently accessed data&lt;/li&gt;
&lt;li&gt;S3, Infrequent Access: lower cost than standard, for data accessed monthly, and requires milliseconds retrival, but there is a cost associated with retrival&lt;/li&gt;
&lt;li&gt;S3, Infrequent Access, One Zone, good for secondary copies of backup, or data that can be recreated, infrequent access for cost saving&lt;/li&gt;
&lt;li&gt;S3, Glacier Instant Retrival, price per storage + price per retrival, can access within milliseconds, for low cost storage for long-lived data&lt;/li&gt;
&lt;li&gt;S3, Glacier Flexible Retrival, expedited: 1-5 mins, standard: 3-5 hrs, bulk: 5-12 hrs (free), for long term low cost storage for backups and archives  with different retrival options&lt;/li&gt;
&lt;li&gt;S3, Glacier Deep Archive: lowest cost, 180 days of minimum storage, for rarely accessed archive data&lt;/li&gt;
&lt;li&gt;S3, Intelligent Tiering: move objects between tiers with monthly monitoring and auto-tiering fee&lt;/li&gt;
&lt;li&gt;It is possible to move objects between these storage classes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://d1.awsstatic.com/reInvent/re21-pdp-tier1/s3/Amazon-S3-Storage-Classes.pdf&#34;&gt;more info&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;lifecycle-rules&#34;&gt;Lifecycle rules&lt;/h3&gt;
&lt;p&gt;Amazon S3 Lifecycle rules allow you to define policies for how Amazon S3 stores objects. You can use Lifecycle rules to specify when objects transition to different storage classes, or when they expire and are deleted. This can help you reduce your storage costs by moving objects to lower-cost storage classes or deleting them when they are no longer needed. You can set up Lifecycle rules at the bucket level or at the object level (for individual objects or for groups of objects). You can also specify different rules for different prefixes or object tags.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transitioning objects between classes is possible&lt;/li&gt;
&lt;li&gt;Transition Actions can be used to configure objects to transition to another storage class&lt;/li&gt;
&lt;li&gt;Transition Actions can also be used for expiration, incomplete multi part uploads etc.&lt;/li&gt;
&lt;li&gt;Rules can be applied to buckets, specific paths of the project or also to tags&lt;/li&gt;
&lt;li&gt;Amazon S3 analytics works exclusively on S3 standard, and S3 IA, and provides analytics on usage&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;performance-chart&#34;&gt;Performance Chart&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/s3_storage_classes.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;encryption&#34;&gt;Encryption&lt;/h3&gt;
&lt;p&gt;Amazon S3 supports several encryption options to help users secure their data at rest. These options include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SSE-S3: This option uses server-side encryption with Amazon S3-managed keys. With this option, Amazon S3 encrypts the data as it is written to disks in its data centers and decrypts it when it is accessed.&lt;/li&gt;
&lt;li&gt;SSE-KMS: This option uses server-side encryption with AWS KMS-managed keys. With this option, users can create, rotate, and manage the keys used to encrypt their data.&lt;/li&gt;
&lt;li&gt;SSE-C: This option allows users to use their own encryption keys to encrypt their data. Users are responsible for securely managing their keys and rotating them as needed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Users can enable encryption when creating a new bucket or when uploading an object to an existing bucket. They can also enable encryption for all objects in an existing bucket by enabling bucket-level encryption.&lt;/p&gt;
&lt;h3 id=&#34;security-policy&#34;&gt;Security Policy&lt;/h3&gt;
&lt;p&gt;Amazon S3 bucket policies allow users to add additional security controls to their S3 buckets and objects. A bucket policy is a JSON document that defines the permissions for an S3 bucket. It can be used to grant permissions to other AWS accounts, or to grant public access to a bucket and its objects.&lt;/p&gt;
&lt;p&gt;With a bucket policy, users can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Grant read and write permissions to a specific AWS account for all objects in a bucket.&lt;/li&gt;
&lt;li&gt;Grant read-only permissions to the anonymous user for all objects in a bucket.&lt;/li&gt;
&lt;li&gt;Grant read and write permissions to a specific AWS account for all objects with a specific prefix (such as &amp;ldquo;private/&amp;rdquo;).&lt;/li&gt;
&lt;li&gt;Deny all access to a specific AWS account for all objects in a bucket.&lt;/li&gt;
&lt;li&gt;It is important for users to carefully consider the permissions they grant in their bucket policy, as it can have wide-ranging effects on the security of the bucket and its contents.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;misc&#34;&gt;Misc&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Amazon S3 VPC Endpoints allow users to access Amazon S3 from within their virtual private cloud (VPC) without the need for an Internet gateway, NAT device, or VPN connection. With VPC Endpoints, users can access S3 from their VPC over an optimized network path, reducing Internet traffic and improving performance.&lt;/li&gt;
&lt;li&gt;Users can create a VPC Endpoint for Amazon S3 in their VPC, and then configure their VPC security groups and IAM policies to allow access to the endpoint. They can then use the endpoint to access Amazon S3 using the Amazon S3 APIs or the AWS Management Console, just as they would over the Internet.&lt;/li&gt;
&lt;li&gt;VPC Endpoints for Amazon S3 are supported in all regions and are available in two types: Gateway Endpoints and Interface Endpoints. Gateway Endpoints are powered by a highly available network gateway, while Interface Endpoints are powered by a highly available Network Load Balancer. Users can choose the endpoint type that best meets their needs.&lt;/li&gt;
&lt;li&gt;Amazon S3 CloudTrail is a service that enables users to record API calls made to Amazon S3 and log the events to an Amazon S3 bucket. This allows users to track changes to their objects, buckets, and Amazon S3 configurations, and to identify and troubleshoot issues.&lt;/li&gt;
&lt;li&gt;With CloudTrail, users can:
&lt;ul&gt;
&lt;li&gt;Track changes to their Amazon S3 objects and bucket metadata.&lt;/li&gt;
&lt;li&gt;Determine who made a change and when it was made.&lt;/li&gt;
&lt;li&gt;Audit changes to their Amazon S3 bucket and object permissions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CloudTrail logs are stored in an Amazon S3 bucket that the user specifies, and they can be delivered to an Amazon CloudWatch Logs log group or an Amazon SNS topic. Users can use the CloudTrail logs to monitor their S3 resources and to ensure compliance with their policies.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kinesis&#34;&gt;Kinesis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Kinesis is a fully managed (alternative to Kafka), cloud-based service that enables users to process and analyze streaming data in real-time. With Kinesis, users can build custom applications that process and analyze data as it arrives, and they can scale these applications to process any volume of data, at any time.&lt;/li&gt;
&lt;li&gt;Kinesis consists of three main components:
&lt;ul&gt;
&lt;li&gt;Producers: Producers are sources of data that send data records to Kinesis streams.&lt;/li&gt;
&lt;li&gt;Kinesis streams: A Kinesis stream is a sequence of data records that are persisted for a set period of time. Users can create and delete streams, and they can specify the number of shards in a stream.&lt;/li&gt;
&lt;li&gt;Consumers: Consumers are applications that read and process data records from Kinesis streams.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Kinesis is designed to be highly available and durable, and it can automatically scale to handle increases in traffic.&lt;/li&gt;
&lt;li&gt;Users can use Kinesis to build custom applications that can process and analyze real-time data streams, and they can use the service to support a wide range of use cases, such as real-time analytics, fraud detection, and Internet of Things (IoT) applications.&lt;/li&gt;
&lt;li&gt;Data is replicated to at least 3 AZ&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kinesis-streams&#34;&gt;Kinesis Streams&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Kinesis Streams is a fully managed, cloud-based service that allows real-time processing of streaming data at high scale.&lt;/li&gt;
&lt;li&gt;It can continuously capture and store terabytes of data per hour from hundreds of thousands of sources, such as website clickstreams, financial transactions, social media feeds, IT logs, and location-tracking events.&lt;/li&gt;
&lt;li&gt;With Kinesis Streams, users can build custom applications that process or analyze the data as it arrives, or they can use the provided Kinesis Data Streams API to load the data into other AWS services, such as Amazon S3, Amazon Redshift, or Amazon Elasticsearch Service, for long-term storage and analysis.&lt;/li&gt;
&lt;li&gt;Streams are divided into shards and partitions&lt;/li&gt;
&lt;li&gt;The maximum throughput of a single shard 1 mb/seconds or 1000 messages/seconds&lt;/li&gt;
&lt;li&gt;Data retention: 24 hours by default. It can go up to 365 days. This is useful for reprocessing/replaying data&lt;/li&gt;
&lt;li&gt;Immutable, 1 mb in size&lt;/li&gt;
&lt;li&gt;Provisioned mode: choose number of shards and scale manually or using an API&lt;/li&gt;
&lt;li&gt;Each shard gets 1mb/s in, 2mb/s out&lt;/li&gt;
&lt;li&gt;On demand mode: each capacity provisioned is 4mb/s&lt;/li&gt;
&lt;li&gt;If you can plan capacity, use provisioned. however, use on demand if capacity is unknown&lt;/li&gt;
&lt;li&gt;Custom code for producer or consumer is possible&lt;/li&gt;
&lt;li&gt;Real time (200 ms latency, possible all the way up to 70ms)&lt;/li&gt;
&lt;li&gt;Automatic scaling with on-demand mode&lt;/li&gt;
&lt;li&gt;Multi consumers is possible from one source&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kinesis-analytics&#34;&gt;Kinesis Analytics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Kinesis Analytics is a fully managed, cloud-based service that allows users to process and analyze streaming data in real-time with SQL.&lt;/li&gt;
&lt;li&gt;With Kinesis Analytics, users can run ad-hoc queries on the data, or they can set up a SQL-based stream processing application to perform transformations on the data as it arrives. SQL or Apache Flink can be used here.&lt;/li&gt;
&lt;li&gt;The output of these queries and transformations can be fed back into Kinesis Streams for further processing, or it can be stored in other AWS services, such as Amazon S3 or Amazon Redshift, for long-term analysis.&lt;/li&gt;
&lt;li&gt;Select columns, continious metric generation, responsive analytics, etc.&lt;/li&gt;
&lt;li&gt;Serverless, scales automatically, pay for resouces consumed but expensive&lt;/li&gt;
&lt;li&gt;Schema discovery&lt;/li&gt;
&lt;li&gt;Lambda can be used for preprocessing&lt;/li&gt;
&lt;li&gt;Two machine learning algorithms:
&lt;ul&gt;
&lt;li&gt;Random cut forest for anomaly detection on numeric columns in a stream, uses recent data to compute the model. A random cut forest (RCF) is a machine learning algorithm that is used for anomaly detection in streaming data. It works by constructing a number of decision trees on randomly selected subsets of the data, and then comparing the score for each new data point to the scores of similar points in the trees. If the score for a new data point is significantly lower than the scores of similar points in the trees, it is considered to be an anomaly. The number of trees in the forest and the size of the subsets of data used to train each tree can be adjusted to control the sensitivity of the model. RCFs are particularly well-suited for detecting anomalies in large, high-dimensional datasets, and they are often used in conjunction with streaming data platforms, such as Amazon Kinesis Streams.&lt;/li&gt;
&lt;li&gt;Hotspots: A hotspots algorithm is a type of machine learning algorithm that is used to identify spatial clusters of events or observations in a dataset. These clusters, which are also known as hotspots, are areas in which the concentration of events or observations is significantly higher than the surrounding areas. Hotspots algorithms are often used in a variety of applications, such as crime mapping, disease surveillance, and marketing analysis. There are several different approaches to identifying hotspots, including spatial clustering methods, spatial scan statistics, and kernel density estimation. These methods can be applied to a variety of types of data, including point data, such as crime incidents or disease cases, and areal data, such as census tracts or zip codes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kinesis-firehose&#34;&gt;Kinesis Firehose&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Kinesis Firehose is a fully managed service&lt;/li&gt;
&lt;li&gt;makes it easy to load streaming data into data stores and analytics tools&lt;/li&gt;
&lt;li&gt;It can capture, transform, and load data streams into Amazon S3, Amazon Redshift, and Amazon Elasticsearch Service, Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards&lt;/li&gt;
&lt;li&gt;Kinesis Firehose is a simple and reliable way to load streaming data into data stores and analytics tools.&lt;/li&gt;
&lt;li&gt;most common is firehose reading from kinesis streams&lt;/li&gt;
&lt;li&gt;near realtime service because it batch writes&lt;/li&gt;
&lt;li&gt;data desitination can be s3, redshift, elastisearch, splunk, new relic, or http endpoint&lt;/li&gt;
&lt;li&gt;60 seconds latency minimum for non full batches&lt;/li&gt;
&lt;li&gt;data ingestion into redshift, s3, elasticsearch, splunk&lt;/li&gt;
&lt;li&gt;automatic scaling&lt;/li&gt;
&lt;li&gt;conversions from csv/json to parquet and orc and requires the use of glue&lt;/li&gt;
&lt;li&gt;and transformation through lambda csv to json is possible&lt;/li&gt;
&lt;li&gt;compression is possible&lt;/li&gt;
&lt;li&gt;automates scaling&lt;/li&gt;
&lt;li&gt;no data storage&lt;/li&gt;
&lt;li&gt;no replay capability&lt;/li&gt;
&lt;li&gt;it is a serverless transformation tool&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kinesis-video-streams&#34;&gt;Kinesis Video Streams&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Web Services (AWS) Kinesis Video Streams is a fully managed service that allows users to stream live video from connected devices to the cloud.&lt;/li&gt;
&lt;li&gt;This service is designed to make it easy to build applications that process and analyze live video streams, as well as store and transmit videos securely at scale.&lt;/li&gt;
&lt;li&gt;With Kinesis Video Streams, users can stream live video from millions of devices and easily build applications for real-time video analytics and machine learning.&lt;/li&gt;
&lt;li&gt;In addition, the service allows users to stream video directly to other AWS services, such as Amazon S3, Amazon Kinesis Data Streams, and Amazon Rekognition, for further processing and analysis.&lt;/li&gt;
&lt;li&gt;Producers: security camera, body-worn cam, aws deeplens, radar data, camera&lt;/li&gt;
&lt;li&gt;One producer per video stream&lt;/li&gt;
&lt;li&gt;Video playback capability&lt;/li&gt;
&lt;li&gt;Sagemaker, rekognition video, 1 hour to 10 years of storage&lt;/li&gt;
&lt;li&gt;Checkpointing via dynamodb, frames to Sagemaker for ML inference, publish to stream, lambda can be used for notification.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;glue&#34;&gt;Glue&lt;/h2&gt;
&lt;h3 id=&#34;glue-data-catalog-and-glue-data-crawlers&#34;&gt;Glue Data Catalog and Glue Data Crawlers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;AWS Glue Data Catalog is a fully managed, cloud-native metadata store that provides a central place to store, annotate, and share metadata across AWS services, applications, and tools.&lt;/li&gt;
&lt;li&gt;It makes it easy to discover and understand data, and facilitates the development of data-driven applications.&lt;/li&gt;
&lt;li&gt;With the Glue Data Catalog, users can create, maintain, and access metadata such as database and table definitions, column names and data types, and data lineage.&lt;/li&gt;
&lt;li&gt;The Glue Data Catalog is integrated with other AWS services such as Amazon Redshift, Amazon Athena, and Amazon EMR, and is accessible through the AWS Management Console, the AWS Glue API, and the AWS Glue ETL (extract, transform, and load) library.&lt;/li&gt;
&lt;li&gt;Schemas are versioned&lt;/li&gt;
&lt;li&gt;Glue crawlers help build the Catalog&lt;/li&gt;
&lt;li&gt;Glue will also extract the partitions, this is helpful for query optimization&lt;/li&gt;
&lt;li&gt;Glue Data Crawlers are a tool within the Amazon Glue service that allows users to extract metadata from their data stores and create table definitions in the Glue Data Catalog.&lt;/li&gt;
&lt;li&gt;This enables the creation of ETL jobs and development endpoints in Glue, which can be used to move and transform data.&lt;/li&gt;
&lt;li&gt;Glue Data Crawlers can connect to various data stores, including Amazon S3 and RDS, as well as any JDBC-compliant data store.&lt;/li&gt;
&lt;li&gt;Custom connectors for other data stores can also be created using the Glue ETL library. To use Glue Data Crawlers, a Glue ETL job or development endpoint must first be created, after which the Glue ETL library can be utilized for data movement and transformation tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;glue-etl&#34;&gt;Glue ETL&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Transform data, clean data, enrich data before doing analysis&lt;/li&gt;
&lt;li&gt;Generate ETL code in python or scala, you can modify the code&lt;/li&gt;
&lt;li&gt;Possible to provide your own spark or pyspark scripts&lt;/li&gt;
&lt;li&gt;Target can be S3, JDBC or in glue data catalog&lt;/li&gt;
&lt;li&gt;Fully managed, cost effective, pay only for the resources consumed&lt;/li&gt;
&lt;li&gt;Jobs are run on a serverless Spark platform&lt;/li&gt;
&lt;li&gt;Glue scheduler to schedule the jobs&lt;/li&gt;
&lt;li&gt;Glue triggers to automate job runs based on events&lt;/li&gt;
&lt;li&gt;Transformations can be bundled (drop, filter, join, map)&lt;/li&gt;
&lt;li&gt;Machine learning transformation (find matches, duplicates even when data do not match exactly, dedup)&lt;/li&gt;
&lt;li&gt;Any apache spark transformation is possible, and changing in format is possible.&lt;/li&gt;
&lt;li&gt;Multiple ways to create glue jobs including visual editors, python notebooks, python script, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;redshift&#34;&gt;Redshift&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Redshift is a fully managed data warehouse service offered by Amazon Web Services (AWS).&lt;/li&gt;
&lt;li&gt;It is designed to handle petabyte-scale data warehouses and make it easy to analyze data using SQL and business intelligence tools.&lt;/li&gt;
&lt;li&gt;Amazon Redshift is based on PostgreSQL, and it supports many of the same data types and functions as PostgreSQL.&lt;/li&gt;
&lt;li&gt;To use Amazon Redshift, users first need to set up a cluster of compute nodes. They can then load data into the cluster and perform SQL queries on the data. Amazon Redshift integrates with various data sources and destinations, including Amazon S3, Amazon EMR, and Amazon RDS.&lt;/li&gt;
&lt;li&gt;It also integrates with a variety of business intelligence tools, such as Quicksight, Tableau, Qlik, and MicroStrategy.&lt;/li&gt;
&lt;li&gt;Amazon Redshift offers a number of features to help users manage their data warehouses, including automatic data compression, data replication, and data security. It also provides a number of performance enhancements, such as columnar storage, data caching, and parallel query execution.&lt;/li&gt;
&lt;li&gt;Overall, Amazon Redshift is a powerful and scalable data warehouse solution for analyzing large datasets in the cloud.&lt;/li&gt;
&lt;li&gt;OLAP&lt;/li&gt;
&lt;li&gt;Uses SQL to analyze structured and semi-structured data across data warehouses, operational databases, and data lakes&lt;/li&gt;
&lt;li&gt;Redshift Spectrum can directly query from S3&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;rds&#34;&gt;RDS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Relational Database Service (RDS) is a fully managed database service offered by Amazon Web Services (AWS).&lt;/li&gt;
&lt;li&gt;It makes it easy to set up, operate, and scale a relational database in the cloud.&lt;/li&gt;
&lt;li&gt;Amazon RDS supports a variety of database engines, including MySQL, MariaDB, PostgreSQL, Oracle, and Microsoft SQL Server.&lt;/li&gt;
&lt;li&gt;With Amazon RDS, users can create and manage a database without the need to install and maintain database software.&lt;/li&gt;
&lt;li&gt;Amazon RDS handles tasks such as hardware provisioning, database setup, patching, and backups.&lt;/li&gt;
&lt;li&gt;It also provides features such as automated failover and read replicas to help users improve availability and scalability.&lt;/li&gt;
&lt;li&gt;Amazon RDS is a popular choice for applications that require a relational database, such as e-commerce, content management, and customer relationship management systems.&lt;/li&gt;
&lt;li&gt;It is particularly well-suited for use cases that require high availability and low latency, such as online transaction processing (OLTP).&lt;/li&gt;
&lt;li&gt;Must provision servers in advance&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;dynamodb&#34;&gt;DynamoDB&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon DynamoDB is a fully managed NoSQL database service offered by Amazon Web Services (AWS).&lt;/li&gt;
&lt;li&gt;It is designed to be scalable, fast, and flexible, making it a good choice for applications that need high performance and low latency.&lt;/li&gt;
&lt;li&gt;DynamoDB stores data in tables, and each table has a primary key that uniquely identifies each item. The primary key can be either a simple primary key (a single attribute) or a composite primary key (a combination of two or more attributes).&lt;/li&gt;
&lt;li&gt;DynamoDB supports both key-value and document data models, and it offers a number of powerful features, such as global secondary indexes, auto scaling, and stream-based data replication.&lt;/li&gt;
&lt;li&gt;DynamoDB is a popular choice for applications that need to store large amounts of data that is frequently read or written, such as online gaming, real-time analytics, and IoT applications.&lt;/li&gt;
&lt;li&gt;It is also well-suited for applications that need to scale rapidly, as it can automatically adjust capacity to meet changing demand.&lt;/li&gt;
&lt;li&gt;Useful to store ML model (or checkpoints)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;opensearch&#34;&gt;OpenSearch&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Previously ElasticSearch&lt;/li&gt;
&lt;li&gt;Amazon OpenSearch is a search service that makes it easy to build and run search applications.&lt;/li&gt;
&lt;li&gt;It is based on the open source Apache Lucene search engine, and it provides a number of features to help users build sophisticated search experiences, such as full-text search, faceted search, and hit highlighting.&lt;/li&gt;
&lt;li&gt;With Amazon OpenSearch, users can index and search large datasets, such as websites, documents, and logs.&lt;/li&gt;
&lt;li&gt;They can also customize the search experience by adding search criteria, filters, and facets, and by displaying search results in various formats.&lt;/li&gt;
&lt;li&gt;Amazon OpenSearch also provides analytics and monitoring capabilities to help users understand how their search applications are being used.&lt;/li&gt;
&lt;li&gt;Amazon OpenSearch is a flexible and scalable search solution that is well-suited for a wide range of applications, such as e-commerce, content management, and data analysis.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;aws-data-pipeline&#34;&gt;AWS Data Pipeline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Data Pipeline is a fully managed data processing service that helps users move and transform data between data stores.&lt;/li&gt;
&lt;li&gt;It is designed to be easy to use and highly reliable, and it can handle data processing tasks of any size.&lt;/li&gt;
&lt;li&gt;With Amazon Data Pipeline, users can create pipelines that move data between data stores, such as Amazon S3, Amazon RDS, and Amazon Redshift.&lt;/li&gt;
&lt;li&gt;They can also use Data Pipeline to transform data, such as by aggregating, filtering, or joining data from different sources. Data Pipeline supports a variety of data formats and sources, and it can be used to schedule and automate data processing tasks.&lt;/li&gt;
&lt;li&gt;Amazon Data Pipeline is a useful tool for a wide range of data processing tasks, such as data warehousing, ETL, and analytics. It is particularly well-suited for use cases that involve moving and transforming large amounts of data, as it can scale to handle data processing needs of any size.&lt;/li&gt;
&lt;li&gt;Data sources can be on premise&lt;/li&gt;
&lt;li&gt;Runs on EC2 but fully managed&lt;/li&gt;
&lt;li&gt;Orchestration service&lt;/li&gt;
&lt;li&gt;Glue is managed, serverless, spark focused, ETL focused, has catalog&lt;/li&gt;
&lt;li&gt;Data Pipeline is orchestation tool, and can do more&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;aws-batch&#34;&gt;AWS Batch&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;For any non-ETL batch is usually better than glue&lt;/li&gt;
&lt;li&gt;Amazon Web Services Batch is a fully managed batch processing service that makes it easy to run batch computing workloads on the AWS Cloud.&lt;/li&gt;
&lt;li&gt;It is designed to be scalable, fault-tolerant, and flexible, and it supports a wide range of workloads, such as machine learning, data processing, and scientific simulations.&lt;/li&gt;
&lt;li&gt;With AWS Batch, users can define batch computing workloads as &amp;ldquo;jobs&amp;rdquo; and &amp;ldquo;tasks,&amp;rdquo; and the service automatically provisions the required compute resources and executes the tasks.&lt;/li&gt;
&lt;li&gt;Users can specify the desired level of concurrency and resource allocation for their jobs, and AWS Batch will automatically scale up or down as needed.&lt;/li&gt;
&lt;li&gt;AWS Batch also integrates with other AWS services, such as Amazon S3 and Amazon ECS, to provide a complete batch processing solution.&lt;/li&gt;
&lt;li&gt;AWS Batch is a useful tool for organizations that need to run large-scale batch computing workloads, such as financial analysis, scientific simulations, and media processing.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;li&gt;Batch can be scheduled using cloudwatch, step functions&lt;/li&gt;
&lt;li&gt;Not just for ETL but absolutely anything at all&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;aws-dms&#34;&gt;AWS DMS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Web Services Database Migration Service (AWS DMS) is a fully managed service that makes it easy to migrate databases to the AWS Cloud.&lt;/li&gt;
&lt;li&gt;It is designed to be reliable, efficient, and flexible, and it supports a wide range of database platforms, including Oracle, MySQL, and Microsoft SQL Server.&lt;/li&gt;
&lt;li&gt;With AWS DMS, users can migrate their databases to the AWS Cloud with minimal downtime.&lt;/li&gt;
&lt;li&gt;The service handles tasks such as data extraction, transformation, and load, and it supports both one-time and ongoing migrations.&lt;/li&gt;
&lt;li&gt;AWS DMS also provides a number of features to help users manage their database migrations, such as change data capture, data transformation, and task scheduling.&lt;/li&gt;
&lt;li&gt;AWS DMS is a useful tool for organizations that want to migrate their databases to the cloud, or that need to replicate their databases across multiple regions for disaster recovery or other purposes.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;li&gt;Supports homogeneous migrations and heterogeneous migrations&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-functions&#34;&gt;Step Functions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Web Services Step Functions is a fully managed service that makes it easy to coordinate the various components of complex, distributed applications.&lt;/li&gt;
&lt;li&gt;It is based on the concepts of tasks and state machines, and it provides a visual workflow editor to help users design and manage their applications.&lt;/li&gt;
&lt;li&gt;With AWS Step Functions, users can define and execute workflows that coordinate multiple AWS services, such as AWS Lambda, Amazon ECS, and AWS Batch.&lt;/li&gt;
&lt;li&gt;The service automatically scales to meet the needs of the workflow, and it provides features such as error handling and retry logic to help users build resilient applications.&lt;/li&gt;
&lt;li&gt;AWS Step Functions is a useful tool for organizations that need to coordinate the various components of complex, distributed applications, such as data pipelines, machine learning workflows, and microservices architectures.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;li&gt;Audit of history of workflow&lt;/li&gt;
&lt;li&gt;Allows waiting&lt;/li&gt;
&lt;li&gt;Maximum execution time of 1 year&lt;/li&gt;
&lt;li&gt;Can be used to train/tune a ML model&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;efs&#34;&gt;EFS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Elastic File System (EFS) is a fully managed, cloud-native file storage service that makes it easy to store and access files from multiple Amazon Elastic Compute Cloud (EC2) instances.&lt;/li&gt;
&lt;li&gt;It is designed to be scalable, highly available, and easy to use, and it supports the Network File System (NFS) protocol.&lt;/li&gt;
&lt;li&gt;With AWS EFS, users can create file systems and store files in them, and they can access the files from multiple EC2 instances at the same time.&lt;/li&gt;
&lt;li&gt;EFS automatically scales up or down as needed to meet the storage and performance needs of the applications, and it provides features such as file system access control and data durability to help users manage their file storage.&lt;/li&gt;
&lt;li&gt;AWS EFS is a useful tool for organizations that need to store and access files from multiple EC2 instances, such as web servers, application servers, and development environments.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ebs&#34;&gt;EBS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Elastic Block Store (EBS) is a fully managed, cloud-native block storage service that makes it easy to store and access data from Amazon Elastic Compute Cloud (EC2) instances.&lt;/li&gt;
&lt;li&gt;It is designed to be scalable, highly available, and easy to use, and it supports a variety of storage types and performance levels.&lt;/li&gt;
&lt;li&gt;With AWS EBS, users can create storage volumes and attach them to EC2 instances, and they can use the volumes to store and access data.&lt;/li&gt;
&lt;li&gt;EBS provides a number of features to help users manage their storage, such as snapshotting, data replication, and encryption.&lt;/li&gt;
&lt;li&gt;It also supports a variety of storage types, including SSD-backed volumes for high performance and HDD-backed volumes for lower cost.&lt;/li&gt;
&lt;li&gt;AWS EBS is a useful tool for organizations that need to store and access data from EC2 instances, such as databases, file systems, and applications.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;li&gt;EBS volumes are attached to specific EC2 instances, and they scale with the needs of the applications running on those instances.&lt;/li&gt;
&lt;li&gt;EFS file systems, on the other hand, can be accessed concurrently by multiple EC2 instances, and they scale automatically to meet the needs of the workload.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;emr&#34;&gt;EMR&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Elastic MapReduce (EMR) is a fully managed, cloud-native big data processing service that makes it easy to process large amounts of data using Apache Hadoop and Apache Spark.&lt;/li&gt;
&lt;li&gt;It is designed to be scalable, highly available, and easy to use, and it integrates with a variety of data sources and destinations.&lt;/li&gt;
&lt;li&gt;With AWS EMR, users can create clusters of Amazon Elastic Compute Cloud (EC2) instances and use them to process and analyze large datasets stored in Amazon S3 or other data stores.&lt;/li&gt;
&lt;li&gt;EMR supports a variety of big data processing frameworks, including Hadoop, Spark, and Flink, and it provides tools to help users manage and monitor their clusters.&lt;/li&gt;
&lt;li&gt;AWS EMR is a useful tool for organizations that need to process and analyze large amounts of data, such as log files, scientific simulations, and machine learning models.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;misc-1&#34;&gt;Misc&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;AWS DataSync: for data migrations from on-premises to AWS storage services&lt;/li&gt;
&lt;li&gt;MQTT: IOT protocol, Standard messaging protocol&lt;/li&gt;
&lt;li&gt;Apache Spark, Apache Hive, Apache Hadoop, and Apache Pig are all open-source technologies that are used for data processing and analysis. However, they are designed for different purposes and have different strengths and weaknesses.
&lt;ul&gt;
&lt;li&gt;Apache Spark is a fast, in-memory data processing engine that is used for real-time data processing and analytics. It is particularly well-suited for use cases that require fast processing times, such as streaming data and interactive data exploration.&lt;/li&gt;
&lt;li&gt;Apache Hive is a data warehousing and SQL-like query language that is used to process and analyze large datasets stored in the Hadoop Distributed File System (HDFS). It is particularly well-suited for use cases that involve complex data transformations and aggregations.&lt;/li&gt;
&lt;li&gt;Apache Hadoop is a distributed computing platform that is used to store and process large amounts of data. It is composed of several modules, including HDFS for storing data, YARN for resource management, and MapReduce for parallel data processing. Hadoop is a popular choice for batch processing and offline data analysis.&lt;/li&gt;
&lt;li&gt;Apache Pig is a high-level data processing language that is used to write and execute MapReduce jobs on Apache Hadoop. It is particularly well-suited for use cases that involve complex data transformations and complex data structures.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>AWS Certified ML - Specialty exam (MLS-C01) - 2. Exploratory Data Analysis</title>
      <link>https://ayushsubedi.github.io/posts/aws_ml_speciality_eda/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/aws_ml_speciality_eda/</guid>
      <description>&lt;h1 id=&#34;exploratory-data-analysis&#34;&gt;Exploratory Data Analysis&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#python-in-data-science-and-machine-learning&#34;&gt;Python in data science and machine learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-data&#34;&gt;Types of Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-distributions&#34;&gt;Data Distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trends-and-seasonality&#34;&gt;Trends and seasonality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#athena&#34;&gt;Athena&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quicksight&#34;&gt;Quicksight&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-visualization&#34;&gt;Types of visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#emr&#34;&gt;EMR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hadoop&#34;&gt;Hadoop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#apache-spark&#34;&gt;Apache Spark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#emr-notebooks,-security-and-instance-types&#34;&gt;EMR Notebooks, Security and Instance Types&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feature-engineering&#34;&gt;Feature Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#imputing-missing-data&#34;&gt;Imputing Missing Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#unbalanced-data&#34;&gt;Unbalanced Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#handling-outliers&#34;&gt;Handling Outliers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#binning,-transoforming,-encoding,-scaling,-and-shuffling&#34;&gt;Binning, Transoforming, Encoding, Scaling, and Shuffling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-sagemaker-ground-truth-and-label-generation&#34;&gt;Amazon Sagemaker Ground Truth and Label Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#misc&#34;&gt;Misc&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This section requires understanding of sanitizing and preparing data for modeling, performing feature engineering, and analyzing and visualizing data for machine learning.&lt;/p&gt;
&lt;p&gt;Exploratory Data Analysis (EDA) is a process of analyzing and summarizing a dataset in order to understand its structure and relationships. In the context of Amazon Web Services (AWS), EDA is often performed on large datasets that are stored in AWS storage services such as Amazon S3 or Amazon EBS.&lt;/p&gt;
&lt;p&gt;To perform EDA on AWS, users can use various tools and services provided by AWS. For example, users can use Amazon Elastic MapReduce (EMR) to process and analyze large datasets using tools such as Apache Spark or Hive. Users can also use Amazon Athena to query datasets stored in Amazon S3 using SQL.&lt;/p&gt;
&lt;p&gt;In addition to these tools, users can also use various AWS services and libraries to visualize and explore the data. For example, users can use Amazon QuickSight to create interactive charts and dashboards, or use libraries such as pandas and matplotlib to create custom visualizations.&lt;/p&gt;
&lt;p&gt;Overall, EDA on AWS involves using a combination of tools and services to understand the structure and relationships within a dataset, and to gain insights that can inform further analysis and decision making.&lt;/p&gt;
&lt;h2 id=&#34;python-in-data-science-and-machine-learning&#34;&gt;Python in data science and machine learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Python code will not be tested in the exam.&lt;/li&gt;
&lt;li&gt;Python is a popular language for data exploration, analysis, and machine learning. It has a number of useful libraries for loading, manipulating, and visualizing data, as well as for building and training machine learning models.&lt;/li&gt;
&lt;li&gt;For data exploration and visualization, some popular libraries include pandas, numpy, and matplotlib. Pandas is a library for working with tabular data, numpy is a library for working with numerical data, and matplotlib is a library for creating charts and plots.&lt;/li&gt;
&lt;li&gt;For machine learning, some popular libraries include scikit-learn, tensorflow, and pytorch. These libraries include a wide range of tools for tasks such as classification, regression, clustering, and deep learning.&lt;/li&gt;
&lt;li&gt;Overall, Python is a powerful and flexible language for data analysis and machine learning, and is widely used in the field.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;types-of-data&#34;&gt;Types of Data&lt;/h2&gt;
&lt;p&gt;There are many different types of data, and the type of data can often influence the analysis and techniques used to understand it. Some common types of data include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Numeric data: This includes data that is represented as numbers, such as integers or floating point values.&lt;/li&gt;
&lt;li&gt;Categorical data: This includes data that consists of categories or groups, such as gender or eye color.&lt;/li&gt;
&lt;li&gt;Ordinal data: This is similar to categorical data, but the categories have a natural ordering, such as low, medium, and high.&lt;/li&gt;
&lt;li&gt;Binary data: This is data that has only two categories, such as true/false or 0/1.&lt;/li&gt;
&lt;li&gt;Time series data: This is data that is collected over time, such as daily stock prices or monthly sales figures.&lt;/li&gt;
&lt;li&gt;Text data: This is data that is represented as text, such as emails or social media posts.&lt;/li&gt;
&lt;li&gt;Image data: This is data that is represented as images, such as photographs or videos.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-distributions&#34;&gt;Data Distributions&lt;/h2&gt;
&lt;h3 id=&#34;normal-distribution&#34;&gt;Normal Distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The normal distribution, also known as the Gaussian distribution, is a continuous probability distribution that is defined by a symmetrical bell-shaped curve.&lt;/li&gt;
&lt;li&gt;It is one of the most widely used and well-known probability distributions in statistics, and is commonly used to model real-valued random variables.&lt;/li&gt;
&lt;li&gt;The normal distribution is completely defined by its mean and standard deviation.&lt;/li&gt;
&lt;li&gt;The mean is the center of the distribution and determines the location of the peak of the curve.&lt;/li&gt;
&lt;li&gt;The standard deviation is a measure of the spread of the distribution and determines the width of the curve.&lt;/li&gt;
&lt;li&gt;A larger standard deviation means that the data is more spread out, while a smaller standard deviation means that the data is more concentrated around the mean.&lt;/li&gt;
&lt;li&gt;The normal distribution has a number of useful properties. For example, the empirical rule states that for a normal distribution, approximately 68% of the data lies within one standard deviation of the mean, 95% of the data lies within two standard deviations of the mean, and 99.7% of the data lies within three standard deviations of the mean.&lt;/li&gt;
&lt;li&gt;This means that if a dataset follows a normal distribution, a large percentage of the data will be concentrated around the mean. Overall, the normal distribution is a widely used and important distribution in statistics, and is often used to model real-valued data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;probability-mass-function-discrete-data-type&#34;&gt;Probability Mass function (discrete data type)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A probability mass function (PMF) is a function that gives the probability of a discrete random variable taking on a particular value. For a random variable X, the PMF is denoted as f(x), and it is defined as the probability that X takes on the value x.&lt;/li&gt;
&lt;li&gt;The PMF is a useful tool for describing the probability distribution of a discrete random variable. It specifies the probability of each possible outcome, and can be used to calculate various statistical quantities such as the mean, variance, and skewness of the distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bernoulli-discrete-data-type&#34;&gt;Bernoulli (discrete data type)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Bernoulli distribution is a discrete probability distribution that models the probability of a binary outcome, such as the result of a coin flip or a yes/no question. It is defined by a single parameter p, which represents the probability of success (the probability of the outcome being &amp;ldquo;yes&amp;rdquo;).&lt;/li&gt;
&lt;li&gt;The Bernoulli distribution is a special case of the binomial distribution, where the number of trials is fixed at n=1. In other words, it models a single binary event, such as the flip of a coin.&lt;/li&gt;
&lt;li&gt;The probability mass function (PMF) of a Bernoulli-distributed random variable X is given by the following formula:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(x) = p^x * (1-p)^(1-x)&lt;/p&gt;
&lt;p&gt;where x is the outcome (0 for &amp;ldquo;no&amp;rdquo; and 1 for &amp;ldquo;yes&amp;rdquo;), and p is the probability of success.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Bernoulli distribution has a mean of p and a variance of p(1-p). It is a simple but widely used distribution, and is often used as a building block for more complex models.&lt;/li&gt;
&lt;li&gt;Overall, the Bernoulli distribution is a useful tool for modeling the probability of a binary outcome, and is widely used in a variety of applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;binomial-discrete-data-type&#34;&gt;Binomial (discrete data type)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The binomial distribution is a discrete probability distribution that is used to model the probability of a certain number of successes in a fixed number of independent trials. It is defined by two parameters: the number of trials (n) and the probability of success in each trial (p).&lt;/li&gt;
&lt;li&gt;The binomial distribution can be used to model a wide variety of situations, such as the probability of flipping a coin and getting a certain number of heads in a row, or the probability of a certain number of defects occurring in a batch of products.&lt;/li&gt;
&lt;li&gt;The probability mass function (PMF) of a binomial-distributed random variable X is given by the following formula:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(x) = (n choose x) * p^x * (1-p)^(n-x)&lt;/p&gt;
&lt;p&gt;where x is the number of successes, n is the number of trials, p is the probability of success in each trial, and &amp;ldquo;choose&amp;rdquo; represents the binomial coefficient.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The binomial distribution has a number of useful properties, such as the fact that the mean and variance can be easily calculated from the parameters n and p. It is also a limiting distribution, meaning that it can be used to approximate other distributions under certain conditions.&lt;/li&gt;
&lt;li&gt;Overall, the binomial distribution is a useful tool for modeling the probability of a certain number of successes in a fixed number of independent trials, and is widely used in a variety of applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;geometric-distribution&#34;&gt;Geometric Distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The geometric distribution is a discrete probability distribution that models the number of Bernoulli trials (i.e., a series of independent &amp;ldquo;success-failure&amp;rdquo; experiments) needed to get a success. It is defined by a single parameter p, which is the probability of success on each trial.&lt;/li&gt;
&lt;li&gt;The probability mass function (PMF) of the geometric distribution is given by:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(k) = (1 - p)^(k-1) * p&lt;/p&gt;
&lt;p&gt;where k is the number of trials needed to get a success and p is the probability of success on each trial.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The geometric distribution has several characteristics:
&lt;ul&gt;
&lt;li&gt;It is a one-sided distribution, which means that it is defined only for k &amp;gt; 0.&lt;/li&gt;
&lt;li&gt;It is a discrete distribution, which means that it is defined for a specific set of values rather than for a continuous range of values.&lt;/li&gt;
&lt;li&gt;It has a mean of 1/p, which is the expected number of trials needed to get a success.&lt;/li&gt;
&lt;li&gt;The geometric distribution is often used in modeling the number of trials needed to get a success, such as the number of ads that need to be shown before a customer clicks on one, or the number of patients that need to be treated before a certain medical condition is cured. It is also used in reliability engineering to model the number of failures before a unit fails.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;poission-discrete-data-type&#34;&gt;Poission (discrete data type)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Poisson distribution is a discrete probability distribution that is used to model the number of times an event occurs within a certain period of time or space. It is commonly used to model events that occur randomly and independently, such as the number of customers arriving at a store or the number of defects in a manufactured product.&lt;/li&gt;
&lt;li&gt;The Poisson distribution is defined by a single parameter, called the rate parameter or the mean rate of occurrence. This parameter is denoted as lambda (λ) and represents the average number of times the event occurs per unit of time or space.&lt;/li&gt;
&lt;li&gt;The probability mass function (PMF) of a Poisson-distributed random variable X is given by the following formula:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(x) = (λ^x * e^(-λ)) / x!&lt;/p&gt;
&lt;p&gt;where x is the number of times the event occurs, λ is the rate parameter, and e is the base of the natural logarithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Poisson distribution has a number of useful properties, such as the fact that the mean and variance are equal to the rate parameter λ. It is also a limiting distribution, meaning that it can be used to approximate other distributions under certain conditions.&lt;/li&gt;
&lt;li&gt;Overall, the Poisson distribution is a useful tool for modeling the number of times an event occurs within a certain period of time or space, and is widely used in a variety of applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;exponential-distribution&#34;&gt;Exponential Distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The exponential distribution is a continuous probability distribution that describes the time between events in a Poisson process, which is a process in which events occur continuously and independently at a constant average rate. It is defined by a single parameter λ (lambda), which is the rate at which the events occur.&lt;/li&gt;
&lt;li&gt;The probability density function (PDF) of the exponential distribution is given by:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(x) = λ * e^(-λx)&lt;/p&gt;
&lt;p&gt;where x is the time between events and e is the base of the natural logarithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The exponential distribution has several characteristics:
&lt;ul&gt;
&lt;li&gt;It is a memoryless distribution, which means that the probability of an event occurring at time t+x, given that it has not occurred by time t, is the same as the probability of the event occurring at time x.&lt;/li&gt;
&lt;li&gt;It has a constant hazard rate, which means that the probability of an event occurring at any given time is constant.&lt;/li&gt;
&lt;li&gt;It is a one-sided distribution, which means that it is defined only for x &amp;gt; 0.&lt;/li&gt;
&lt;li&gt;The exponential distribution is often used in modeling the time between failures of equipment, the time between arrivals at a service facility, and the time between phone calls at a call center. It is also used in survival analysis to model the time until an event occurs, such as death or failure.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;weibull-distribution&#34;&gt;Weibull Distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The Weibull distribution is a continuous probability distribution that is often used to model time-to-failure data in reliability engineering. It is defined by two parameters: shape and scale.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The probability density function (PDF) of the Weibull distribution is given by:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(x) = (shape/scale) * (x/scale)^(shape-1) * e^(-(x/scale)^shape)&lt;/p&gt;
&lt;p&gt;where x is the time to failure, shape is the shape parameter, and scale is the scale parameter.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Weibull distribution has several characteristics:
&lt;ul&gt;
&lt;li&gt;It is a one-sided distribution, which means that it is defined only for x &amp;gt; 0.&lt;/li&gt;
&lt;li&gt;It has a shape parameter that controls the shape of the curve. If the shape parameter is less than 1, the curve is &amp;ldquo;skewed&amp;rdquo; to the right, meaning that it has a longer tail on the right side. - If the shape parameter is greater than 1, the curve is &amp;ldquo;skewed&amp;rdquo; to the left, meaning that it has a longer tail on the left side. If the shape parameter is equal to 1, the curve is symmetrical.&lt;/li&gt;
&lt;li&gt;It has a scale parameter that controls the spread of the curve. If the scale parameter is large, the curve is spread out and has a longer tail. If the scale parameter is small, the curve is more concentrated and has a shorter tail.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The Weibull distribution is often used in reliability engineering to model the time until failure of a component or system. It is also used in other fields, such as meteorology, to model wind speed and in economics to model stock returns.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;trends-and-seasonality&#34;&gt;Trends and seasonality&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Trends and seasonality are two common patterns that can occur in time series data.&lt;/li&gt;
&lt;li&gt;A trend is a long-term increase or decrease in the data. It can be either linear, meaning that the data increases or decreases at a constant rate, or nonlinear, meaning that the rate of change varies over time. Trends can be caused by various factors such as changes in consumer demand, economic conditions, or technological innovations.&lt;/li&gt;
&lt;li&gt;Seasonality is a pattern that repeats over a specific time period, such as annually or monthly. It can be caused by factors such as weather patterns, holidays, or consumer behavior.&lt;/li&gt;
&lt;li&gt;Both trends and seasonality can have important implications for forecasting and decision making. For example, if a company sees a trend of increasing sales, it may decide to ramp up production or hire more staff. If a company sees seasonal fluctuations in demand, it may need to adjust its inventory or staffing levels accordingly.&lt;/li&gt;
&lt;li&gt;To analyze trends and seasonality in time series data, various techniques can be used such as smoothing methods, decomposition methods, and autoregressive models. It is important to correctly identify and account for these patterns in order to make accurate forecasts and informed decisions.&lt;/li&gt;
&lt;li&gt;Additive time series data is characterized by a constant trend and constant seasonality over time. This means that the trend and seasonality do not change, and the data can be modeled using a linear function. The relationship between the data and the trend and seasonality components can be represented as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Data = Trend + Seasonality + Noise&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiplicative time series data, on the other hand, is characterized by a varying trend and varying seasonality. This means that the trend and seasonality change over time, and the data cannot be modeled using a linear function. The relationship between the data and the trend and seasonality components can be represented as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Data = Trend * Seasonality * Noise&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It is important to correctly identify whether a time series is additive or multiplicative, as this can influence the choice of modeling techniques and the interpretation of the results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Also this is possible:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Additive trend and additive seasonality&lt;/li&gt;
&lt;li&gt;Additive trend and multiplicative seasonality&lt;/li&gt;
&lt;li&gt;Multiplicative trend and additive seasonality&lt;/li&gt;
&lt;li&gt;Multiplicative trend and multiplicative seasonality&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;athena&#34;&gt;Athena&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Athena is a serverless, interactive query service that allows users to analyze data stored in Amazon S3 using SQL.&lt;/li&gt;
&lt;li&gt;It is designed to be fast and easy to use, and can be used to analyze data from a wide variety of sources such as logs, streaming data, and data lakes.&lt;/li&gt;
&lt;li&gt;To use Athena, users first define a data schema by creating tables that point to the data stored in Amazon S3.&lt;/li&gt;
&lt;li&gt;They can then use SQL to query the data and analyze it using various functions and aggregations. Athena supports a wide range of SQL functions and data types, and users can also use custom user-defined functions (UDFs) to extend its capabilities.&lt;/li&gt;
&lt;li&gt;It is also highly scalable, and can handle queries on large datasets with minimal performance degradation.&lt;/li&gt;
&lt;li&gt;Presto under the hood&lt;/li&gt;
&lt;li&gt;Supports multiple formats&lt;/li&gt;
&lt;li&gt;Unstructured, semi structured or structured&lt;/li&gt;
&lt;li&gt;Ad hoc queries, querying data before loading to Redshift, analyze Cloudtrail, integration with Jupyter, Zepplin, Integration with quicksight, integration with ODBC, JDBC&lt;/li&gt;
&lt;li&gt;AWS Glue datalog can extract the schema for Athena to use&lt;/li&gt;
&lt;li&gt;Pay as you go, inexpensive, converting to columner saves a lot of money, Glue and S3 have their own charges&lt;/li&gt;
&lt;li&gt;IAM policies, encryption is possible, TLS is possible&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;quicksight&#34;&gt;Quicksight&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon QuickSight is a business intelligence and data visualization service provided by Amazon Web Services (AWS).&lt;/li&gt;
&lt;li&gt;It allows users to create interactive dashboards and charts to visualize and analyze data from a wide variety of sources.&lt;/li&gt;
&lt;li&gt;To use QuickSight, users first need to connect it to their data sources, which can include data stored in Amazon S3, Amazon Redshift, Amazon RDS, and other AWS data stores, as well as external data sources such as spreadsheets and databases.&lt;/li&gt;
&lt;li&gt;Once the data is connected, users can use QuickSight&amp;rsquo;s visual interface to create charts, graphs, and other visualizations to explore and analyze the data.&lt;/li&gt;
&lt;li&gt;QuickSight offers a range of features and tools to help users analyze and understand their data.&lt;/li&gt;
&lt;li&gt;These include built-in analytics functions, support for custom SQL queries, and the ability to share and collaborate on dashboards with other users.&lt;/li&gt;
&lt;li&gt;Overall, Amazon QuickSight is a powerful and easy-to-use tool for creating interactive dashboards and visualizations, and is widely used in a variety of applications such as business intelligence, data exploration, and data reporting.&lt;/li&gt;
&lt;li&gt;Ad-hoc analysis&lt;/li&gt;
&lt;li&gt;Can do calculated columns etc.&lt;/li&gt;
&lt;li&gt;SPICE: Super Fast Parallel, In memory Calculation engine 10 gb&lt;/li&gt;
&lt;li&gt;Quicksight is quick because of SPICE&lt;/li&gt;
&lt;li&gt;Quicksights machine learning insights: Anomaly detection using Random cut forest, Forecasting and auto narratives (not too mature).&lt;/li&gt;
&lt;li&gt;Multifactor authentication&lt;/li&gt;
&lt;li&gt;Works with vpc, and provides row-level security&lt;/li&gt;
&lt;li&gt;Users defined via IAM or email signup&lt;/li&gt;
&lt;li&gt;AugoGraph feature selects the best graph for the respective data type&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;types-of-visualization&#34;&gt;Types of visualization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bar charts: These are used to compare categories or groups of data. They can be vertical or horizontal, and can be used to show both quantitative and categorical data.&lt;/li&gt;
&lt;li&gt;Line charts: These are used to show trends over time or other continuous variables. They can be used to show multiple data series on the same chart.&lt;/li&gt;
&lt;li&gt;Scatter plots: These are used to show the relationship between two numeric variables. They can be used to show correlations, patterns, and trends in the data.&lt;/li&gt;
&lt;li&gt;Pie charts: These are used to show proportions or percentages. They are most commonly used to show how a whole is divided into parts.&lt;/li&gt;
&lt;li&gt;Histograms: These are used to show the distribution of a continuous variable. They show the frequency or density of data points within different ranges or bins.&lt;/li&gt;
&lt;li&gt;Box plots: These are used to show the distribution and spread of a continuous variable. They show the minimum, first quartile, median, third quartile, and maximum values of the data.&lt;/li&gt;
&lt;li&gt;Heatmaps: These are used to show patterns and trends in data organized in a grid. They use color to represent the data, with warmer colors indicating higher values and cooler colors indicating lower values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;emr&#34;&gt;EMR&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Elastic MapReduce (EMR) is a fully managed, cloud-native big data processing service that makes it easy to process large amounts of data using Apache Hadoop and Apache Spark.&lt;/li&gt;
&lt;li&gt;It is designed to be scalable, highly available, and easy to use, and it integrates with a variety of data sources and destinations.&lt;/li&gt;
&lt;li&gt;With AWS EMR, users can create clusters of Amazon Elastic Compute Cloud (EC2) instances and use them to process and analyze large datasets stored in Amazon S3 or other data stores.&lt;/li&gt;
&lt;li&gt;EMR supports a variety of big data processing frameworks, including Hadoop, Spark, and Flink, and it provides tools to help users manage and monitor their clusters.&lt;/li&gt;
&lt;li&gt;AWS EMR is a useful tool for organizations that need to process and analyze large amounts of data, such as log files, scientific simulations, and machine learning models.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;li&gt;Provides notebooks&lt;/li&gt;
&lt;li&gt;Master nodes (manages the cluster), Core node (holds HDFS data and run tasks), Task node (only runs tasks)&lt;/li&gt;
&lt;li&gt;HDFS is epimerical&lt;/li&gt;
&lt;li&gt;Transient cluster vs Long running cluster&lt;/li&gt;
&lt;li&gt;IAM configure permissions&lt;/li&gt;
&lt;li&gt;CloudTrail: audit requests&lt;/li&gt;
&lt;li&gt;Data Pipeline: schedule and start clusters&lt;/li&gt;
&lt;li&gt;EMRFS: access s3 as if it were HDFS, uses DynamoDB to track consistency&lt;/li&gt;
&lt;li&gt;EBS for HDFS is also possible&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;hadoop&#34;&gt;Hadoop&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Apache Hadoop is an open-source software framework for storing and processing large amounts of data in a distributed computing environment.&lt;/li&gt;
&lt;li&gt;It is designed to handle data that is too large or complex for traditional database systems, and can process and analyze data in parallel across a large number of servers.&lt;/li&gt;
&lt;li&gt;Hadoop consists of two main components: the Hadoop Distributed File System (HDFS) and the MapReduce programming model.&lt;/li&gt;
&lt;li&gt;HDFS is a distributed file system that stores data across a large number of servers, and MapReduce is a programming model that allows developers to write programs that can process and analyze large amounts of data in parallel.&lt;/li&gt;
&lt;li&gt;Hadoop is commonly used for tasks such as data analysis, machine learning, and log processing. It is also often used in conjunction with other tools and technologies such as Apache Spark, Apache Hive, and Apache Pig to build more complex data processing pipelines.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;apache-spark&#34;&gt;Apache Spark&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Apache Spark is an open-source, distributed computing system that is designed for fast and flexible data processing.&lt;/li&gt;
&lt;li&gt;It is a popular choice for tasks such as data analytics, machine learning, and real-time stream processing.&lt;/li&gt;
&lt;li&gt;Spark is built on top of the Hadoop distributed file system (HDFS) and is designed to be highly scalable and efficient.&lt;/li&gt;
&lt;li&gt;It can process and analyze data in parallel across a large number of servers, and supports a wide range of programming languages including Python, Java, R, and Scala.&lt;/li&gt;
&lt;li&gt;One of the main benefits of Spark is its ability to process data in memory, which allows it to be much faster than other distributed computing systems that rely on disk-based storage.&lt;/li&gt;
&lt;li&gt;It also includes a number of useful libraries and tools for tasks such as machine learning, graph processing, and stream processing.&lt;/li&gt;
&lt;li&gt;in memory caching, DAGs&lt;/li&gt;
&lt;li&gt;Batch processing and real time analytics, graph processing, machine learning&lt;/li&gt;
&lt;li&gt;Spark context, cluster manager via spark or yarn, executors&lt;/li&gt;
&lt;li&gt;Spark core&lt;/li&gt;
&lt;li&gt;Spark RDD, DataFrames and Datasets are built on top of RDD, and they are most commonly used at the moment&lt;/li&gt;
&lt;li&gt;Spark Streaming is possible (works in mini batches). Unbounded database table&lt;/li&gt;
&lt;li&gt;MlLib (distributed machine learning)&lt;/li&gt;
&lt;li&gt;Graphx (distributed graph processing)&lt;/li&gt;
&lt;li&gt;Zepplin can run spark code interactively, and can also use charts/plots&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;spark-mllib&#34;&gt;Spark MLlib&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MLlib is a machine learning library for Apache Spark. It is designed to provide scalable and efficient machine learning algorithms that can be used on big data.&lt;/li&gt;
&lt;li&gt;MLlib includes a wide range of machine learning algorithms and utility functions, including algorithms for classification, regression, clustering, collaborative filtering, and dimensionality reduction.&lt;/li&gt;
&lt;li&gt;It also includes tools for feature engineering, such as feature extraction, transformation, and selection.&lt;/li&gt;
&lt;li&gt;MLlib is designed to be easy to use, and includes APIs for several programming languages including Python, Java, R, and Scala.&lt;/li&gt;
&lt;li&gt;It is also designed to be highly scalable, and can be used to build machine learning models on large datasets distributed across multiple servers.&lt;/li&gt;
&lt;li&gt;Classification: logistic regression and naive bayes&lt;/li&gt;
&lt;li&gt;Regression&lt;/li&gt;
&lt;li&gt;Decision trees&lt;/li&gt;
&lt;li&gt;Recommendation engine (ALS)&lt;/li&gt;
&lt;li&gt;Clustering (K-means)&lt;/li&gt;
&lt;li&gt;LDA (topic modeling)&lt;/li&gt;
&lt;li&gt;ML workflow utilities (pipelines, feature transformation, persistence)&lt;/li&gt;
&lt;li&gt;SVD, PCA and statistics&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;emr-notebooks-security-and-instance-types&#34;&gt;EMR Notebooks, Security and Instance Types&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon EMR Notebooks is a service that allows users to create and manage Jupyter notebooks on Amazon Elastic MapReduce (EMR) clusters. EMR is a cloud-based big data processing service, and Jupyter notebooks are interactive, web-based documents that can contain code, text, and visualizations.&lt;/li&gt;
&lt;li&gt;EMR Notebooks provides a simple and flexible way to analyze and visualize data stored in Amazon S3 or other data stores using a variety of tools and libraries such as Apache Spark, Python, and R. Users can create and edit notebooks using a web-based editor, and can also use notebooks to run and debug code, create visualizations, and collaborate with other users.&lt;/li&gt;
&lt;li&gt;EMR Notebooks is fully integrated with EMR, which means that users can easily spin up and down EMR clusters to process and analyze large datasets, and can also access other EMR features such as security and data access controls.&lt;/li&gt;
&lt;li&gt;Similar concept to Zeppelin, with more AWS integration&lt;/li&gt;
&lt;li&gt;Notebooks backed up to s3&lt;/li&gt;
&lt;li&gt;Provision clusters from the notebooks&lt;/li&gt;
&lt;li&gt;Hosted inside a vpc&lt;/li&gt;
&lt;li&gt;Accessed only via aws console&lt;/li&gt;
&lt;li&gt;IAM policies, Kerberos (a computer-network authentication protocol that works on the basis of tickets to allow nodes communicating over a non-secure network to prove their identity to one another in a secure manner), SSH, IAM roles&lt;/li&gt;
&lt;li&gt;Spot instances are good choice for task nodes, only use on core or master if you are testing or very cost sensitive, however, you are risking partial data loss&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;feature-engineering&#34;&gt;Feature Engineering&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Feature engineering is the process of creating new features or transforming existing features in a dataset in order to improve the performance of a machine learning model. It is a crucial step in the machine learning process, and can have a significant impact on the model&amp;rsquo;s accuracy and effectiveness.&lt;/li&gt;
&lt;li&gt;There are many different techniques that can be used in feature engineering, including:
&lt;ul&gt;
&lt;li&gt;Feature selection: This involves selecting a subset of the most relevant features from a dataset to use in a model. This can help to reduce overfitting, improve model interpretability, and reduce training time.&lt;/li&gt;
&lt;li&gt;Feature extraction: This involves creating new features from existing data by combining or transforming the original features. For example, a new feature could be created by taking the square root of an existing feature.&lt;/li&gt;
&lt;li&gt;Feature transformation: This involves transforming the scale or distribution of a feature in order to improve model performance. For example, data may need to be normalized or standardized in order to be used in some models.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;imputing-missing-data&#34;&gt;Imputing Missing Data&lt;/h2&gt;
&lt;p&gt;There are several ways to impute (or fill in) missing data, and the best method will depend on the specific dataset and the nature of the missing data. Some common methods for imputing missing data include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mean imputation: This involves replacing missing values with the mean (or average) of the non-missing values. This is a simple method that can be useful for numerical data, but can be biased if the data has a skewed distribution.&lt;/li&gt;
&lt;li&gt;Median imputation: This is similar to mean imputation, but uses the median (or middle value) instead of the mean. It can be less affected by outliers than mean imputation and may be a better choice for skewed data.&lt;/li&gt;
&lt;li&gt;Mode imputation: This involves replacing missing values with the most frequent (or mode) value in the dataset. It is often used for categorical data.&lt;/li&gt;
&lt;li&gt;Regression imputation: This involves using a regression model to predict the missing values based on the other available features. It can be a more powerful method, but requires a good understanding of the relationships between the features and the target variable.&lt;/li&gt;
&lt;li&gt;Nearest Neighbors (k-NN) imputation is a method for imputing missing data that uses the k-NN algorithm to fill in missing values based on the values of the nearest neighbors. It is a simple and intuitive method that is often used in machine learning and data analysis. To use k-NN imputation, the first step is to identify the nearest neighbors of the data point with missing values. This is typically done using a distance measure such as Euclidean distance, and the number of neighbors (k) is a user-defined parameter. Once the nearest neighbors have been identified, the missing values can be imputed by averaging the values of the neighbors. k-NN imputation can be a useful method for filling in missing data, especially when the data is highly correlated and the relationships between the features are well understood. However, it can be sensitive to the choice of k, and may not always produce the best results.&lt;/li&gt;
&lt;li&gt;Dropping NA&amp;rsquo;s: This method can be useful in some cases, such as when the number of missing values is small and removing them does not significantly affect the overall size of the dataset. However, it can also lead to a loss of information and may not be appropriate if the missing values are prevalent or if they are likely to be informative.&lt;/li&gt;
&lt;li&gt;MICE (multiple imputation by chained equations): Multiple imputation by chained equations (MICE) is a method for imputing missing data that involves creating multiple imputed datasets and combining them to produce a final result. It is a more advanced method that can be more robust and accurate than other imputation methods, especially when the data has a complex structure and the relationships between the features are not well understood. The final result is produced by combining the imputed datasets using appropriate statistical methods, such as averaging or weighted averaging. MICE can be a powerful method for imputing missing data, but it can also be time-consuming and requires a good understanding of statistical modeling.&lt;/li&gt;
&lt;li&gt;Categorical is usually not trivial&lt;/li&gt;
&lt;li&gt;Just get more data (if possible)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;unbalanced-data&#34;&gt;Unbalanced Data&lt;/h2&gt;
&lt;p&gt;There are several approaches for handling imbalanced data in machine learning, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Balancing the data: This can be done by oversampling the minority class or undersampling the majority class.&lt;/li&gt;
&lt;li&gt;Using a different evaluation metric: Instead of using accuracy, try using precision, recall, or F1 score, which are more sensitive to imbalanced data.&lt;/li&gt;
&lt;li&gt;Adjusting the class weight: Some algorithms allow you to adjust the weight of each class, which can be used to give more importance to the minority class.&lt;/li&gt;
&lt;li&gt;Using a different algorithm: Some algorithms are more robust to imbalanced data than others. For example, tree-based algorithms like random forests and gradient boosting tend to perform better on imbalanced data than algorithms like logistic regression.&lt;/li&gt;
&lt;li&gt;Using data augmentation: If you are working with image data, you can use data augmentation techniques to generate additional minority class examples.&lt;/li&gt;
&lt;li&gt;Anomaly detection: If the minority class represents anomalies or rare events, you can treat the problem as an anomaly detection problem rather than a classification problem.&lt;/li&gt;
&lt;li&gt;Synthetic minority oversampling technique (SMOTE): This is a popular method for oversampling the minority class by synthesizing new examples rather than simply replicating existing ones.&lt;/li&gt;
&lt;li&gt;Cost-sensitive learning: In this approach, the cost of misclassifying examples from the minority class is higher than the cost of misclassifying examples from the majority class.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;handling-outliers&#34;&gt;Handling Outliers&lt;/h2&gt;
&lt;p&gt;There are several ways to identify outliers in a dataset:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Visualization: One of the easiest ways to identify outliers is to plot the data using a box plot or histogram. Outliers will typically be located outside the &amp;ldquo;whiskers&amp;rdquo; of the box plot or outside the range of the histogram.&lt;/li&gt;
&lt;li&gt;Statistical tests: You can use statistical tests to identify outliers in a dataset. For example, you can use the Z-score method to identify outliers by calculating the distance of each data point from the mean in terms of standard deviations. Data points with a Z-score greater than a certain threshold (such as 3 or 4) can be considered outliers.&lt;/li&gt;
&lt;li&gt;Data transformation: Transforming the data, such as taking the log of the data, can make outliers more obvious.&lt;/li&gt;
&lt;li&gt;Anomaly detection algorithms: There are also machine learning algorithms specifically designed for detecting anomalies or outliers in a dataset. These algorithms include density-based methods, distance-based methods, and model-based methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are several approaches for handling outliers in a dataset:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ignore the outlier: This is an option if the outlier does not have a significant impact on the model or if the outlier is the result of a data entry error.&lt;/li&gt;
&lt;li&gt;Drop the outlier: This is an option if the outlier is not representative of the population being studied and if the outlier has a significant impact on the model.&lt;/li&gt;
&lt;li&gt;Transform the data: Some algorithms are more robust to outliers, such as decision trees and random forests. Transforming the data, such as using the log transformation, can also make the model more robust to outliers.&lt;/li&gt;
&lt;li&gt;Use robust models: Some models, such as linear regression with the Huber loss function, are less sensitive to outliers than other models.&lt;/li&gt;
&lt;li&gt;Anomaly detection: If the outlier represents an anomaly or rare event, you can treat the problem as an anomaly detection problem rather than a classification or regression problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;binning-transforming-encoding-scaling-and-shuffling&#34;&gt;Binning, Transforming, Encoding, Scaling, and Shuffling&lt;/h2&gt;
&lt;h3 id=&#34;binning&#34;&gt;Binning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Binning is a process of transforming numerical data into categorical data by dividing the data into a set of bins or intervals. This can be useful for reducing the number of unique values in a dataset, which can make it easier to visualize and analyze the data.&lt;/li&gt;
&lt;li&gt;For example, if you have a dataset with a large range of numerical values, you could use binning to group the values into a smaller set of intervals. This would allow you to plot the data on a histogram or bar chart, which would be more informative than a scatter plot.&lt;/li&gt;
&lt;li&gt;There are several ways to determine the size and number of bins to use for binning data, including:
&lt;ul&gt;
&lt;li&gt;Fixed width bins: In this approach, you specify the size of the bins and the data is divided into intervals of that size.&lt;/li&gt;
&lt;li&gt;Fixed number of bins: In this approach, you specify the number of bins and the data is divided into that number of intervals.&lt;/li&gt;
&lt;li&gt;Optimal bin width: In this approach, the optimal bin width is determined using a statistical method, such as the Scott&amp;rsquo;s normal reference rule or the Freedman-Diaconis rule.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;transforming&#34;&gt;Transforming&lt;/h3&gt;
&lt;p&gt;Data transformation is a process of converting data from one format or representation to another. This can be useful for several reasons, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data cleaning: Data transformation can be used to fix or remove errors or inconsistencies in the data.&lt;/li&gt;
&lt;li&gt;Data preparation: Data transformation can be used to prepare the data for analysis by formatting the data in a specific way or creating new variables.&lt;/li&gt;
&lt;li&gt;Data reduction: Data transformation can be used to reduce the size or complexity of the data, such as by aggregating the data or removing unnecessary variables.&lt;/li&gt;
&lt;li&gt;Data normalization: Data transformation can be used to scale the data to a common range, such as by normalizing the data to have a mean of 0 and a standard deviation of 1.&lt;/li&gt;
&lt;li&gt;Data transformation can also be used to make the data more amenable to a specific algorithm or technique, such as by binning numerical data or encoding categorical data.&lt;/li&gt;
&lt;li&gt;There are many different types of data transformation techniques, including scaling, centering, normalization, aggregation, imputation, and encoding. The appropriate transformation technique will depend on the specific characteristics of the data and the goals of the analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;encoding&#34;&gt;Encoding&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Encoding is the process of converting data from one format into another, often for the purpose of efficient storage or transmission. In the context of machine learning, encoding is often used to convert categorical data, which can&amp;rsquo;t be represented as numerical values, into numerical form.&lt;/li&gt;
&lt;li&gt;There are several types of encoding techniques that can be used, including:
&lt;ul&gt;
&lt;li&gt;One-hot encoding: This technique converts each categorical value into a new binary column, with a value of 1 indicating the presence of the categorical value and a value of 0 indicating its absence.&lt;/li&gt;
&lt;li&gt;Label encoding: This technique converts each categorical value into a numerical value, such as an integer. However, this can lead to problems if the numerical values are interpreted as having a meaningful order.&lt;/li&gt;
&lt;li&gt;Count encoding: This technique encodes the categorical values by the count of each value in the dataset.&lt;/li&gt;
&lt;li&gt;Binary encoding: This technique encodes the categorical values as binary code.&lt;/li&gt;
&lt;li&gt;Target encoding: This technique encodes the categorical values using the mean of the target variable for each value.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;scaling-and-normalizing&#34;&gt;Scaling and normalizing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Scaling and normalization are techniques used to transform variables so that they have a comparable scale. This can be useful for a variety of reasons, such as:&lt;/li&gt;
&lt;li&gt;Some machine learning algorithms are sensitive to the scale of the input variables, and can perform poorly if the variables are on a different scale. Scaling the variables to the same scale can improve the performance of these algorithms.&lt;/li&gt;
&lt;li&gt;Scaling the variables can also make it easier to compare the magnitude of the variables.&lt;/li&gt;
&lt;li&gt;There are several ways to scale and normalize data:
&lt;ul&gt;
&lt;li&gt;Min-max scaling: This scales the variables to a specific range, such as 0-1 or -1 to 1.&lt;/li&gt;
&lt;li&gt;Standardization: This scales the variables so that they have a mean of 0 and a standard deviation of 1.&lt;/li&gt;
&lt;li&gt;Normalization: This scales the variables so that they have a unit norm (a length of 1).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;shuffling&#34;&gt;Shuffling&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Shuffling is the process of randomly rearranging the rows of a dataset. This can be useful for several reasons:&lt;/li&gt;
&lt;li&gt;Machine learning algorithms often expect the data to be in a random order. Shuffling the data before training a model can help ensure that the model is not biased by the order of the data.&lt;/li&gt;
&lt;li&gt;Shuffling the data can also help ensure that the training and test sets are representative of the overall dataset. If the data is not shuffled and the rows are ordered in a certain way, the training and test sets may not be representative of the overall dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tokenization&#34;&gt;Tokenization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements, known as tokens. Tokenization is an important preprocessing step for many natural language processing (NLP) tasks, such as text classification and information retrieval.&lt;/li&gt;
&lt;li&gt;There are several approaches to tokenization, including:
&lt;ul&gt;
&lt;li&gt;Word tokenization: This involves dividing the text into words.&lt;/li&gt;
&lt;li&gt;Sentence tokenization: This involves dividing the text into sentences.&lt;/li&gt;
&lt;li&gt;Word-level tokenization: This involves dividing the text into words and punctuation, such as &amp;ldquo;don&amp;rsquo;t&amp;rdquo; being split into &amp;ldquo;do&amp;rdquo; and &amp;ldquo;n&amp;rsquo;t.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;N-gram tokenization: This involves dividing the text into contiguous sequences of n items, such as bigrams (pairs of words) or trigrams (triplets of words).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;stemming-and-lemmatization&#34;&gt;Stemming and lemmatization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stemming and lemmatization are techniques used to normalize words to their base form, known as a stem or lemma. These techniques are often used as a preprocessing step for natural language processing (NLP) tasks, such as text classification and information retrieval.&lt;/li&gt;
&lt;li&gt;Stemming involves removing the suffixes from a word to obtain the root form of the word. For example, the stem of the word &amp;ldquo;jumping&amp;rdquo; might be &amp;ldquo;jump,&amp;rdquo; and the stem of the word &amp;ldquo;stemmer,&amp;rdquo; might be &amp;ldquo;stem.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Lemmatization, on the other hand, involves determining the base form of a word based on its part of speech and meaning. For example, the lemma of the word &amp;ldquo;was&amp;rdquo; might be &amp;ldquo;be,&amp;rdquo; and the lemma of the word &amp;ldquo;better&amp;rdquo; might be &amp;ldquo;good.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Stemming and lemmatization can be useful for reducing the dimensionality of the data and improving the performance of NLP models, but they can also remove some of the context and meaning of the words.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-sagemaker-ground-truth-and-label-generation&#34;&gt;Amazon Sagemaker Ground Truth and Label Generation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon SageMaker Ground Truth is a fully managed data labeling service that allows users to build highly accurate training datasets for machine learning.&lt;/li&gt;
&lt;li&gt;It offers a variety of labeling methods, including human labeling, active learning, and automatic labeling.&lt;/li&gt;
&lt;li&gt;SageMaker Ground Truth also includes workflows for common data labeling tasks, such as image classification, object detection, and semantic segmentation.&lt;/li&gt;
&lt;li&gt;It also provides tools for managing and tracking the data labeling process, including the ability to set up labeling jobs, track their progress, and review the results.&lt;/li&gt;
&lt;li&gt;This helps users ensure that their data is labeled accurately and efficiently.&lt;/li&gt;
&lt;li&gt;Ambiguous data is sent to humans&lt;/li&gt;
&lt;li&gt;Mechanical Turk: Amazon Mechanical Turk (MTurk) is a cloud platform that enables organizations to use a network of human workers to perform tasks that are typically difficult or time-consuming for computers to perform. These tasks, known as Human Intelligence Tasks (HITs), can include data labeling, transcription, image annotation, and many other types of work.&lt;/li&gt;
&lt;li&gt;Amazon SageMaker Ground Truth Plus helps you to create high-quality training datasets without having to build labeling applications or manage a labeling workforce.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;misc&#34;&gt;Misc&lt;/h2&gt;
&lt;h3 id=&#34;synthetic-features&#34;&gt;Synthetic Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Synthetic features are artificially created features that are derived from existing features in a dataset. They are often used to improve the performance of machine learning models by providing additional information that may not be present in the original features. Synthetic features can be created using various techniques, such as combining or transforming existing features, or by applying statistical or mathematical operations to the data.&lt;/li&gt;
&lt;li&gt;One example of a synthetic feature is a polynomial feature, which is created by taking the product of a feature with itself or with other features. Polynomial features can capture nonlinear relationships between features and the target variable, and can improve the performance of linear models on nonlinear problems. Other examples of synthetic features include interactions between features, binned features, and dummy variables.&lt;/li&gt;
&lt;li&gt;Synthetic features can be useful for improving the performance of machine learning models, especially when the original features are not sufficient for making accurate predictions. However, care should be taken when creating synthetic features, as adding too many of them can lead to overfitting and degrade model performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;stop-words&#34;&gt;Stop words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stop words are common words that are typically filtered out before natural language processing (NLP) tasks, such as text classification or text mining, because they do not provide meaningful information. Examples of stop words include words like &amp;ldquo;a,&amp;rdquo; &amp;ldquo;an,&amp;rdquo; &amp;ldquo;the,&amp;rdquo; &amp;ldquo;and,&amp;rdquo; and &amp;ldquo;but.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;There are several ways to identify and handle stop words:
&lt;ul&gt;
&lt;li&gt;Use a list of stop words: Many NLP libraries and frameworks, such as NLTK and scikit-learn, include a pre-defined list of stop words that can be used to filter out common words.&lt;/li&gt;
&lt;li&gt;Identify stop words using term frequency-inverse document frequency (TF-IDF): This method involves calculating the importance of each word in a document or corpus and filtering out the least important words, which are often stop words.&lt;/li&gt;
&lt;li&gt;Customize the stop word list: You can customize the list of stop words based on the specific needs of your task. For example, if you are working with domain-specific language, you may need to add domain-specific words to the stop word list.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;To handle stop words, you can simply filter them out of the dataset before performing the NLP task. This can be done by comparing the words in the dataset to the stop word list and removing any words that are on the list.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s important to carefully consider whether or not to filter out stop words, as they can sometimes provide important context or meaning. For example, in the phrase &amp;ldquo;not good,&amp;rdquo; the word &amp;ldquo;not&amp;rdquo; is a stop word that changes the meaning of the phrase.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tf-idf&#34;&gt;TF-IDF&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Term Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure that is used to evaluate the importance of a word in a document or corpus. The importance of a word is determined by its frequency in the document and in the corpus as a whole.&lt;/li&gt;
&lt;li&gt;TF-IDF is calculated as the product of the term frequency (TF) and the inverse document frequency (IDF). The term frequency is the number of times a word appears in the document, and the inverse document frequency is the logarithm of the number of documents in the corpus divided by the number of documents that contain the word.&lt;/li&gt;
&lt;li&gt;TF-IDF is often used as a weighting factor in information retrieval and text mining, and can be useful for tasks such as document classification, clustering, and keyword extraction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;correlation&#34;&gt;Correlation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Correlation is a statistical measure that indicates the strength and direction of a linear relationship between two variables. A positive correlation indicates that as one variable increases, the other variable also increases. A negative correlation indicates that as one variable increases, the other variable decreases.&lt;/li&gt;
&lt;li&gt;The correlation coefficient, denoted by &amp;ldquo;r,&amp;rdquo; is a measure of the strength of the relationship between the variables. It ranges from -1 to 1, where -1 indicates a strong negative correlation, 0 indicates no correlation, and 1 indicates a strong positive correlation.&lt;/li&gt;
&lt;li&gt;Correlation can be useful for understanding the relationship between two variables and predicting one variable based on the other. However, it&amp;rsquo;s important to remember that correlation does not necessarily imply causation, meaning that a correlation between two variables does not necessarily mean that one variable causes the other.&lt;/li&gt;
&lt;li&gt;There are several methods for calculating the correlation between variables, including Pearson&amp;rsquo;s correlation coefficient, Spearman&amp;rsquo;s rank correlation coefficient, and Kendall&amp;rsquo;s tau. The appropriate method will depend on the characteristics of the data and the goals of the analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;p-value&#34;&gt;p-value&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The p-value is a statistical measure that is used to assess the significance of a hypothesis test. It is the probability of obtaining a test statistic at least as extreme as the one observed, assuming that the null hypothesis is true.&lt;/li&gt;
&lt;li&gt;The null hypothesis is a statement that there is no statistical relationship between the variables being tested. The alternative hypothesis is the opposite of the null hypothesis and states that there is a statistical relationship between the variables.&lt;/li&gt;
&lt;li&gt;To interpret the p-value, you compare it to a significance level, which is a predetermined cutoff value. If the p-value is less than the significance level, you can reject the null hypothesis and conclude that there is a statistical relationship between the variables. If the p-value is greater than the significance level, you fail to reject the null hypothesis and cannot conclude that there is a statistical relationship between the variables.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s important to carefully consider the appropriate significance level and the limitations of the p-value, as it can be affected by factors such as sample size and the distribution of the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;elbow-plot&#34;&gt;Elbow plot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An elbow plot is a graphical method used to determine the appropriate number of clusters to use in a cluster analysis. It plots the within-cluster sum of squared distances (WCSS) for each possible number of clusters, and the number of clusters is chosen at the &amp;ldquo;elbow&amp;rdquo; point, where the change in WCSS begins to level off.&lt;/li&gt;
&lt;li&gt;To create an elbow plot, you first perform a cluster analysis for a range of possible number of clusters, and then plot the WCSS for each number of clusters. The WCSS can be calculated as the sum of the squared distance between each point and its cluster centroid.&lt;/li&gt;
&lt;li&gt;The &amp;ldquo;elbow&amp;rdquo; point is generally considered to be the point where the WCSS starts to decrease more slowly, indicating that adding more clusters is not significantly improving the fit of the model.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s important to carefully consider the limitations of the elbow method, as it can be affected by the shape of the data and may not always clearly identify the appropriate number of clusters. Other methods, such as the silhouette method, can also be used to determine the number of clusters in a dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;summary-statistics&#34;&gt;Summary Statistics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Summary statistics are quantitative measures that describe and summarize a dataset. They provide a quick and easy way to get a sense of the characteristics and patterns in the data.&lt;/li&gt;
&lt;li&gt;Some common summary statistics include:&lt;/li&gt;
&lt;li&gt;Mean: The mean is the arithmetic average of the data. It is calculated by summing all the values and dividing by the number of values.&lt;/li&gt;
&lt;li&gt;Median: The median is the middle value of the data when it is sorted in ascending order. It is a measure of central tendency that is resistant to outliers.&lt;/li&gt;
&lt;li&gt;Mode: The mode is the most frequent value in the data.&lt;/li&gt;
&lt;li&gt;Range: The range is the difference between the maximum and minimum values in the data.&lt;/li&gt;
&lt;li&gt;Variance: The variance is a measure of the spread or dispersion of the data. It is calculated as the sum of the squared differences between each value and the mean, divided by the number of values.&lt;/li&gt;
&lt;li&gt;Standard deviation: The standard deviation is the square root of the variance. It is a measure of the spread of the data that is in the same units as the original data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;distance-norms&#34;&gt;Distance Norms&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Distance norms, also known as metrics, are functions that define a distance between two points in a space. These functions take two points as inputs and return a non-negative number that represents the distance between them. Different distance norms can be used depending on the characteristics of the data and the requirements of the application.&lt;/li&gt;
&lt;li&gt;Some common distance norms include:
&lt;ul&gt;
&lt;li&gt;Euclidean distance: This is the most common distance norm and is based on the Pythagorean theorem. It is defined as the square root of the sum of the squares of the differences between the coordinates of the two points.&lt;/li&gt;
&lt;li&gt;Manhattan distance: This distance norm is based on the sum of the absolute differences of the coordinates of the two points. It is also known as the &amp;ldquo;taxi cab&amp;rdquo; distance because it represents the distance a taxi cab would need to travel to get from one point to the other.&lt;/li&gt;
&lt;li&gt;Minkowski distance: This is a generalization of the Euclidean and Manhattan distances. It is defined as the sum of the absolute differences of the coordinates of the two points, raised to a power. The value of the power determines whether the distance is more similar to the Euclidean or Manhattan distance.&lt;/li&gt;
&lt;li&gt;Cosine distance: This distance norm is based on the cosine similarity between two vectors. It is often used in text analysis to measure the similarity between documents.&lt;/li&gt;
&lt;li&gt;Jaccard distance: This distance norm is based on the Jaccard similarity coefficient and is often used to compare the similarity of sets. It is defined as the size of the intersection of the sets divided by the size of the union of the sets.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;qq-plots&#34;&gt;QQ plots&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A Q-Q plot (short for &amp;ldquo;quantile-quantile plot&amp;rdquo;) is a graphical way to compare two probability distributions by plotting their quantiles against each other. It is a plot of the sorted data against an idealized distribution with a uniform distribution. The purpose of a Q-Q plot is to check whether two datasets come from the same distribution.&lt;/li&gt;
&lt;li&gt;To create a Q-Q plot, you first need to specify the distribution that you want to use as the reference distribution. Then, you sort both datasets and plot the quantiles of one dataset against the quantiles of the other dataset. If the two datasets come from the same distribution, the points in the Q-Q plot will lie approximately on a straight line. If the points do not lie on a straight line, it suggests that the two datasets come from different distributions.&lt;/li&gt;
&lt;li&gt;Q-Q plots are often used to check whether a dataset follows a particular distribution, such as a normal distribution. They are also useful for comparing datasets to see whether they come from the same distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;parametric-and-non-parametric-test&#34;&gt;Parametric and Non-Parametric test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A parametric test is a statistical test that assumes that the data comes from a population with a known probability distribution, such as a normal distribution. The test uses parameters of the distribution (such as the mean and standard deviation) to make statistical inferences about the population.&lt;/li&gt;
&lt;li&gt;Parametric tests are based on the assumption that the data follows a particular probability distribution, and they are generally more powerful (i.e., able to detect differences with smaller sample sizes) than nonparametric tests, which do not make any assumptions about the distribution of the data. However, if the assumption of a known distribution is not met, the results of a parametric test may be less reliable.&lt;/li&gt;
&lt;li&gt;Examples of parametric tests include the t-test, the ANOVA test, and the linear regression analysis. These tests are commonly used to compare means, variances, and relationships between variables.&lt;/li&gt;
&lt;li&gt;Nonparametric tests, on the other hand, do not assume that the data comes from a particular distribution and are more robust to departures from normality. Examples of nonparametric tests include the Wilcoxon rank-sum test and the Kruskal-Wallis test.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;markov-chain-model&#34;&gt;Markov Chain model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A Markov chain is a mathematical system that undergoes transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the system arrived at its current state, the possible future states are fixed.&lt;/li&gt;
&lt;li&gt;A Markov chain is often represented by a state transition diagram, which shows all the possible states that the system can be in, and the transitions between these states. The transitions are governed by transition probabilities, which specify the probability of moving from one state to another.&lt;/li&gt;
&lt;li&gt;Markov chains have many applications in various fields, including economics, computer science, and physics. They are used to model systems that change over time and have a finite number of states. Some examples of systems that can be modeled using Markov chains include:
&lt;ul&gt;
&lt;li&gt;The behavior of a customer moving through a website, where the states represent the different pages on the website and the transitions represent the clicks that the customer makes to move from one page to another.&lt;/li&gt;
&lt;li&gt;The weather, where the states represent different weather conditions (such as sunny, cloudy, or rainy) and the transitions represent the probability of the weather changing from one condition to another.&lt;/li&gt;
&lt;li&gt;The movement of a particle through a lattice, where the states represent the different positions that the particle can occupy and the transitions represent the probability of the particle moving from one position to another.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;optimization&#34;&gt;Optimization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Optimization is the process of finding the best solution to a problem, given certain constraints. In mathematics, optimization problems involve finding the maximum or minimum value of a function, subject to certain constraints. These constraints can be equality constraints, which specify that a certain relationship must hold among the variables, or inequality constraints, which specify that a certain relationship must not hold among the variables.&lt;/li&gt;
&lt;li&gt;There are many different methods for solving optimization problems, including gradient descent, the simplex method, and the interior point method. The choice of method depends on the specific problem and the desired properties of the solution.&lt;/li&gt;
&lt;li&gt;Optimization is used in many fields, including engineering, economics, and machine learning. Some examples of optimization problems include:
&lt;ul&gt;
&lt;li&gt;Finding the optimal allocation of resources, such as the allocation of capital to different investments, or the allocation of production capacity to different products.&lt;/li&gt;
&lt;li&gt;Finding the optimal design of a system, such as the design of an aircraft or the design of a supply chain.&lt;/li&gt;
&lt;li&gt;Finding the optimal parameters of a machine learning model, such as the weights of a neural network or the regularization parameters of a linear model.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;objective-function&#34;&gt;Objective Function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An objective function is a mathematical function that is used to represent the goal of an optimization problem. The goal of the optimization problem is to find the values of the variables that either maximize or minimize the objective function, subject to certain constraints.&lt;/li&gt;
&lt;li&gt;The objective function is also known as the cost function, the loss function, or the criterion function. It is a measure of how well a given solution meets the requirements of the problem. In general, the objective function is a scalar-valued function, meaning that it maps a vector of variables to a single scalar value.&lt;/li&gt;
&lt;li&gt;The objective function is an important component of an optimization problem, as it defines the goal of the optimization and determines the solution of the problem. The objective function is often defined in terms of the decision variables (the variables that are being optimized) and the parameters of the problem (the constants that define the problem).&lt;/li&gt;
&lt;li&gt;For example, in a linear programming problem, the objective function is a linear function that represents the cost or profit associated with a particular allocation of resources. In a nonlinear programming problem, the objective function is a nonlinear function that represents the cost or performance of a system.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;exponential-smoothing-for-outlier-detection&#34;&gt;Exponential smoothing for outlier detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Exponential smoothing is a time series forecasting method that is used to predict future values based on historical data. It is based on the idea of giving more weight to more recent observations and less weight to observations that are further in the past.&lt;/li&gt;
&lt;li&gt;Exponential smoothing can be used for outlier detection by analyzing the residuals (i.e., the differences between the predicted values and the actual values) of the time series. If the residuals contain outliers (i.e., values that are significantly different from the majority of the residuals), it may indicate that there is something unusual or unexpected happening in the time series.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;box-cox-transformation&#34;&gt;Box-Cox Transformation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Box-Cox transformation is a way to transform non-normal dependent variables into a normal shape. It is named after George Box and David Cox, who introduced the transformation in 1964. The transformation is defined as:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Y = (X^L - 1) / L&lt;/p&gt;
&lt;p&gt;where X is the variable to be transformed, Y is the transformed variable, and L is a parameter that needs to be estimated. When L = 0, the Box-Cox transformation becomes the log transformation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Box-Cox transformation can be useful in regression analysis, ANOVA, and other statistical tests when the assumptions of normality are not met. It can also be useful in improving the interpretability of the model by making the relationship between the dependent and independent variables more linear.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pca-for-visualization-and-eda&#34;&gt;PCA for visualization and EDA&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Principal component analysis (PCA) is a statistical technique that is used to reduce the dimensionality of a data set.&lt;/li&gt;
&lt;li&gt;It does this by identifying the directions in which the data vary the most, and then projecting the data onto a lower-dimensional space.&lt;/li&gt;
&lt;li&gt;This can be useful for visualization, as it can allow you to plot high-dimensional data in a 2D or 3D space.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>AWS Certified ML - Specialty exam (MLS-C01) - 3b. Modeling</title>
      <link>https://ayushsubedi.github.io/posts/aws_ml_speciality_modeling/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/aws_ml_speciality_modeling/</guid>
      <description>&lt;h1 id=&#34;modeling&#34;&gt;Modeling&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#activation-functions&#34;&gt;Activation Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#convolutional-neural-network&#34;&gt;Convolutional Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#recurrent-neural-networks&#34;&gt;Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modern-nlp-with-bert-and-gpt-and-transfer-learning&#34;&gt;Modern NLP with BERT and GPT, and Transfer Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deep-learning-on-ec2-and-emr&#34;&gt;Deep Learning on EC2 and EMR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tuning-neural-networks&#34;&gt;Tuning Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regularization-techniques-for-neural-networks-dropout-early-stopping&#34;&gt;Regularization Techniques for Neural Networks (Dropout, Early Stopping)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#l1-and-l2-regularization&#34;&gt;L1 and L2 Regularization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#grief-with-gradients-the-vanishing-gradient-problem&#34;&gt;Grief with Gradients The Vanishing Gradient problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-confusion-matrix&#34;&gt;The Confusion Matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#precision-recall-f1-auc-and-more&#34;&gt;Precision, Recall, F1, AUC, and more&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ensemble-methods-bagging-and-boosting&#34;&gt;Ensemble Methods Bagging and Boosting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introducing-amazon-sagemaker&#34;&gt;Introducing Amazon SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-learner-in-sagemaker&#34;&gt;Linear Learner in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#xgboost-in-sagemaker&#34;&gt;XGBoost in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#seq2seq-in-sagemaker&#34;&gt;Seq2Seq in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deepar-in-sagemaker&#34;&gt;DeepAR in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#blazingtext-in-sagemaker&#34;&gt;BlazingText in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#object2vec-in-sagemaker&#34;&gt;Object2Vec in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#object-detection-in-sagemaker&#34;&gt;Object Detection in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#image-classification-in-sagemaker&#34;&gt;Image Classification in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#semantic-segmentation-in-sagemaker&#34;&gt;Semantic Segmentation in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-cut-forest-in-sagemaker&#34;&gt;Random Cut Forest in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#neural-topic-model-in-sagemaker&#34;&gt;Neural Topic Model in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#latent-dirichlet-allocation-lda-in-sagemaker&#34;&gt;Latent Dirichlet Allocation (LDA) in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-nearest-neighbors-knn-in-sagemaker&#34;&gt;K-Nearest-Neighbors (KNN) in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-means-clustering-in-sagemaker&#34;&gt;K-Means Clustering in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#principal-component-analysis-pca-in-sagemaker&#34;&gt;Principal Component Analysis (PCA) in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#factorization-machines-in-sagemaker&#34;&gt;Factorization Machines in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ip-insights-in-sagemaker&#34;&gt;IP Insights in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reinforcement-learning-in-sagemaker&#34;&gt;Reinforcement Learning in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#automatic-model-tuning&#34;&gt;Automatic Model Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#apache-spark-with-sagemaker&#34;&gt;Apache Spark with SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-studio-and-sagemaker-experiments&#34;&gt;SageMaker Studio, and SageMaker Experiments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-debugger&#34;&gt;SageMaker Debugger&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-autopilot-automl&#34;&gt;SageMaker Autopilot / AutoML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-model-monitor&#34;&gt;SageMaker Model Monitor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-recent-features-jumpstart-data-wrangler-features-store-edge-manager&#34;&gt;Other recent features (JumpStart, Data Wrangler, Features Store, Edge Manager)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-canvas&#34;&gt;SageMaker Canvas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bias-measures-in-sagemaker-canvas&#34;&gt;Bias Measures in SageMaker Canvas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-training-compiler&#34;&gt;SageMaker Training Compiler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-comprehend&#34;&gt;Amazon Comprehend&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-translate&#34;&gt;Amazon Translate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-transcribe&#34;&gt;Amazon Transcribe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-polly&#34;&gt;Amazon Polly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-rekognition&#34;&gt;Amazon Rekognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-forecast&#34;&gt;Amazon Forecast&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-forecast-algorithms&#34;&gt;Amazon Forecast Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-lex&#34;&gt;Amazon Lex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-personalize&#34;&gt;Amazon Personalize&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lightning-round-textract-deeplens-deepracher-lookout-and-monitron&#34;&gt;Lightning round! TexTract, DeepLens, DeepRacher, Lookout, and Monitron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#torchserve-aws-neuron-and-aws-panorama&#34;&gt;TorchServe, AWS Neuron, and AWS Panorama&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deep-composer-fraud-detection-codeguru-and-contact-lens&#34;&gt;Deep Composer, Fraud Detection, CodeGuru, and Contact Lens&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-kendra-and-amazon-augmented-ai-a2i&#34;&gt;Amazon Kendra and Amazon Augmented AI (A2I)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This section covers framing business problems as machine learning problems, selecting the appropriate model(s) for a given machine learning problem, training machine learning models, performing hyperparameter optimization, and evaluate machine learning models.&lt;/p&gt;
&lt;h2 id=&#34;deeplearning-frameworks&#34;&gt;Deeplearning Frameworks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Tensorflow/Keras (Google)&lt;/li&gt;
&lt;li&gt;PyTorch (Meta)&lt;/li&gt;
&lt;li&gt;MXNet (Apache, and therefore AWS leans towards this)&lt;/li&gt;
&lt;li&gt;Scikit-Learn (for simple DL)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;activation-functions&#34;&gt;Activation Functions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Apply a non linear transformation&lt;/li&gt;
&lt;li&gt;Given the input, what should by output be&lt;/li&gt;
&lt;li&gt;Can be applied in between layers, or in the output layer&lt;/li&gt;
&lt;li&gt;Step Function, Sigmoid, TanH, ReLU, Leaky ReLU&lt;/li&gt;
&lt;li&gt;Binary Step Function is either on or off, cannot handle multiple classification, vertical slopes do not work with calculus&lt;/li&gt;
&lt;li&gt;Sigmoid: 0 to 1&lt;/li&gt;
&lt;li&gt;TanH: -1 to 1&lt;/li&gt;
&lt;li&gt;For Sigmoid and TanH there is a vanishing gradient problem (value changes slowly for high or low value)&lt;/li&gt;
&lt;li&gt;Sigmoid and TanH are computationally expensive&lt;/li&gt;
&lt;li&gt;ReLu: fast to compute, for inputs that are zero or negative, it is a linear function (dying relu problem)&lt;/li&gt;
&lt;li&gt;Leaky ReLU solves this&lt;/li&gt;
&lt;li&gt;Parametric ReLU, slope in the negative part is learned via backpropagation, complicated&lt;/li&gt;
&lt;li&gt;Exponential Linear Unit (ELU)&lt;/li&gt;
&lt;li&gt;Maxout: usually not worth the effort&lt;/li&gt;
&lt;li&gt;Softmax: usually the final layer of a classification model&lt;/li&gt;
&lt;li&gt;RNN&amp;rsquo;s do well with Tanh&lt;/li&gt;
&lt;li&gt;Sigmoid if more that one classification is required for the same thing&lt;/li&gt;
&lt;li&gt;For everything else, start with ReLU&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;convolutional-neural-network&#34;&gt;Convolutional Neural Network&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;CNN vs MLP (Multilayer perceptron)&lt;/li&gt;
&lt;li&gt;They have convolutional layers&lt;/li&gt;
&lt;li&gt;Some filters may detect edges, lines, shapes etc. and deeper layers can detect objects&lt;/li&gt;
&lt;li&gt;Feature location invariant, Shift Invariant, Space Invariant Artificial Neural Networks&lt;/li&gt;
&lt;li&gt;Image and video recognition, recommender systems, image classification, image segmentations,&lt;/li&gt;
&lt;li&gt;Machine translation, Sentence Classification, Sentiment analysis&lt;/li&gt;
&lt;li&gt;AlexNet, LeNet, GoogLeNet, ResNet as an example&lt;/li&gt;
&lt;li&gt;source data must be of appropriate dimensions&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;recurrent-neural-networks&#34;&gt;Recurrent Neural Networks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;deals with sequences in time (predict stock prices, understand words in a sentence, translation etc)&lt;/li&gt;
&lt;li&gt;time series data, sequence of arbitrary length&lt;/li&gt;
&lt;li&gt;captions for images, order matters&lt;/li&gt;
&lt;li&gt;structure and context is relevant&lt;/li&gt;
&lt;li&gt;machine generated music&lt;/li&gt;
&lt;li&gt;past behaviour of neuron impacts the future&lt;/li&gt;
&lt;li&gt;Sequence to Sequence: predict stock prices based on series of historic data&lt;/li&gt;
&lt;li&gt;Sequence to vector: words in a sentence to sentiment&lt;/li&gt;
&lt;li&gt;Vector to sequence: create captions from an image&lt;/li&gt;
&lt;li&gt;Encoder -&amp;gt; Decoder: Sequence -&amp;gt; vector -&amp;gt; sequence, machine translation&lt;/li&gt;
&lt;li&gt;Backpropogation through time&lt;/li&gt;
&lt;li&gt;Ends up looking like a really really deep neural network&lt;/li&gt;
&lt;li&gt;Therefore, we use truncated backpropagation through time&lt;/li&gt;
&lt;li&gt;State from earlier time steps get diluted over time, Long Short-Term memory cell LSTM cell&lt;/li&gt;
&lt;li&gt;GRU cell: Gated Recurrent Unit, Simplified LSTM which performs almost as well&lt;/li&gt;
&lt;li&gt;Traning RNN&amp;rsquo;s is hard, very sensitive to topologies, choice of hyperparameters, very resource intensive, a wrong choice can lead to a RNN that does not converge at all.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;modern-nlp-with-bert-and-gpt-and-transfer-learning&#34;&gt;Modern NLP with BERT and GPT, and Transfer Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Transformer deep learning architectures&lt;/li&gt;
&lt;li&gt;BERT, RoBERTa, T5, GPT2, GPT3, etc&lt;/li&gt;
&lt;li&gt;DistilBERT: uses knowledge distillation to reduce model size by 40%&lt;/li&gt;
&lt;li&gt;BERT: Bi-directional Encoder Representations from Transformers&lt;/li&gt;
&lt;li&gt;GPT: Generative Pre-trained Transformer&lt;/li&gt;
&lt;li&gt;Transfer Learning&lt;/li&gt;
&lt;li&gt;Model zoos: hugging face offer pre trained models to start with&lt;/li&gt;
&lt;li&gt;Hugging face DLC (deep learning containers)&lt;/li&gt;
&lt;li&gt;Transfer Learning, retrain=True vs False&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;deep-learning-on-ec2emr&#34;&gt;Deep Learning on EC2/EMR&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;EMR supports Apache MXNet and GPU instance types&lt;/li&gt;
&lt;li&gt;Appropriate instance types for deep learning&lt;/li&gt;
&lt;li&gt;P3, P2, G3&lt;/li&gt;
&lt;li&gt;Deep Learning AMI&amp;rsquo;s&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tuning-neural-networks&#34;&gt;Tuning Neural Networks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Neural nets are trained by gradient descent or sth similar&lt;/li&gt;
&lt;li&gt;We start at some random point, and sample different solutions seeking to minimize some cost functions, over many epochs&lt;/li&gt;
&lt;li&gt;how far apart these samples are is the learning rate&lt;/li&gt;
&lt;li&gt;learning rate is an example of a hyperparameter&lt;/li&gt;
&lt;li&gt;batch size is also a hyperparameter, smaller batch size can work out of local minima&lt;/li&gt;
&lt;li&gt;small batch size tend to not get stuck in local minima&lt;/li&gt;
&lt;li&gt;large batch sizes can converge on the wrong solution at random&lt;/li&gt;
&lt;li&gt;large learning rates can overshoot the correct solution&lt;/li&gt;
&lt;li&gt;small learning rates increate training time&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;regularization-techniques-for-neural-networks-dropout-early-stopping&#34;&gt;Regularization Techniques for Neural Networks (Dropout, Early Stopping)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Regularization helps with avoiding overfitting&lt;/li&gt;
&lt;li&gt;build simple model, dropout, early stopping can also help with avoiding overfitting&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;l1-and-l2-regularization&#34;&gt;L1 and L2 Regularization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;L1: sum of abs value of weights: perform feature selection, computationally inefficient, sparse output&lt;/li&gt;
&lt;li&gt;L2: sum of square of weights, all features considered but weighted, computationally efficient, dence output&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;grief-with-gradients-the-vanishing-gradient-problem&#34;&gt;Grief with Gradients The Vanishing Gradient problem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;vanishing gradient propogate to deeper layer&lt;/li&gt;
&lt;li&gt;slope is approaching zero&lt;/li&gt;
&lt;li&gt;it could be the local miminum or global where the convergence is happening&lt;/li&gt;
&lt;li&gt;long short term memory RNN can be used&lt;/li&gt;
&lt;li&gt;resnet also helps with vanishing gradient problem&lt;/li&gt;
&lt;li&gt;better activation function (relu is a good choice)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-confusion-matrix&#34;&gt;The Confusion Matrix&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;sometimes accuracy does not tell the whole story&lt;/li&gt;
&lt;li&gt;TP, TN, FP, FN&lt;/li&gt;
&lt;li&gt;Confusion matrix shows this&lt;/li&gt;
&lt;li&gt;multi class confusion matrix: heatmap&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;precision-recall-f1-auc-and-more&#34;&gt;Precision, Recall, F1, AUC, and more&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Precision/Correct Positives/Percent of relevant results: when you are a lot about false positives: TP/(TP+FP)&lt;/li&gt;
&lt;li&gt;Recall/Sensitivity/True Positive Rate:  TP/(TP + FN): when you care about false negatives&lt;/li&gt;
&lt;li&gt;F1 score: harmonic mean of Precision and Recall&lt;/li&gt;
&lt;li&gt;Specificity: TN/(TN+FP)&lt;/li&gt;
&lt;li&gt;RMSE, AMSE, etc.&lt;/li&gt;
&lt;li&gt;ROC curve: Receiver Operating Characteristic Curve: Plot of true positive rate (recall) vs false positive rate at various threshold setting.&lt;/li&gt;
&lt;li&gt;AUC curve: area under the ROC curve.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ensemble-methods-bagging-and-boosting&#34;&gt;Ensemble Methods Bagging and Boosting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bagging: Generate N new training sets by random sampling with replacement, each resampled model can be trained in parallel&lt;/li&gt;
&lt;li&gt;Boosting: Observations are weighted, training is sequential&lt;/li&gt;
&lt;li&gt;XGBoost is the latest hotness, boosting generally yields better accuracy, bagging avoids overfitting, bagging is easier to parallelize&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introducing-amazon-sagemaker&#34;&gt;Introducing Amazon SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;built to handle the entire machine learning workflow&lt;/li&gt;
&lt;li&gt;deploy model, evaluate results in production, fetch, clean and prepare data, train and evaluate a model&lt;/li&gt;
&lt;li&gt;training data will be in s3, sagemakaker docker EC2 for inference&lt;/li&gt;
&lt;li&gt;spins as many hosts, spins as many endpoints&lt;/li&gt;
&lt;li&gt;Sagemaker notebook: notebook instance on EC2, has access to s3, scikit learn, spark, tensorflow, ability to deploy trained models for making predictions at scale&lt;/li&gt;
&lt;li&gt;hyperparameter tuning from notebook&lt;/li&gt;
&lt;li&gt;Sagemaker console&lt;/li&gt;
&lt;li&gt;Data comes from S3, ideal format is RecordIO/Protobuf/csv&lt;/li&gt;
&lt;li&gt;Can also ingest from Athena, EMR, Redshift, Amazon Keyspaces DB&lt;/li&gt;
&lt;li&gt;Apache Spark integrates with Sagemaker&lt;/li&gt;
&lt;li&gt;Scikit learn, numpy, pandas all work&lt;/li&gt;
&lt;li&gt;Create training job&lt;/li&gt;
&lt;li&gt;save your trained model to s3&lt;/li&gt;
&lt;li&gt;can be deployed using persistent endpoint for making individual predictions on demand&lt;/li&gt;
&lt;li&gt;or batch transform to get prediction for and entire dataset&lt;/li&gt;
&lt;li&gt;inference pipelines&lt;/li&gt;
&lt;li&gt;sagemaker neo for deploying to edge devices&lt;/li&gt;
&lt;li&gt;elastic inference for accelerating deep learning models&lt;/li&gt;
&lt;li&gt;automatic scaling of endpoints as needed&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;linear-learner-in-sagemaker&#34;&gt;Linear Learner in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Linear learer can handle both classification and regression&lt;/li&gt;
&lt;li&gt;can do classification using Linear Learner threshold&lt;/li&gt;
&lt;li&gt;as long as a line will fit&lt;/li&gt;
&lt;li&gt;RecordIO wrapped protobuf float32, or csv (first column assumed to be the label)&lt;/li&gt;
&lt;li&gt;File or pipe mode both supported&lt;/li&gt;
&lt;li&gt;pipe mode will be more efficient&lt;/li&gt;
&lt;li&gt;if s3 is taking to long to train, pipe is a simple optimization&lt;/li&gt;
&lt;li&gt;training data should be normalized&lt;/li&gt;
&lt;li&gt;input data should be shuffled&lt;/li&gt;
&lt;li&gt;uses SGD&lt;/li&gt;
&lt;li&gt;multiple models are optimized in parallel&lt;/li&gt;
&lt;li&gt;tune l1, l2 regularization&lt;/li&gt;
&lt;li&gt;balance multiclass weights: give each class equal importance in loss functions&lt;/li&gt;
&lt;li&gt;learning rate, mini batch size, l1 regualization&lt;/li&gt;
&lt;li&gt;multi gpu does not help&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;xgboost-in-sagemaker&#34;&gt;XGBoost in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;eXtreme gradient boosting&lt;/li&gt;
&lt;li&gt;boosted group of decision trees&lt;/li&gt;
&lt;li&gt;gradient descent&lt;/li&gt;
&lt;li&gt;winning a lot of kaggle competitions&lt;/li&gt;
&lt;li&gt;fast&lt;/li&gt;
&lt;li&gt;classification/regression&lt;/li&gt;
&lt;li&gt;CSV/libsvm/recordIO-protobuf/parquet&lt;/li&gt;
&lt;li&gt;models are searilized/deserialized with pickle&lt;/li&gt;
&lt;li&gt;can use as a framework withing notebooks&lt;/li&gt;
&lt;li&gt;or as a built in sagemaker algorithm&lt;/li&gt;
&lt;li&gt;subsample (prevent overfitting)&lt;/li&gt;
&lt;li&gt;ETA (step size shrinkage, prevents overfitting)&lt;/li&gt;
&lt;li&gt;Gamma (minimul loss reduction to create a partition)&lt;/li&gt;
&lt;li&gt;Alpha (L1 regularization term, larger = more conservative)&lt;/li&gt;
&lt;li&gt;Lambda (L2 regularization term, larger = more conservative)&lt;/li&gt;
&lt;li&gt;eval_metric: Optimize on AUC, example: if you care about false positives more than accuracy&lt;/li&gt;
&lt;li&gt;scale_pos_weight: adjusts balance of positive and negative weights, helpful for unbalanced classes&lt;/li&gt;
&lt;li&gt;max_depth : too high may overfit&lt;/li&gt;
&lt;li&gt;Xgboost with cpu: M5 is a good choice (optimize for memory and not compute)&lt;/li&gt;
&lt;li&gt;Xgboost with gpu: tree_method hyperparameter: gpu_hist, cheaper and faster, P3 is good choice&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;seq2seq-in-sagemaker&#34;&gt;Seq2Seq in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;sequence to sequence (example machine translation, text summarization, speech to text)&lt;/li&gt;
&lt;li&gt;implemented with RNN&amp;rsquo;s and CNN&amp;rsquo;s with attention&lt;/li&gt;
&lt;li&gt;RecordIO-Protobuf tokens must be integers&lt;/li&gt;
&lt;li&gt;start with tokenized text files&lt;/li&gt;
&lt;li&gt;convert to protobuf using sample code&lt;/li&gt;
&lt;li&gt;must provide training data, validation data and vocabulary files&lt;/li&gt;
&lt;li&gt;training machine translation can take days, pretrained models are available&lt;/li&gt;
&lt;li&gt;public training datasets are avaialable for specific translation tasks&lt;/li&gt;
&lt;li&gt;batch_size, optimizer_type, learning_rate, num_layers_encoder, num_layers_decoder, can optimize on accuracy, bleu score (compares against multiple reference translations), perplexity (cross-entropy)&lt;/li&gt;
&lt;li&gt;cannot be parallelized&lt;/li&gt;
&lt;li&gt;can only use gpu instance&lt;/li&gt;
&lt;li&gt;can use multi gpu within an instance machine&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;deepar-in-sagemaker&#34;&gt;DeepAR in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Forecasting one dimensional time series data&lt;/li&gt;
&lt;li&gt;uses rnn&amp;rsquo;s&lt;/li&gt;
&lt;li&gt;allows you to train the same model over several related time series&lt;/li&gt;
&lt;li&gt;finds frequencies and seasonality&lt;/li&gt;
&lt;li&gt;json lines format, Gzip or Parquet&lt;/li&gt;
&lt;li&gt;each record must contain, start and target&lt;/li&gt;
&lt;li&gt;each record can contain dynamic features and categorical features&lt;/li&gt;
&lt;li&gt;always include entire time series for training, testing and inference&lt;/li&gt;
&lt;li&gt;use entire dataset as test set&lt;/li&gt;
&lt;li&gt;do not use very large values for prediction (&amp;gt;400)&lt;/li&gt;
&lt;li&gt;train on many time series&lt;/li&gt;
&lt;li&gt;contect length, epochs, mini batch size, learning rate, num cells&lt;/li&gt;
&lt;li&gt;can use cpu or gpu&lt;/li&gt;
&lt;li&gt;single or multi machine&lt;/li&gt;
&lt;li&gt;cpu only for inferene&lt;/li&gt;
&lt;li&gt;may need larger instances for tuning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;blazingtext-in-sagemaker&#34;&gt;BlazingText in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Text classification: predict labels for a sentence, useful in web searches, information retrieal, supervised&lt;/li&gt;
&lt;li&gt;Word2vec: creates a vector representation of workds&lt;/li&gt;
&lt;li&gt;semantically similar words are represented by vectors close to each otehr&lt;/li&gt;
&lt;li&gt;this is called a word embedding&lt;/li&gt;
&lt;li&gt;it is useful for nlp, but is not an nlp algorithm itself&lt;/li&gt;
&lt;li&gt;it only works on individual words, not sentences or documents&lt;/li&gt;
&lt;li&gt;for supervised mode, one sentence per line, first word in the sentence is the string &lt;em&gt;label&lt;/em&gt; followed by the label&lt;/li&gt;
&lt;li&gt;Also, &amp;ldquo;augmented manifest text format&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Word3vec just wants a text file with one training sentence per line&lt;/li&gt;
&lt;li&gt;There are multiple modes:&lt;/li&gt;
&lt;li&gt;Cbow (Continuous Bag of Words)&lt;/li&gt;
&lt;li&gt;Skip-gram&lt;/li&gt;
&lt;li&gt;Batch skip-gram (Distributed computation over many CPU nodes)&lt;/li&gt;
&lt;li&gt;Word2vec: mode, learning rate, window size, verctor dim, negative samples&lt;/li&gt;
&lt;li&gt;Text classification: epochs, learning rate, word ngrams, vector dim&lt;/li&gt;
&lt;li&gt;For cbow and skipgram, recommend a single ml.p3.2xlarge, any single CPU or single GPU instance will work&lt;/li&gt;
&lt;li&gt;for batch_skipgram, can use single or multiple CPU instances&lt;/li&gt;
&lt;li&gt;for text classification C5 recommended if less than 2GB training data, for larger datasets use a single GPU instance ml.p2.xlarge or ml.p3.2xlarge&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;object2vec-in-sagemaker&#34;&gt;Object2Vec in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;creates low-dimensional dense embeddings of high-dimensional objects&lt;/li&gt;
&lt;li&gt;compute nearest neighbors of objects&lt;/li&gt;
&lt;li&gt;visualize clusters&lt;/li&gt;
&lt;li&gt;genre prediction&lt;/li&gt;
&lt;li&gt;recommendations&lt;/li&gt;
&lt;li&gt;data must be tokenized into integers&lt;/li&gt;
&lt;li&gt;training data consists of pairs of tokens and or sequenses of tokens&lt;/li&gt;
&lt;li&gt;process data into json lines and shuffle it&lt;/li&gt;
&lt;li&gt;train with two input channels, two encoders, and a comparator&lt;/li&gt;
&lt;li&gt;encoder choices: average-pooled embeddings, cnn&amp;rsquo;s, bidirectional lstm&lt;/li&gt;
&lt;li&gt;comparator is followed by feed-fowrard neural network&lt;/li&gt;
&lt;li&gt;usual suspect: dropout, early stopping, epochs, learning rate, bbatch size, layers, activation function, optimizer, weight decay&lt;/li&gt;
&lt;li&gt;Enc1_network, enc2_network&lt;/li&gt;
&lt;li&gt;instance types: can only train on a single machine (cpu or gpu, multi-gpu ok)&lt;/li&gt;
&lt;li&gt;inference: use ml.p2.2xlarge&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;object-detection-in-sagemaker&#34;&gt;Object Detection in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;identify all objects in an image with bounding box&lt;/li&gt;
&lt;li&gt;detects and classifies objects with a single deep neural network&lt;/li&gt;
&lt;li&gt;classes are accompanied by confidence scores&lt;/li&gt;
&lt;li&gt;can train from scratch, or use pretrained models based on imagenet&lt;/li&gt;
&lt;li&gt;recodrio or image format&lt;/li&gt;
&lt;li&gt;with image format, supply a json file for annotation data for each image&lt;/li&gt;
&lt;li&gt;takes and image input, outputs all instances of objects in teh imagte with categories and confidence scores&lt;/li&gt;
&lt;li&gt;uses cnn with single shot multibox detector ssd algorithm, the base being vgg-16 or resnet-50&lt;/li&gt;
&lt;li&gt;transfer learning mode/incrementatl training: use pretrained model for the base network instead of random inintial weights&lt;/li&gt;
&lt;li&gt;uses flip, rescale, and jitter internally to avoid overfitting&lt;/li&gt;
&lt;li&gt;mini batch size, learning rate, optimizer&lt;/li&gt;
&lt;li&gt;gpu instances for training&lt;/li&gt;
&lt;li&gt;multi gpu multi machines&lt;/li&gt;
&lt;li&gt;for inference cpu is enough&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;image-classification-in-sagemaker&#34;&gt;Image Classification in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;assign one or more labels to an image&lt;/li&gt;
&lt;li&gt;does not tell you where objects are&lt;/li&gt;
&lt;li&gt;mxnet recordio (not protobuf)&lt;/li&gt;
&lt;li&gt;raw jpg or png&lt;/li&gt;
&lt;li&gt;.lst files to associate image index and class&lt;/li&gt;
&lt;li&gt;augmented manifest image format enables pipe mode&lt;/li&gt;
&lt;li&gt;resnet cnn under the hood&lt;/li&gt;
&lt;li&gt;full training mode&lt;/li&gt;
&lt;li&gt;transfer learning mode&lt;/li&gt;
&lt;li&gt;default image is 224 224 3&lt;/li&gt;
&lt;li&gt;bbatch size, learning rate, optimizer&lt;/li&gt;
&lt;li&gt;weight decay, beta 1, beta 2, eps, gamma&lt;/li&gt;
&lt;li&gt;gpu instance fr training&lt;/li&gt;
&lt;li&gt;cpu or gpu for inference&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;semantic-segmentation-in-sagemaker&#34;&gt;Semantic Segmentation in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;pixel level object classificaion&lt;/li&gt;
&lt;li&gt;different from image classification&lt;/li&gt;
&lt;li&gt;useful for self driving vehicles, medical imaging, robot sensing&lt;/li&gt;
&lt;li&gt;produces a semantic mask&lt;/li&gt;
&lt;li&gt;jpg or img with annotations&lt;/li&gt;
&lt;li&gt;augmented manifest image format supported for pipe mode&lt;/li&gt;
&lt;li&gt;jpg images accepted for inference&lt;/li&gt;
&lt;li&gt;mxnet gluon and gluon cv&lt;/li&gt;
&lt;li&gt;fully convolution network, pyramid scene parsing, deeplabv3&lt;/li&gt;
&lt;li&gt;resnet50, renet101, both rained on imagenet&lt;/li&gt;
&lt;li&gt;incremental training, or scratch&lt;/li&gt;
&lt;li&gt;epochs, learning rate, batch size, optimizer, algorithm, backbone&lt;/li&gt;
&lt;li&gt;only gpu for training (p2 or p3), and only on one maching&lt;/li&gt;
&lt;li&gt;cpu or gpu for inference&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;random-cut-forest-in-sagemaker&#34;&gt;Random Cut Forest in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;anomaly detection&lt;/li&gt;
&lt;li&gt;unsupervised&lt;/li&gt;
&lt;li&gt;detect unexpected spikes in time series data&lt;/li&gt;
&lt;li&gt;breaks in periodicity&lt;/li&gt;
&lt;li&gt;unclassifiable data points&lt;/li&gt;
&lt;li&gt;assigns and anamoly score to each data points&lt;/li&gt;
&lt;li&gt;recordio protobuf or csv&lt;/li&gt;
&lt;li&gt;can use file or pipe mode on either&lt;/li&gt;
&lt;li&gt;optional test channel for computation&lt;/li&gt;
&lt;li&gt;creates a forest of trees where each tree is a partition of the training data, looks at expected change in complexity of the tree as a result of adding a point into it&lt;/li&gt;
&lt;li&gt;data is sampled randomly and then trained&lt;/li&gt;
&lt;li&gt;rcf shows up in kinesis analytics as well, it can work on streaming data as well.&lt;/li&gt;
&lt;li&gt;num_trees, num_samples_per_tree (should be chosen such that 1/num_samples_per_tree approximates the ratio of anomalous to normal data)&lt;/li&gt;
&lt;li&gt;does not take advantage of gpu&lt;/li&gt;
&lt;li&gt;ml.c5.xl for inference&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;neural-topic-model-in-sagemaker&#34;&gt;Neural Topic Model in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;organize documents into topics&lt;/li&gt;
&lt;li&gt;classify or summarize documents based on topics&lt;/li&gt;
&lt;li&gt;it is not just tf/idf&lt;/li&gt;
&lt;li&gt;unsupervised: algorithm is neural variational inference&lt;/li&gt;
&lt;li&gt;four data channels, train, validation, test and auxiliary&lt;/li&gt;
&lt;li&gt;record io or csv&lt;/li&gt;
&lt;li&gt;words muyst be tokenized into integers&lt;/li&gt;
&lt;li&gt;file or pipe mode&lt;/li&gt;
&lt;li&gt;you define how many topics you want, these topics are latent representation based on top ranking words&lt;/li&gt;
&lt;li&gt;one of two modelling algorithms sagemaker offers&lt;/li&gt;
&lt;li&gt;batch size, num_topics&lt;/li&gt;
&lt;li&gt;gpu or cpu&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;latent-dirichlet-allocation-lda-in-sagemaker&#34;&gt;Latent Dirichlet Allocation (LDA) in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;latent dirichlet allocation&lt;/li&gt;
&lt;li&gt;another topic modeling algorithm but not based on deep learning&lt;/li&gt;
&lt;li&gt;unsupervised: topics are unlabeled, they are just grouping of documents with a shared subseet of words&lt;/li&gt;
&lt;li&gt;can be used for other purposes as well&lt;/li&gt;
&lt;li&gt;train channel, optional test channel&lt;/li&gt;
&lt;li&gt;protobuf or csv&lt;/li&gt;
&lt;li&gt;each document has counts for every word in vocabulary&lt;/li&gt;
&lt;li&gt;pipe mode: only supported with proto&lt;/li&gt;
&lt;li&gt;unsupervised, generates however many topics you specify&lt;/li&gt;
&lt;li&gt;per-word log likelyhood&lt;/li&gt;
&lt;li&gt;num_topics, alpha0&lt;/li&gt;
&lt;li&gt;cpu single instance, cannot parallelize&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;k-nearest-neighbors-knn-in-sagemaker&#34;&gt;K-Nearest-Neighbors (KNN) in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;simple classification or regression algorithm&lt;/li&gt;
&lt;li&gt;classification: k closest points&lt;/li&gt;
&lt;li&gt;regression: average values&lt;/li&gt;
&lt;li&gt;train channel, test channel emits accuracy or MSE&lt;/li&gt;
&lt;li&gt;protobuf or csv (first column is label)&lt;/li&gt;
&lt;li&gt;data is sampled, sagemaker includes dimensionality reduction stage, build an index for looking up neighbors, serialize the model, query the model for given K&lt;/li&gt;
&lt;li&gt;hyperparameter K, sample_size&lt;/li&gt;
&lt;li&gt;cpu or gpu&lt;/li&gt;
&lt;li&gt;cpu or gpu for inference&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;k-means-clustering-in-sagemaker&#34;&gt;K-Means Clustering in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;unsupervised clustering&lt;/li&gt;
&lt;li&gt;divide data into k groups, where members of a group are as similar as possible to each other&lt;/li&gt;
&lt;li&gt;web scale k means clustering&lt;/li&gt;
&lt;li&gt;training input: train channel, train in shardedbys3key and testing: fullyreplicated&lt;/li&gt;
&lt;li&gt;recordio or csv&lt;/li&gt;
&lt;li&gt;file or pipemode&lt;/li&gt;
&lt;li&gt;every ovservation is mapped to n-dimensional space&lt;/li&gt;
&lt;li&gt;works to optimize the center of k clusters&lt;/li&gt;
&lt;li&gt;algorithm: k means++ tries to make initial clusters far away, lloyd&amp;rsquo;s method&lt;/li&gt;
&lt;li&gt;mini_batch_size, extra_center_factor, init_method&lt;/li&gt;
&lt;li&gt;cpu or gpu, but cpu recommended&lt;/li&gt;
&lt;li&gt;only one gpu per instance used on gpu&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;principal-component-analysis-pca-in-sagemaker&#34;&gt;Principal Component Analysis (PCA) in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;dimensionality reduction&lt;/li&gt;
&lt;li&gt;unsupervised&lt;/li&gt;
&lt;li&gt;covariance matrix is created, then SVD&lt;/li&gt;
&lt;li&gt;two modesL regular: for sparse matrix, randomized: for large number of observations and features&lt;/li&gt;
&lt;li&gt;algorithm_mode and subtract_mean&lt;/li&gt;
&lt;li&gt;gpu or cpu&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;factorization-machines-in-sagemaker&#34;&gt;Factorization Machines in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;dealing with sparse data&lt;/li&gt;
&lt;li&gt;item recommendations&lt;/li&gt;
&lt;li&gt;supervised: classification or regression&lt;/li&gt;
&lt;li&gt;limited to pair-wise interactions&lt;/li&gt;
&lt;li&gt;protobuf with float32&lt;/li&gt;
&lt;li&gt;bias, factors, and linear terms&lt;/li&gt;
&lt;li&gt;cpu or gpu, cpu recommended&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ip-insights-in-sagemaker&#34;&gt;IP Insights in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;finding fishy behaviour&lt;/li&gt;
&lt;li&gt;unsupervised learning of ip address&lt;/li&gt;
&lt;li&gt;identifies suspicious behaviour from ip addresses&lt;/li&gt;
&lt;li&gt;user names, account ids, not need to pre process&lt;/li&gt;
&lt;li&gt;training channel, optional validation (computes auc score)&lt;/li&gt;
&lt;li&gt;csv only (entity, ips)&lt;/li&gt;
&lt;li&gt;neural network to learn latent vector representations of entities and ip addresses&lt;/li&gt;
&lt;li&gt;entities are hashed and embedded&lt;/li&gt;
&lt;li&gt;automatically generates negative samples during training by randomly pairing entities and ips&lt;/li&gt;
&lt;li&gt;num_entity vectors, vector_dim, epochs, learning rate, batch size&lt;/li&gt;
&lt;li&gt;cpu or gpu&lt;/li&gt;
&lt;li&gt;gpu recommended&lt;/li&gt;
&lt;li&gt;multiple gpu can be used withing an instance&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reinforcement-learning-in-sagemaker&#34;&gt;Reinforcement Learning in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;agent and environment&lt;/li&gt;
&lt;li&gt;supply chain management, hvac systems, industrial robots, dialog systems, autonomous vehicles&lt;/li&gt;
&lt;li&gt;yields fast on-line performance once the space has been explored&lt;/li&gt;
&lt;li&gt;Q learning: environment, actions, state/action part&lt;/li&gt;
&lt;li&gt;uses a deep learning framework with tensorflow and mxnet&lt;/li&gt;
&lt;li&gt;supports intel coach and ray rllib toolkits&lt;/li&gt;
&lt;li&gt;custom, open-source or commercial environments supported&lt;/li&gt;
&lt;li&gt;can distribute trining and environment rollout&lt;/li&gt;
&lt;li&gt;multi core and multi instance&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;automatic-model-tuning&#34;&gt;Automatic Model Tuning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;define the hyperparameters you care about&lt;/li&gt;
&lt;li&gt;sagemaker spins up a hyperparameter tuning job that trains as many combinations as you will allow&lt;/li&gt;
&lt;li&gt;it learns as it goes, so it does not have to try every possible combination&lt;/li&gt;
&lt;li&gt;intelligent&lt;/li&gt;
&lt;li&gt;do not optimize too many hyperparameters at once&lt;/li&gt;
&lt;li&gt;limit your ranges to as samall range&lt;/li&gt;
&lt;li&gt;use logarithmic scales&lt;/li&gt;
&lt;li&gt;do not run too many training jobs concurently&lt;/li&gt;
&lt;li&gt;make sure training jobs running on multiple instance report the correct objective metric in the end&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;apache-spark-with-sagemaker&#34;&gt;Apache Spark with SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;apache spark allows for preprocessing and also has mllib&lt;/li&gt;
&lt;li&gt;combination of sagemaker and spark is possible&lt;/li&gt;
&lt;li&gt;preprocess with spark, and instead of using mllib, you can use sagemaker estimator, you can use kmeans, pca, xgboost&lt;/li&gt;
&lt;li&gt;sagemakermodel, can be used to make inferences&lt;/li&gt;
&lt;li&gt;connect notebook to a remote emr&lt;/li&gt;
&lt;li&gt;fit, transform in sagemaker&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-studio-and-sagemaker-experiments&#34;&gt;SageMaker Studio, and SageMaker Experiments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;visual ide&lt;/li&gt;
&lt;li&gt;sagemaker notebooks&lt;/li&gt;
&lt;li&gt;sagemaker experiments&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-debugger&#34;&gt;SageMaker Debugger&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;saves internal model state at periodical intervals&lt;/li&gt;
&lt;li&gt;gradients/tensors over time is saved&lt;/li&gt;
&lt;li&gt;define rules for detecting unwanted conditions while training&lt;/li&gt;
&lt;li&gt;a debug job is run for each rule&lt;/li&gt;
&lt;li&gt;logs and fires a cloudwatch event when the rule is hit&lt;/li&gt;
&lt;li&gt;sagemaker studio debugger dashboards&lt;/li&gt;
&lt;li&gt;auto generated training reports&lt;/li&gt;
&lt;li&gt;built in rules: monitor system bottlenecks, profile model framework operations, debug model parameters&lt;/li&gt;
&lt;li&gt;supported framewords and algorithms: tensorflow, pytorch, mxnet, xgboost, sagemaker generic estimator&lt;/li&gt;
&lt;li&gt;debugger api&amp;rsquo;s available in github&lt;/li&gt;
&lt;li&gt;smdebug is the library&lt;/li&gt;
&lt;li&gt;Sagemaker debugger insights dashboard&lt;/li&gt;
&lt;li&gt;profiler report, hardware system metrics, framework metrics&lt;/li&gt;
&lt;li&gt;built in actions to receive notifications or stop training&lt;/li&gt;
&lt;li&gt;profiling system resource usage and training&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-autopilot--automl&#34;&gt;SageMaker Autopilot / AutoML&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;automates algorithm selection, data preprocessing, model tuning&lt;/li&gt;
&lt;li&gt;it does all the trial and error for you&lt;/li&gt;
&lt;li&gt;automl&lt;/li&gt;
&lt;li&gt;automatic model creation&lt;/li&gt;
&lt;li&gt;model leaderboard&lt;/li&gt;
&lt;li&gt;ranks&lt;/li&gt;
&lt;li&gt;can add in human guidance&lt;/li&gt;
&lt;li&gt;human in the loop&lt;/li&gt;
&lt;li&gt;with or without code in sagemaker studio&lt;/li&gt;
&lt;li&gt;problem types: binary/multiclass classification&lt;/li&gt;
&lt;li&gt;linear learner, xgboost, mlp&lt;/li&gt;
&lt;li&gt;data must be tabular csv&lt;/li&gt;
&lt;li&gt;autopilot explainability&lt;/li&gt;
&lt;li&gt;integrates with sagemaker clarify&lt;/li&gt;
&lt;li&gt;transparency on how models arrive at predictions&lt;/li&gt;
&lt;li&gt;feature attributions: uses shap baselines/shapley values, research from cooperative game theory, assigns each feature an importance value for a give prediction&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-model-monitor&#34;&gt;SageMaker Model Monitor&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;get alery on quality deviations on your deployed models via cloudwatch&lt;/li&gt;
&lt;li&gt;visualize data drift&lt;/li&gt;
&lt;li&gt;detect anomalies and outliers&lt;/li&gt;
&lt;li&gt;detect new features&lt;/li&gt;
&lt;li&gt;no code required&lt;/li&gt;
&lt;li&gt;data is stored in s3, monitoring jobs are scheduled via a monitoring schedule, metrics are emitted to cloudwatch, integrates with quicksight, tensorboard etc.&lt;/li&gt;
&lt;li&gt;drift in statistical properties of the features&lt;/li&gt;
&lt;li&gt;drift in model quality&lt;/li&gt;
&lt;li&gt;bias drift&lt;/li&gt;
&lt;li&gt;feature attribution drift&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;other-recent-features-jumpstart-data-wrangler-features-store-edge-manager&#34;&gt;Other recent features (JumpStart, Data Wrangler, Features Store, Edge Manager)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;jumpstart: one click models and algorithms from model zoos: 150 open source models in nlp, object detection, image classification etc&lt;/li&gt;
&lt;li&gt;data wrangler: import transform analayze and export data withing sagemaker studio&lt;/li&gt;
&lt;li&gt;feature studio: find, discover and share features in studio:online and offline modes&lt;/li&gt;
&lt;li&gt;sagemaker edge manager: software agent for edge devices, models optimized with agemaker neo, collects and samples data for monitoring, labeling and retraining&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-canvas&#34;&gt;SageMaker Canvas&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;no code machine learning for business analysts&lt;/li&gt;
&lt;li&gt;upload csv data, select a column to predict, build it and make predictions&lt;/li&gt;
&lt;li&gt;can also join datasets&lt;/li&gt;
&lt;li&gt;classification or regressions&lt;/li&gt;
&lt;li&gt;automatic data cleaning, missing values, outlier and duplicates&lt;/li&gt;
&lt;li&gt;share models and datasets with sagemaker studio&lt;/li&gt;
&lt;li&gt;import from redshift is possible&lt;/li&gt;
&lt;li&gt;time series must be enabled via IAM&lt;/li&gt;
&lt;li&gt;vpc&lt;/li&gt;
&lt;li&gt;a little expensive&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;bias-measures-in-sagemaker-clarify&#34;&gt;Bias Measures in SageMaker Clarify&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;class imbalance&lt;/li&gt;
&lt;li&gt;difference in proportions of labels&lt;/li&gt;
&lt;li&gt;kullback-leibler divergence, jensen-shannon divergence&lt;/li&gt;
&lt;li&gt;lp-norm&lt;/li&gt;
&lt;li&gt;total variation distance&lt;/li&gt;
&lt;li&gt;kolmogorov-smirnov&lt;/li&gt;
&lt;li&gt;conditional demographic disparity&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-training-compiler&#34;&gt;SageMaker Training Compiler&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;integrates into AWS deep learning containers&lt;/li&gt;
&lt;li&gt;compile and optimize training jobs on gpu&lt;/li&gt;
&lt;li&gt;can accelerate training up to 50%&lt;/li&gt;
&lt;li&gt;converts models into hardware-optimized instructions&lt;/li&gt;
&lt;li&gt;tested with hugging face transformers library, or bring your own model&lt;/li&gt;
&lt;li&gt;ensure gpu instance are used in ml.p3, ml.p4&lt;/li&gt;
&lt;li&gt;pytorch models must use pytorch xla&amp;rsquo;s model save function&lt;/li&gt;
&lt;li&gt;enable dubug flask in compiler_config parameter to enable debugging&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-comprehend&#34;&gt;Amazon Comprehend&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;nlp and text analytics&lt;/li&gt;
&lt;li&gt;input social media, emails, web pages, documents, transcripts, medical records (comprehend medical)&lt;/li&gt;
&lt;li&gt;extract key phrases, entities, sentiment, language, syntax, topics, and document classifications&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-translate&#34;&gt;Amazon Translate&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;translates text&lt;/li&gt;
&lt;li&gt;uses deep learning&lt;/li&gt;
&lt;li&gt;supports custom terminology for proper names&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-transcribe&#34;&gt;Amazon Transcribe&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;speech to text&lt;/li&gt;
&lt;li&gt;speaker identification&lt;/li&gt;
&lt;li&gt;channel identification&lt;/li&gt;
&lt;li&gt;language identification&lt;/li&gt;
&lt;li&gt;custom vocabularies&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-polly&#34;&gt;Amazon Polly&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;text to speech&lt;/li&gt;
&lt;li&gt;polly is parrot&lt;/li&gt;
&lt;li&gt;lexicons&lt;/li&gt;
&lt;li&gt;ssml (speech synthesis markup language)&lt;/li&gt;
&lt;li&gt;speech marks&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-rekognition&#34;&gt;Amazon Rekognition&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;compute vision&lt;/li&gt;
&lt;li&gt;object and scene detection&lt;/li&gt;
&lt;li&gt;image moderation, facial analysis, celebrity recognition, face comparison, text in image, video analysis&lt;/li&gt;
&lt;li&gt;kinesis video stream h.264 encoded, 5-30 fps&lt;/li&gt;
&lt;li&gt;can use lambda to trigger image analysis upon upload&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-forecast&#34;&gt;Amazon Forecast&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;fully managed service to deliver highly accurate forecasts with ml&lt;/li&gt;
&lt;li&gt;automl chooses the best model for your time series data&lt;/li&gt;
&lt;li&gt;arima, deepar, ets, npts, prophet&lt;/li&gt;
&lt;li&gt;works with any time series&lt;/li&gt;
&lt;li&gt;inventory planning, financial planning, resource planning, based on dataset groups, predictors and forecasts&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-forecast-algorithms&#34;&gt;Amazon Forecast Algorithms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;cnnqr: convolutional neural network quantile regression, best for large datasets with hundreds of time series, accepts related historical time series data and metadata&lt;/li&gt;
&lt;li&gt;deepar+ : recurrent neural network, best for large datasets, accepts related forward-looking time series and metadata&lt;/li&gt;
&lt;li&gt;prophet: additive model with non linear trends and seasonality&lt;/li&gt;
&lt;li&gt;npts: non parametric time series: good for sparse data&lt;/li&gt;
&lt;li&gt;arima: simple datasets&lt;/li&gt;
&lt;li&gt;ets: exponential smoothing&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-lex&#34;&gt;Amazon Lex&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;chatbot engine&lt;/li&gt;
&lt;li&gt;lambda to fulfill intent from text&lt;/li&gt;
&lt;li&gt;can deploy to aws mobile sdk, facebook messenger, slack, twilio&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-personalize&#34;&gt;Amazon Personalize&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;fully managed recommendation engine&lt;/li&gt;
&lt;li&gt;api access: feed in data, provide schema in avro, javascript or sdk, get recommendations, get personalized ranking&lt;/li&gt;
&lt;li&gt;real time or batch recommendations&lt;/li&gt;
&lt;li&gt;recommendations for new users and new items&lt;/li&gt;
&lt;li&gt;contextual recommendations&lt;/li&gt;
&lt;li&gt;similar items&lt;/li&gt;
&lt;li&gt;datasets, recipes, solutions, compaignhs&lt;/li&gt;
&lt;li&gt;hidden_dimensions, bptt, recency_mask, min/max_user_history_length_percentile, exploration_weight, exploration_item_age_cut_off&lt;/li&gt;
&lt;li&gt;necessary to maintain recency&lt;/li&gt;
&lt;li&gt;bucket policy&lt;/li&gt;
&lt;li&gt;data ingestion: per gb, training per training hour, inference per tps-hour, batch recommendations: per user or per item&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lightning-round-textract-deeplens-deepracher-lookout-and-monitron&#34;&gt;Lightning round! TexTract, DeepLens, DeepRacher, Lookout, and Monitron&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;TexTract: ocr with forms, fields, tables support&lt;/li&gt;
&lt;li&gt;DeepLens: deep learning enabled video camera, integrated with rekognition, sagemaker, polly, tensorflow, mxnet, caffe&lt;/li&gt;
&lt;li&gt;DeepRacer: reinforcement learning powered 1/18 scale race car&lt;/li&gt;
&lt;li&gt;Lookout: equipment, metrics and vision: detect defects in silicon wafers, circuit boards etc.&lt;/li&gt;
&lt;li&gt;Monitron: end to end system for monitoring equipment and predictive maintenance&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;torchserve-aws-neuron-and-aws-panorama&#34;&gt;TorchServe, AWS Neuron, and AWS Panorama&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;TorchServe: model serving framework for pytorch&lt;/li&gt;
&lt;li&gt;AWS Neuron: ml inferentia chip, Ec2 inf1 instance type&lt;/li&gt;
&lt;li&gt;Panorama: computer vision at the edge&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;deep-composer-fraud-detection-codeguru-and-contact-lens&#34;&gt;Deep Composer, Fraud Detection, CodeGuru, and Contact Lens&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;DeepComposer: ai powered keyboard&lt;/li&gt;
&lt;li&gt;fraud detection: upload your own data&lt;/li&gt;
&lt;li&gt;Codeguru: automated code reviews, finds lines of code that hurt performance&lt;/li&gt;
&lt;li&gt;contact lens: for customer support call centers, ingests audio, sentiment analysis&lt;/li&gt;
&lt;li&gt;finds utterances that correlate with successful calls&lt;/li&gt;
&lt;li&gt;categorize calls automatically&lt;/li&gt;
&lt;li&gt;measure talk speed and interruptions&lt;/li&gt;
&lt;li&gt;theme detection: discovers emerging issues&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-kendra-and-amazon-augmented-ai-a2i&#34;&gt;Amazon Kendra and Amazon Augmented AI (A2I)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Enterprise search with natural languate&lt;/li&gt;
&lt;li&gt;combines data from sharepoint, intranet, sharing services, jdbc, s4 into one searchable repo&lt;/li&gt;
&lt;li&gt;ml powered, uses thumbs up/down&lt;/li&gt;
&lt;li&gt;relevance tuning, boost strength of document freshness&lt;/li&gt;
&lt;li&gt;Kendra: Alexa&amp;rsquo;s sister&lt;/li&gt;
&lt;li&gt;AugmentedAI: human review of ml predictions, mechanical turk workforce or vendors&lt;/li&gt;
&lt;li&gt;integrated into textract and rekognition&lt;/li&gt;
&lt;li&gt;integrates with sagemaker&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>AWS Certified ML - Specialty exam (MLS-C01) - 4. Machine Learning Implementation and Operations</title>
      <link>https://ayushsubedi.github.io/posts/aws_ml_speciality_ml/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/aws_ml_speciality_ml/</guid>
      <description>&lt;h1 id=&#34;machine-learning-implementation-and-operations&#34;&gt;Machine Learning Implementation and Operations&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#section-intro-machine-learning-implementation-and-operations&#34;&gt;Section Intro: Machine Learning Implementation and Operations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemakers-inner-details-and-production-variants&#34;&gt;SageMaker&amp;rsquo;s Inner Details and Production Variants&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-on-the-edge-sagemaker-neo-and-iot-greengrass&#34;&gt;SageMaker On the Edge: SageMaker Neo and IoT Greengrass&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-security-encryption-at-rest-and-in-transit&#34;&gt;SageMaker Security: Encryption at Rest and In Transit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-security-vpcs-iam-logging-and-monitoring&#34;&gt;SageMaker Security: VPC&amp;rsquo;s, IAM, Logging, and Monitoring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-resource-management-instance-types-and-spot-training&#34;&gt;SageMaker Resource Management: Instance Types and Spot Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-resource-management-elastic-inference-automatic-scaling-azs&#34;&gt;SageMaker Resource Management: Elastic Inference, Automatic Scaling, AZ&amp;rsquo;s&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-serverless-inference-and-inference-recommender&#34;&gt;SageMaker Serverless Inference and Inference Recommender&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-inference-pipelines&#34;&gt;SageMaker Inference Pipelines&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This section covers building machine learning solutions for performance, availability, scalability, resiliency, and fault tolerance, recommending and implementing the appropriate machine learning services and features for a given problem, applying basic AWS security practices to machine learning solutions and deploying and operationalizing machine learning solutions.&lt;/p&gt;
&lt;h2 id=&#34;section-intro-machine-learning-implementation-and-operations&#34;&gt;Section Intro: Machine Learning Implementation and Operations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;scaling, productionalization and security&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemakers-inner-details-and-production-variants&#34;&gt;SageMaker&amp;rsquo;s Inner Details and Production Variants&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;all models in agemaker are hosted in docker containers&lt;/li&gt;
&lt;li&gt;the docker container is registered with ECR&lt;/li&gt;
&lt;li&gt;pre built deep learning, scikit learn and spark ml&lt;/li&gt;
&lt;li&gt;pre built tensorflow, mxnet, chainer, pytorch etc. horovod or parameter server is a way to distribute tensorflow training&lt;/li&gt;
&lt;li&gt;you can also use any script or algorithm within sagemaker&lt;/li&gt;
&lt;li&gt;the containers are isolated and contain all dependencies&lt;/li&gt;
&lt;li&gt;Dockerfile structure&lt;/li&gt;
&lt;li&gt;using your own image&lt;/li&gt;
&lt;li&gt;you can test muliple models on live traffic using Production Variants (Roll out variant weights)&lt;/li&gt;
&lt;li&gt;A/B test is posible&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-on-the-edge-sagemaker-neo-and-iot-greengrass&#34;&gt;SageMaker On the Edge: SageMaker Neo and IoT Greengrass&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Train once, run anywhere&lt;/li&gt;
&lt;li&gt;supports multiple architecture&lt;/li&gt;
&lt;li&gt;optimizes code for specific devices&lt;/li&gt;
&lt;li&gt;consists of a compiler and a runtime&lt;/li&gt;
&lt;li&gt;Neo compiled models can be deployed to an https endpoint, must be the same instance type used for compilation&lt;/li&gt;
&lt;li&gt;or you can deploy to iot greengrass&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-security-encryption-at-rest-and-in-transit&#34;&gt;SageMaker Security: Encryption at Rest and In Transit&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;IAM&lt;/li&gt;
&lt;li&gt;MFA&lt;/li&gt;
&lt;li&gt;SSL/TLS&lt;/li&gt;
&lt;li&gt;Cloudtrail to log API and user activity&lt;/li&gt;
&lt;li&gt;encryption&lt;/li&gt;
&lt;li&gt;AWS key mangement service is accepted by notebooks and all sagemaker jobs&lt;/li&gt;
&lt;li&gt;s3 can be encrypted as well&lt;/li&gt;
&lt;li&gt;All traffic supports TLS/SSL&lt;/li&gt;
&lt;li&gt;inter node training communication may be optionally encrypted&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-security-vpcs-iam-logging-and-monitoring&#34;&gt;SageMaker Security: VPC&amp;rsquo;s, IAM, Logging, and Monitoring&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;training jobs run in VPC&lt;/li&gt;
&lt;li&gt;private VPC&lt;/li&gt;
&lt;li&gt;s3 vpc endpoints&lt;/li&gt;
&lt;li&gt;IAM user permissions for CreateTrainingJob, CreateModel, CreateEndpointConfig, CreateTransformJob, CreateHyperParameterTuningJob, CreateNotebookInstance, UpdateNotebookInstance&lt;/li&gt;
&lt;li&gt;Predefined policies for AmazonSageMakerReadOnly, AmazonSageMakerFullAccess, AdministratorAccess, DataScientist&lt;/li&gt;
&lt;li&gt;cloudwatch can log, monitor and alarm on invocations and latency of endpoints, health of instance nodes, ground truth (active workers)&lt;/li&gt;
&lt;li&gt;cloudtrail records actions from users, roles, and services within Sagemaker: log files are delivered to s3 for auditing purposes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-resource-management-instance-types-and-spot-training&#34;&gt;SageMaker Resource Management: Instance Types and Spot Training&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;depends but usually gpu for training and cpu for inference&lt;/li&gt;
&lt;li&gt;EC2 spot training: checkpoints to s3 so training can resume&lt;/li&gt;
&lt;li&gt;can increate training time&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-resource-management-elastic-inference-automatic-scaling-azs&#34;&gt;SageMaker Resource Management: Elastic Inference, Automatic Scaling, AZ&amp;rsquo;s&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;accelerates deep learning inference at a fraction of a cost&lt;/li&gt;
&lt;li&gt;EI accelerator may be added alongside a CPU instance&lt;/li&gt;
&lt;li&gt;EI accelerators can also be applied to notebooks&lt;/li&gt;
&lt;li&gt;works with tensorflow, pytorch, mxnet, onnx may be used to export models to mxnet&lt;/li&gt;
&lt;li&gt;works with custom containers built with El-enabled Tensorflow, Pytorch or mxnet&lt;/li&gt;
&lt;li&gt;works with image classification and object detection built in algorithms&lt;/li&gt;
&lt;li&gt;Automatic scaling: can be used to define target metrics, min or max capacity, cooldown periods, works with cloudwatch, dynamically adjusts number of instances for a production variant, load test your configuration before using it.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-serverless-inference-and-inference-recommender&#34;&gt;SageMaker Serverless Inference and Inference Recommender&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;specify your container, memory requirement, concurrency requirements&lt;/li&gt;
&lt;li&gt;underlying capacity is automatically provisioned and scaled&lt;/li&gt;
&lt;li&gt;good for infrequent or unpredictable traffic, will scale down to zero when there are no requests&lt;/li&gt;
&lt;li&gt;chared based on usage&lt;/li&gt;
&lt;li&gt;monitor via cloudwatch&lt;/li&gt;
&lt;li&gt;Inference Recommender  recommends what instance type and configuration for your model&lt;/li&gt;
&lt;li&gt;automates load testing and model tuning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-inference-pipelines&#34;&gt;SageMaker Inference Pipelines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;linear sequence of 2-15 containers&lt;/li&gt;
&lt;li&gt;any combination of pre-trained built-in algorithms or your own algorithms in Docker&lt;/li&gt;
&lt;li&gt;combine pre-processing, predictions, post-processing&lt;/li&gt;
&lt;li&gt;Spark ML and scikit-learn&lt;/li&gt;
&lt;li&gt;chaining multiple inference containers into a pipeline of results&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Term Frequecy Inverse Document Frequency (TFIDF)</title>
      <link>https://ayushsubedi.github.io/posts/tfidf/</link>
      <pubDate>Sun, 25 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/tfidf/</guid>
      <description>&lt;h1 id=&#34;wikipedia-search-using-tfidf&#34;&gt;Wikipedia search using TFIDF&lt;/h1&gt;
&lt;h2 id=&#34;term-frequecy-inverse-document-frequency&#34;&gt;Term Frequecy Inverse Document Frequency&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/tfidf.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;please, call, the, number, below, do, not, us, please call, call the, the number, number below, please do, do not, not call, call us&lt;/p&gt;
&lt;p&gt;dimension = [2, 16]&lt;/p&gt;
&lt;h1 id=&#34;example-of-unigram-tfidf&#34;&gt;Example of unigram TFIDF&lt;/h1&gt;
&lt;h2 id=&#34;imports&#34;&gt;Imports&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; pd
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; os
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pyspark
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyspark.sql &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; SparkSession
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyspark.sql.types &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyspark.sql.functions &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; udf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyspark.ml.feature &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; HashingTF, IDF, Tokenizer
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;sparksession&#34;&gt;SparkSession&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;spark &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SparkSession&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;builder \
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;appName(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tfidf&amp;#39;&lt;/span&gt;)\
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;config(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;spark.jars&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;../jars/snowflake-jdbc-3.13.6.jar, ../jars/spark-snowflake_2.12-2.9.0-spark_3.1.jar&amp;#39;&lt;/span&gt;) \
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;getOrCreate()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;spark&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sparkContext&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;setLogLevel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;WARN&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;22/12/27 13:35:58 WARN Utils: Your hostname, SPMBP136.local resolves to a loopback address: 127.0.0.1; using 192.168.0.101 instead (on interface en6)
22/12/27 13:35:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
22/12/27 13:35:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable


Setting default log level to &amp;quot;WARN&amp;quot;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;file_path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;../datasets/wiki.csv&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wiki &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; spark&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;csv&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;option(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;header&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(file_path)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wiki&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;+---+--------------------+-------------------+--------------------+
| ID|               Title|               Time|            Document|
+---+--------------------+-------------------+--------------------+
| 12|           Anarchism|2008-12-30 06:23:05|&amp;quot;Anarchism (somet...|
| 25|              Autism|2008-12-24 20:41:05|&amp;quot;Autism is a brai...|
| 39|              Albedo|2008-12-29 18:19:09|&amp;quot;The albedo of an...|
|290|                   A|2008-12-27 04:33:16|&amp;quot;The letter A is ...|
|303|             Alabama|2008-12-29 08:15:47|&amp;quot;Alabama (formall...|
|305|            Achilles|2008-12-30 06:18:01|&amp;quot;thumb\n\nIn Gree...|
|307|     Abraham Lincoln|2008-12-28 20:18:23|&amp;quot;Abraham Lincoln ...|
|308|           Aristotle|2008-12-29 23:54:48|&amp;quot;Aristotle (Greek...|
|309|An American in Paris|2008-09-27 19:29:28|&amp;quot;An American in P...|
|324|       Academy Award|2008-12-28 17:50:43|&amp;quot;The Academy Awar...|
|330|             Actrius|2008-05-23 15:24:32|Actrius (Actresse...|
|332|     Animalia (book)|2008-12-18 11:12:34|thumbAnimalia (IS...|
|334|International Ato...|2008-11-21 22:40:20|International Ato...|
|336|            Altruism|2008-12-27 03:57:17|&amp;quot;Altruism is self...|
|339|            Ayn Rand|2008-12-30 08:03:06|&amp;quot;Ayn Rand (,  – M...|
|340|        Alain Connes|2008-09-03 13:41:39|Alain Connes (bor...|
|344|          Allan Dwan|2008-11-14 05:28:58|Allan Dwan (April...|
|358|             Algeria|2008-12-29 02:54:36|&amp;quot;Algeria (, al-Ja...|
|359|List of character...|2008-12-23 20:20:21|&amp;quot;This is a list o...|
|569|        Anthropology|2008-12-28 23:04:30|&amp;quot;Anthropology (, ...|
+---+--------------------+-------------------+--------------------+
only showing top 20 rows
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wiki&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;filter(wiki&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Document&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isNull())&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;1
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wiki &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; wiki&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;filter(&lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt;wiki&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Document&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isNull())
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wiki&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;+---+--------------------+-------------------+--------------------+
| ID|               Title|               Time|            Document|
+---+--------------------+-------------------+--------------------+
| 12|           Anarchism|2008-12-30 06:23:05|&amp;quot;Anarchism (somet...|
| 25|              Autism|2008-12-24 20:41:05|&amp;quot;Autism is a brai...|
| 39|              Albedo|2008-12-29 18:19:09|&amp;quot;The albedo of an...|
|290|                   A|2008-12-27 04:33:16|&amp;quot;The letter A is ...|
|303|             Alabama|2008-12-29 08:15:47|&amp;quot;Alabama (formall...|
|305|            Achilles|2008-12-30 06:18:01|&amp;quot;thumb\n\nIn Gree...|
|307|     Abraham Lincoln|2008-12-28 20:18:23|&amp;quot;Abraham Lincoln ...|
|308|           Aristotle|2008-12-29 23:54:48|&amp;quot;Aristotle (Greek...|
|309|An American in Paris|2008-09-27 19:29:28|&amp;quot;An American in P...|
|324|       Academy Award|2008-12-28 17:50:43|&amp;quot;The Academy Awar...|
|330|             Actrius|2008-05-23 15:24:32|Actrius (Actresse...|
|332|     Animalia (book)|2008-12-18 11:12:34|thumbAnimalia (IS...|
|334|International Ato...|2008-11-21 22:40:20|International Ato...|
|336|            Altruism|2008-12-27 03:57:17|&amp;quot;Altruism is self...|
|339|            Ayn Rand|2008-12-30 08:03:06|&amp;quot;Ayn Rand (,  – M...|
|340|        Alain Connes|2008-09-03 13:41:39|Alain Connes (bor...|
|344|          Allan Dwan|2008-11-14 05:28:58|Allan Dwan (April...|
|358|             Algeria|2008-12-29 02:54:36|&amp;quot;Algeria (, al-Ja...|
|359|List of character...|2008-12-23 20:20:21|&amp;quot;This is a list o...|
|569|        Anthropology|2008-12-28 23:04:30|&amp;quot;Anthropology (, ...|
+---+--------------------+-------------------+--------------------+
only showing top 20 rows
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tokenizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Tokenizer(inputCol&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Document&amp;#34;&lt;/span&gt;, outputCol&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;words&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wordsData &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform(wiki)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wordsData&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;+---+--------------------+-------------------+--------------------+--------------------+
| ID|               Title|               Time|            Document|               words|
+---+--------------------+-------------------+--------------------+--------------------+
| 12|           Anarchism|2008-12-30 06:23:05|&amp;quot;Anarchism (somet...|[&amp;quot;anarchism, (som...|
| 25|              Autism|2008-12-24 20:41:05|&amp;quot;Autism is a brai...|[&amp;quot;autism, is, a, ...|
| 39|              Albedo|2008-12-29 18:19:09|&amp;quot;The albedo of an...|[&amp;quot;the, albedo, of...|
|290|                   A|2008-12-27 04:33:16|&amp;quot;The letter A is ...|[&amp;quot;the, letter, a,...|
|303|             Alabama|2008-12-29 08:15:47|&amp;quot;Alabama (formall...|[&amp;quot;alabama, (forma...|
|305|            Achilles|2008-12-30 06:18:01|&amp;quot;thumb\n\nIn Gree...|[&amp;quot;thumb\n\nin, gr...|
|307|     Abraham Lincoln|2008-12-28 20:18:23|&amp;quot;Abraham Lincoln ...|[&amp;quot;abraham, lincol...|
|308|           Aristotle|2008-12-29 23:54:48|&amp;quot;Aristotle (Greek...|[&amp;quot;aristotle, (gre...|
|309|An American in Paris|2008-09-27 19:29:28|&amp;quot;An American in P...|[&amp;quot;an, american, i...|
|324|       Academy Award|2008-12-28 17:50:43|&amp;quot;The Academy Awar...|[&amp;quot;the, academy, a...|
|330|             Actrius|2008-05-23 15:24:32|Actrius (Actresse...|[actrius, (actres...|
|332|     Animalia (book)|2008-12-18 11:12:34|thumbAnimalia (IS...|[thumbanimalia, (...|
|334|International Ato...|2008-11-21 22:40:20|International Ato...|[international, a...|
|336|            Altruism|2008-12-27 03:57:17|&amp;quot;Altruism is self...|[&amp;quot;altruism, is, s...|
|339|            Ayn Rand|2008-12-30 08:03:06|&amp;quot;Ayn Rand (,  – M...|[&amp;quot;ayn, rand, (,, ...|
|340|        Alain Connes|2008-09-03 13:41:39|Alain Connes (bor...|[alain, connes, (...|
|344|          Allan Dwan|2008-11-14 05:28:58|Allan Dwan (April...|[allan, dwan, (ap...|
|358|             Algeria|2008-12-29 02:54:36|&amp;quot;Algeria (, al-Ja...|[&amp;quot;algeria, (,, al...|
|359|List of character...|2008-12-23 20:20:21|&amp;quot;This is a list o...|[&amp;quot;this, is, a, li...|
|569|        Anthropology|2008-12-28 23:04:30|&amp;quot;Anthropology (, ...|[&amp;quot;anthropology, (...|
+---+--------------------+-------------------+--------------------+--------------------+
only showing top 20 rows
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hashingTF &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; HashingTF(inputCol&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;words&amp;#34;&lt;/span&gt;, outputCol&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rawFeatures&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;featuredData &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; hashingTF&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform(wordsData)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;featuredData&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;+---+--------------------+-------------------+--------------------+--------------------+--------------------+
| ID|               Title|               Time|            Document|               words|         rawFeatures|
+---+--------------------+-------------------+--------------------+--------------------+--------------------+
| 12|           Anarchism|2008-12-30 06:23:05|&amp;quot;Anarchism (somet...|[&amp;quot;anarchism, (som...|(262144,[15157,27...|
| 25|              Autism|2008-12-24 20:41:05|&amp;quot;Autism is a brai...|[&amp;quot;autism, is, a, ...|(262144,[15,1546,...|
| 39|              Albedo|2008-12-29 18:19:09|&amp;quot;The albedo of an...|[&amp;quot;the, albedo, of...|(262144,[7853,240...|
|290|                   A|2008-12-27 04:33:16|&amp;quot;The letter A is ...|[&amp;quot;the, letter, a,...|(262144,[6037,942...|
|303|             Alabama|2008-12-29 08:15:47|&amp;quot;Alabama (formall...|[&amp;quot;alabama, (forma...|(262144,[1797,256...|
|305|            Achilles|2008-12-30 06:18:01|&amp;quot;thumb\n\nIn Gree...|[&amp;quot;thumb\n\nin, gr...|(262144,[10758,16...|
|307|     Abraham Lincoln|2008-12-28 20:18:23|&amp;quot;Abraham Lincoln ...|[&amp;quot;abraham, lincol...|(262144,[2564,460...|
|308|           Aristotle|2008-12-29 23:54:48|&amp;quot;Aristotle (Greek...|[&amp;quot;aristotle, (gre...|(262144,[2767,356...|
|309|An American in Paris|2008-09-27 19:29:28|&amp;quot;An American in P...|[&amp;quot;an, american, i...|(262144,[2366,670...|
|324|       Academy Award|2008-12-28 17:50:43|&amp;quot;The Academy Awar...|[&amp;quot;the, academy, a...|(262144,[2931,328...|
|330|             Actrius|2008-05-23 15:24:32|Actrius (Actresse...|[actrius, (actres...|(262144,[6558,674...|
|332|     Animalia (book)|2008-12-18 11:12:34|thumbAnimalia (IS...|[thumbanimalia, (...|(262144,[2284,609...|
|334|International Ato...|2008-11-21 22:40:20|International Ato...|[international, a...|(262144,[847,925,...|
|336|            Altruism|2008-12-27 03:57:17|&amp;quot;Altruism is self...|[&amp;quot;altruism, is, s...|(262144,[5675,680...|
|339|            Ayn Rand|2008-12-30 08:03:06|&amp;quot;Ayn Rand (,  – M...|[&amp;quot;ayn, rand, (,, ...|(262144,[528,1091...|
|340|        Alain Connes|2008-09-03 13:41:39|Alain Connes (bor...|[alain, connes, (...|(262144,[154,1595...|
|344|          Allan Dwan|2008-11-14 05:28:58|Allan Dwan (April...|[allan, dwan, (ap...|(262144,[1578,181...|
|358|             Algeria|2008-12-29 02:54:36|&amp;quot;Algeria (, al-Ja...|[&amp;quot;algeria, (,, al...|(262144,[3852,492...|
|359|List of character...|2008-12-23 20:20:21|&amp;quot;This is a list o...|[&amp;quot;this, is, a, li...|(262144,[14376,19...|
|569|        Anthropology|2008-12-28 23:04:30|&amp;quot;Anthropology (, ...|[&amp;quot;anthropology, (...|(262144,[57138,10...|
+---+--------------------+-------------------+--------------------+--------------------+--------------------+
only showing top 20 rows
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;idf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; IDF(inputCol&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rawFeatures&amp;#34;&lt;/span&gt;, outputCol&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;features&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;idfModel &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; idf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(featuredData)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rescaledData &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; idfModel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform(featuredData)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rescaledData&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;22/12/27 13:36:11 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB
+---+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+
| ID|               Title|               Time|            Document|               words|         rawFeatures|            features|
+---+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+
| 12|           Anarchism|2008-12-30 06:23:05|&amp;quot;Anarchism (somet...|[&amp;quot;anarchism, (som...|(262144,[15157,27...|(262144,[15157,27...|
| 25|              Autism|2008-12-24 20:41:05|&amp;quot;Autism is a brai...|[&amp;quot;autism, is, a, ...|(262144,[15,1546,...|(262144,[15,1546,...|
| 39|              Albedo|2008-12-29 18:19:09|&amp;quot;The albedo of an...|[&amp;quot;the, albedo, of...|(262144,[7853,240...|(262144,[7853,240...|
|290|                   A|2008-12-27 04:33:16|&amp;quot;The letter A is ...|[&amp;quot;the, letter, a,...|(262144,[6037,942...|(262144,[6037,942...|
|303|             Alabama|2008-12-29 08:15:47|&amp;quot;Alabama (formall...|[&amp;quot;alabama, (forma...|(262144,[1797,256...|(262144,[1797,256...|
|305|            Achilles|2008-12-30 06:18:01|&amp;quot;thumb\n\nIn Gree...|[&amp;quot;thumb\n\nin, gr...|(262144,[10758,16...|(262144,[10758,16...|
|307|     Abraham Lincoln|2008-12-28 20:18:23|&amp;quot;Abraham Lincoln ...|[&amp;quot;abraham, lincol...|(262144,[2564,460...|(262144,[2564,460...|
|308|           Aristotle|2008-12-29 23:54:48|&amp;quot;Aristotle (Greek...|[&amp;quot;aristotle, (gre...|(262144,[2767,356...|(262144,[2767,356...|
|309|An American in Paris|2008-09-27 19:29:28|&amp;quot;An American in P...|[&amp;quot;an, american, i...|(262144,[2366,670...|(262144,[2366,670...|
|324|       Academy Award|2008-12-28 17:50:43|&amp;quot;The Academy Awar...|[&amp;quot;the, academy, a...|(262144,[2931,328...|(262144,[2931,328...|
|330|             Actrius|2008-05-23 15:24:32|Actrius (Actresse...|[actrius, (actres...|(262144,[6558,674...|(262144,[6558,674...|
|332|     Animalia (book)|2008-12-18 11:12:34|thumbAnimalia (IS...|[thumbanimalia, (...|(262144,[2284,609...|(262144,[2284,609...|
|334|International Ato...|2008-11-21 22:40:20|International Ato...|[international, a...|(262144,[847,925,...|(262144,[847,925,...|
|336|            Altruism|2008-12-27 03:57:17|&amp;quot;Altruism is self...|[&amp;quot;altruism, is, s...|(262144,[5675,680...|(262144,[5675,680...|
|339|            Ayn Rand|2008-12-30 08:03:06|&amp;quot;Ayn Rand (,  – M...|[&amp;quot;ayn, rand, (,, ...|(262144,[528,1091...|(262144,[528,1091...|
|340|        Alain Connes|2008-09-03 13:41:39|Alain Connes (bor...|[alain, connes, (...|(262144,[154,1595...|(262144,[154,1595...|
|344|          Allan Dwan|2008-11-14 05:28:58|Allan Dwan (April...|[allan, dwan, (ap...|(262144,[1578,181...|(262144,[1578,181...|
|358|             Algeria|2008-12-29 02:54:36|&amp;quot;Algeria (, al-Ja...|[&amp;quot;algeria, (,, al...|(262144,[3852,492...|(262144,[3852,492...|
|359|List of character...|2008-12-23 20:20:21|&amp;quot;This is a list o...|[&amp;quot;this, is, a, li...|(262144,[14376,19...|(262144,[14376,19...|
|569|        Anthropology|2008-12-28 23:04:30|&amp;quot;Anthropology (, ...|[&amp;quot;anthropology, (...|(262144,[57138,10...|(262144,[57138,10...|
+---+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+
only showing top 20 rows
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;search&#34;&gt;Search&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;search_article&lt;/span&gt;(keyword):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# get the hash val from keyword&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    schema &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; StructType([StructField(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;words&amp;#34;&lt;/span&gt;, ArrayType(StringType()))])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    temp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; spark&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;createDataFrame(([[[keyword]]]), schema)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;toDF(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;words&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    temp_unhashed &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; hashingTF&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform(temp)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;select(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rawFeatures&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;collect()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    val &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int(temp_unhashed[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rawFeatures&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;indices[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;#&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    termExtractor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; udf(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x:float(x[val]), FloatType())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    final &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rescaledData&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;withColumn(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;score&amp;#39;&lt;/span&gt;, termExtractor(rescaledData&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;features))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    final &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; final&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;filter(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;score&amp;gt;0&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;orderBy(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;score&amp;#34;&lt;/span&gt;, ascending&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; final&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;select(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ID&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Title&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;score&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;search_article(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mystery&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;22/12/27 13:36:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB


[Stage 11:===================&amp;gt;                                      (1 + 2) / 3]

+----+--------------------+--------+
|  ID|               Title|   score|
+----+--------------------+--------+
| 984|     Agatha Christie|5.521461|
| 986|          The Plague|5.521461|
|1307|The Alan Parsons ...|5.521461|
+----+--------------------+--------+
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;search_article(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;comic&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;22/12/27 13:36:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB
+----+--------------------+----------+
|  ID|               Title|     score|
+----+--------------------+----------+
| 931|The Amazing Spide...|14.4849415|
|2101|             Asterix|  9.656628|
|1549|             Agathon|  9.656628|
|2023|           Aeschylus|  9.656628|
|1028|        Aristophanes|  9.656628|
|1614|              Alexis|  4.828314|
|1784|  Athenian democracy|  4.828314|
+----+--------------------+----------+
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;search_article(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;revolution&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;22/12/27 13:36:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB
+----+--------------------+---------+
|  ID|               Title|    score|
+----+--------------------+---------+
|1973| American Revolution|12.052151|
|2273|            AFC Ajax|4.0173836|
| 339|            Ayn Rand|4.0173836|
| 572|Agricultural science|4.0173836|
| 771|American Revoluti...|4.0173836|
| 915|       Andrey Markov|4.0173836|
| 930|       Alvin Toffler|4.0173836|
|1030|     Austrian School|4.0173836|
|1057|      Anatole France|4.0173836|
|1192| Artistic revolution|4.0173836|
|1316|      Annales School|4.0173836|
|1676|Alfonso XII of Spain|4.0173836|
|1363|  André-Marie Ampère|4.0173836|
|2075|  Aircraft hijacking|4.0173836|
|1784|  Athenian democracy|4.0173836|
|1844|          Archimedes|4.0173836|
|2070|Act of Settlement...|4.0173836|
+----+--------------------+---------+
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;search_article(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;football&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;22/12/27 13:36:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB
+----+--------------------+---------+
|  ID|               Title|    score|
+----+--------------------+---------+
|2273|            AFC Ajax|54.596165|
|2357|American Football...|46.196754|
|2174|        Arsenal F.C.|29.397936|
|2358|           A.S. Roma| 25.19823|
|2102|   Arizona Cardinals|20.998526|
|2103|     Atlanta Falcons| 16.79882|
| 615|American Football...| 16.79882|
| 925|Alumni Athletic Club|12.599115|
|2289|  AZ (football club)| 4.199705|
|2310|       Arthur Miller| 4.199705|
|1797|                Acre| 4.199705|
|2363|Alessandro Scarlatti| 4.199705|
|2382|               Aalen| 4.199705|
|1016|       Achill Island| 4.199705|
+----+--------------------+---------+
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;search_article(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;emirates&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;22/12/27 13:36:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB
+----+------------+--------+
|  ID|       Title|   score|
+----+------------+--------+
|2174|Arsenal F.C.|6.214608|
+----+------------+--------+
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;search_article(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;the&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;22/12/27 13:36:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB
+----+--------------------+---------+
|  ID|               Title|    score|
+----+--------------------+---------+
|1854| Geography of Africa|56.093544|
|2273|            AFC Ajax|43.326492|
|2023|           Aeschylus|41.968296|
|1216|              Athens|30.287798|
| 717|             Alberta|26.213207|
|2358|           A.S. Roma|23.904272|
| 841|      Attila the Hun|23.360992|
|1285|Geography of Alabama|23.089354|
|2338|Rise and Fall of ...|21.323696|
|1440|       Abydos, Egypt|19.150581|
| 904|           Aluminium| 18.87894|
|1905|              Ambush|18.199842|
|1962|  Apparent magnitude|17.928204|
|1557|Agrippina the You...|17.792383|
|1613|  Alexios I Komnenos|17.792383|
|1234|     Acoustic theory|17.520744|
|2064|      Antonio Canova|15.619268|
|1686| Alfonso V of Aragon| 15.07599|
|1451|APL (programming ...| 15.07599|
|2274|Arthur Stanley Ed...| 14.80435|
+----+--------------------+---------+
only showing top 20 rows
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning Glossary</title>
      <link>https://ayushsubedi.github.io/ml_glossary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/ml_glossary/</guid>
      <description>&lt;h1 id=&#34;machine-learning-glossary&#34;&gt;Machine Learning Glossary&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#basic-machine-learning&#34;&gt;Basic Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#confusion-matrix&#34;&gt;Confusion Matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data&#34;&gt;Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#design-of-experiments&#34;&gt;Design of Experiments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#game-theory&#34;&gt;Game Theory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-quality&#34;&gt;Model Quality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#non-parametric-tests&#34;&gt;Non-Parametric Tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimization&#34;&gt;Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#probability-based-models&#34;&gt;Probability based models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regression&#34;&gt;Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variable-selection&#34;&gt;Variable Selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#misc&#34;&gt;Misc&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;basic-machine-learning&#34;&gt;Basic Machine Learning&lt;/h2&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of machine learning, an algorithm is a set of instructions that a computer follows in order to learn from data.&lt;/li&gt;
&lt;li&gt;Machine learning algorithms take input data and use statistical analysis to predict an output value within an acceptable range.&lt;/li&gt;
&lt;li&gt;The goal of a machine learning algorithm is to improve its prediction accuracy over time by adjusting the parameters of the model based on the input data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;change-detection&#34;&gt;Change detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Change detection is a process in which a system is able to identify changes in a given environment over time.&lt;/li&gt;
&lt;li&gt;In the context of machine learning, change detection involves using algorithms to analyze data from a given environment in order to identify any changes that have occurred.&lt;/li&gt;
&lt;li&gt;This can be useful in a variety of different applications, including monitoring changes in financial markets, detecting changes in customer behavior, or identifying changes in the physical environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;classification&#34;&gt;Classification&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Classification is a supervised learning problem in which the model is trained to predict a discrete label or class for a given input data.&lt;/li&gt;
&lt;li&gt;The goal is to predict the class or category that a new instance belongs to, based on the training data.&lt;/li&gt;
&lt;li&gt;For example, a classifier could be trained to predict whether an email is spam or not spam, based on the contents of the email. The input data would be the contents of the email, and the output class would be either &amp;ldquo;spam&amp;rdquo; or &amp;ldquo;not spam&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;There are many different algorithms that can be used for classification, including logistic regression, support vector machines (SVMs), and decision trees. The choice of algorithm depends on the characteristics of the data and the desired complexity of the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;classifier&#34;&gt;Classifier&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A classifier is a machine learning model that is trained to predict a discrete class or category for a given input data. Classifiers are used in a variety of applications, including spam filtering, image classification, and natural language processing.&lt;/li&gt;
&lt;li&gt;There are many different types of classifiers, including logistic regression, support vector machines (SVMs), and decision trees. The choice of classifier depends on the characteristics of the data and the desired complexity of the model.&lt;/li&gt;
&lt;li&gt;To train a classifier, the model is presented with a labeled dataset that includes input data and the corresponding correct class or category. The model then &amp;ldquo;learns&amp;rdquo; to predict the correct class by finding patterns in the training data. Once trained, the classifier can then be used to predict the class for new, unseen data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cluster&#34;&gt;Cluster&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of machine learning, a cluster refers to a group of data points that are similar to one another. Clustering is an unsupervised learning problem in which the goal is to divide the data into distinct groups, or clusters, such that the data points within each cluster are more similar to one another than they are to data points in other clusters.&lt;/li&gt;
&lt;li&gt;There are many different algorithms that can be used for clustering, including k-means clustering and hierarchical clustering. The choice of algorithm depends on the characteristics of the data and the desired properties of the clusters.&lt;/li&gt;
&lt;li&gt;Clustering can be used for a variety of purposes, including data compression, anomaly detection, and generating hypotheses for further testing. It is a useful tool for exploring and understanding the structure of a dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cluster-center&#34;&gt;Cluster center&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of clustering, a cluster center is a representative data point for a cluster. It is typically the mean or median of the points in the cluster, depending on the specific clustering algorithm being used.&lt;/li&gt;
&lt;li&gt;In k-means clustering, for example, the cluster center is the mean of all the data points in the cluster. The k-means algorithm works by iteratively assigning each data point to the cluster with the closest cluster center and then updating the cluster center to be the mean of the points in the cluster.&lt;/li&gt;
&lt;li&gt;In hierarchical clustering, the cluster center can be thought of as the point at the center of the cluster, which is determined by the specific linkage criterion being used.&lt;/li&gt;
&lt;li&gt;The cluster center is used to represent the &amp;ldquo;typical&amp;rdquo; data point in a cluster, and can be useful for understanding the characteristics of the cluster and for visualization purposes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;clustering&#34;&gt;Clustering&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Clustering is an unsupervised learning problem in which the goal is to divide a dataset into distinct groups, or clusters, such that the data points within each cluster are more similar to one another than they are to data points in other clusters. Clustering is a useful tool for exploring and understanding the structure of a dataset, and can be used for a variety of purposes, including data compression, anomaly detection, and generating hypotheses for further testing.&lt;/li&gt;
&lt;li&gt;There are many different algorithms that can be used for clustering, including k-means clustering, hierarchical clustering, and density-based clustering. The choice of algorithm depends on the characteristics of the data and the desired properties of the clusters.&lt;/li&gt;
&lt;li&gt;In k-means clustering, for example, the goal is to partition the data into a specified number (k) of clusters by iteratively assigning each data point to the cluster with the closest cluster center and then updating the cluster center to be the mean of the points in the cluster. Hierarchical clustering, on the other hand, involves creating a hierarchy of clusters, where at each step, the two closest clusters are merged together. Density-based clustering algorithms, such as DBSCAN, identify clusters as areas of higher density surrounded by areas of lower density.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cusum&#34;&gt;CUSUM&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CUSUM is an acronym for &amp;ldquo;Cumulative Sum.&amp;rdquo; It is a statistical algorithm that is used to detect small shifts in the mean of a process over time. It is often used in quality control and reliability engineering to monitor processes and detect changes that may indicate a problem or deviation from the norm.&lt;/li&gt;
&lt;li&gt;The CUSUM algorithm works by keeping track of a running total of the difference between the observed values and the expected or target value. When the running total exceeds a pre-determined threshold, it indicates that the process has shifted and may need to be corrected or investigated.&lt;/li&gt;
&lt;li&gt;CUSUM charts are often used to visualize the performance of the CUSUM algorithm, with the running total being plotted on the y-axis and the time steps on the x-axis. The chart can then be used to identify when the running total exceeds the threshold and to identify any trends or patterns in the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deep-learning&#34;&gt;Deep learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Deep learning is a subfield of machine learning that is inspired by the structure and function of the brain, specifically the neural networks that make up the brain. It involves the use of artificial neural networks, which are computational models inspired by the structure and function of the brain, to learn from data and make decisions.&lt;/li&gt;
&lt;li&gt;Deep learning algorithms learn by example, just like humans do. They learn by being presented with a large amount of labeled data and adjusting the internal parameters of the network to optimize performance on a specific task. The &amp;ldquo;deep&amp;rdquo; in deep learning refers to the fact that these algorithms typically have multiple layers of artificial neurons, with each layer learning to extract higher-level features of the data.&lt;/li&gt;
&lt;li&gt;Deep learning has been successful in a wide range of applications, including image and speech recognition, natural language processing, and autonomous driving. It has revolutionized the field of machine learning and has enabled the development of many practical applications that were previously thought to be impossible.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dimension&#34;&gt;Dimension&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of machine learning, a dimension refers to a particular feature or attribute of a dataset. For example, if you are working with a dataset that includes information about houses (such as price, number of bedrooms, square footage, and location), each of these features would be considered a separate dimension.&lt;/li&gt;
&lt;li&gt;The number of dimensions in a dataset is often referred to as the &amp;ldquo;dimensionality&amp;rdquo; of the dataset. High-dimensional datasets, which have a large number of dimensions, can be difficult to work with and visualize, as it can be challenging to represent the relationships between all of the dimensions in a meaningful way.&lt;/li&gt;
&lt;li&gt;In machine learning, techniques such as dimensionality reduction can be used to reduce the number of dimensions in a dataset, while still preserving the important information. This can be useful for tasks such as visualization and training machine learning models, which may be more efficient and effective on lower-dimensional data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;expectation-maximization-algorithm-em-algorithm&#34;&gt;Expectation-maximization algorithm (EM algorithm)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The EM algorithm (Expectation-Maximization algorithm) is a widely used method for estimating the parameters of a statistical model when there is missing or incomplete data. It is an iterative algorithm that alternates between two steps: the expectation (E) step and the maximization (M) step.&lt;/li&gt;
&lt;li&gt;In the E step, the algorithm estimates the expected value of the complete data likelihood function (a measure of the probability of the data given the model parameters) based on the current parameter values. In the M step, the algorithm updates the parameter values to maximize the expected complete data likelihood. The process is then repeated until convergence, at which point the parameter estimates are considered to be optimal.&lt;/li&gt;
&lt;li&gt;The EM algorithm is widely used in a variety of applications, including machine learning, natural language processing, and bioinformatics. It is particularly useful when the data are incomplete or when the model is a mixture model (i.e., a model that consists of a mixture of different underlying distributions).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;heuristic&#34;&gt;Heuristic&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In machine learning, a heuristic is a simplified, approximate solution to a problem that is used to quickly find a satisfactory answer. It is often used in situations where finding the optimal solution is computationally infeasible or impractical.&lt;/li&gt;
&lt;li&gt;Heuristics are often used in machine learning as a way to quickly search through a large space of possible solutions and find a good, but not necessarily optimal, solution. They can be useful for tasks such as optimization, feature selection, and model selection.&lt;/li&gt;
&lt;li&gt;Heuristics are often designed to be domain-specific and are based on the specific characteristics of the problem at hand. They can be useful for providing a rough estimate or approximation of the solution, but they may not always be reliable or accurate. In general, heuristics should be used with caution and should be validated against more rigorous methods where possible.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;𝑘-means-algorithm&#34;&gt;𝑘-means algorithm&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The k-means algorithm is a method for clustering data into a specified number (k) of distinct clusters. It is an iterative algorithm that works by first randomly initializing k cluster centers, and then iteratively assigning each data point to the cluster with the closest cluster center and updating the cluster center to be the mean of the points in the cluster.&lt;/li&gt;
&lt;li&gt;The k-means algorithm has the following steps:
&lt;ul&gt;
&lt;li&gt;Initialize k cluster centers randomly.&lt;/li&gt;
&lt;li&gt;Assign each data point to the cluster with the closest cluster center.&lt;/li&gt;
&lt;li&gt;Update the cluster centers to be the mean of the points in the cluster.&lt;/li&gt;
&lt;li&gt;Repeat steps 2 and 3 until the cluster assignments stop changing or a maximum number of iterations is reached.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The k-means algorithm is sensitive to the initial cluster assignments, so it is common to run the algorithm multiple times with different random initializations to ensure that the final clusters are stable. The algorithm is also sensitive to outliers and may produce suboptimal clusters if the data contain outliers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;𝑘-nearest-neighbor-knn&#34;&gt;𝑘-Nearest-Neighbor (KNN)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The k-nearest neighbor (KNN) algorithm is a method for classifying objects based on the closest training examples in the feature space. It is a non-parametric method, which means that it does not make any assumptions about the underlying distribution of the data.&lt;/li&gt;
&lt;li&gt;The KNN algorithm works by calculating the distance between the new data point and all the training data, and then selecting the k training points that are closest to the new data point. The class label of the new data point is then determined by majority vote among the k nearest neighbors.&lt;/li&gt;
&lt;li&gt;The value of k is a hyperparameter of the KNN algorithm and must be chosen by the practitioner. A larger value of k will make the model more robust to noise, but a smaller value may be more sensitive to the underlying structure of the data.&lt;/li&gt;
&lt;li&gt;KNN is a simple and effective method for classification, but it can be computationally expensive for large datasets, as it requires calculating the distance between the new data point and all the training examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kernel&#34;&gt;Kernel&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of machine learning, a kernel is a function that takes in two inputs and returns a scalar value. Kernels are used in a variety of machine learning algorithms, including support vector machines (SVMs) and kernel principal component analysis (PCA).&lt;/li&gt;
&lt;li&gt;In SVMs, kernels are used to define a similarity measure between two data points. The kernel function is applied to the data points to transform them into a higher-dimensional space, where it is then possible to find a linear separation between the classes. By using a kernel function, it is possible to learn a non-linear decision boundary in the original feature space using a linear classifier in the transformed space.&lt;/li&gt;
&lt;li&gt;In kernel PCA, kernels are used to define a similarity measure between data points in the original space, and the resulting kernel matrix is used to perform PCA in the feature space. This allows for non-linear dimensionality reduction, which can be useful for data that is not linearly separable.&lt;/li&gt;
&lt;li&gt;There are many different kernel functions that can be used, including linear kernels, polynomial kernels, and radial basis function (RBF) kernels. The choice of kernel depends on the characteristics of the data and the desired properties of the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;margin&#34;&gt;Margin&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of machine learning, the margin is the distance between the decision boundary (i.e., the line or hyperplane that separates the classes) and the nearest training data points. The margin is an important concept in certain types of algorithms, such as support vector machines (SVMs), where the goal is to find the decision boundary that has the largest margin.&lt;/li&gt;
&lt;li&gt;In SVMs, the margin is the distance between the decision boundary and the closest data points from each class. The margin is maximized when the decision boundary is as far as possible from the closest data points from each class, which leads to a model that is more robust and generalizable to new data.&lt;/li&gt;
&lt;li&gt;The margin can also be thought of as a measure of the confidence of the classifier. A larger margin indicates that the classifier is more confident in its predictions, as it is based on a wider separation between the classes.&lt;/li&gt;
&lt;li&gt;The margin is an important consideration when training a machine learning model, as a model with a large margin is often preferred to a model with a small margin, as it is likely to be more robust and generalizable to new data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;machine-learning&#34;&gt;Machine learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Machine learning is a field of artificial intelligence that involves the use of computational models to learn from data and make predictions or decisions without being explicitly programmed. It involves the development of algorithms that can automatically improve their performance through experience.&lt;/li&gt;
&lt;li&gt;There are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning.&lt;/li&gt;
&lt;li&gt;In supervised learning, the goal is to learn a function that maps input data to output labels, based on a labeled training dataset. The model is trained on the training data and then evaluated on a separate test dataset to evaluate its performance. Examples of supervised learning tasks include classification and regression.&lt;/li&gt;
&lt;li&gt;In unsupervised learning, the goal is to discover patterns or relationships in the data without any prior knowledge or labeled training data. Examples of unsupervised learning tasks include clustering and dimensionality reduction.&lt;/li&gt;
&lt;li&gt;In reinforcement learning, the goal is to learn a policy that maximizes a reward signal. The model is trained by interacting with its environment and receiving feedback in the form of rewards or punishments. Reinforcement learning is used in a variety of applications, including robotics and control systems.&lt;/li&gt;
&lt;li&gt;Machine learning has been successful in a wide range of applications, including image and speech recognition, natural language processing, and autonomous driving. It has revolutionized many fields and has enabled the development of practical applications that were previously thought to be impossible.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;neural-network&#34;&gt;Neural network&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A neural network is a type of machine learning model inspired by the structure and function of the brain. It is composed of layers of interconnected &amp;ldquo;neurons,&amp;rdquo; which process and transmit information. Neural networks are able to learn and adapt to new data by adjusting the strengths of the connections between neurons.&lt;/li&gt;
&lt;li&gt;The basic building block of a neural network is the neuron, which is a simple computational unit that receives input, processes it, and produces an output. The input is passed through multiple layers of neurons, with each layer learning to extract higher-level features of the data. The output of the final layer is the prediction or decision made by the neural network.&lt;/li&gt;
&lt;li&gt;There are many different types of neural networks, including feedforward neural networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs). The choice of neural network architecture depends on the characteristics of the data and the desired properties of the model.&lt;/li&gt;
&lt;li&gt;Neural networks have been successful in a wide range of applications, including image and speech recognition, natural language processing, and autonomous driving. They have revolutionized the field of machine learning and have enabled the development of many practical applications that were previously thought to be impossible.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;supervised-learning&#34;&gt;Supervised learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Supervised learning is a type of machine learning in which the model is trained on a labeled dataset, where the correct output is provided for each example in the training set. The goal of supervised learning is to learn a function that can map input data to the correct output labels.&lt;/li&gt;
&lt;li&gt;Supervised learning algorithms can be divided into two main categories: regression and classification.&lt;/li&gt;
&lt;li&gt;In regression, the goal is to predict a continuous value, such as the price of a house or the likelihood of a customer churning. Examples of regression algorithms include linear regression and support vector regression.&lt;/li&gt;
&lt;li&gt;In classification, the goal is to predict a discrete label or class, such as whether an email is spam or not spam. Examples of classification algorithms include logistic regression, k-nearest neighbors, and decision trees.&lt;/li&gt;
&lt;li&gt;Supervised learning is the most widely used type of machine learning and has been successful in a wide range of applications, including image and speech recognition, natural language processing, and fraud detection. It requires a labeled dataset to train the model, which can be expensive and time-consuming to obtain.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;support-vector-machine-svm&#34;&gt;Support vector machine (SVM)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Support vector machine (SVM) is a type of supervised learning algorithm that can be used for classification or regression. It is based on the idea of finding a hyperplane in a high-dimensional space that maximally separates the classes.&lt;/li&gt;
&lt;li&gt;In the case of classification, the goal is to find a hyperplane that separates the data points into different classes as well as possible. The SVM algorithm finds the hyperplane that has the largest margin, or distance, between the closest data points of each class. This maximizes the separation between the classes and leads to a more robust and generalizable model.&lt;/li&gt;
&lt;li&gt;In the case of regression, the goal is to find a hyperplane that predicts the output value for a given input value. The SVM algorithm finds the hyperplane that minimizes the error between the predicted and actual values.&lt;/li&gt;
&lt;li&gt;SVMs are effective in high-dimensional spaces and are widely used in a variety of applications, including image and speech recognition, natural language processing, and bioinformatics. They are also robust to noise and can handle datasets with a large number of features. However, they can be computationally expensive to train and are not well-suited for very large datasets.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;unsupervised-learning&#34;&gt;Unsupervised learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Unsupervised learning is a type of machine learning in which the model is not given any labeled training data and must find patterns or relationships in the data on its own. The goal of unsupervised learning is to discover the underlying structure of the data, without any prior knowledge or assumptions.&lt;/li&gt;
&lt;li&gt;Unsupervised learning algorithms can be divided into two main categories: clustering and dimensionality reduction.&lt;/li&gt;
&lt;li&gt;In clustering, the goal is to group the data points into distinct clusters such that the points within each cluster are more similar to one another than they are to points in other clusters. Examples of clustering algorithms include k-means clustering and hierarchical clustering.&lt;/li&gt;
&lt;li&gt;In dimensionality reduction, the goal is to reduce the number of dimensions (features) in the data while preserving as much of the information as possible. This can be useful for tasks such as visualization and feature selection. Examples of dimensionality reduction algorithms include principal component analysis (PCA) and t-SNE (t-distributed stochastic neighbor embedding).&lt;/li&gt;
&lt;li&gt;Unsupervised learning is useful for exploring and understanding the structure of a dataset, and can be used for tasks such as anomaly detection and data compression. It does not require labeled data and can be used with data that has not been labeled or has incomplete labels. However, it can be more difficult to evaluate the performance of unsupervised learning algorithms, as there is no ground truth to compare the results to.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;voronoi-diagram&#34;&gt;Voronoi diagram&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A Voronoi diagram is a graphical representation of the partitioning of a plane into regions based on the distance to a set of points. It is named after Russian mathematician Georgy Voronoi, who developed the concept in 1908.&lt;/li&gt;
&lt;li&gt;In a Voronoi diagram, the plane is divided into a set of cells, with each cell corresponding to one of the input points. The points are called the &amp;ldquo;generators&amp;rdquo; of the Voronoi diagram. Each cell consists of all points that are closer to its generator than to any other generator. The boundary between cells is called a Voronoi edge, and the points where Voronoi edges intersect are called Voronoi vertices.&lt;/li&gt;
&lt;li&gt;Voronoi diagrams have a wide range of applications, including computer graphics, image processing, and spatial analysis. They are used to model the spatial distribution of points and can be used to optimize the placement of facilities, such as warehouses or cell phone towers, to minimize the distance to the nearest facility. They are also used in computer games to determine the visibility of objects on the screen and in the design of efficient algorithms for solving problems in computational geometry.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;confusion-matrix&#34;&gt;Confusion Matrix&lt;/h2&gt;
&lt;h3 id=&#34;accuracy&#34;&gt;Accuracy&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Accuracy is a measure of how well a model correctly predicts the outcome of a given data sample. It is commonly used in classification problems, where the model is trying to predict a label for a given input.&lt;/li&gt;
&lt;li&gt;The accuracy score is calculated by dividing the number of correct predictions made by the model by the total number of predictions made. This value is then expressed as a percentage. For example, if a model made 100 predictions and 75 of them were correct, the accuracy score would be 75%.&lt;/li&gt;
&lt;li&gt;To calculate the accuracy score, you need a set of predictions made by the model and the corresponding true labels for those predictions. You can then compare the predictions to the true labels to see how many were correct.&lt;/li&gt;
&lt;li&gt;Here is an example of how to calculate the accuracy score in Python:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;def accuracy_score(y_true, y_pred):
    # Calculate the number of correct predictions
    correct = sum(y_true == y_pred)
    # Calculate the total number of predictions
    total = len(y_true)
    # Calculate the accuracy score as a percentage
    return correct / total * 100
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Here, y_true is a list of the true labels and y_pred is a list of the predictions made by the model. The function first calculates the number of correct predictions and then divides that by the total number of predictions to get the accuracy as a decimal. It then multiplies that value by 100 to express the accuracy as a percentage.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;confusion-matrix-1&#34;&gt;Confusion matrix&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A confusion matrix is a table that is used to evaluate the performance of a classification algorithm. It helps to visualize the correct and incorrect predictions made by the model and allows you to see which classes are being predicted accurately and which are not.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The rows of the matrix represent the actual classes of the samples and the columns represent the predicted classes. The diagonal elements of the matrix represent the number of samples that have been correctly classified, while the off-diagonal elements represent the number of misclassified samples.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Here is an example of a confusion matrix:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;              Predicted Positive    Predicted Negative
Actual Positive          TP                  FP
Actual Negative          FN                  TN
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;In this example, TP (true positive) is the number of samples that are actually positive and have been correctly predicted as positive. TN (true negative) is the number of samples that are actually negative and have been correctly predicted as negative. FP (false positive) is the number of samples that are actually negative but have been predicted as positive. FN (false negative) is the number of samples that are actually positive but have been predicted as negative.&lt;/li&gt;
&lt;li&gt;To calculate the values for the confusion matrix, you need a set of predictions made by the model and the corresponding true labels for those predictions. You can then compare the predictions to the true labels to see how many were correct and how many were incorrect.&lt;/li&gt;
&lt;li&gt;Here is an example of how to calculate a confusion matrix in Python:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;from sklearn.metrics import confusion_matrix

y_true = [1, 0, 1, 1, 0, 1]
y_pred = [1, 1, 1, 1, 0, 0]

confusion_matrix(y_true, y_pred)
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;This will output the following confusion matrix:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;array([[2, 1],
       [1, 2]])
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;diagnostic-odds-ratio&#34;&gt;Diagnostic odds ratio&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The diagnostic odds ratio (DOR) is a measure of the accuracy of a diagnostic test. It is used to compare the accuracy of two or more diagnostic tests or to compare the accuracy of a diagnostic test to a reference standard.&lt;/li&gt;
&lt;li&gt;The DOR is calculated as the ratio of the odds of a positive test result in patients with the condition being tested for to the odds of a positive test result in patients without the condition.&lt;/li&gt;
&lt;li&gt;Here is the formula for calculating the DOR:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;DOR = (TP / FP) / (FN / TN)
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Where TP (true positive) is the number of samples that are actually positive and have been correctly predicted as positive, TN (true negative) is the number of samples that are actually negative and have been correctly predicted as negative, FP (false positive) is the number of samples that are actually negative but have been predicted as positive, and FN (false negative) is the number of samples that are actually positive but have been predicted as negative.&lt;/li&gt;
&lt;li&gt;The DOR can range from 0 to infinity, with higher values indicating a more accurate diagnostic test. A DOR of 1 indicates that the test is no better than a coin flip, while a DOR of infinity indicates perfect accuracy.&lt;/li&gt;
&lt;li&gt;To calculate the DOR, you need a set of predictions made by the diagnostic test and the corresponding true labels for those predictions. You can then use the formula above to calculate the DOR.&lt;/li&gt;
&lt;li&gt;Here is an example of how to calculate the DOR in Python:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;def diagnostic_odds_ratio(y_true, y_pred):
    tp = sum((y_true == 1) &amp;amp; (y_pred == 1))
    tn = sum((y_true == 0) &amp;amp; (y_pred == 0))
    fp = sum((y_true == 0) &amp;amp; (y_pred == 1))
    fn = sum((y_true == 1) &amp;amp; (y_pred == 0))
    dor = (tp / fp) / (fn / tn)
    return dor
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Here, y_true is a list of the true labels and y_pred is a list of the predictions made by the diagnostic test. The function calculates the values for TP, TN, FP, and FN using boolean masks and then uses these values to calculate the DOR using the formula above.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;fall-out&#34;&gt;Fall out&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Fallout (also known as false positive rate or type I error) is a measure of the performance of a diagnostic test or classification algorithm. It is the percentage of negative samples that are incorrectly classified as positive.&lt;/li&gt;
&lt;li&gt;In the context of a diagnostic test, fallout represents the probability that a person without the condition being tested for will receive a positive test result. In the context of a classification algorithm, fallout represents the percentage of negative samples that are incorrectly classified as positive.&lt;/li&gt;
&lt;li&gt;Here is the formula for calculating fallout:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Fallout = FP / (FP + TN)
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Where FP (false positive) is the number of samples that are actually negative but have been predicted as positive, and TN (true negative) is the number of samples that are actually negative and have been correctly predicted as negative.&lt;/li&gt;
&lt;li&gt;To calculate fallout, you need a set of predictions made by the diagnostic test or classification algorithm and the corresponding true labels for those predictions. You can then use the formula above to calculate the fallout.&lt;/li&gt;
&lt;li&gt;Here is an example of how to calculate fallout in Python:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;def fallout(y_true, y_pred):
    fp = sum((y_true == 0) &amp;amp; (y_pred == 1))
    tn = sum((y_true == 0) &amp;amp; (y_pred == 0))
    fallout = fp / (fp + tn)
    return fallout
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Here, y_true is a list of the true labels and y_pred is a list of the predictions made by the diagnostic test or classification algorithm. The function calculates the values for FP and TN using boolean masks and then uses these values to calculate the fallout using the formula above.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;false-negative-fn&#34;&gt;False negative (FN)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A false negative (FN) is a prediction made by a diagnostic test or classification algorithm that is incorrect. It refers to a situation where the test or algorithm predicts a negative result for a sample that is actually positive.&lt;/li&gt;
&lt;li&gt;In the context of a diagnostic test, a false negative means that the test failed to detect the presence of a condition in a person who actually has the condition. In the context of a classification algorithm, a false negative means that the algorithm failed to correctly classify a positive sample.&lt;/li&gt;
&lt;li&gt;False negatives are often more serious than false positives, as they can have more serious consequences. For example, if a diagnostic test for a disease returns a false negative result, the person may not receive the necessary treatment and their condition may worsen.&lt;/li&gt;
&lt;li&gt;To calculate the number of false negatives, you need a set of predictions made by the diagnostic test or classification algorithm and the corresponding true labels for those predictions. You can then compare the predictions to the true labels to see how many were incorrect.&lt;/li&gt;
&lt;li&gt;Here is an example of how to calculate the number of false negatives in Python:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;def false_negatives(y_true, y_pred):
    fn = sum((y_true == 1) &amp;amp; (y_pred == 0))
    return fn
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Here, y_true is a list of the true labels and y_pred is a list of the predictions made by the diagnostic test or classification algorithm. The function calculates the number of false negatives using a boolean mask that compares the true labels to the predictions&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;false-negative-rate&#34;&gt;False negative rate&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The false negative rate (FNR) is a measure of the performance of a diagnostic test or classification algorithm. It is the percentage of positive samples that are incorrectly classified as negative.&lt;/li&gt;
&lt;li&gt;In the context of a diagnostic test, the false negative rate represents the probability that a person with the condition being tested for will receive a negative test result. In the context of a classification algorithm, the false negative rate represents the percentage of positive samples that are incorrectly classified as negative.&lt;/li&gt;
&lt;li&gt;Here is the formula for calculating the false negative rate:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;FNR = FN / (FN + TP)
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Where FN (false negative) is the number of samples that are actually positive but have been predicted as negative, and TP (true positive) is the number of samples that are actually positive and have been correctly predicted as positive.&lt;/li&gt;
&lt;li&gt;To calculate the false negative rate, you need a set of predictions made by the diagnostic test or classification algorithm and the corresponding true labels for those predictions. You can then use the formula above to calculate the false negative rate.&lt;/li&gt;
&lt;li&gt;Here is an example of how to calculate the false negative rate in Python:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;def false_negative_rate(y_true, y_pred):
    fn = sum((y_true == 1) &amp;amp; (y_pred == 0))
    tp = sum((y_true == 1) &amp;amp; (y_pred == 1))
    fnr = fn / (fn + tp)
    return fnr
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Here, y_true is a list of the true labels and y_pred is a list of the predictions made by the diagnostic test or classification algorithm. The function calculates the values for FN and TP using boolean masks and then uses these values to calculate the false negative rate using the formula above.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;false-positive-fp&#34;&gt;False positive (FP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A false positive (FP) is a prediction made by a diagnostic test or classification algorithm that is incorrect. It refers to a situation where the test or algorithm predicts a positive result for a sample that is actually negative.&lt;/li&gt;
&lt;li&gt;In the context of a diagnostic test, a false positive means that the test detected the presence of a condition in a person who actually does not have the condition. In the context of a classification algorithm, a false positive means that the algorithm incorrectly classified a negative sample.&lt;/li&gt;
&lt;li&gt;False positives can sometimes be less serious than false negatives, as they may lead to unnecessary follow-up tests or treatment. However, they can also be costly and cause anxiety for the person being tested.&lt;/li&gt;
&lt;li&gt;To calculate the number of false positives, you need a set of predictions made by the diagnostic test or classification algorithm and the corresponding true labels for those predictions. You can then compare the predictions to the true labels to see how many were incorrect.&lt;/li&gt;
&lt;li&gt;Here is an example of how to calculate the number of false positives in Python:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;def false_positives(y_true, y_pred):
    fp = sum((y_true == 0) &amp;amp; (y_pred == 1))
    return fp
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Here, y_true is a list of the true labels and y_pred is a list of the predictions made by the diagnostic test or classification algorithm. The function calculates the number of false positives using a boolean mask that compares the true labels to the predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;false-positive-rate&#34;&gt;False positive rate&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of diagnostic tests, the false positive rate is the probability that a patient with a negative disease status will receive a positive test result. In other words, it is the probability of a false alarm. A high false positive rate means that there is a high probability of a patient being told they have a disease when they actually do not. This can lead to unnecessary anxiety and further testing, and can also reduce the overall credibility of the diagnostic test.&lt;/li&gt;
&lt;li&gt;The false positive rate is often considered in conjunction with the sensitivity and specificity of a diagnostic test. Sensitivity is the probability of a positive test result given that the patient actually has the disease, and specificity is the probability of a negative test result given that the patient does not have the disease. Together, these measures can give a more complete picture of the performance of a diagnostic test.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;false-omission-rate&#34;&gt;False omission rate&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of diagnostic tests, the false omission rate, also known as the false negative rate, is the probability that a patient with a positive disease status will receive a negative test result. A high false negative rate means that there is a high probability of a patient being told they do not have a disease when they actually do. This can have serious consequences, as the patient may not receive the necessary treatment.&lt;/li&gt;
&lt;li&gt;The false negative rate is often considered in conjunction with the sensitivity and specificity of a diagnostic test. Sensitivity is the probability of a positive test result given that the patient actually has the disease, and specificity is the probability of a negative test result given that the patient does not have the disease. Together, these measures can give a more complete picture of the performance of a diagnostic test.&lt;/li&gt;
&lt;li&gt;For example, consider a diagnostic test for a particular disease. The test has a sensitivity of 90%, meaning that it correctly identifies 90% of patients with the disease. It also has a specificity of 95%, meaning that it correctly identifies 95% of patients who do not have the disease. However, if the disease is relatively rare, the false negative rate may still be unacceptably high. For example, if the prevalence of the disease is 1%, and the test has a false negative rate of 10%, then out of 100 patients with the disease, the test will correctly identify only 81 of them (90% sensitivity), while 19 will be misdiagnosed as not having the disease (10% false negative rate). This could lead to a significant number of missed diagnoses.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hit-rate&#34;&gt;Hit rate&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Hit rate, also known as the hit ratio, is a measure of the accuracy of a classifier, predictor, or other machine learning model. It is the number of times the model correctly predicts the outcome (a &amp;ldquo;hit&amp;rdquo;) divided by the total number of predictions made. For example, if a model makes 100 predictions and is correct 70 times, the hit rate is 70%.&lt;/li&gt;
&lt;li&gt;Hit rate is often used as a measure of performance for models that make binary predictions (e.g., &amp;ldquo;positive&amp;rdquo; or &amp;ldquo;negative&amp;rdquo;). In this case, a hit is a correct prediction of the positive or negative class, and the hit rate is the proportion of positive or negative predictions that are correct.&lt;/li&gt;
&lt;li&gt;Hit rate is related to the true positive rate and the false positive rate, which are measures of the performance of a binary classifier. The true positive rate is the proportion of positive cases that are correctly classified as positive, while the false positive rate is the proportion of negative cases that are incorrectly classified as positive. Together, these measures can give a more complete picture of the performance of a classifier.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;miss-rate&#34;&gt;Miss rate&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Miss rate, also known as the miss ratio or false negative rate, is a measure of the accuracy of a classifier, predictor, or other machine learning model. It is the number of times the model incorrectly predicts the outcome (a &amp;ldquo;miss&amp;rdquo;) divided by the total number of predictions made. For example, if a model makes 100 predictions and is incorrect 30 times, the miss rate is 30%.&lt;/li&gt;
&lt;li&gt;Miss rate is often used as a measure of performance for models that make binary predictions (e.g., &amp;ldquo;positive&amp;rdquo; or &amp;ldquo;negative&amp;rdquo;). In this case, a miss is an incorrect prediction of the positive or negative class, and the miss rate is the proportion of positive or negative predictions that are incorrect.&lt;/li&gt;
&lt;li&gt;Miss rate is related to the true positive rate and the false positive rate, which are measures of the performance of a binary classifier. The true positive rate is the proportion of positive cases that are correctly classified as positive, while the false positive rate is the proportion of negative cases that are incorrectly classified as positive. Together, these measures can give a more complete picture of the performance of a classifier.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;negative-likelihood-ratio&#34;&gt;Negative likelihood ratio&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The negative likelihood ratio (NLR) is a measure of the performance of a diagnostic test or other classifier. It is the ratio of the probability of a negative test result given that the patient does not have the disease (specificity) to the probability of a negative test result given that the patient does have the disease (1 - sensitivity). The NLR is used to assess the ability of a test to rule out the presence of a disease.&lt;/li&gt;
&lt;li&gt;The NLR can be calculated using the following formula: NLR = (1 - sensitivity) / specificity&lt;/li&gt;
&lt;li&gt;A diagnostic test with a high NLR (greater than 1) is said to have a high negative predictive value, meaning that it is good at ruling out the presence of a disease. A test with a low NLR (less than 1) has a low negative predictive value, meaning that it is not good at ruling out the presence of a disease.&lt;/li&gt;
&lt;li&gt;The NLR is often used in conjunction with the positive likelihood ratio (PLR), which is the ratio of the probability of a positive test result given that the patient has the disease (sensitivity) to the probability of a positive test result given that the patient does not have the disease (1 - specificity). The PLR is used to assess the ability of a test to detect the presence of a disease. Together, the NLR and PLR can give a more complete picture of the performance of a diagnostic test.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;negative-predictive-value&#34;&gt;Negative predictive value&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The negative predictive value (NPV) is a measure of the performance of a diagnostic test or other classifier. It is the probability that a patient with a negative test result does not have the disease. The NPV is used to assess the ability of a test to rule out the presence of a disease.&lt;/li&gt;
&lt;li&gt;The NPV can be calculated using the following formula: NPV = TN / (TN + FN)
where TN is the number of true negatives (patients with a negative test result who do not have the disease) and FN is the number of false negatives (patients with a negative test result who do have the disease).&lt;/li&gt;
&lt;li&gt;A diagnostic test with a high NPV (close to 1) is said to have a high negative predictive value, meaning that it is good at ruling out the presence of a disease. A test with a low NPV (close to 0) has a low negative predictive value, meaning that it is not good at ruling out the presence of a disease.&lt;/li&gt;
&lt;li&gt;The NPV is often used in conjunction with the positive predictive value (PPV), which is the probability that a patient with a positive test result does have the disease. The PPV is used to assess the ability of a test to detect the presence of a disease. Together, the NPV and PPV can give a more complete picture of the performance of a diagnostic test.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;positive-likelihood-ratio&#34;&gt;Positive likelihood ratio&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The positive likelihood ratio (PLR) is a measure of the performance of a diagnostic test or other classifier. It is the ratio of the probability of a positive test result given that the patient has the disease (sensitivity) to the probability of a positive test result given that the patient does not have the disease (1 - specificity). The PLR is used to assess the ability of a test to detect the presence of a disease.&lt;/li&gt;
&lt;li&gt;The PLR can be calculated using the following formula: PLR = sensitivity / (1 - specificity)&lt;/li&gt;
&lt;li&gt;A diagnostic test with a high PLR (greater than 1) is said to have a high positive predictive value, meaning that it is good at detecting the presence of a disease. A test with a low PLR (less than 1) has a low positive predictive value, meaning that it is not good at detecting the presence of a disease.&lt;/li&gt;
&lt;li&gt;The PLR is often used in conjunction with the negative likelihood ratio (NLR), which is the ratio of the probability of a negative test result given that the patient does not have the disease (specificity) to the probability of a negative test result given that the patient does have the disease (1 - sensitivity). The NLR is used to assess the ability of a test to rule out the presence of a disease. Together, the PLR and NLR can give a more complete picture of the performance of a diagnostic test.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;positive-predictive-value&#34;&gt;Positive predictive value&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The positive predictive value (PPV) is a measure of the performance of a diagnostic test or other classifier. It is the probability that a patient with a positive test result does have the disease. The PPV is used to assess the ability of a test to detect the presence of a disease.&lt;/li&gt;
&lt;li&gt;The PPV can be calculated using the following formula: PPV = TP / (TP + FP)
where TP is the number of true positives (patients with a positive test result who do have the disease) and FP is the number of false positives (patients with a positive test result who do not have the disease).&lt;/li&gt;
&lt;li&gt;A diagnostic test with a high PPV (close to 1) is said to have a high positive predictive value, meaning that it is good at detecting the presence of a disease. A test with a low PPV (close to 0) has a low positive predictive value, meaning that it is not good at detecting the presence of a disease.&lt;/li&gt;
&lt;li&gt;The PPV is often used in conjunction with the negative predictive value (NPV), which is the probability that a patient with a negative test result does not have the disease. The NPV is used to assess the ability of a test to rule out the presence of a disease. Together, the PPV and NPV can give a more complete picture of the performance of a diagnostic test.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;precision&#34;&gt;Precision&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of statistical hypothesis testing and machine learning, precision is a measure of the accuracy of a classifier, predictor, or other model. It is the number of true positive predictions made by the model divided by the total number of positive predictions made by the model. Precision is used to evaluate the performance of a model that makes binary predictions (e.g., &amp;ldquo;positive&amp;rdquo; or &amp;ldquo;negative&amp;rdquo;).&lt;/li&gt;
&lt;li&gt;For example, consider a model that makes 100 predictions, of which 70 are positive and 30 are negative. If the model is correct in 60 of the positive predictions and all of the negative predictions, the precision of the model is 60/70 = 0.86. This means that of all the positive predictions made by the model, 86% are correct.&lt;/li&gt;
&lt;li&gt;Precision is often used in conjunction with the recall, which is the number of true positive predictions made by the model divided by the total number of actual positive cases. Precision and recall are both used to evaluate the performance of a binary classifier, and can be balanced against each other to achieve the desired trade-off in a particular application.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;recall&#34;&gt;Recall&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of statistical hypothesis testing and machine learning, recall is a measure of the accuracy of a classifier, predictor, or other model. It is the number of true positive predictions made by the model divided by the total number of actual positive cases. Recall is used to evaluate the performance of a model that makes binary predictions (e.g., &amp;ldquo;positive&amp;rdquo; or &amp;ldquo;negative&amp;rdquo;).&lt;/li&gt;
&lt;li&gt;For example, consider a model that makes 100 predictions, of which 70 are positive and 30 are negative. If the model is correct in 60 of the positive predictions and all of the negative predictions, and there are 80 actual positive cases, the recall of the model is 60/80 = 0.75. This means that of all the actual positive cases, 75% are correctly predicted by the model.&lt;/li&gt;
&lt;li&gt;Recall is often used in conjunction with the precision, which is the number of true positive predictions made by the model divided by the total number of positive predictions made by the model. Precision and recall are both used to evaluate the performance of a binary classifier, and can be balanced against each other to achieve the desired trade-off in a particular application.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sensitivity&#34;&gt;Sensitivity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Sensitivity, also known as the true positive rate or the recall, is a measure of the performance of a diagnostic test or other classifier. It is the probability of a positive test result given that the patient actually has the disease. Sensitivity is used to evaluate the ability of a test to detect the presence of a disease.&lt;/li&gt;
&lt;li&gt;The sensitivity of a diagnostic test can be calculated using the following formula: sensitivity = TP / (TP + FN) where TP is the number of true positives (patients with a positive test result who do have the disease) and FN is the number of false negatives (patients with a negative test result who do have the disease).&lt;/li&gt;
&lt;li&gt;A diagnostic test with a high sensitivity (close to 1) is said to have a high true positive rate, meaning that it is good at detecting the presence of a disease. A test with a low sensitivity (close to 0) has a low true positive rate, meaning that it is not good at detecting the presence of a disease.&lt;/li&gt;
&lt;li&gt;Sensitivity is often used in conjunction with the specificity of a diagnostic test, which is the probability of a negative test result given that the patient does not have the disease. Together, sensitivity and specificity can give a more complete picture of the performance of a diagnostic test.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;specificity&#34;&gt;Specificity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Specificity, also known as the true negative rate, is a measure of the performance of a diagnostic test or other classifier. It is the probability of a negative test result given that the patient does not have the disease. Specificity is used to evaluate the ability of a test to rule out the presence of a disease.&lt;/li&gt;
&lt;li&gt;The specificity of a diagnostic test can be calculated using the following formula: specificity = TN / (TN + FP) where TN is the number of true negatives (patients with a negative test result who do not have the disease) and FP is the number of false positives (patients with a positive test result who do not have the disease).&lt;/li&gt;
&lt;li&gt;A diagnostic test with a high specificity (close to 1) is said to have a high true negative rate, meaning that it is good at ruling out the presence of a disease. A test with a low specificity (close to 0) has a low true negative rate, meaning that it is not good at ruling out the presence of a disease.&lt;/li&gt;
&lt;li&gt;Specificity is often used in conjunction with the sensitivity of a diagnostic test, which is the probability of a positive test result given that the patient does have the disease. Together, sensitivity and specificity can give a more complete picture of the performance of a diagnostic test.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;true-negative-tn&#34;&gt;True negative (TN)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A true negative is a prediction made by a diagnostic test or other classifier that an event or condition is absent, and the event or condition is indeed absent. In the context of statistical hypothesis testing and machine learning, a true negative is a prediction made by a model that an instance belongs to the negative class, and the instance does indeed belong to the negative class.&lt;/li&gt;
&lt;li&gt;True negatives are typically represented by the letter TN in performance metrics such as sensitivity, specificity, and the positive and negative predictive values. These metrics are used to evaluate the accuracy of a diagnostic test or other classifier. For example, the sensitivity of a test is the proportion of true positive predictions made by the test to the total number of actual positive cases, while the specificity of a test is the proportion of true negative predictions made by the test to the total number of actual negative cases.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;true-positive-tp&#34;&gt;True positive (TP)&lt;/h3&gt;
&lt;p&gt;A true positive is a prediction made by a diagnostic test or other classifier that an event or condition is present, and the event or condition is indeed present. In the context of statistical hypothesis testing and machine learning, a true positive is a prediction made by a model that an instance belongs to the positive class, and the instance does indeed belong to the positive class.&lt;/p&gt;
&lt;p&gt;True positives are typically represented by the letter TP in performance metrics such as sensitivity, specificity, and the positive and negative predictive values. These metrics are used to evaluate the accuracy of a diagnostic test or other classifier. For example, the sensitivity of a test is the proportion of true positive predictions made by the test to the total number of actual positive cases, while the specificity of a test is the proportion of true negative predictions made by the test to the total number of actual negative cases.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;h3 id=&#34;attribute&#34;&gt;Attribute&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of data modeling and database design, an attribute is a property or characteristic of an entity, typically represented as a column in a database table. An attribute can be a simple data value (e.g., a string, integer, or date) or a complex data structure (e.g., an array or object).&lt;/li&gt;
&lt;li&gt;For example, consider a database table that represents a collection of users. Each user in the table might have attributes such as name, email, and date of birth. These attributes can be used to describe the characteristics of each user in the table.&lt;/li&gt;
&lt;li&gt;In the context of machine learning, an attribute is a feature or characteristic of a data instance that can be used for prediction or classification. For example, in a dataset of customer data, each customer might have attributes such as age, income, and location, which could be used to predict their purchasing behavior.&lt;/li&gt;
&lt;li&gt;In both cases, the attributes of an entity or data instance are used to describe and differentiate it from other entities or instances in the same data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;box-and-whisker-plot&#34;&gt;Box and whisker plot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A box and whisker plot (also known as a box plot) is a graphical representation of a set of numerical data that summarizes several important features of the data using a simple and visually effective display. It is typically used to visualize the distribution of the data and to identify any outliers or unusual observations.&lt;/li&gt;
&lt;li&gt;To create a box and whisker plot, the data is first sorted into numerical order. The middle 50% of the data is then represented by a box, which extends from the lower quartile (the 25th percentile) to the upper quartile (the 75th percentile). The lower and upper quartiles are the points that divide the data into four equal parts.&lt;/li&gt;
&lt;li&gt;The median (the 50th percentile) is represented by a line inside the box. The median is the middle value of the data, such that half of the data is above it and half is below it.&lt;/li&gt;
&lt;li&gt;The &amp;ldquo;whiskers&amp;rdquo; of the plot extend from the box to the minimum and maximum values of the data, unless there are outliers present, in which case the whiskers extend only to the most extreme data points that are not outliers. Outliers are data points that are significantly farther from the main body of the data than the rest of the data. They are typically plotted separately as individual points on the plot.&lt;/li&gt;
&lt;li&gt;Box and whisker plots are useful for comparing the distributions of different sets of data, or for identifying patterns and trends in a single set of data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;categorical-data&#34;&gt;Categorical data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Categorical data is data that can be divided into categories or groups. These categories are usually based on some shared characteristics or qualities. Categorical data can be either nominal, meaning the categories do not have any specific order or ranking, or ordinal, meaning the categories are ranked or ordered in some way.&lt;/li&gt;
&lt;li&gt;Examples of categorical data include:
-Nominal data:
&lt;ul&gt;
&lt;li&gt;Gender (male, female)&lt;/li&gt;
&lt;li&gt;Eye color (brown, blue, green)&lt;/li&gt;
&lt;li&gt;Type of animal (cat, dog, bird)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ordinal data:
&lt;ul&gt;
&lt;li&gt;Educational degree (high school, bachelor&amp;rsquo;s degree, master&amp;rsquo;s degree)&lt;/li&gt;
&lt;li&gt;Customer satisfaction ratings (very satisfied, satisfied, neutral, dissatisfied, very frustrated)&lt;/li&gt;
&lt;li&gt;Military rank (private, sergeant, lieutenant, captain)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Categorical data is often used in statistical analysis, and it is important to understand the type of data you are working with in order to choose the appropriate statistical techniques and analysis tools.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;collective-outlier&#34;&gt;Collective outlier&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A collective outlier is a group of data points that are significantly different from the rest of the data. Collective outliers can occur when there is a group of data points that have a different distribution or pattern from the rest of the data. These data points may be the result of a measurement error, an unusual event, or a different process or population.&lt;/li&gt;
&lt;li&gt;Collective outliers can be difficult to identify, as they may not stand out as clearly as individual outliers. It is important to carefully examine the data and consider the context in which it was collected to determine if a group of data points may be collective outliers.&lt;/li&gt;
&lt;li&gt;There are several methods for detecting collective outliers, including visual inspection of the data, statistical tests, and machine learning algorithms. Once identified, it is important to determine the cause of the collective outliers and consider whether they should be included in the analysis or removed from the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contextual-outlier&#34;&gt;Contextual outlier&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A contextual outlier is a data point that is unusual or unexpected in the context in which it occurs, but may not be unusual if considered in a different context. Contextual outliers can occur when there are differences in the populations, processes, or environments being studied, or when the data is being collected for different purposes or using different methods.&lt;/li&gt;
&lt;li&gt;For example, if you are studying the height of adult men and women, a data point representing the height of a 6-foot-tall woman might be considered a contextual outlier, as it is unusual compared to the rest of the data on women&amp;rsquo;s height, but not necessarily unusual compared to the overall distribution of heights in the population.&lt;/li&gt;
&lt;li&gt;It is important to consider the context in which the data was collected when identifying and analyzing contextual outliers. This can help to identify any underlying causes of the outlier and determine whether it is appropriate to include the outlier in the analysis or exclude it from the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;covariate&#34;&gt;Covariate&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A covariate is a variable that is correlated with another variable and is included in a statistical model to control for its effect. Covariates are often used in statistical analysis to adjust for differences between groups or to better understand the relationship between two variables.&lt;/li&gt;
&lt;li&gt;For example, in a study of the relationship between age and blood pressure, age might be included as a covariate to control for its effect on blood pressure. This is because age is known to be related to blood pressure, and including it as a covariate in the statistical model can help to isolate the relationship between blood pressure and other factors being studied.&lt;/li&gt;
&lt;li&gt;In general, covariates are used to improve the accuracy and validity of statistical models by accounting for the influence of other variables that might confound the relationship being studied.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-point&#34;&gt;Data point&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A data point is a single piece of data or a single observation in a dataset. Data points can represent a wide variety of things, depending on the context in which the data was collected. For example, a data point might represent a person&amp;rsquo;s age, the number of sales made by a company in a given month, the temperature at a specific location on a given day, or the result of a laboratory experiment.&lt;/li&gt;
&lt;li&gt;Data points are usually organized and stored in a dataset, which can be a table, spreadsheet, or other structured format. A dataset typically contains multiple data points, and each data point is often represented by a row in the dataset.&lt;/li&gt;
&lt;li&gt;Data points are used in statistical analysis to understand patterns, trends, and relationships within the data. By examining individual data points and the relationships between them, it is possible to draw conclusions and make predictions about the population or system being studied.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;detrending&#34;&gt;Detrending&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Detrending is the process of removing trends or long-term patterns from data in order to better understand short-term fluctuations or changes. Detrending is often used in time series analysis, where the goal is to identify and analyze patterns in data that occur over time.&lt;/li&gt;
&lt;li&gt;There are several methods for detrending data, including:
&lt;ul&gt;
&lt;li&gt;Subtracting the mean: This method involves calculating the mean value of the data over a certain period of time, and then subtracting that value from each data point.&lt;/li&gt;
&lt;li&gt;Fitting a trend line: This method involves fitting a line to the data using a statistical model, such as a linear or polynomial model, and then subtracting the predicted values from the actual data.&lt;/li&gt;
&lt;li&gt;Differencing: This method involves subtracting each data point from the previous data point, which removes any trend that is present in the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Detrending can help to identify and analyze shorter-term patterns or cycles in the data, and can be useful for forecasting or predicting future values. However, it is important to carefully consider the appropriateness of detrending for a particular dataset, as removing trends can also remove important information about the underlying process or system being studied.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;eigenvalue&#34;&gt;Eigenvalue&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An eigenvalue is a special number that is associated with a linear transformation or matrix. In mathematics, a linear transformation is a function that maps one set of numbers (called vectors) to another set of numbers, in such a way that the transformation preserves certain properties of the original vectors. Matrices are used to represent linear transformations, and the eigenvalues of a matrix are a measure of its overall behavior or characteristics.&lt;/li&gt;
&lt;li&gt;The eigenvalues of a matrix are the values that satisfy a particular equation involving the matrix and a vector. These values can be real numbers or complex numbers, and each matrix has a set of eigenvalues that are unique to that matrix.&lt;/li&gt;
&lt;li&gt;Eigenvalues are used in a variety of mathematical and statistical contexts, including image processing, machine learning, and data analysis. They are often used to understand the behavior or characteristics of a matrix or linear transformation, and can be used to identify patterns or trends in data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;eigenvector&#34;&gt;Eigenvector&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An eigenvector is a special type of vector that is associated with a linear transformation or matrix. In mathematics, a vector is a set of numbers that can be used to represent quantities such as position, velocity, or force. A linear transformation is a function that maps one set of vectors to another set of vectors, in such a way that the transformation preserves certain properties of the original vectors. Matrices are used to represent linear transformations, and the eigenvectors of a matrix are vectors that are unchanged (up to a scale factor) by the matrix.&lt;/li&gt;
&lt;li&gt;The eigenvectors of a matrix are the vectors that satisfy a particular equation involving the matrix and the vector. These vectors can have any number of dimensions, and each matrix has a set of eigenvectors that are unique to that matrix.&lt;/li&gt;
&lt;li&gt;Eigenvectors are used in a variety of mathematical and statistical contexts, including image processing, machine learning, and data analysis. They are often used to understand the behavior or characteristics of a matrix or linear transformation, and can be used to identify patterns or trends in data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;feature&#34;&gt;Feature&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of machine learning, features are pieces of data or characteristics that are used as inputs for a model. A machine learning model is a mathematical model that is trained to perform a specific task, such as classifying objects, predicting a numerical value, or generating text. In order to train a model, it is necessary to provide a set of input data, called features, along with the corresponding output data, called labels.&lt;/li&gt;
&lt;li&gt;The choice of features can have a significant impact on the performance of a machine learning model. Good features should be relevant to the task being performed and should contain enough information to allow the model to make accurate predictions or decisions. In some cases, it may be necessary to transform or engineer the features in order to extract the relevant information or to improve the model&amp;rsquo;s performance.&lt;/li&gt;
&lt;li&gt;For example, in a machine learning model that is used to classify images of animals, the features might include the pixel values of the images, or characteristics such as the shape or color of the objects in the images. In a model that is used to predict the price of a house, the features might include characteristics of the house, such as the size, location, and age, as well as external factors such as the local housing market.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;imputation&#34;&gt;Imputation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Imputation is the process of estimating or replacing missing or incomplete data in a dataset. Missing data can occur for a variety of reasons, such as errors in data collection, missing values in a database, or respondents who do not answer certain questions in a survey. Imputation is often necessary in order to use the available data for statistical analysis or machine learning tasks.&lt;/li&gt;
&lt;li&gt;There are several methods for imputing missing data, including:
&lt;ul&gt;
&lt;li&gt;Mean imputation: This method involves replacing missing values with the mean or average value of the data.&lt;/li&gt;
&lt;li&gt;Median imputation: This method involves replacing missing values with the median value of the data.&lt;/li&gt;
&lt;li&gt;Mode imputation: This method involves replacing missing values with the most frequent or common value in the data.&lt;/li&gt;
&lt;li&gt;Regression imputation: This method involves using a statistical model, such as linear regression, to predict the missing values based on the other variables in the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;It is important to carefully consider the appropriate method for imputing missing data, as the choice of method can affect the accuracy and validity of the results.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;observation&#34;&gt;Observation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An observation is a single piece of data or a single measure of a variable. Observations can be collected in a variety of ways, depending on the context and the purpose of the study. For example, observations might be collected through experiments, surveys, or measurements.&lt;/li&gt;
&lt;li&gt;Observations are used to collect and analyze data in order to understand patterns, trends, and relationships within the data. By examining individual observations and the relationships between them, it is possible to draw conclusions and make predictions about the population or system being studied.&lt;/li&gt;
&lt;li&gt;Observations can be either qualitative, meaning they describe a characteristic or attribute of an object or phenomenon, or quantitative, meaning they represent a numerical measurement. Observations are usually organized and stored in a dataset, which can be a table, spreadsheet, or other structured format. A dataset typically contains multiple observations, and each observation is often represented by a row in the dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pca&#34;&gt;PCA&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Principal component analysis (PCA) is a statistical technique that is used to reduce the dimensionality of a dataset by identifying and projecting the data onto a smaller set of orthogonal (uncorrelated) dimensions, called principal components.&lt;/li&gt;
&lt;li&gt;PCA is often used as a preprocessing step for machine learning algorithms, as it can help to remove noise and redundancy from the data, and make the data easier to visualize and analyze. It can also help to identify patterns and trends in the data, and to identify the most important variables or features in the dataset.&lt;/li&gt;
&lt;li&gt;To perform PCA, the data is first standardized, so that all of the variables have a mean of zero and a standard deviation of one. The data is then decomposed into a set of orthogonal principal components, which are ranked in order of their importance or variability in the data. The first principal component represents the direction in the data that has the highest variance, and the subsequent principal components represent directions that have decreasing variance.&lt;/li&gt;
&lt;li&gt;PCA is a powerful tool for analyzing and understanding complex datasets, and it has a wide range of applications in fields such as machine learning, data mining, and image processing.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;point-outlier&#34;&gt;Point outlier&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A point outlier is a data point that is significantly different from the rest of the data. Point outliers can occur when there is an unusual or unexpected measurement, an error in data collection, or a different process or population being studied.&lt;/li&gt;
&lt;li&gt;Point outliers can be identified by visual inspection of the data, or by using statistical tests or machine learning algorithms. It is important to carefully consider the cause of the outlier and determine whether it is appropriate to include the outlier in the analysis or exclude it from the data.&lt;/li&gt;
&lt;li&gt;In some cases, point outliers may be the result of errors or mistakes in data collection, and it may be appropriate to remove them from the data. In other cases, point outliers may represent unusual or unexpected events or observations, and it may be important to include them in the analysis in order to better understand the underlying process or system being studied.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;predictor&#34;&gt;Predictor&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A predictor is a variable that is used to predict or estimate the value of another variable, called the response variable. In statistical analysis, predictor variables are often used to build models that can be used to make predictions or estimations about the response variable.&lt;/li&gt;
&lt;li&gt;For example, in a study of the relationship between age and blood pressure, age might be used as a predictor variable to predict blood pressure. In this case, age would be considered a predictor because it is believed to have an effect on blood pressure, and the goal is to use it to predict or estimate blood pressure in a given population.&lt;/li&gt;
&lt;li&gt;Predictor variables can be either continuous, meaning they can take on any value within a certain range, or categorical, meaning they belong to a specific category or group. The type of predictor variables and the relationship between them and the response variable can influence the choice of statistical techniques and models that are used to analyze the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;quantitative-data&#34;&gt;Quantitative data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Quantitative data is data that is numerical and can be measured or counted. Quantitative data is often used in statistical analysis to understand patterns, trends, and relationships within the data.&lt;/li&gt;
&lt;li&gt;There are two main types of quantitative data: continuous data and discrete data. Continuous data can take on any value within a certain range, such as weight, height, or temperature. Discrete data can only take on specific values, such as the number of students in a class or the number of emails a person receives in a day.&lt;/li&gt;
&lt;li&gt;Examples of quantitative data include:
&lt;ul&gt;
&lt;li&gt;Age&lt;/li&gt;
&lt;li&gt;Income&lt;/li&gt;
&lt;li&gt;Height&lt;/li&gt;
&lt;li&gt;Weight&lt;/li&gt;
&lt;li&gt;Temperature&lt;/li&gt;
&lt;li&gt;Distance&lt;/li&gt;
&lt;li&gt;Time&lt;/li&gt;
&lt;li&gt;Sales revenue&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Quantitative data is often used in statistical analysis to understand patterns, trends, and relationships within the data. It can be analyzed using statistical techniques such as mean, median, mode, standard deviation, and correlation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;response&#34;&gt;Response&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In statistical analysis, the response (also known as the dependent variable) is the variable that is being predicted or estimated based on the values of one or more predictor variables (also known as independent variables).&lt;/li&gt;
&lt;li&gt;For example, in a study of the relationship between age and blood pressure, blood pressure might be the response variable, and age might be a predictor variable. In this case, the goal might be to use age to predict or estimate blood pressure in a given population.&lt;/li&gt;
&lt;li&gt;The response variable is often the main focus of statistical analysis, and the goal is usually to understand how the predictor variables influence the response variable. The choice of predictor variables and the relationship between them and the response variable can influence the choice of statistical techniques and models that are used to analyze the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;scaling&#34;&gt;Scaling&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Scaling is the process of transforming data so that it is on the same scale or within the same range. Scaling is often necessary when comparing data from different sources or when the data has a wide range of values.&lt;/li&gt;
&lt;li&gt;There are several methods for scaling data, including:
&lt;ul&gt;
&lt;li&gt;Min-Max scaling: This method scales the data to a specific range, such as 0 to 1, by&lt;/li&gt;
&lt;li&gt;subtracting the minimum value from each data point and dividing by the range of the data.&lt;/li&gt;
&lt;li&gt;Standardization: This method scales the data so that it has a mean of zero and standard deviation of one.&lt;/li&gt;
&lt;li&gt;Z-score normalization: This method scales the data so that it has a mean of zero and&lt;/li&gt;
&lt;li&gt;a standard deviation of one, and transforms it into a standard normal distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Scaling can be useful for improving the performance of machine learning algorithms, as it can help to prevent certain features from dominating the model due to their large scale. Scaling can also be useful for visualizing the data and comparing different variables or datasets.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;standardization&#34;&gt;Standardization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of machine learning, standardization refers to the process of transforming data features so that they have zero mean and unit variance. This is often done to ensure that all features are on the same scale, which can be important for some machine learning algorithms to function properly.&lt;/li&gt;
&lt;li&gt;For example, suppose that you have a dataset with two features, one that ranges from 0 to 100 and another that ranges from 0 to 1. Without standardization, the feature with a larger range will dominate the model. By standardizing the data, both features will be transformed to have the same scale, which can lead to better performance from the machine learning model.&lt;/li&gt;
&lt;li&gt;Standardization is typically done by subtracting the mean of each feature from the feature values and dividing by the standard deviation of the feature. This ensures that the resulting feature values have zero mean and unit variance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;structured-data&#34;&gt;Structured data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Structured data is data that is organized in a specific way and follows a clear set of rules. It is typically stored in a tabular form, with rows representing individual instances or observations and columns representing the attributes or features of the data. Structured data can be easily processed and analyzed by machines because it follows a well-defined format.&lt;/li&gt;
&lt;li&gt;Examples of structured data include databases, spreadsheets, and tables in a relational database management system (RDBMS). Structured data is often contrasted with unstructured data, which does not follow a fixed format and is more difficult for machines to process and analyze.&lt;/li&gt;
&lt;li&gt;In the context of machine learning, structured data refers to data that is organized in a way that can be easily fed into a machine learning model. This often involves formatting the data into a tabular form with rows representing individual observations and columns representing the features or attributes of the data. Machine learning algorithms are typically designed to work with structured data, so it is important to ensure that the data is properly structured before using it for training or testing a model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;time-series-data&#34;&gt;Time series data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Time series data is a type of data that is collected over time at regular intervals. It is typically used to analyze trends and patterns in data over time. Time series data can be represented as a sequence of data points, where each data point represents the value of a particular variable at a specific time.&lt;/li&gt;
&lt;li&gt;Examples of time series data include stock prices, weather data, and traffic data. Time series data can be used in a variety of applications, including financial forecasting, demand forecasting, and anomaly detection.&lt;/li&gt;
&lt;li&gt;In the context of machine learning, time series data can be used to train models to make predictions about future values of a particular variable based on its past values. This can be done using techniques such as time series forecasting, which involves using machine learning algorithms to model the temporal dependencies in the data and make predictions about future values.&lt;/li&gt;
&lt;li&gt;Time series data is often analyzed using specialized tools and techniques, such as autoregressive integrated moving average (ARIMA) models and long short-term memory (LSTM) neural networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;unstructured-data&#34;&gt;Unstructured data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Unstructured data is data that does not follow a specific format or structure. It is often unorganized and does not fit neatly into a traditional database or spreadsheet. Examples of unstructured data include natural language text, images, audio and video files, and social media posts.&lt;/li&gt;
&lt;li&gt;Unstructured data is difficult for machines to process and analyze because it does not follow a fixed format. This makes it more challenging to extract insights and information from unstructured data compared to structured data, which is organized in a well-defined format and can be easily processed by machines.&lt;/li&gt;
&lt;li&gt;In the context of machine learning, unstructured data can be used as input to train models, but it often requires preprocessing and feature engineering to extract relevant features that can be used by the model.&lt;/li&gt;
&lt;li&gt;This can involve techniques such as natural language processing (NLP) for text data, image processing for image data, and audio processing for audio data. The extracted features can then be used to train machine learning models, which can be used to make predictions or classify the data in some way.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;design-of-experiments&#34;&gt;Design of Experiments&lt;/h2&gt;
&lt;h3 id=&#34;ab-testing&#34;&gt;A/B testing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A/B testing, also known as split testing or bucket testing, is a statistical hypothesis testing procedure used to compare the results of two versions of a product or service. It is commonly used in the fields of marketing and user experience to determine which version is more effective.&lt;/li&gt;
&lt;li&gt;In A/B testing, a random sample of users is selected and divided into two groups, referred to as the control group and the treatment group. The control group is exposed to the current version of the product or service, while the treatment group is exposed to the new version. The results of the two groups are then compared to determine if the new version is an improvement over the current version.&lt;/li&gt;
&lt;li&gt;A/B testing is often used to test changes to websites, apps, and other products or services to determine their impact on user behavior. It is a powerful tool for making data-driven decisions because it allows you to measure the impact of a change in a controlled and statistically rigorous way.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;analysis-of-variance&#34;&gt;Analysis of Variance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Analysis of variance (ANOVA) is a statistical test used to compare the mean of a continuous variable between two or more groups. It is used to determine whether there is a significant difference between the means of the groups, and if so, where the difference lies.&lt;/li&gt;
&lt;li&gt;ANOVA is based on the idea of partitioning the total variance in a dataset into different components, such as the variance within each group and the variance between groups. By comparing the size of these components, ANOVA can determine whether the differences between the group means are statistically significant or if they are likely due to random chance.&lt;/li&gt;
&lt;li&gt;ANOVA can be used with both categorical and continuous independent variables, and it is a widely used tool in a variety of fields, including psychology, sociology, and economics. There are several different types of ANOVA tests, including one-way ANOVA, two-way ANOVA, and repeated measures ANOVA, which are used in different situations depending on the design of the study.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;balanced-design&#34;&gt;Balanced design&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A balanced design is a type of experimental design in which the number of observations in each group is equal. Balanced designs are often used in experiments to ensure that the groups are comparable and that any differences between the groups can be attributed to the independent variable being tested.&lt;/li&gt;
&lt;li&gt;For example, suppose that you are conducting an experiment to test the effectiveness of a new drug. You might use a balanced design by dividing the study participants into two groups: one group that receives the drug and another group that receives a placebo. By ensuring that the two groups are equal in size and composition, you can control for other factors that might influence the results and increase the reliability of your findings.&lt;/li&gt;
&lt;li&gt;Balanced designs can be contrasted with unbalanced designs, in which the number of observations in each group is unequal. Unbalanced designs can be more prone to bias and may not be as reliable as balanced designs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;blocking&#34;&gt;Blocking&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of experimental design, blocking refers to the process of dividing the study subjects into groups, or &amp;ldquo;blocks,&amp;rdquo; based on certain factors that could potentially affect the outcome of the experiment. The goal of blocking is to control for these factors and reduce the potential for extraneous variability in the results.&lt;/li&gt;
&lt;li&gt;For example, suppose that you are conducting an experiment to test the effectiveness of a new teaching method. You might use blocking by dividing the students into groups based on their prior knowledge of the subject matter, in order to control for differences in their initial understanding. By ensuring that the groups are balanced with respect to this factor, you can increase the reliability of your findings and reduce the risk of confounding variables influencing the results.&lt;/li&gt;
&lt;li&gt;Blocking is often used in conjunction with randomization, in which the subjects within each block are randomly assigned to the different treatment groups. This helps to further control for extraneous variables and increase the internal validity of the experiment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;control&#34;&gt;Control&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of experimental design, a control group is a group of subjects that does not receive the treatment being tested. The control group is used for comparison with the experimental group, which does receive the treatment. By comparing the results of the two groups, researchers can determine the effect of the treatment on the outcome of interest.&lt;/li&gt;
&lt;li&gt;The control group is an important element of experimental design because it helps to control for extraneous variables that might influence the results. For example, suppose that you are conducting an experiment to test the effectiveness of a new drug. By including a control group that does not receive the drug, you can control for other factors that might affect the outcome, such as the placebo effect or the natural course of the disease.&lt;/li&gt;
&lt;li&gt;In order to be effective, the control group should be similar to the experimental group in all aspects except for the treatment being tested. This helps to ensure that any differences between the two groups can be attributed to the treatment, rather than other factors.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;design-of-experiments-1&#34;&gt;Design of experiments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The design of experiments (DOE) refers to the systematic and scientific approach to planning, conducting, analyzing, and interpreting experiments. It is a powerful tool for understanding the relationships between variables and for making informed decisions based on data.&lt;/li&gt;
&lt;li&gt;The goal of DOE is to identify the key factors that affect the outcome of an experiment and to determine the optimal combination of these factors. This is typically done by manipulating the levels of the different variables and observing the resulting changes in the outcome.&lt;/li&gt;
&lt;li&gt;There are many different types of experimental designs, including randomized controlled trials, cross-over designs, and factorial designs. The choice of design depends on the specific research question being addressed and the resources available for the experiment.&lt;/li&gt;
&lt;li&gt;DOE is widely used in a variety of fields, including medicine, engineering, and the social sciences. It is an important tool for scientific research and for making data-driven decisions in a variety of settings.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;exploitation&#34;&gt;Exploitation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Exploitation refers to the act of using something or someone to achieve a benefit or gain, often in a way that is unfair or unethical. In the context of machine learning, exploitation can refer to the use of data or algorithms in ways that unfairly advantage certain individuals or groups, or that violate the privacy or autonomy of those whose data is being used.&lt;/li&gt;
&lt;li&gt;For example, exploitation in machine learning could involve using sensitive personal data for purposes that were not disclosed to the individual when the data was collected, or using algorithms that are biased against certain groups. Such practices can lead to negative consequences for those affected by the exploitation, including loss of privacy, discrimination, or loss of opportunities.&lt;/li&gt;
&lt;li&gt;It is important to be aware of the potential for exploitation in machine learning and to take steps to ensure that data and algorithms are used ethically and responsibly. This can involve adopting ethical principles and guidelines, such as those put forth by organizations like the Association for Computing Machinery (ACM) and the International Association for AI and Ethics (IAAIE).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;exploration&#34;&gt;Exploration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of design of experiments (DOE), exploration refers to the process of systematically varying the levels of the input factors in order to better understand the response of the system being studied. Exploration is an important aspect of DOE because it helps to identify the important factors that influence the response, as well as the relationships between these factors and the response.&lt;/li&gt;
&lt;li&gt;This information can be used to optimize the system by identifying the optimal levels of the input factors for a desired response. Exploration can be carried out using a variety of DOE techniques, such as factorial designs, response surface methodology, and DOE software tools.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;factorial-design&#34;&gt;Factorial design&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A factorial design is a type of experimental design in which multiple levels of multiple input factors are tested simultaneously. This allows researchers to study the combined effect of multiple factors on a response, as well as the interaction between the factors.&lt;/li&gt;
&lt;li&gt;Factorial designs are commonly used in DOE because they are efficient and can provide a lot of information about the system being studied. For example, if there are two factors being studied, each at two levels, a 2x2 factorial design would involve testing all four possible combinations of the factor levels. This allows researchers to see how the response changes as each factor is varied independently, as well as how the response changes when the factors are combined.&lt;/li&gt;
&lt;li&gt;Factorial designs can have more than two factors and more than two levels per factor. The number of treatment combinations in a factorial design increases quickly as the number of factors and levels increases, so it is important to carefully plan the design to ensure that it is both practical and efficient.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;fractional-factorial-design&#34;&gt;Fractional factorial design&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A fractional factorial design is a type of experimental design that is similar to a full factorial design, but involves testing only a fraction of the possible combinations of factor levels. This allows researchers to study the effects of multiple factors with a smaller number of experimental runs.&lt;/li&gt;
&lt;li&gt;Fractional factorial designs are useful when there are a large number of factors that need to be studied, or when it is not practical or cost-effective to test all possible combinations of factor levels. However, because not all combinations of factor levels are tested, a fractional factorial design may not be as accurate as a full factorial design.&lt;/li&gt;
&lt;li&gt;There are several types of fractional factorial designs, including two-level fractional factorial designs, which involve testing only a fraction of the possible combinations of two levels of each factor, and Plackett-Burman designs, which are a type of fractional factorial design that is commonly used to identify the important factors in a system.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;full-factorial-design&#34;&gt;Full factorial design&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A full factorial design is a type of experimental design in which all possible combinations of the levels of multiple input factors are tested. This allows researchers to study the combined effect of multiple factors on a response, as well as the interaction between the factors.&lt;/li&gt;
&lt;li&gt;Full factorial designs are commonly used in design of experiments (DOE) because they provide a lot of information about the system being studied. For example, if there are two factors being studied, each at two levels, a full factorial design would involve testing all four possible combinations of the factor levels. This allows researchers to see how the response changes as each factor is varied independently, as well as how the response changes when the factors are combined.&lt;/li&gt;
&lt;li&gt;Full factorial designs can have more than two factors and more than two levels per factor. The number of treatment combinations in a full factorial design increases quickly as the number of factors and levels increases, so it is important to carefully plan the design to ensure that it is both practical and efficient.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multi-armed-bandit&#34;&gt;Multi-armed bandit&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A multi-armed bandit is a type of optimization problem that involves balancing the exploration of different options (the &amp;ldquo;arms&amp;rdquo; of the bandit) with the exploitation of the best option known so far. The goal is to maximize the reward over time by choosing the arm that is most likely to provide the highest reward at each step.&lt;/li&gt;
&lt;li&gt;The multi-armed bandit problem is often used to model situations in which there is a trade-off between exploration and exploitation. For example, in online advertising, a website owner may need to choose which ads to display to a user. The website owner may not know which ad will be the most effective at converting the user into a customer, so they must balance the need to explore different ads with the need to exploit the most effective ad.&lt;/li&gt;
&lt;li&gt;There are various algorithms that can be used to solve the multi-armed bandit problem, such as the epsilon-greedy algorithm and the upper confidence bound (UCB) algorithm. These algorithms use different approaches to balance exploration and exploitation, and can be modified to suit the specific needs of a given application.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;response-surface&#34;&gt;Response surface&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Response surface methodology (RSM) is a statistical technique used to model and optimize the relationship between one or more input variables (also known as factors or independent variables) and an output variable (also known as the response). RSM involves designing experiments to study the response of a system to different levels of the input variables, and then fitting a mathematical model to the data to represent the relationship between the variables.&lt;/li&gt;
&lt;li&gt;The response surface is the graphical representation of the response of the system as a function of the input variables. It is usually a two-dimensional plot showing the response as a function of two input variables, although it can also be a three-dimensional plot for systems with three or more input variables. The response surface can be used to identify the optimal combination of input variables that produce the desired response, as well as to understand the nature of the relationship between the variables.&lt;/li&gt;
&lt;li&gt;RSM is commonly used in engineering and scientific research to optimize processes and products, and it can be applied to a wide range of systems and industries.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;game-theory&#34;&gt;Game Theory&lt;/h2&gt;
&lt;h3 id=&#34;cooperative-game-theory&#34;&gt;Cooperative game theory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cooperative game theory is a branch of game theory that studies situations in which multiple players can form coalitions and make binding agreements in order to achieve a common goal. In cooperative game theory, the players are assumed to be rational and to act in their own self-interest, but they are also able to communicate and make agreements with each other.&lt;/li&gt;
&lt;li&gt;One important concept in cooperative game theory is the concept of the &amp;ldquo;value&amp;rdquo; of a game, which is the maximum payoff that can be achieved by the players if they cooperate. The value of a game can be determined using various solution concepts, such as the Shapley value, the nucleolus, and the core. These solution concepts provide a way to divide the value of the game among the players in a fair and stable way.&lt;/li&gt;
&lt;li&gt;Cooperative game theory is used in a variety of fields, including economics, political science, and computer science. It is particularly useful for studying situations in which the players have conflicting interests, but may still be able to cooperate in order to achieve a mutually beneficial outcome.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;game-theory-1&#34;&gt;Game theory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Game theory is the study of mathematical models of strategic interactions between rational decision-makers. It has applications in a wide range of disciplines, including economics, political science, and psychology, as well as in biology and computer science.&lt;/li&gt;
&lt;li&gt;In game theory, a &amp;ldquo;game&amp;rdquo; is defined as a situation in which multiple players, called &amp;ldquo;players,&amp;rdquo; have to make decisions that will affect the outcome of the game. Each player has a set of possible actions they can take, called a &amp;ldquo;strategy,&amp;rdquo; and a corresponding payoff that depends on the strategies chosen by all the players. The players are assumed to be rational and to act in their own self-interest, trying to maximize their payoff.&lt;/li&gt;
&lt;li&gt;There are two main types of games in game theory: cooperative games and non-cooperative games. In cooperative games, the players can form coalitions and make binding agreements, while in non-cooperative games, the players act independently and cannot make agreements.&lt;/li&gt;
&lt;li&gt;Game theory has been used to study a wide range of real-world situations, including auctions, negotiation, and voting systems. It has also been used to analyze strategic interactions in biology, such as predator-prey relationships and the evolution of social behavior.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mixed-strategyrandomized-strategy&#34;&gt;Mixed strategy/randomized strategy&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In game theory, a mixed strategy is a strategy in which a player randomly selects one of several pure strategies with a specified probability. A pure strategy is a strategy in which the player always chooses a particular action, while a mixed strategy allows the player to choose among several different actions with some probability.&lt;/li&gt;
&lt;li&gt;Mixed strategies are often used to model situations in which a player has incomplete information about the other player&amp;rsquo;s strategies or preferences, or in which the payoffs for each action are not fixed and may vary from one round of the game to the next.&lt;/li&gt;
&lt;li&gt;In non-cooperative games, mixed strategies can be used to find a Nash equilibrium, which is a situation in which no player has an incentive to deviate from their current strategy given the strategies of the other players. In a Nash equilibrium, each player&amp;rsquo;s mixed strategy is a best response to the mixed strategies of the other players.&lt;/li&gt;
&lt;li&gt;Mixed strategies can also be used in cooperative games, although they are not always necessary to find a solution. In cooperative games, mixed strategies can be used to divide the value of the game among the players in a fair and stable way.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;prisoners-dilemma&#34;&gt;Prisoner&amp;rsquo;s dilemma&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The prisoner&amp;rsquo;s dilemma is a classic example of a game used to illustrate the concept of game theory. It is a non-cooperative game that involves two players who must decide whether to cooperate with each other or to defect (i.e., not cooperate).&lt;/li&gt;
&lt;li&gt;In the prisoner&amp;rsquo;s dilemma, the players are assumed to be two prisoners who are being held in separate cells and are offered the following deal: if both prisoners defect, each one will serve a two-year prison sentence; if one defects and the other cooperates, the defector will go free while the cooperator will serve a three-year prison sentence; and if both cooperate, each one will serve a one-year prison sentence.&lt;/li&gt;
&lt;li&gt;The prisoner&amp;rsquo;s dilemma is interesting because the rational choice for each player, given the other player&amp;rsquo;s choice, is to defect. However, if both players defect, they both end up with a worse outcome than if they had cooperated. This illustrates the concept of the &amp;ldquo;prisoner&amp;rsquo;s dilemma,&amp;rdquo; in which individual rationality leads to a suboptimal outcome for both players.&lt;/li&gt;
&lt;li&gt;The prisoner&amp;rsquo;s dilemma has been used to model a wide range of real-world situations, including negotiations, international relations, and the evolution of social behavior.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pure-strategy&#34;&gt;Pure strategy&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In game theory, a pure strategy is a strategy in which a player always chooses a particular action. A pure strategy is contrasted with a mixed strategy, in which a player randomly selects one of several actions with a specified probability.&lt;/li&gt;
&lt;li&gt;Pure strategies are often used to model situations in which a player has complete information about the other player&amp;rsquo;s strategies or preferences, or in which the payoffs for each action are fixed and do not vary from one round of the game to the next.&lt;/li&gt;
&lt;li&gt;In non-cooperative games, pure strategies can be used to find a Nash equilibrium, which is a situation in which no player has an incentive to deviate from their current strategy given the strategies of the other players. In a Nash equilibrium, each player&amp;rsquo;s pure strategy is a best response to the pure strategies of the other players.&lt;/li&gt;
&lt;li&gt;Pure strategies can also be used in cooperative games, although they are not always necessary to find a solution. In cooperative games, pure strategies can be used to divide the value of the game among the players in a fair and stable way.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sequential-game&#34;&gt;Sequential game&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A sequential game is a type of game in which the players take turns making decisions, and the actions of each player depend on the actions of the previous players. In a sequential game, the players have the opportunity to observe the actions of the other players before making their own decisions, which allows them to adjust their strategies based on the actions of the other players.&lt;/li&gt;
&lt;li&gt;Sequential games can be either cooperative or non-cooperative. In cooperative sequential games, the players can communicate and make binding agreements with each other, while in non-cooperative sequential games, the players act independently and cannot make agreements.&lt;/li&gt;
&lt;li&gt;There are several solution concepts that can be used to analyze sequential games, including the subgame perfect equilibrium, the backward induction solution, and the trembling hand perfect equilibrium. These solution concepts provide a way to predict the outcomes of sequential games and to understand the strategic interactions between the players.&lt;/li&gt;
&lt;li&gt;Sequential games are often used to model real-world situations in which the players have the opportunity to observe and learn from each other&amp;rsquo;s actions, such as in auctions and negotiations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;simultaneous-game&#34;&gt;Simultaneous game&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A simultaneous game is a type of game in which all of the players make their decisions at the same time, without knowing the decisions of the other players. In a simultaneous game, the players have to make their decisions based on their beliefs about the other players&amp;rsquo; strategies or preferences, rather than on the actual actions of the other players.&lt;/li&gt;
&lt;li&gt;Simultaneous games can be either cooperative or non-cooperative. In cooperative simultaneous games, the players can communicate and make binding agreements with each other, while in non-cooperative simultaneous games, the players act independently and cannot make agreements.&lt;/li&gt;
&lt;li&gt;There are several solution concepts that can be used to analyze simultaneous games, including the Nash equilibrium, the correlated equilibrium, and the rationalizability concept. These solution concepts provide a way to predict the outcomes of simultaneous games and to understand the strategic interactions between the players.&lt;/li&gt;
&lt;li&gt;Simultaneous games are often used to model real-world situations in which the players make their decisions simultaneously and do not have the opportunity to observe each other&amp;rsquo;s actions, such as in auctions and political elections.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;stable-equilibrium&#34;&gt;Stable equilibrium&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An equilibrium is a state in which no player has an incentive to change their behavior given the behavior of the other players. In game theory, an equilibrium is considered &amp;ldquo;stable&amp;rdquo; if it is the unique outcome of the game and if all the players are satisfied with the outcome.&lt;/li&gt;
&lt;li&gt;There are several types of stable equilibria in game theory, including the Nash equilibrium, the correlated equilibrium, and the rationalizability concept. These solution concepts provide a way to predict the outcomes of games and to understand the strategic interactions between the players.&lt;/li&gt;
&lt;li&gt;Stable equilibria are important because they provide a way to predict the behavior of players in strategic situations. They are often used to model real-world situations in which the players have conflicting interests and must make decisions that will affect the outcome of the game.&lt;/li&gt;
&lt;li&gt;In order for an equilibrium to be stable, it must be the unique outcome of the game and all the players must be satisfied with the outcome. This means that if any player has an incentive to deviate from the equilibrium, the equilibrium is not stable.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;zero-sum-game&#34;&gt;Zero-sum game&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A zero-sum game is a type of game in which the total gain or loss of the players is always zero. This means that the gain of one player is exactly balanced by the loss of the other player(s).&lt;/li&gt;
&lt;li&gt;In a zero-sum game, the players are in direct competition with each other, and the outcome of the game depends on the relative skill of the players. If one player wins, the other player(s) must lose an equal amount.&lt;/li&gt;
&lt;li&gt;Examples of zero-sum games include poker, chess, and the prisoner&amp;rsquo;s dilemma. In these games, one player&amp;rsquo;s gain is exactly offset by the other player&amp;rsquo;s loss, so the total gain or loss of the players is always zero.&lt;/li&gt;
&lt;li&gt;Zero-sum games are important in game theory because they provide a simple and well-defined framework for analyzing strategic interactions between players. They are also important in economics, where they are used to model situations in which the total resources available to the players are fixed and cannot be increased or decreased.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;model-quality&#34;&gt;Model Quality&lt;/h2&gt;
&lt;h3 id=&#34;akaike-information-criterion-aic&#34;&gt;Akaike information criterion (AIC)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;AIC stands for &amp;ldquo;Akaike&amp;rsquo;s Information Criterion.&amp;rdquo; It is a statistical measure that is used to evaluate the quality of a statistical model. The AIC is based on the idea that the best model is the one that strikes the right balance between fit to the data and parsimony (i.e., simplicity).&lt;/li&gt;
&lt;li&gt;The AIC is calculated as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;AIC = 2k - 2ln(L)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where k is the number of parameters in the model and L is the maximum likelihood of the model. The AIC is a measure of the relative quality of a model, with lower values indicating a better model.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The AIC is often used in model selection, where it is used to compare the relative quality of different models. It can also be used to compare the quality of nested models, where one model is a special case of another model.&lt;/li&gt;
&lt;li&gt;The AIC is widely used in statistics and is particularly useful for comparing models with different numbers of parameters. It has been applied in a wide range of fields, including economics, engineering, and the natural sciences.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bayesian-information-criterion-bic&#34;&gt;Bayesian Information criterion (BIC)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Bayesian Information Criterion (BIC) is a statistical measure that is used to evaluate the quality of a statistical model. It is based on the idea that the best model is the one that strikes the right balance between fit to the data and parsimony (i.e., simplicity).&lt;/li&gt;
&lt;li&gt;The BIC is calculated as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;BIC = kln(n) - 2ln(L)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where k is the number of parameters in the model, n is the number of data points, and L is the maximum likelihood of the model. The BIC is a measure of the relative quality of a model, with lower values indicating a better model.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The BIC is often used in model selection, where it is used to compare the relative quality of different models. It can also be used to compare the quality of nested models, where one model is a special case of another model.&lt;/li&gt;
&lt;li&gt;The BIC is widely used in statistics and is particularly useful for comparing models with different numbers of parameters. It has been applied in a wide range of fields, including economics, engineering, and the natural sciences.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;causation&#34;&gt;Causation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Causation refers to the relationship between an event (the cause) and a second event (the effect), where the second event is the result of the first. In order for an event to be considered the cause of another event, it must be shown that there is a clear link between the two events and that the first event directly led to the second event.&lt;/li&gt;
&lt;li&gt;There are several factors that are often used to establish causation, including the following:
&lt;ul&gt;
&lt;li&gt;Temporal precedence: The cause must occur before the effect.&lt;/li&gt;
&lt;li&gt;Covariation: The cause and effect must vary together.&lt;/li&gt;
&lt;li&gt;Control: When other variables are controlled for, the cause and effect should still be related.&lt;/li&gt;
&lt;li&gt;Plausibility: The proposed cause must be scientifically plausible.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Establishing causation can be challenging, particularly in complex systems where there may be multiple potential causes and it is difficult to control for all other variables. In these cases, it is often necessary to use statistical methods to assess the strength of the relationship between the cause and effect.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;corrected-aic&#34;&gt;Corrected AIC&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Corrected AIC, also known as AICc, is a variant of Akaike&amp;rsquo;s Information Criterion (AIC) that is used to evaluate the quality of a statistical model. Like the AIC, the AICc is based on the idea that the best model is the one that strikes the right balance between fit to the data and parsimony (i.e., simplicity).&lt;/li&gt;
&lt;li&gt;The AICc is calculated as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;AICc = AIC + (2k(k + 1)) / (n - k - 1)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where k is the number of parameters in the model, n is the number of data points, and AIC is Akaike&amp;rsquo;s Information Criterion. The AICc is a measure of the relative quality of a model, with lower values indicating a better model.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The AICc is often used in model selection, where it is used to compare the relative quality of different models. It is particularly useful for comparing models with small sample sizes, as it adjusts for the bias that can occur when using the AIC with small sample sizes.&lt;/li&gt;
&lt;li&gt;The AICc is widely used in statistics and has been applied in a wide range of fields, including economics, engineering, and the natural sciences.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;correlation&#34;&gt;Correlation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Correlation is a statistical measure of the relationship between two variables. It is a way to describe the degree to which two variables are related to each other.&lt;/li&gt;
&lt;li&gt;The correlation between two variables is usually represented by the correlation coefficient, which can range from -1 to 1. A correlation coefficient of -1 indicates a perfect negative correlation, meaning that as one variable increases, the other decreases. A correlation coefficient of 1 indicates a perfect positive correlation, meaning that as one variable increases, the other also increases. A correlation coefficient of 0 indicates no correlation.&lt;/li&gt;
&lt;li&gt;Correlation does not imply causation, meaning that the presence of a correlation between two variables does not necessarily mean that one variable is causing the other. It is possible for two variables to be correlated without there being a causal relationship between them.&lt;/li&gt;
&lt;li&gt;Correlation is an important statistical concept that is used in a wide range of fields, including economics, psychology, and the natural sciences. It is often used to understand the relationship between different variables and to predict future outcomes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cross-validation&#34;&gt;Cross-validation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cross-validation is a method used to evaluate the performance of a statistical model. It involves dividing the data into a training set, which is used to train the model, and a test set, which is used to evaluate the model.&lt;/li&gt;
&lt;li&gt;There are several types of cross-validation, including the following:
&lt;ul&gt;
&lt;li&gt;K-fold cross-validation: The data is divided into k folds, and the model is trained on k-1 folds and tested on the remaining fold. This process is repeated k times, with a different fold being used as the test set each time.&lt;/li&gt;
&lt;li&gt;Leave-one-out cross-validation: The model is trained on all but one data point, and then tested on the left-out data point. This process is repeated for each data point, resulting in a model being trained and tested n times, where n is the number of data points.&lt;/li&gt;
&lt;li&gt;Stratified cross-validation: The data is divided into folds such that the proportions of different classes in the folds are similar to the proportions in the entire dataset. This is useful when the classes are imbalanced.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cross-validation is a useful tool for evaluating the performance of a statistical model and for selecting the best model for a given dataset. It helps to ensure that the model is not overfitted to the training data and that it generalizes well to unseen data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hypothesis-test&#34;&gt;Hypothesis test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A hypothesis test is a statistical procedure used to test whether a hypothesis about a population parameter is true or false. It involves collecting data from a sample and using it to make a decision about the hypothesis.&lt;/li&gt;
&lt;li&gt;The process of conducting a hypothesis test usually involves the following steps:
&lt;ul&gt;
&lt;li&gt;State the null hypothesis and the alternative hypothesis. The null hypothesis is the assumption that there is no relationship between the variables being tested, while the alternative hypothesis is the assumption that there is a relationship.&lt;/li&gt;
&lt;li&gt;Select a sample and collect data. The sample should be representative of the population being studied.&lt;/li&gt;
&lt;li&gt;Choose a test statistic and a critical value. The test statistic is a measure of the difference between the sample and the null hypothesis, while the critical value is a predetermined threshold that is used to decide whether to reject or accept the null hypothesis.&lt;/li&gt;
&lt;li&gt;Calculate the p-value. The p-value is the probability of obtaining a test statistic as extreme as the one observed, given that the null hypothesis is true.&lt;/li&gt;
&lt;li&gt;Make a decision. If the p-value is less than the critical value, the null hypothesis is rejected in favor of the alternative hypothesis. If the p-value is greater than the critical value, the null hypothesis is not rejected.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hypothesis tests are an important tool for making decisions about statistical relationships and are widely used in a variety of fields, including psychology, economics, and the natural sciences.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;k-fold-cross-validation&#34;&gt;k-fold cross-validation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;K-fold cross-validation is a method used to evaluate the performance of a statistical model. It involves dividing the data into k folds (also known as &amp;ldquo;subsets&amp;rdquo;) and training the model k times, each time using a different fold as the test set and the remaining folds as the training set. The performance of the model is then averaged across the k iterations.&lt;/li&gt;
&lt;li&gt;For example, in 5-fold cross-validation, the data is divided into 5 folds, and the model is trained and tested 5 times. Each time, a different fold is used as the test set, and the model is trained on the other 4 folds. The performance of the model is then averaged across the 5 iterations.&lt;/li&gt;
&lt;li&gt;K-fold cross-validation is a useful tool for evaluating the performance of a model and for selecting the best model for a given dataset. It helps to ensure that the model is not overfitted to the training data and that it generalizes well to unseen data.&lt;/li&gt;
&lt;li&gt;K-fold cross-validation is a widely used method in machine learning and is particularly useful for small datasets, where it can provide a more reliable estimate of model performance than other methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;likelihood&#34;&gt;Likelihood&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In statistics, the likelihood of a model is a measure of how well the model fits the data. It is defined as the probability of observing the data given the model and a set of parameters.&lt;/li&gt;
&lt;li&gt;The likelihood is often used to compare the fit of different models to the same data. A higher likelihood indicates a better fit, while a lower likelihood indicates a poorer fit.&lt;/li&gt;
&lt;li&gt;The likelihood is often used in maximum likelihood estimation, a method used to estimate the parameters of a statistical model. In maximum likelihood estimation, the parameters of the model are chosen to maximize the likelihood of the model given the data.&lt;/li&gt;
&lt;li&gt;The likelihood is an important concept in statistics that is used in a wide range of applications, including hypothesis testing, model selection, and statistical inference. It provides a way to evaluate the fit of a model to the data and to compare the fit of different models to the same data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;maximum-likelihood&#34;&gt;Maximum likelihood&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Maximum likelihood is a method used to estimate the parameters of a statistical model. It is based on the idea of finding the set of parameters that maximize the likelihood of the model given the data.&lt;/li&gt;
&lt;li&gt;The likelihood of a model is a measure of how well the model fits the data. It is defined as the probability of observing the data given the model and a set of parameters. In maximum likelihood estimation, the parameters of the model are chosen to maximize the likelihood of the model given the data.&lt;/li&gt;
&lt;li&gt;Maximum likelihood estimation has several desirable properties, including being asymptotically efficient (i.e., the estimators converge to the true values as the sample size increases) and being relatively easy to implement. It is widely used in a variety of fields, including economics, psychology, and the natural sciences.&lt;/li&gt;
&lt;li&gt;Maximum likelihood estimation is often used in conjunction with other statistical methods, such as hypothesis testing and model selection, to make inferences about the underlying population from which the data were collected.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;missing-data&#34;&gt;Missing data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Missing data refers to data that is not available or that has not been collected. It is a common problem in statistical analysis and can occur for a variety of reasons, including errors in data collection, missing values in the data, and data that is not recorded.&lt;/li&gt;
&lt;li&gt;Missing data can be a problem because it can bias the results of statistical analyses. For example, if the missing data is not randomly distributed, it can lead to sampling bias and affect the validity of the conclusions.&lt;/li&gt;
&lt;li&gt;There are several approaches for dealing with missing data, including the following:
&lt;ul&gt;
&lt;li&gt;Complete case analysis: This involves removing any cases with missing data from the analysis. This is the simplest approach, but it can lead to biased results if the missing data is not missing at random.&lt;/li&gt;
&lt;li&gt;Imputation: This involves replacing the missing values with estimates based on the available data. There are several methods for imputing missing data, including mean imputation, regression imputation, and multiple imputation.&lt;/li&gt;
&lt;li&gt;Maximum likelihood: This involves using a statistical model to estimate the missing data based on the observed data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The best approach for dealing with missing data depends on the nature of the missing data and the goals of the analysis. It is important to carefully consider the implications of missing data and choose an appropriate approach to ensure the validity of the results.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;random-effects&#34;&gt;Random effects&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In statistics, a random effect is a variable that is included in a statistical model to account for the fact that the data is a sample from a larger population. Random effects are used to model the variability between different groups or individuals in the population.&lt;/li&gt;
&lt;li&gt;For example, consider a study that aims to investigate the relationship between diet and blood pressure. In this study, the researchers might collect data from several different groups of people, such as men and women, or people from different countries. If the researchers want to account for the fact that the data is a sample from a larger population, they might include a random effect for group in their statistical model. This would allow them to estimate the average effect of diet on blood pressure within each group, as well as the overall effect across all groups.&lt;/li&gt;
&lt;li&gt;Random effects are often used in mixed-effects models, which are used to analyze data that has both fixed and random effects. They are an important tool for understanding the sources of variability in data and for making inferences about the population from which the data were collected.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;real-effects&#34;&gt;Real effects&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In statistics, a real effect is a variable that is included in a statistical model to represent an underlying relationship or effect that is believed to exist in the population. Real effects are often used to test hypotheses about the relationships between variables and to estimate the strength and direction of those relationships.&lt;/li&gt;
&lt;li&gt;For example, consider a study that aims to investigate the relationship between diet and blood pressure. In this study, the researchers might collect data from a sample of people and include a real effect for diet in their statistical model. This would allow them to estimate the average effect of diet on blood pressure in the population and to test whether this effect is statistically significant.&lt;/li&gt;
&lt;li&gt;Real effects are often contrasted with random effects, which are used to account for the fact that the data is a sample from a larger population. While real effects represent underlying relationships in the population, random effects represent the variability between different groups or individuals in the population.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sum-of-squared-errors&#34;&gt;Sum-of-squared errors&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The sum of squared errors (SSE) is a measure of the deviation of a set of values from a predicted value. It is often used in statistical analysis to evaluate the fit of a model to a set of data.&lt;/li&gt;
&lt;li&gt;The SSE is calculated as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;SSE = ∑(observed value - predicted value)^2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where the sum is taken over all the data points.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The SSE is a measure of the sum of the squared differences between the observed values and the predicted values. It is a common measure of the error or deviation of a set of values from a predicted value, and it is often used to compare the fit of different models to the same data.&lt;/li&gt;
&lt;li&gt;In general, a smaller SSE indicates a better fit of the model to the data, while a larger SSE indicates a poorer fit. The SSE is often used in conjunction with other measures of fit, such as the coefficient of determination (R^2), to evaluate the quality of a statistical model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;test-datatest-set&#34;&gt;Test data/test set&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A test set is a set of data that is used to evaluate the performance of a statistical model. It is separate from the training set, which is used to fit the model, and is used to assess how well the model generalizes to new, unseen data.&lt;/li&gt;
&lt;li&gt;The test set is often used to estimate the accuracy of the model, as well as other performance metrics such as precision, recall, and F1 score. It is a crucial step in the model development process, as it allows the model to be evaluated on data that it has not seen before and provides a way to assess the generalizability of the model.&lt;/li&gt;
&lt;li&gt;The test set is usually chosen to be representative of the data that the model will encounter in real-world use. It is important to ensure that the test set is independent of the training set and that it is not used in any way to fit the model.&lt;/li&gt;
&lt;li&gt;The test set is an important tool for evaluating the performance of a statistical model and for comparing the performance of different models. It is widely used in a variety of fields, including machine learning, data mining, and statistical analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;training-datatraining-set&#34;&gt;Training data/training set&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The training data or training set is a set of data that is used to fit a statistical model. It is used to learn the parameters of the model and to improve the model&amp;rsquo;s ability to make predictions on new, unseen data.&lt;/li&gt;
&lt;li&gt;The training set is usually a subset of the total dataset and is chosen to be representative of the data that the model will encounter in real-world use. It is important to ensure that the training set is representative of the data that the model will encounter in order to improve the model&amp;rsquo;s ability to generalize to new data.&lt;/li&gt;
&lt;li&gt;The training set is used to fit the model by adjusting the model&amp;rsquo;s parameters to minimize the error between the predicted values and the observed values. Once the model has been trained on the training set, it can be evaluated on a separate test set to assess its performance on new data.&lt;/li&gt;
&lt;li&gt;The training set is an important tool for building and evaluating statistical models and is widely used in a variety of fields, including machine learning, data mining, and statistical analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;validation-datavalidation-set&#34;&gt;Validation data/validation set&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The validation data or validation set is a set of data that is used to evaluate the performance of a statistical model. It is used to tune the model&amp;rsquo;s hyperparameters and to select the best model among a set of candidates.&lt;/li&gt;
&lt;li&gt;The validation set is usually a subset of the total dataset and is used to assess the model&amp;rsquo;s ability to generalize to new, unseen data. It is important to ensure that the validation set is independent of the training set and is not used to fit the model in any way.&lt;/li&gt;
&lt;li&gt;The validation set is used to compare the performance of different models and to select the best model based on a predetermined criterion, such as the accuracy of the model or the Akaike Information Criterion (AIC). Once the best model has been selected, it can be evaluated on a separate test set to assess its performance on new data.&lt;/li&gt;
&lt;li&gt;The validation set is an important tool for building and evaluating statistical models and is widely used in a variety of fields, including machine learning, data mining, and statistical analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;non-parametric-tests&#34;&gt;Non-Parametric Tests&lt;/h2&gt;
&lt;h3 id=&#34;mann-whitney-test&#34;&gt;Mann-Whitney test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Mann-Whitney test is a nonparametric statistical test used to compare the means of two independent samples. It is used when the data is not normally distributed or when the variances of the two samples are not equal.&lt;/li&gt;
&lt;li&gt;The Mann-Whitney test is based on the ranks of the data rather than the raw data values. It involves ranking the data from the two samples and comparing the ranks of the observations from the two samples.&lt;/li&gt;
&lt;li&gt;The Mann-Whitney test is used to test the hypothesis that the two samples come from the same population. If the null hypothesis is rejected, it indicates that there is a statistically significant difference between the means of the two samples.&lt;/li&gt;
&lt;li&gt;The Mann-Whitney test is a widely used statistical test and is particularly useful when the assumptions of other tests, such as the t-test, are not met. It is an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mcnemars-test&#34;&gt;McNemar&amp;rsquo;s test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;McNemar&amp;rsquo;s test is a statistical test used to compare the proportions of two dependent samples. It is used when the data is in the form of pairs, such as before and after measurements on the same group of individuals.&lt;/li&gt;
&lt;li&gt;The McNemar&amp;rsquo;s test is used to test the hypothesis that the proportions of the two samples are equal. It is based on the difference between the two proportions and is used to determine whether the difference is statistically significant.&lt;/li&gt;
&lt;li&gt;The McNemar&amp;rsquo;s test is a nonparametric test, which means that it does not assume that the data follows a specific distribution. It is often used when the assumptions of other tests, such as the chi-squared test, are not met.&lt;/li&gt;
&lt;li&gt;The McNemar&amp;rsquo;s test is an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. It is widely used in a variety of fields, including psychology, medicine, and the social sciences.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;nonparametric-test&#34;&gt;Nonparametric test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A nonparametric test is a statistical test that does not assume that the data follows a specific distribution. Nonparametric tests are often used when the assumptions of parametric tests, such as the t-test or the ANOVA test, are not met or when the sample size is too small to make such assumptions.&lt;/li&gt;
&lt;li&gt;Nonparametric tests are based on the ranks or the frequencies of the data rather than the raw data values. They are often used to compare the means or proportions of two or more groups or to test for associations between variables.&lt;/li&gt;
&lt;li&gt;Some examples of nonparametric tests include the Mann-Whitney test, the Wilcoxon signed-rank test, the Kruskal-Wallis test, the chi-squared test, and the McNemar&amp;rsquo;s test.&lt;/li&gt;
&lt;li&gt;Nonparametric tests are an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. They are widely used in a variety of fields, including psychology, medicine, and the social sciences.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;paired-samples&#34;&gt;Paired samples&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Paired samples are two sets of measurements that are taken on the same group of individuals or units. Paired samples are often used in statistical analysis to compare the means or proportions of the two samples and to test for statistical significance.&lt;/li&gt;
&lt;li&gt;Paired samples are often used when the two samples are dependent, meaning that the measurements in one sample are related to the measurements in the other sample. For example, paired samples might be used to compare the scores of the same group of individuals on two different tests, or to compare the blood pressure of the same group of individuals before and after a treatment.&lt;/li&gt;
&lt;li&gt;Paired samples can be analyzed using parametric or nonparametric statistical tests, depending on the assumptions of the data. Some examples of statistical tests for paired samples include the paired t-test, the Wilcoxon signed-rank test, and the McNemar&amp;rsquo;s test.&lt;/li&gt;
&lt;li&gt;Paired samples are an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. They are widely used in a variety of fields, including psychology, medicine, and the social sciences.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;parametric-test&#34;&gt;Parametric test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A parametric test is a statistical test that assumes that the data follows a specific distribution, such as the normal distribution. Parametric tests are based on the parameters of the distribution and are used to test hypotheses about the population means or proportions.&lt;/li&gt;
&lt;li&gt;Parametric tests are often more powerful than nonparametric tests, which means that they can detect smaller differences between the samples. However, they are also more sensitive to violations of the assumptions of the test, such as normality and homoscedasticity.&lt;/li&gt;
&lt;li&gt;Some examples of parametric tests include the t-test, the ANOVA test, and the linear regression model.&lt;/li&gt;
&lt;li&gt;Parametric tests are an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. They are widely used in a variety of fields, including psychology, medicine, and the social sciences.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;wilcoxon-signed-rank-test-one-sample&#34;&gt;Wilcoxon signed rank test (one sample)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Wilcoxon signed-rank test is a nonparametric statistical test used to compare the median of a single sample to a hypothesized value. It is used when the data are not normally distributed or when the sample size is small.&lt;/li&gt;
&lt;li&gt;The Wilcoxon signed-rank test is based on the ranks of the differences between the observations and the hypothesized value. It involves ranking the differences and testing the hypothesis that the median of the ranked differences is equal to zero.&lt;/li&gt;
&lt;li&gt;The Wilcoxon signed-rank test is used to test the hypothesis that the median of the sample is equal to the hypothesized value. If the null hypothesis is rejected, it indicates that there is a statistically significant difference between the median of the sample and the hypothesized value.&lt;/li&gt;
&lt;li&gt;The Wilcoxon signed-rank test is an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. It is widely used in a variety of fields, including psychology, medicine, and the social sciences.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;wilcoxon-signed-rank-test&#34;&gt;Wilcoxon signed rank test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Wilcoxon signed-rank test is a nonparametric statistical test used to compare the means of two related or dependent samples. It is used when the data are not normally distributed or when the variances of the two samples are not equal.&lt;/li&gt;
&lt;li&gt;The Wilcoxon signed-rank test is based on the ranks of the differences between the observations in the two samples. It involves ranking the differences and testing the hypothesis that the median of the ranked differences is equal to zero.&lt;/li&gt;
&lt;li&gt;The Wilcoxon signed-rank test is used to test the hypothesis that the means of the two samples are equal. If the null hypothesis is rejected, it indicates that there is a statistically significant difference between the means of the two samples.&lt;/li&gt;
&lt;li&gt;The Wilcoxon signed-rank test is an important tool for understanding the relationship between variables and for making inferences about the underlying populations from which the samples were drawn. It is widely used in a variety of fields, including psychology, medicine, and the social sciences.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;optimization&#34;&gt;Optimization&lt;/h2&gt;
&lt;h3 id=&#34;approximate-dynamic-program&#34;&gt;Approximate dynamic program&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Approximate dynamic programming is a method for solving optimization problems that involves iteratively improving approximate solutions to a problem. It is often used when the exact solution to the problem is computationally intractable, but it is possible to compute approximate solutions that are good enough for a particular application.&lt;/li&gt;
&lt;li&gt;In approximate dynamic programming, a sequence of approximate solutions is generated, with each successive solution being an improvement upon the previous one. This process is often done using techniques from machine learning, such as supervised learning, reinforcement learning, or unsupervised learning, to learn a function that can be used to generate the approximate solutions.&lt;/li&gt;
&lt;li&gt;Approximate dynamic programming can be used in a wide variety of applications, including resource allocation, scheduling, control systems, and decision making. It is a powerful tool for solving optimization problems in real-time, and has been applied in a number of different fields, including economics, engineering, and computer science.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;arc&#34;&gt;Arc&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of optimization, an arc is a continuous path between two points in a graph or network. Arcs are often used to represent connections or relationships between variables or points in a problem.&lt;/li&gt;
&lt;li&gt;In mathematical optimization, arcs can be used to represent constraints or limitations on the solution to a problem. For example, in a transportation optimization problem, arcs might represent the routes that can be taken between different locations, and the cost of traveling along each route. The optimization problem would then involve finding the lowest cost path that satisfies all of the constraints represented by the arcs.&lt;/li&gt;
&lt;li&gt;Arcs can also be used to represent relationships between variables in a problem. For example, in a linear programming problem, arcs might represent the flow of a resource between different locations or activities. The optimization problem would then involve finding the values for the variables (such as the amount of the resource to be allocated to each location or activity) that maximize or minimize some objective function, subject to the constraints represented by the arcs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;assignment-problem&#34;&gt;Assignment problem&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The assignment problem is a type of optimization problem that involves finding the optimal way to assign a set of resources to a set of tasks. The goal of the assignment problem is to minimize the total cost of the assignment, where the cost of an assignment is the sum of the cost of assigning each resource to its corresponding task.
-The assignment problem can be represented as a bipartite graph, with one set of vertices representing the resources and the other set representing the tasks. Arcs are then drawn between the vertices, with the cost of assigning a resource to a task being represented by the weight of the corresponding arc. The assignment problem then involves finding a complete matching (a set of arcs such that every vertex is incident to exactly one arc) in the graph that minimizes the total arc weight.&lt;/li&gt;
&lt;li&gt;The assignment problem can be solved using a number of different algorithms, including the Hungarian algorithm, the auction algorithm, and the primal-dual algorithm. It has a wide range of applications, including resource allocation, scheduling, and transportation optimization.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bellmans-equation&#34;&gt;Bellman&amp;rsquo;s equation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bellman&amp;rsquo;s equation is a mathematical equation that is used in dynamic programming to compute the value of a given state in a Markov decision process. It is named after Richard Bellman, who introduced the concept of dynamic programming in the 1950s.&lt;/li&gt;
&lt;li&gt;In dynamic programming, a Markov decision process is represented as a sequence of states, transitions, and rewards. At each time step, the decision maker can choose from a set of actions that will transition the system to a new state. The value of a state is defined as the expected sum of future rewards that can be obtained by starting in that state and following an optimal policy.&lt;/li&gt;
&lt;li&gt;Bellman&amp;rsquo;s equation is used to compute the value of a given state by considering all of the possible actions that can be taken from that state and the resulting rewards and next states. It is typically written as:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;V(s) = max[R(s,a) + γV(s&amp;#39;)]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where V(s) is the value of state s, R(s,a) is the reward for taking action a in state s, s&amp;rsquo; is the next state resulting from taking action a in state s, and γ is a discount factor that determines the importance of future rewards relative to immediate rewards.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bellman&amp;rsquo;s equation is used to solve many different types of optimization problems, including problems in economics, engineering, and computer science.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;binary-integer-program&#34;&gt;Binary integer program&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A binary integer program (BIP) is a type of mathematical optimization problem in which the variables are restricted to be binary (i.e., either 0 or 1) and the objective function and constraints are linear. BIPs are often used to model decision-making problems in which the variables represent the selection or assignment of resources, and the objective function and constraints represent the costs and limitations of the problem.&lt;/li&gt;
&lt;li&gt;BIP problems can be expressed in the following standard form:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;maximize c^T x
subject to Ax &amp;lt;= b
x is binary
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where x is a vector of binary variables, c is a vector of coefficients representing the objective function, A is a matrix of coefficients representing the constraints, and b is a vector of constants representing the right-hand side of the constraints.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BIP problems can be solved using a variety of algorithms, including branch and bound, cutting plane, and branch and cut. They have a wide range of applications, including resource allocation, scheduling, and transportation optimization.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;binary-variable&#34;&gt;Binary variable&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In optimization, a binary variable is a type of decision variable that can take on only two values: 0 or 1. Binary variables are often used to represent choices or assignments in optimization problems, where a value of 0 indicates that the corresponding choice or assignment is not made, and a value of 1 indicates that it is made.&lt;/li&gt;
&lt;li&gt;Binary variables are commonly used in mathematical optimization to model problems in which the variables represent the selection or assignment of resources. For example, in a scheduling problem, a binary variable might be used to represent whether or not a particular machine is assigned to a particular task. In this case, a value of 0 would indicate that the machine is not assigned to the task, and a value of 1 would indicate that it is.&lt;/li&gt;
&lt;li&gt;Binary variables can be included in optimization problems using a number of different modeling languages and software packages, such as AMPL, GAMS, and CPLEX. They are often used in conjunction with other types of variables, such as continuous or integer variables, to model more complex optimization problems.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;chance-constraint&#34;&gt;Chance constraint&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A chance constraint is a type of constraint that is used in optimization problems to ensure that a certain probability is achieved. In a chance constraint, the constraint is expressed in terms of a probability, and the solution to the optimization problem must satisfy the constraint with a certain probability, which is usually specified in advance.&lt;/li&gt;
&lt;li&gt;Chance constraints are often used in optimization problems to model uncertainty or risk. For example, in a transportation optimization problem, a chance constraint might be used to ensure that a certain percentage of shipments arrive at their destination on time. In this case, the probability would represent the likelihood that a shipment will arrive on time, and the constraint would specify the minimum acceptable probability.&lt;/li&gt;
&lt;li&gt;Chance constraints can be difficult to handle in optimization problems, because they introduce a probabilistic element that is not present in traditional constraints. As a result, special techniques are often needed to solve optimization problems with chance constraints, such as Monte Carlo simulation or approximation methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;clique&#34;&gt;Clique&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In graph theory, a clique is a subset of vertices in an undirected graph such that every two distinct vertices in the clique are adjacent, that is, they are connected by an edge. A clique is said to be maximal if it is not a subset of any other clique in the graph.&lt;/li&gt;
&lt;li&gt;Cliques have a number of interesting properties and have been studied extensively in the field of graph theory. For example, it is easy to determine whether a given set of vertices forms a clique, and it is also easy to find the maximum size of a clique in a given graph.&lt;/li&gt;
&lt;li&gt;Cliques have a wide range of applications in computer science and other fields. They are often used in network analysis to identify groups of nodes that are highly connected, and they have also been used in machine learning and data mining to identify patterns and trends in data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;concave-function&#34;&gt;Concave function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In mathematics, a concave function is a function that is always below its tangent lines. Equivalently, a concave function is a function for which the line segment connecting any two points on the graph of the function lies above the graph.&lt;/li&gt;
&lt;li&gt;Concave functions have a number of interesting properties and are often used in optimization problems. For example, the graph of a concave function is always curved downward, and it has a single global minimum. As a result, it is often relatively easy to find the global minimum of a concave function using optimization algorithms.&lt;/li&gt;
&lt;li&gt;Concave functions are used in a wide variety of applications, including economics, engineering, and computer science. They are often used to model cost and utility functions, and they are also used to model constraints in optimization problems.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;constraint&#34;&gt;Constraint&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of optimization, a constraint is a condition that must be satisfied by the solution to a problem. Constraints are used in optimization to specify the limits and requirements of a problem, and they help to define the feasible region of the problem, which is the set of all possible solutions that satisfy the constraints.&lt;/li&gt;
&lt;li&gt;Constraints can be expressed in a variety of ways, depending on the type of optimization problem being solved. In linear programming, for example, constraints are typically expressed as linear inequalities or equations. In nonlinear programming, constraints can be expressed as nonlinear functions.&lt;/li&gt;
&lt;li&gt;Constraints play a central role in optimization problems, as they help to define the space of possible solutions and the objective that the optimization algorithm is trying to maximize or minimize. Constraints can be used to represent a wide range of requirements and limitations, including capacity limits, resource availability, and physical laws.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convex-function&#34;&gt;Convex function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In mathematics, a convex function is a function that is always above its tangent lines. Equivalently, a convex function is a function for which the line segment connecting any two points on the graph of the function lies below the graph.&lt;/li&gt;
&lt;li&gt;Convex functions have a number of interesting properties and are often used in optimization problems. For example, the graph of a convex function is always curved upwards, and it has a single global minimum. As a result, it is often relatively easy to find the global minimum of a convex function using optimization algorithms.&lt;/li&gt;
&lt;li&gt;Convex functions are used in a wide variety of applications, including economics, engineering, and computer science. They are often used to model cost and utility functions, and they are also used to model constraints in optimization problems. Convex optimization is a field of optimization that focuses specifically on optimization problems with convex objective functions and convex constraints.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convex-optimization-model&#34;&gt;Convex optimization model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Convex optimization is a subfield of optimization that studies optimization problems for which the objective function and the feasible region are both convex. Convex optimization problems can be formulated and solved in a variety of ways. They can be expressed as linear programming problems, quadratic programming problems, second-order cone programming problems, and semidefinite programming problems, among others.&lt;/li&gt;
&lt;li&gt;One of the key features of convex optimization problems is that they have a unique global minimum, which can be found efficiently using algorithms such as gradient descent or interior point methods. Additionally, convex optimization problems satisfy strong duality, which means that the solution to the primal problem (the original optimization problem) can be obtained from the solution to the dual problem (a related optimization problem).&lt;/li&gt;
&lt;li&gt;Convex optimization has a wide range of applications in fields such as machine learning, control engineering, and economics. It is used to solve problems such as training neural networks, designing control systems, and finding equilibrium in market models.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convex-quadratic-function&#34;&gt;Convex quadratic function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A convex quadratic function is a function of the form:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;f(x) = x^T Q x + q^T x + c
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where Q is a symmetric matrix, q is a vector, and c is a scalar.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The function f is convex if and only if Q is positive semidefinite, i.e., all of its eigenvalues are nonnegative. If Q is positive definite, then f is strictly convex, meaning that it has a unique global minimum.&lt;/li&gt;
&lt;li&gt;Convex quadratic functions can be minimized using a variety of algorithms, such as gradient descent, Newton&amp;rsquo;s method, and interior point methods. They are often used in convex optimization problems as a simple and efficient way to model objective functions or constraints.&lt;/li&gt;
&lt;li&gt;Examples of convex quadratic functions include the negative log likelihood of a Gaussian distribution, the objective function of a least squares regression problem, and the objective function of a support vector machine.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convex-quadratic-program&#34;&gt;Convex quadratic program&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A convex quadratic program (CQP) is an optimization problem of the form:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;minimize x^T Q x + q^T x
subject to Ax &amp;lt;= b
l &amp;lt;= x &amp;lt;= u
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where x is the optimization variable, Q is a symmetric matrix, q is a vector, A is a matrix, b is a vector, l is a vector of lower bounds on x, and u is a vector of upper bounds on x.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The objective function f(x) = x^T Q x + q^T x is a convex quadratic function, and the feasible region defined by Ax &amp;lt;= b and l &amp;lt;= x &amp;lt;= u is a convex set. Therefore, the problem is a convex optimization problem.&lt;/li&gt;
&lt;li&gt;CQPs can be solved using a variety of algorithms, such as gradient descent, Newton&amp;rsquo;s method, and interior point methods. They are often used to model problems in fields such as machine learning, control engineering, and economics.&lt;/li&gt;
&lt;li&gt;Examples of CQPs include the problem of training a support vector machine, the problem of designing a linear controller, and the problem of finding an equilibrium in a market model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convex-set&#34;&gt;Convex set&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A convex set is a subset of a vector space that contains all the points on the line segments connecting any two of its points. Equivalently, a set is convex if for any two points x and y in the set and for any scalar t in the interval [0, 1], the point (1 - t)x + ty is also in the set.&lt;/li&gt;
&lt;li&gt;Convex sets have several useful properties. For example, if a function is defined on a convex set and is minimized over that set, then it has a unique global minimum. Additionally, the intersection of any two convex sets is convex, and the convex hull of any set is convex.&lt;/li&gt;
&lt;li&gt;Convex sets are important in optimization because many optimization problems can be formulated as minimizing a function over a convex set. Such problems are called convex optimization problems, and they can be solved efficiently using algorithms such as gradient descent or interior point methods.&lt;/li&gt;
&lt;li&gt;Examples of convex sets include the set of all points in a plane that are contained in a circle, the set of all positive semidefinite matrices, and the set of all points in a Euclidean space that satisfy a system of linear inequalities.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;diet-problem&#34;&gt;Diet problem&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The diet problem is a classic example of a linear programming problem, which is a type of optimization problem. It involves finding the optimal combination of foods to consume in order to meet certain nutritional requirements at the lowest cost.&lt;/li&gt;
&lt;li&gt;The diet problem can be formulated as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;minimize c^T x
subject to Ax &amp;lt;= b
x &amp;gt;= 0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where x is a vector of decision variables representing the amounts of each food to consume, c is a vector of costs per unit of each food, A is a matrix representing the nutritional content of each food, and b is a vector representing the required intake of each nutrient.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The objective is to minimize the total cost of the diet, subject to the constraints that the nutritional requirements are met.&lt;/li&gt;
&lt;li&gt;The diet problem can be solved using linear programming techniques, such as the simplex algorithm or the interior point method. It has applications in fields such as nutrition, public health, and economics.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dynamic-programming&#34;&gt;Dynamic programming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dynamic programming is a method for solving optimization problems by breaking them down into smaller subproblems and storing the solutions to these subproblems in a table or array. The solutions to the subproblems are then combined to obtain the solution to the original problem.&lt;/li&gt;
&lt;li&gt;Dynamic programming is particularly useful for problems that exhibit the following two properties:
&lt;ul&gt;
&lt;li&gt;Optimal substructure: The optimal solution to a problem can be obtained by combining the optimal solutions to its subproblems.&lt;/li&gt;
&lt;li&gt;Overlapping subproblems: Many of the subproblems in the problem are identical, or &amp;ldquo;overlap,&amp;rdquo; meaning that they can be solved just once and the solution can be reused many times.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dynamic programming algorithms are typically implemented using recursion, and they can be either top-down (starting with the original problem and breaking it down into subproblems) or bottom-up (starting with the subproblems and combining them to solve the original problem).&lt;/li&gt;
&lt;li&gt;Examples of problems that can be solved using dynamic programming include the knapsack problem, the shortest path problem, and the longest common subsequence problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;edge&#34;&gt;Edge&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of machine learning, an edge refers to the boundary between different classes or clusters in a dataset. For example, in a classification problem, an edge may represent the boundary between different categories of data points, such as between points that belong to the &amp;ldquo;positive&amp;rdquo; class and points that belong to the &amp;ldquo;negative&amp;rdquo; class.&lt;/li&gt;
&lt;li&gt;Edges can be used to inform the design of machine learning models, particularly in the context of supervised learning. For example, a model that is designed to classify data points into different categories might use edges to define the decision boundaries between classes. In this case, the model would aim to find a line or curve that maximally separates the points in one class from those in another class.&lt;/li&gt;
&lt;li&gt;Edges can also be used as features in machine learning models. For example, in a computer vision problem, edges in an image might be used as input to a model that is designed to classify objects in the image.&lt;/li&gt;
&lt;li&gt;Overall, the concept of an edge is important in machine learning because it represents the separation between different classes or clusters of data, and this separation can be used to inform the design and behavior of machine learning models.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;feasible-solution&#34;&gt;Feasible solution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A feasible solution to an optimization problem is a solution that satisfies all of the constraints of the problem. In other words, it is a solution that lies within the feasible region defined by the constraints.&lt;/li&gt;
&lt;li&gt;For example, consider the following linear programming problem:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;minimize c^T x
subject to Ax &amp;lt;= b
x &amp;gt;= 0
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;In this problem, x is the optimization variable, and the constraints Ax &amp;lt;= b and x &amp;gt;= 0 define the feasible region. A feasible solution is a vector x that lies within this region, i.e., it satisfies the constraints Ax &amp;lt;= b and x &amp;gt;= 0.&lt;/li&gt;
&lt;li&gt;Feasible solutions are important in optimization because they represent the set of possible solutions to the problem. The goal of an optimization algorithm is to find the optimal solution, which is the feasible solution that minimizes (or maximizes) the objective function.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;fixed-charge&#34;&gt;Fixed charge&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of optimization, a fixed charge is a cost that is independent of the decision variables and is not included in the objective function. Instead, it is treated as a constraint on the problem.&lt;/li&gt;
&lt;li&gt;For example, consider the following linear programming problem:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;minimize c^T x
subject to Ax &amp;lt;= b
x &amp;gt;= 0
F(x) &amp;lt;= f
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;In this problem, x is the optimization variable, c is a vector of costs per unit of each decision variable, and Ax &amp;lt;= b and x &amp;gt;= 0 define the feasible region. The constraint F(x) &amp;lt;= f represents a fixed charge on the problem.&lt;/li&gt;
&lt;li&gt;Fixed charges are important in optimization because they can represent costs or constraints that are not captured by the objective function or the constraints on the decision variables. For example, a fixed charge might represent a budget constraint, a regulatory requirement, or a capacity constraint.&lt;/li&gt;
&lt;li&gt;Optimization algorithms can be used to find the optimal solution to a problem with fixed charges by taking these constraints into account. The optimal solution is the feasible solution that minimizes (or maximizes) the objective function subject to all of the constraints on the problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;flow&#34;&gt;Flow&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of optimization, flow typically refers to the movement of goods, resources, or people from one location to another. Optimization problems that involve flow often involve finding the optimal allocation of resources or the optimal path for goods or people to follow.&lt;/li&gt;
&lt;li&gt;Examples of optimization problems that involve flow include network flow problems, transportation problems, and logistics problems. These problems can be formulated as linear programming problems, integer programming problems, or network flow problems, depending on the specifics of the problem.&lt;/li&gt;
&lt;li&gt;In a network flow problem, the goal is to find the optimal flow of goods or resources through a network of nodes and edges, subject to capacity constraints on the edges and demand or supply constraints at the nodes. In a transportation problem, the goal is to find the optimal allocation of goods from a set of sources to a set of destinations, subject to capacity constraints on the transportation vehicles and demand constraints at the destinations. In a logistics problem, the goal is to find the optimal route or schedule for moving goods from one location to another, subject to time and resource constraints.&lt;/li&gt;
&lt;li&gt;Overall, the concept of flow is important in optimization because it represents the movement of goods, resources, or people, and the optimization of this flow can lead to improved efficiency and cost savings.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;global-optimummaximumminimum&#34;&gt;Global optimum/maximum/minimum&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The global optimum (or global maximum or minimum, depending on the context) of a function is the point at which the function achieves its highest (or lowest) value. For a function defined over a continuous domain, the global optimum is the point at which the function has a local minimum (or maximum) and there are no other points with a lower (or higher) value.&lt;/li&gt;
&lt;li&gt;In optimization, the goal is often to find the global optimum of an objective function subject to certain constraints. For example, in a linear programming problem, the goal is to find the point at which the objective function is minimized (or maximized) subject to a set of linear constraints. In this case, the global optimum is the point at which the objective function has the lowest (or highest) value among all points that satisfy the constraints.&lt;/li&gt;
&lt;li&gt;The global optimum is important because it represents the best possible solution to an optimization problem. In contrast, a local optimum is a point at which the objective function has a local minimum (or maximum) but may not be the global optimum.&lt;/li&gt;
&lt;li&gt;The global optimum can be found using a variety of optimization algorithms, such as gradient descent, Newton&amp;rsquo;s method, and interior point methods. These algorithms can be used to find the global optimum of a wide range of optimization problems, including linear programming problems, nonlinear programming problems, and convex optimization problems.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;greedy-algorithm&#34;&gt;Greedy algorithm&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A graph is a mathematical structure used to represent relationships between objects. It consists of a set of vertices (also called nodes) and a set of edges connecting the vertices.&lt;/li&gt;
&lt;li&gt;The vertices in a graph represent the objects, and the edges represent the relationships between the objects. The edges can be directed (meaning that they have a specific starting and ending vertex) or undirected (meaning that they do not have a specific direction).&lt;/li&gt;
&lt;li&gt;Graphs are commonly used to represent networks, such as social networks, transportation networks, and communication networks. They are also used to represent data structures, such as trees and maps, and to model optimization problems, such as the shortest path problem and the traveling salesman problem.&lt;/li&gt;
&lt;li&gt;There are many different types of graphs, including directed graphs, undirected graphs, weighted graphs (where the edges have weights or costs associated with them), and bipartite graphs (where the vertices can be divided into two disjoint sets and the edges only connect vertices in different sets).&lt;/li&gt;
&lt;li&gt;Overall, the concept of a graph is a fundamental one in mathematics and computer science, and it has many applications in fields such as data analysis, machine learning, and operations research.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;improving-direction&#34;&gt;Improving direction&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A graph is a mathematical structure used to represent relationships between objects. It consists of a set of vertices (also called nodes) and a set of edges connecting the vertices.&lt;/li&gt;
&lt;li&gt;The vertices in a graph represent the objects, and the edges represent the relationships between the objects. The edges can be directed (meaning that they have a specific starting and ending vertex) or undirected (meaning that they do not have a specific direction).&lt;/li&gt;
&lt;li&gt;Graphs are commonly used to represent networks, such as social networks, transportation networks, and communication networks. They are also used to represent data structures, such as trees and maps, and to model optimization problems, such as the shortest path problem and the traveling salesman problem.&lt;/li&gt;
&lt;li&gt;There are many different types of graphs, including directed graphs, undirected graphs, weighted graphs (where the edges have weights or costs associated with them), and bipartite graphs (where the vertices can be divided into two disjoint sets and the edges only connect vertices in different sets).&lt;/li&gt;
&lt;li&gt;Overall, the concept of a graph is a fundamental one in mathematics and computer science, and it has many applications in fields such as data analysis, machine learning, and operations research.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;initialization&#34;&gt;Initialization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of optimization, initialization refers to the process of setting the initial values of the decision variables or other parameters of the optimization algorithm. These initial values are used to begin the optimization process, and they can have a significant impact on the convergence and performance of the algorithm.&lt;/li&gt;
&lt;li&gt;The choice of initial values can depend on the specific optimization problem being solved and the optimization algorithm being used. For example, in a gradient descent algorithm, the initial values of the decision variables might be set to random values or to the solution of a related optimization problem. In an interior point algorithm, the initial values of the decision variables and the algorithm parameters might be chosen based on the properties of the problem, such as the condition number of the constraint matrix.&lt;/li&gt;
&lt;li&gt;Initialization is important in optimization because it can affect the convergence and performance of the algorithm. Careful initialization can help to ensure that the algorithm converges to a good solution and does not get stuck in a local minimum (or maximum). On the other hand, poor initialization can lead to slow convergence or failure to find the optimal solution.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;integer-program&#34;&gt;Integer program&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An integer program is an optimization problem in which some or all of the decision variables are required to be integers. Integer programming problems are often used to model problems that involve discrete choices or decisions, such as the selection of a set of products to manufacture or the allocation of resources to different projects.&lt;/li&gt;
&lt;li&gt;Integer programming problems can be formulated in a variety of ways, depending on the specific constraints and objective of the problem. For example, a common form of integer programming problem is the linear integer programming problem, which has the following form:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;minimize c^T x
subject to Ax &amp;lt;= b
x &amp;gt;= 0
x is integer
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;In this problem, x is the optimization variable, and c is a vector of costs per unit of each decision variable. The constraints Ax &amp;lt;= b and x &amp;gt;= 0 define the feasible region, and the constraint x is integer requires that the decision variables must be integers.&lt;/li&gt;
&lt;li&gt;Integer programming problems can be difficult to solve, because the feasible region is typically discrete and may not be smooth or continuous. Specialized algorithms, such as branch and bound and cutting plane algorithms, can be used to solve integer programming problems.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;linear-equation&#34;&gt;Linear equation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A linear equation is an equation in which the highest power of the variable(s) is 1. For example, the equation y = 2x + 1 is a linear equation because the highest power of x is 1. Linear equations can take many forms, but they all have the property that the highest power of the variable(s) is 1.&lt;/li&gt;
&lt;li&gt;Linear equations can be written in the standard form:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ax + by = c
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where a and b are constants, and x and y are variables. The standard form of a linear equation is useful because it allows us to easily identify the slope and y-intercept of the line described by the equation. The slope of the line is represented by the coefficient of x (a), and the y-intercept is the point where the line crosses the y-axis (the value of y when x = 0), which is represented by the constant term (c).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear equations can also be written in slope-intercept form:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;y = mx + b
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where m is the slope of the line and b is the y-intercept. This form of the equation is useful when we want to find the equation of a line given its slope and y-intercept.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear equations can have one or more variables, and they can have any number of terms. For example, the equation 2x + 3y - 4z = 5 is a linear equation because the highest power of any of the variables (x, y, and z) is 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;linear-function&#34;&gt;Linear function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A linear function is a function of the form f(x) = mx + b, where x is the input variable and f(x) is the output variable. The constants m and b are called the slope and y-intercept of the function, respectively. The slope is a measure of how steep the line described by the function is, and the y-intercept is the point where the line crosses the y-axis (the value of f(x) when x = 0).&lt;/li&gt;
&lt;li&gt;Linear functions have the property that the graph of the function is a straight line. The slope of the line is determined by the value of m, and the y-intercept is determined by the value of b. For example, the function f(x) = 2x + 1 has a slope of 2 and a y-intercept of (0, 1).&lt;/li&gt;
&lt;li&gt;Linear functions are useful in many applications because they are easy to work with and understand. They are also widely used in mathematics and science because they often provide a good approximation to real-world phenomena that exhibit linear behavior.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;linear-inequality&#34;&gt;Linear inequality&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A linear inequality is an inequality that involves a linear function. A linear function is a function of the form f(x) = mx + b, where m and b are constants and x is a variable. The graph of a linear inequality is a region of the coordinate plane that satisfies the inequality.&lt;/li&gt;
&lt;li&gt;Linear inequalities can be represented in one of two ways: in standard form or in slope-intercept form. In standard form, a linear inequality is written as:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ax + by &amp;gt; c

or

ax + by &amp;lt; c

or

ax + by ≥ c

or

ax + by ≤ c
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where a, b, and c are constants and x and y are variables. The standard form of a linear inequality is useful because it allows us to easily identify the slope and y-intercept of the line described by the inequality.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In slope-intercept form, a linear inequality is written as:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;y &amp;gt; mx + b

or

y &amp;lt; mx + b
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where m is the slope of the line and b is the y-intercept. This form of the inequality is useful when we want to find the inequality that defines a particular region of the coordinate plane.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The solution to a linear inequality is the set of all points that satisfy the inequality. The graph of a linear inequality is a region of the coordinate plane that includes all of the points that satisfy the inequality. The graph of a linear inequality is often represented by shading the region of the coordinate plane that satisfies the inequality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;linear-program&#34;&gt;Linear program&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A linear program (LP) is a mathematical optimization problem in which the objective function and the constraints are all linear. Linear programs are used to find the maximum or minimum value of a linear objective function subject to a set of linear inequality or equality constraints.&lt;/li&gt;
&lt;li&gt;Linear programs have the following general form:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;maximize c1x1 + c2x2 + ... + cnxn

subject to:
a11x1 + a12x2 + ... + a1nxn ≤ b1
a21x1 + a22x2 + ... + a2nxn ≤ b2
...
am1x1 + am2x2 + ... + amnxn ≤ bm
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where x1, x2, &amp;hellip;, xn are the decision variables, c1, c2, &amp;hellip;, cn are the objective coefficients, aij are the constraint coefficients, and b1, b2, &amp;hellip;, bm are the right-hand side values.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear programs can be solved using a variety of techniques, including simplex method, interior point method, and duality. These techniques are used to find the values of the decision variables that maximize or minimize the objective function subject to the constraints.&lt;/li&gt;
&lt;li&gt;Linear programs are widely used in a variety of fields, including economics, engineering, and operations research, to model and solve real-world problems involving optimization.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;local-optimummaximumminimum&#34;&gt;Local optimum/maximum/minimum&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A local optimum, maximum, or minimum is a point in a function where the function has a locally best value. In other words, it is a point where the function has a value that is better than the values of the function in the immediate vicinity of the point.&lt;/li&gt;
&lt;li&gt;For example, consider a function f(x) defined on the real numbers. If there exists a value x0 such that f(x0) is greater than or equal to f(x) for all x in a certain interval around x0, then x0 is a local maximum of the function. Similarly, if there exists a value x0 such that f(x0) is less than or equal to f(x) for all x in a certain interval around x0, then x0 is a local minimum of the function.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s important to note that a local optimum, maximum, or minimum is not necessarily the global optimum, maximum, or minimum of the function. The global optimum, maximum, or minimum is the point where the function has the best value over its entire domain. For example, if f(x) has a local maximum at x0, it does not necessarily mean that f(x0) is the highest possible value that the function can take on. It could be that there exists another point x1 where the function has an even higher value. In this case, x1 would be the global maximum of the function.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;louvain-algorithm&#34;&gt;Louvain algorithm&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Louvain algorithm is a fast and efficient method for community detection in large networks. It is a heuristic algorithm that is used to find the community structure of a network by optimizing a measure called modularity. Modularity is a measure of the quality of a partition of a network into communities, and it is defined as the fraction of the edges that fall within the communities minus the expected fraction of edges that fall within the communities in a random network with the same degree distribution as the original network.&lt;/li&gt;
&lt;li&gt;The Louvain algorithm operates in two phases. In the first phase, it starts with each node in its own community and iteratively merges pairs of communities based on the modularity gain of the merge. In the second phase, it aggregates the nodes in the same community into a supernode and repeats the process on the reduced network until the modularity cannot be improved further.&lt;/li&gt;
&lt;li&gt;The Louvain algorithm is fast and scalable, making it well-suited for large networks. It has been applied to a wide variety of networks, including social networks, biological networks, and transportation networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;markov-decision-process&#34;&gt;Markov decision process&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A Markov decision process (MDP) is a mathematical framework for modeling decision-making problems in which an agent must choose actions in a sequence of steps in order to maximize some reward. MDPs are used in many areas of artificial intelligence, including reinforcement learning, to solve problems involving optimization under uncertainty.&lt;/li&gt;
&lt;li&gt;An MDP is defined by a set of states, a set of actions, a transition model, and a reward function. The states represent the possible situations that the agent can be in. The actions represent the choices available to the agent at each step. The transition model specifies the probability of transitioning from one state to another as a result of taking a particular action. The reward function specifies the rewards that the agent receives for being in a particular state or taking a particular action.&lt;/li&gt;
&lt;li&gt;The goal of an MDP is to find a policy, which is a function that specifies the action to take in each state. The optimal policy is the policy that maximizes the expected cumulative reward over time. MDPs can be solved using various algorithms, such as value iteration, policy iteration, and Q-learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mathematical-programming&#34;&gt;Mathematical programming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Mathematical programming is a branch of applied mathematics that deals with the optimization of systems described by mathematical models. It is a broad field that encompasses a variety of optimization techniques, including linear programming, nonlinear programming, integer programming, and mixed-integer programming.&lt;/li&gt;
&lt;li&gt;The goal of mathematical programming is to find the values of the decision variables that optimize an objective function subject to a set of constraints. The objective function and the constraints are typically represented as a system of equations or inequalities that must be satisfied.&lt;/li&gt;
&lt;li&gt;Mathematical programming is used in a wide range of applications, including economics, engineering, and operations research. It is used to model and solve real-world problems involving the optimization of resources, such as time, money, and materials.&lt;/li&gt;
&lt;li&gt;There are many algorithms and software packages available for solving mathematical programming problems. These algorithms and software packages use a variety of techniques, such as linear algebra, gradient descent, and optimization algorithms, to find the optimal solution to the problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;maximization-problem&#34;&gt;Maximization problem&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A maximization problem is a type of optimization problem in which the goal is to find the maximum value of a function. Maximization problems are commonly encountered in a variety of fields, including economics, engineering, and operations research.&lt;/li&gt;
&lt;li&gt;A maximization problem is typically written in the form:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;maximize f(x)

subject to:
g1(x) ≤ b1
g2(x) ≤ b2
...
gn(x) ≤ bn
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where f(x) is the objective function to be maximized, g1(x), g2(x), &amp;hellip;, gn(x) are the constraint functions, and b1, b2, &amp;hellip;, bn are the right-hand side values. The variables x1, x2, &amp;hellip;, xn are the decision variables, and the values of these variables that maximize the objective function subject to the constraints are called the optimal solutions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are many algorithms and software packages available for solving maximization problems. These algorithms and software packages use a variety of techniques, such as linear algebra, gradient descent, and optimization algorithms, to find the optimal solution to the problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;maximum-flow-problem&#34;&gt;Maximum flow problem&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The maximum flow problem is a problem in graph theory that involves finding the maximum flow that can be sent through a network from a source to a sink. The problem can be formalized as follows: given a weighted directed graph with a source vertex s and a sink vertex t, find the maximum flow from s to t such that the flow on any edge does not exceed its capacity.&lt;/li&gt;
&lt;li&gt;The maximum flow problem is a fundamental problem in computer science and has numerous applications, including network design, transportation planning, and resource allocation. It is also closely related to the minimum cut problem, which involves finding the minimum-capacity cut that separates the source from the sink in the network.&lt;/li&gt;
&lt;li&gt;There are many algorithms for solving the maximum flow problem, including the Ford-Fulkerson algorithm and the Dinic algorithm. These algorithms are used to find the maximum flow through a network by iteratively increasing the flow along paths from the source to the sink until no additional flow is possible.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;minimization-problem&#34;&gt;Minimization problem&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A minimization problem is a type of optimization problem in which the goal is to find the minimum value of a function. Minimization problems are commonly encountered in a variety of fields, including economics, engineering, and operations research.&lt;/li&gt;
&lt;li&gt;A minimization problem is typically written in the form:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;minimize f(x)

subject to:
g1(x) ≤ b1
g2(x) ≤ b2
...
gn(x) ≤ bn
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where f(x) is the objective function to be minimized, g1(x), g2(x), &amp;hellip;, gn(x) are the constraint functions, and b1, b2, &amp;hellip;, bn are the right-hand side values. The variables x1, x2, &amp;hellip;, xn are the decision variables, and the values of these variables that minimize the objective function subject to the constraints are called the optimal solutions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are many algorithms and software packages available for solving minimization problems. These algorithms and software packages use a variety of techniques, such as linear algebra, gradient descent, and optimization algorithms, to find the optimal solution to the problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;modularity&#34;&gt;Modularity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Modularity is a measure of the quality of a partition of a network into communities. It is defined as the fraction of the edges that fall within the communities minus the expected fraction of edges that fall within the communities in a random network with the same degree distribution as the original network.&lt;/li&gt;
&lt;li&gt;Modularity is often used as a metric for evaluating the quality of community detection algorithms. It is a widely used measure in the field of network science and has been applied to a variety of real-world networks, including social networks, biological networks, and technological networks.&lt;/li&gt;
&lt;li&gt;Modularity is a useful measure because it captures the intuition that a good partition of a network into communities should have a higher density of edges within the communities than between the communities. A high value of modularity indicates that the communities in the partition are well-defined and distinct.&lt;/li&gt;
&lt;li&gt;There are many algorithms for optimizing modularity, including the Louvain algorithm and the spectral clustering algorithm. These algorithms are used to find the partition of a network into communities that maximizes the modularity.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;network&#34;&gt;Network&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A network is a group of interconnected entities or nodes. Networks can be found in many different contexts, such as social networks, transportation networks, and computer networks.&lt;/li&gt;
&lt;li&gt;In the context of social networks, a network is a group of people who are connected to each other by some type of relationship, such as friendship, kinship, or professional association. In transportation networks, a network is a group of locations connected by transportation links, such as roads, railways, or air routes. In computer networks, a network is a group of computers and other devices connected by communication channels, such as cables or wireless connections, for the purpose of exchanging data.&lt;/li&gt;
&lt;li&gt;Networks can be represented using a graph, which is a mathematical structure consisting of vertices (also called nodes) and edges. The nodes represent the entities in the network, and the edges represent the relationships or connections between the entities.&lt;/li&gt;
&lt;li&gt;Networks are often analyzed using techniques from graph theory and network science. These techniques are used to study the structure and properties of networks, such as the number of connections per node, the connectivity of the network, and the centrality of the nodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;network-optimization-problem&#34;&gt;Network optimization problem&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A network is a group of interconnected entities or nodes. Networks can be found in many different contexts, such as social networks, transportation networks, and computer networks.&lt;/li&gt;
&lt;li&gt;In the context of social networks, a network is a group of people who are connected to each other by some type of relationship, such as friendship, kinship, or professional association. In transportation networks, a network is a group of locations connected by transportation links, such as roads, railways, or air routes. In computer networks, a network is a group of computers and other devices connected by communication channels, such as cables or wireless connections, for the purpose of exchanging data.&lt;/li&gt;
&lt;li&gt;Networks can be represented using a graph, which is a mathematical structure consisting of vertices (also called nodes) and edges. The nodes represent the entities in the network, and the edges represent the relationships or connections between the entities.&lt;/li&gt;
&lt;li&gt;Networks are often analyzed using techniques from graph theory and network science. These techniques are used to study the structure and properties of networks, such as the number of connections per node, the connectivity of the network, and the centrality of the nodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;node&#34;&gt;Node&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of a graph, a node is a vertex or a point that represents an entity or an object in the graph. Nodes are typically represented by circles or points in a graph, and they are connected to other nodes by edges.&lt;/li&gt;
&lt;li&gt;In the context of a tree, a node is a point at which one or more branches originate. The top node in a tree is called the root, and the nodes that do not have any children are called leaf nodes.&lt;/li&gt;
&lt;li&gt;In the context of a network, a node is a device or a point in the network that is connected to other nodes by communication links. Nodes in a network can be computers, routers, switches, or any other device that is capable of sending and receiving data.&lt;/li&gt;
&lt;li&gt;In the context of a graph or network, nodes can have attributes, such as a label or a weight, which describe the properties of the node. The structure and properties of nodes in a graph or network are often studied using techniques from graph theory and network science.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;non-convex-program&#34;&gt;Non-convex program&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A non-convex program is a type of optimization problem in which the objective function or the constraint functions are not convex. A convex function is a function that satisfies the property of convexity, which means that the line connecting any two points on the graph of the function lies above the graph. A non-convex function is a function that does not satisfy this property.&lt;/li&gt;
&lt;li&gt;Non-convex programs are more difficult to solve than convex programs because they may have multiple local optima, rather than a single global optimum. This means that there may be multiple points that are locally optimal, but the globally optimal solution may not be attainable by starting from any of these local optima.&lt;/li&gt;
&lt;li&gt;Non-convex programs can be solved using a variety of techniques, including local search algorithms, global search algorithms, and gradient-based algorithms. These algorithms are used to find the optimal solution to the problem by exploring the solution space and searching for points that improve the objective function.&lt;/li&gt;
&lt;li&gt;Examples of non-convex programs include nonlinear programming, integer programming, and mixed-integer programming. Non-convex programs are useful for modeling and solving real-world problems involving the optimization of resources, such as time, money, and materials.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;non-negativity-constraints&#34;&gt;Non-negativity constraints&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Non-negativity constraints are constraints that require the decision variables in an optimization problem to be non-negative. In other words, the decision variables are required to be greater than or equal to zero.&lt;/li&gt;
&lt;li&gt;Non-negativity constraints are common in optimization problems because many real-world problems involve quantities that cannot be negative, such as the number of items produced, the amount of money spent, or the volume of a fluid.&lt;/li&gt;
&lt;li&gt;Non-negativity constraints are typically written in the form:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;x1 ≥ 0
x2 ≥ 0
...
xn ≥ 0

where x1, x2, ..., xn are the decision variables.

Non-negativity constraints can be incorporated into an optimization problem by adding them as inequality constraints to the problem. For example, consider the following linear programming problem:

maximize c1x1 + c2x2 + ... + cnxn

subject to:
a11x1 + a12x2 + ... + a1nxn ≤ b1
a21x1 + a22x2 + ... + a2nxn ≤ b2
...
am1x1 + am2x2 + ... + amnxn ≤ bm
x1 ≥ 0
x2 ≥ 0
...
xn ≥ 0
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;In this problem, the non-negativity constraints x1 ≥ 0, x2 ≥ 0, &amp;hellip;, xn ≥ 0 ensure that the decision variables are non-negative.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;objective-function&#34;&gt;Objective function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In optimization, an objective function is a function that represents the goal of the optimization. The goal of the optimization is to find the values of the decision variables that either maximize or minimize the objective function.&lt;/li&gt;
&lt;li&gt;The objective function is typically written as a mathematical expression that depends on the decision variables. For example, in a linear programming problem, the objective function is a linear function of the decision variables. In a nonlinear programming problem, the objective function may be a nonlinear function of the decision variables.&lt;/li&gt;
&lt;li&gt;The objective function is typically written in the form:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;f(x1, x2, ..., xn)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where x1, x2, &amp;hellip;, xn are the decision variables.
-`The objective function is an important component of an optimization problem because it determines the goal of the optimization. The values of the decision variables that optimize the objective function are called the optimal solutions.&lt;/p&gt;
&lt;h3 id=&#34;optimal-solution&#34;&gt;Optimal solution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In optimization, an optimal solution is a set of values for the decision variables that either maximizes or minimizes the objective function. The objective function is a mathematical expression that represents the goal of the optimization, and the decision variables are the variables that are being optimized.&lt;/li&gt;
&lt;li&gt;The optimal solution to an optimization problem is the solution that satisfies all of the constraints of the problem and either maximizes or minimizes the objective function, depending on the type of optimization problem.&lt;/li&gt;
&lt;li&gt;There may be multiple optimal solutions to an optimization problem, or there may be none. If there are multiple optimal solutions, the problem is said to have multiple optima. If there are no optimal solutions, the problem is said to be infeasible.&lt;/li&gt;
&lt;li&gt;The optimal solution to an optimization problem can be found using a variety of algorithms and techniques, depending on the specific problem and the structure of the objective function and constraints. These techniques may include linear programming, nonlinear programming, and heuristics.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;optimization-1&#34;&gt;Optimization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Optimization is the process of finding the best solution to a problem among a set of possible solutions. Optimization problems are common in many fields, including economics, engineering, and operations research.&lt;/li&gt;
&lt;li&gt;Optimization problems can be classified into several categories based on the type of objective function and the type of constraints. For example, linear programming involves optimizing a linear objective function subject to linear constraints, while nonlinear programming involves optimizing a nonlinear objective function subject to nonlinear constraints.&lt;/li&gt;
&lt;li&gt;The goal of optimization is to find the values of the decision variables that either maximize or minimize the objective function subject to the constraints of the problem. The values of the decision variables that optimize the objective function are called the optimal solutions.&lt;/li&gt;
&lt;li&gt;There are many algorithms and techniques for solving optimization problems, including linear programming, nonlinear programming, integer programming, and heuristics. These algorithms and techniques use a variety of approaches, such as linear algebra, gradient descent, and optimization algorithms, to find the optimal solution to the problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;robust-solution&#34;&gt;Robust solution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A robust solution is a solution that is resistant to changes in the input data or assumptions of the problem. In other words, a robust solution is a solution that performs well under a wide range of conditions or scenarios.&lt;/li&gt;
&lt;li&gt;Robust solutions are often desired in optimization problems because real-world problems often involve uncertainty or variability in the input data or assumptions. A robust solution is able to withstand such uncertainty or variability and still produce good results.&lt;/li&gt;
&lt;li&gt;There are several ways to design robust solutions in optimization. One approach is to use robust optimization, which is a methodology that seeks to find solutions that are robust to uncertainty in the input data. Robust optimization involves optimizing an objective function that is a function of both the decision variables and the uncertain parameters, subject to constraints on both the decision variables and the uncertain parameters.&lt;/li&gt;
&lt;li&gt;Another approach is to use sensitivity analysis to identify the key parameters or assumptions that have the greatest impact on the solution, and to design the solution in a way that is insensitive to variations in these parameters. This can be done by using techniques such as scenario analysis, which involves analyzing the solution for a range of different scenarios.&lt;/li&gt;
&lt;li&gt;Robust solutions are useful for modeling and solving real-world problems because they are able to perform well under a wide range of conditions and uncertainties.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;shortest-path-problem&#34;&gt;Shortest path problem&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The shortest path problem is a problem in graph theory that involves finding the shortest path between two nodes in a graph. The shortest path is the path with the minimum number of edges or the minimum distance between the two nodes.&lt;/li&gt;
&lt;li&gt;The shortest path problem is a fundamental problem in computer science and has numerous applications, including network design, transportation planning, and resource allocation. It is closely related to the minimum spanning tree problem, which involves finding the minimum set of edges that connects all of the nodes in a graph.&lt;/li&gt;
&lt;li&gt;There are many algorithms for solving the shortest path problem, including Dijkstra&amp;rsquo;s algorithm and the A* algorithm. These algorithms are used to find the shortest path through a graph by exploring the edges of the graph and updating the shortest known distance to each node as the algorithm progresses.&lt;/li&gt;
&lt;li&gt;The shortest path problem can be generalized to include additional constraints or objectives, such as finding the shortest path with the minimum number of edges, the minimum cost, or the minimum time. These variations of the shortest path problem are known as the single-source shortest path problem, the single-pair shortest path problem, and the all-pairs shortest path problem, respectively.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solution-in-the-optimization-sense&#34;&gt;Solution (in the optimization sense)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In optimization, a solution is a set of values for the decision variables that satisfies the constraints of the problem. The decision variables are the variables that are being optimized, and the constraints are the limitations or requirements that must be satisfied in the solution.&lt;/li&gt;
&lt;li&gt;The solution to an optimization problem is the set of values for the decision variables that either maximizes or minimizes the objective function, depending on the type of optimization problem. The objective function is a mathematical expression that represents the goal of the optimization.&lt;/li&gt;
&lt;li&gt;There may be multiple solutions to an optimization problem, or there may be none. If there are multiple solutions, the problem is said to have multiple optima. If there are no solutions, the problem is said to be infeasible.&lt;/li&gt;
&lt;li&gt;The solution to an optimization problem can be found using a variety of algorithms and techniques, depending on the specific problem and the structure of the objective function and constraints. These techniques may include linear programming, nonlinear programming, and heuristics.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;state&#34;&gt;State&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of optimization, a state is a set of values for the variables that defines the current configuration of the system being optimized. The variables in the state may include the decision variables, which are the variables that are being optimized, as well as other variables that describe the system, such as the state variables and the parameters.&lt;/li&gt;
&lt;li&gt;In the context of a dynamic optimization problem, the state at a given time represents the configuration of the system at that time. The state of the system at each time point is typically represented by a vector of variables, and the evolution of the state over time is described by a system of differential equations or a difference equation.&lt;/li&gt;
&lt;li&gt;In the context of a discrete optimization problem, the state at a given time represents the configuration of the system at that time, and the state at each time point is typically represented by a vector of variables. The state of the system evolves over time as the decision variables are updated according to the optimization algorithm.&lt;/li&gt;
&lt;li&gt;The state of the system plays an important role in optimization because it determines the objective function and the constraints of the problem. The optimal solution to the optimization problem is the set of values for the decision variables that either maximizes or minimizes the objective function subject to the constraints of the problem, given the current state of the system.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;step-size&#34;&gt;Step size&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In optimization, the step size is a parameter that determines the size of the steps taken by an optimization algorithm as it searches for the optimal solution to a problem. The step size is often used in gradient-based optimization algorithms, such as gradient descent and stochastic gradient descent, which use the gradient of the objective function to guide the search for the optimal solution.&lt;/li&gt;
&lt;li&gt;The step size plays a crucial role in the convergence of the optimization algorithm. If the step size is too small, the algorithm may take a long time to converge to the optimal solution. If the step size is too large, the algorithm may overshoot the optimal solution or even diverge.&lt;/li&gt;
&lt;li&gt;There are several approaches to setting the step size in an optimization algorithm. One approach is to use a fixed step size, which is a constant value that is chosen manually or based on some heuristics. Another approach is to use a variable step size, which is a step size that changes over the course of the optimization. Variable step sizes can be determined using techniques such as line search or trust region methods.&lt;/li&gt;
&lt;li&gt;The step size is an important hyperparameter in optimization algorithms and can have a significant impact on the performance of the algorithm. It is important to choose an appropriate step size for the specific optimization problem and the specific optimization algorithm being used.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;stochastic-dynamic-program&#34;&gt;Stochastic dynamic program&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A stochastic dynamic program (SDP) is a type of optimization problem that involves finding the optimal decision rule for a system that evolves over time in the presence of uncertainty. SDPs are used to model and solve problems in which the future states of the system are uncertain and depend on both the current state of the system and the actions taken by the decision-maker.&lt;/li&gt;
&lt;li&gt;In an SDP, the decision variables are the actions that are taken at each time point, and the objective is to maximize or minimize a function of the actions and the future states of the system. The constraints of the problem may include both state constraints, which limit the possible values of the future states, and action constraints, which limit the possible values of the actions.&lt;/li&gt;
&lt;li&gt;SDPs are solved using dynamic programming algorithms, which involve breaking the optimization problem into smaller subproblems and solving these subproblems recursively. The solution to the SDP is the optimal decision rule, which is a function that maps the current state of the system to the optimal action.&lt;/li&gt;
&lt;li&gt;SDPs are useful for modeling and solving real-world problems involving the optimization of resources over time in the presence of uncertainty, such as resource allocation problems and risk management problems.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;stochastic-optimization&#34;&gt;Stochastic optimization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stochastic optimization is a type of optimization that involves finding the optimal solution to a problem in the presence of uncertainty. Stochastic optimization problems are characterized by randomness or uncertainty in the input data or assumptions of the problem.&lt;/li&gt;
&lt;li&gt;In stochastic optimization, the objective is to find the optimal solution that is robust to the uncertainty or variability in the input data. This is often achieved by minimizing the expected value of the objective function, which is the average value of the objective function over the distribution of the uncertain parameters.&lt;/li&gt;
&lt;li&gt;Stochastic optimization can be used to solve a variety of problems, including resource allocation problems, portfolio optimization problems, and risk management problems.&lt;/li&gt;
&lt;li&gt;There are many algorithms and techniques for solving stochastic optimization problems, including stochastic gradient descent, Monte Carlo simulation, and dynamic programming. These algorithms and techniques use a variety of approaches, such as sampling and statistical techniques, to find the optimal solution to the problem.&lt;/li&gt;
&lt;li&gt;Stochastic optimization is useful for modeling and solving real-world problems because it allows for the incorporation of uncertainty or variability into the optimization process. This is particularly important in situations where the input data or assumptions of the problem are uncertain or subject to change.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;uncertainty&#34;&gt;Uncertainty&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Uncertainty in optimization refers to the presence of randomness or variability in the input data or assumptions of an optimization problem. Uncertainty can arise in many forms, such as random errors in the data, unknown parameters, or stochastic processes.&lt;/li&gt;
&lt;li&gt;Uncertainty can be incorporated into an optimization problem in several ways. One approach is to use stochastic optimization, which is a type of optimization that involves finding the optimal solution to a problem in the presence of uncertainty. In stochastic optimization, the objective is to find the optimal solution that is robust to the uncertainty or variability in the input data. This is often achieved by minimizing the expected value of the objective function, which is the average value of the objective function over the distribution of the uncertain parameters.&lt;/li&gt;
&lt;li&gt;Another approach is to use robust optimization, which is a methodology that seeks to find solutions that are robust to uncertainty in the input data. Robust optimization involves optimizing an objective function that is a function of both the decision variables and the uncertain parameters, subject to constraints on both the decision variables and the uncertain parameters.&lt;/li&gt;
&lt;li&gt;Uncertainty can be a challenging aspect of optimization because it can make it difficult to predict the behavior of the optimization problem and to determine the optimal solution. However, accounting for uncertainty in the optimization process can be important for modeling and solving real-world problems because it allows for the incorporation of variability and randomness into the optimization process.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;vertex&#34;&gt;Vertex&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In optimization, a vertex is a point on the feasible region of an optimization problem that satisfies all of the constraints of the problem. The feasible region is the set of points that satisfy the constraints of the problem, and a vertex is a point on the boundary of the feasible region.&lt;/li&gt;
&lt;li&gt;In a linear programming problem, the feasible region is a polyhedron, and the vertices are the points where the constraints intersect. The optimal solution to the linear programming problem is either a vertex of the feasible region or a point on a constraint that is not a vertex.&lt;/li&gt;
&lt;li&gt;In a nonlinear programming problem, the feasible region is a more complex shape, and the vertices may or may not be part of the optimal solution. The optimal solution to a nonlinear programming problem is typically found using algorithms such as gradient descent or conjugate gradient, which search for the optimal solution by moving from one point to another along the feasible region.&lt;/li&gt;
&lt;li&gt;The vertices of the feasible region are important in optimization because they represent the extreme points of the region, and the optimal solution may be found at a vertex or along a constraint. Understanding the structure of the feasible region and the location of the vertices can be helpful for solving optimization problems.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;probability-based-models&#34;&gt;Probability based models&lt;/h2&gt;
&lt;h3 id=&#34;action&#34;&gt;Action&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In probability-based models, an action is a decision or course of action that is taken by a decision-maker in a given situation. The action is chosen based on the available information and the objectives of the decision-maker.&lt;/li&gt;
&lt;li&gt;In probability-based models, the action is typically represented by a random variable, which is a variable that represents the possible outcomes of the action. The probability of each outcome is determined by the information available to the decision-maker and the objectives of the decision.&lt;/li&gt;
&lt;li&gt;Probability-based models are used in many fields, including economics, finance, and operations research, to model and solve decision-making problems involving uncertainty or risk. These models are used to determine the optimal action in a given situation, given the available information and the objectives of the decision-maker.&lt;/li&gt;
&lt;li&gt;Examples of probability-based models include decision trees, Markov decision processes, and Bayesian networks. These models are used to represent the uncertain outcomes of the action and to determine the optimal action based on the probabilities of the outcomes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;arrival-rate&#34;&gt;Arrival rate&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The arrival rate is a measure of the frequency at which events or customers arrive at a system or service. In the context of queueing theory, the arrival rate is the rate at which customers arrive at a service or queue, and is typically measured in units of time, such as customers per minute or customers per hour.&lt;/li&gt;
&lt;li&gt;The arrival rate is an important parameter in queueing models because it determines the number of customers that are waiting to be served at a given time. The arrival rate is often used in conjunction with other parameters, such as the service rate, to analyze the performance of a queueing system.&lt;/li&gt;
&lt;li&gt;The arrival rate can be constant or variable, depending on the nature of the system being analyzed. For example, in a service system with a fixed arrival rate, the number of customers arriving at the system is constant over time. In a system with a variable arrival rate, the number of customers arriving at the system may vary over time.&lt;/li&gt;
&lt;li&gt;The arrival rate can be estimated using historical data or by analyzing the characteristics of the system or the customers. The arrival rate is an important factor in the design and analysis of queueing systems and is used to determine the capacity and performance of the system.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;balking&#34;&gt;Balking&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of queueing theory, balking refers to the behavior of customers who decide not to join a queue or wait for service when confronted with a long wait. Customers may balk for a variety of reasons, such as time constraints, impatience, or dissatisfaction with the service.&lt;/li&gt;
&lt;li&gt;Balking is an important consideration in the analysis and design of queueing systems because it can have a significant impact on the performance of the system. When customers balk, the system experiences a reduction in the number of customers being served, which can affect the utilization and efficiency of the system.&lt;/li&gt;
&lt;li&gt;Balking can be modeled using a balking function, which is a function that describes the probability that a customer will balk as a function of the waiting time or the number of customers in the queue. The balking function can be used to predict the impact of balking on the performance of the queueing system and to identify strategies for reducing the rate of balking.&lt;/li&gt;
&lt;li&gt;Balking is an important factor in the analysis of queueing systems and is often taken into account in the design of service systems to ensure that the system is efficient and effective in serving customers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bayes-theorembayes-rule&#34;&gt;Bayes&amp;rsquo; theorem/Bayes&amp;rsquo; rule&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bayes&amp;rsquo; theorem is a fundamental principle in probability theory that describes the relationship between the probability of an event and the probability of other related events. It is used to calculate the probability of an event based on the probability of other events that are related to it.&lt;/li&gt;
&lt;li&gt;The theorem is named after Thomas Bayes, an 18th-century mathematician and statistician who developed the theorem to describe the probability of an event based on prior knowledge or evidence.&lt;/li&gt;
&lt;li&gt;Bayes&amp;rsquo; theorem is often expressed as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;P(A|B) = (P(B|A) * P(A)) / P(B)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where P(A|B) is the conditional probability of event A given event B, P(B|A) is the conditional probability of event B given event A, P(A) is the probability of event A, and P(B) is the probability of event B.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bayes&amp;rsquo; theorem is used in many fields, including statistics, machine learning, and artificial intelligence, to update the probability of an event based on new evidence or information. It is a powerful tool for making decisions under uncertainty and is widely used in statistical analysis and data modeling.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;continuous-time-simulation&#34;&gt;Continuous-time simulation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Continuous-time simulation is a type of simulation in which the system being simulated is represented by a set of continuous variables that change over time. Continuous-time simulation is used to model and analyze systems in which the state of the system is continuously changing, such as physical systems, chemical processes, and biological systems.&lt;/li&gt;
&lt;li&gt;In continuous-time simulation, the evolution of the system over time is typically described using differential equations, which are mathematical equations that describe the rate of change of a variable with respect to time. The differential equations are used to compute the values of the variables at each time step, and the simulation is run for a specified period of time.&lt;/li&gt;
&lt;li&gt;Continuous-time simulation is useful for modeling and analyzing systems that involve continuous processes or phenomena, such as fluid flow, heat transfer, and chemical reactions. It is also useful for analyzing the behavior of systems over long periods of time, as it allows for the modeling of small changes in the system that may have significant impacts over time&lt;/li&gt;
&lt;li&gt;Continuous-time simulation is a powerful tool for understanding and predicting the behavior of complex systems and is used in a variety of fields, including engineering, science, and business.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;decision-point&#34;&gt;Decision point&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A decision point is a point in time at which a decision must be made. Decision points are often encountered in decision-making processes and can involve choices between different options or courses of action.&lt;/li&gt;
&lt;li&gt;Decision points can occur at various stages of a process or in different contexts, such as business, finance, or personal decision-making. In many cases, decision points involve a trade-off between different objectives or conflicting goals, and the decision must be made based on the available information and the desired outcomes.&lt;/li&gt;
&lt;li&gt;Decision points can be modeled and analyzed using decision analysis techniques, such as decision trees, utility analysis, and decision tables. These techniques are used to evaluate the different options and to identify the optimal decision based on the desired outcomes and the probabilities or consequences of each option.&lt;/li&gt;
&lt;li&gt;Decision points are an important aspect of decision-making and can have significant consequences on the outcomes of a process or the overall success of an endeavor. It is important to carefully consider the options and to make informed decisions at decision points in order to achieve the desired outcomes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deterministic-simulation&#34;&gt;Deterministic simulation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Deterministic simulation is a type of simulation in which the system being simulated is represented by a set of fixed, known variables, and the evolution of the system over time is determined by the values of these variables. Deterministic simulation is used to model and analyze systems in which the behavior of the system is completely determined by the initial conditions and the underlying rules or laws governing the system.&lt;/li&gt;
&lt;li&gt;In deterministic simulation, the values of the variables are known with certainty, and the evolution of the system is determined by the values of these variables and the rules governing the system. The simulation is run for a specified period of time, and the values of the variables are computed at each time step based on the rules of the system.&lt;/li&gt;
&lt;li&gt;Deterministic simulation is useful for modeling and analyzing systems that are well-understood and can be accurately represented by a set of fixed variables, such as physical systems, chemical processes, and mathematical models. It is also useful for analyzing the behavior of systems over short periods of time, as it does not allow for the modeling of randomness or uncertainty.&lt;/li&gt;
&lt;li&gt;Deterministic simulation is a powerful tool for understanding and predicting the behavior of simple systems and is used in a variety of fields, including engineering, science, and business.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;discrete-event-simulation&#34;&gt;Discrete-event simulation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Discrete-event simulation is a type of simulation in which the system being simulated is represented by a set of discrete variables that change at specific points in time. Discrete-event simulation is used to model and analyze systems in which the state of the system changes in discrete steps or events, such as manufacturing systems, computer networks, and business processes.&lt;/li&gt;
&lt;li&gt;In discrete-event simulation, the evolution of the system over time is represented by a series of discrete events, such as the arrival of a customer at a service system or the completion of a manufacturing process. The simulation is run for a specified period of time, and the events are scheduled and executed at specific times based on the rules of the system.&lt;/li&gt;
&lt;li&gt;Discrete-event simulation is useful for modeling and analyzing systems that involve discrete events or processes, such as manufacturing systems, transportation systems, and communication networks. It is also useful for analyzing the behavior of systems over short periods of time, as it allows for the modeling of complex interactions between the events.&lt;/li&gt;
&lt;li&gt;Discrete-event simulation is a powerful tool for understanding and predicting the behavior of complex systems and is used in a variety of fields, including engineering, science, and business.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;empirical-bayes-model&#34;&gt;Empirical Bayes model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An empirical Bayes model is a statistical model that uses observed data to estimate the parameters of a Bayesian model. Bayesian models are a type of statistical model that involves the use of prior knowledge or assumptions to make inferences about the probability of an event. The parameters of a Bayesian model represent the degree of belief or uncertainty about the event.&lt;/li&gt;
&lt;li&gt;In an empirical Bayes model, the parameters of the model are estimated from observed data rather than being specified a priori. This allows the model to adapt to the data and to make more accurate predictions about the probability of the event.&lt;/li&gt;
&lt;li&gt;Empirical Bayes models are used in a variety of fields, including statistics, machine learning, and artificial intelligence, to estimate the parameters of Bayesian models and to make predictions about the probability of an event. They are particularly useful for making predictions in situations where there is limited prior knowledge or data about the event.&lt;/li&gt;
&lt;li&gt;Empirical Bayes models are a powerful tool for modeling and analyzing data and are used in a wide range of applications, including risk assessment, resource allocation, and decision-making under uncertainty.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;entity&#34;&gt;Entity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In probability-based models, an entity refers to an object or concept that can be described or represented by a set of characteristics or variables. In statistical modeling, an entity can be a person, a group, an event, or any other thing that can be represented by data.&lt;/li&gt;
&lt;li&gt;The probability of an event or outcome is often calculated based on the characteristics or variables associated with the entity. For example, in a medical study, the entity might be a patient, and the probability of the patient experiencing a certain outcome might be calculated based on factors such as age, gender, and medical history.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;fifo&#34;&gt;FIFO&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;FIFO stands for &amp;ldquo;first-in, first-out.&amp;rdquo; In probability-based models, FIFO is a queueing discipline that refers to the way in which items or entities are processed or served. Under a FIFO system, the first item that enters the queue is the first one to be served or processed. This is in contrast to other queueing disciplines, such as LIFO (last-in, first-out) or priority-based systems, in which the order of service or processing is determined by some other criterion.&lt;/li&gt;
&lt;li&gt;In probability models, FIFO systems are often used to model real-world situations in which items are processed in the order in which they arrive. For example, a FIFO system might be used to model the way in which customers are served in a bank or a grocery store, where the first customer in line is the first one to be assisted. The probability of a customer being served within a certain time period might be calculated based on the number of customers already in the queue and the rate at which they are being served.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;interarrival-time&#34;&gt;Interarrival time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In probability and statistics, interarrival time refers to the time that elapses between the arrival of successive entities at a particular location or system. For example, in a queueing system, the interarrival time is the time between the arrival of successive customers. In a communication network, the interarrival time is the time between the arrival of successive packets of data.&lt;/li&gt;
&lt;li&gt;Interarrival times are often modeled in probability-based systems in order to understand and predict the flow of entities through the system. For example, in a queueing system, the interarrival times of customers might be modeled in order to understand the workload on the system and predict how long customers will have to wait before being served. In a communication network, interarrival times might be modeled in order to understand the capacity of the network and predict how long it will take for data to be transmitted.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kendall-notation&#34;&gt;Kendall notation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Kendall notation is a formal way of describing the relationships between entities in a system. It is used to model the behavior of systems in fields such as computer science, engineering, and operations research.&lt;/li&gt;
&lt;li&gt;In Kendall notation, a system is represented by a graph, with the entities in the system represented as nodes, and the relationships between the entities represented as edges. The graph is then annotated with labels that describe the type of relationship that exists between the entities.&lt;/li&gt;
&lt;li&gt;There are several types of labels that can be used in Kendall notation, including:
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;m&amp;rdquo; for a mutual relationship between two entities, in which each entity has an effect on the other&lt;/li&gt;
&lt;li&gt;&amp;ldquo;o&amp;rdquo; for an one-way relationship, in which one entity has an effect on the other but not vice versa&lt;/li&gt;
&lt;li&gt;&amp;ldquo;r&amp;rdquo; for a reciprocal relationship, in which two entities have an effect on each other but the effect is not necessarily equal&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Kendall notation is a useful tool for understanding and analyzing the behavior of complex systems. It can be used to identify and model the relationships between different entities in a system, and to analyze how changes in one part of the system may affect other parts of the system.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;lifo&#34;&gt;LIFO&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;LIFO is an acronym for Last In, First Out, and is a method of organizing and manipulating data in a data structure such as a stack or queue.&lt;/li&gt;
&lt;li&gt;In a LIFO data structure, the most recent item added to the structure is the first one to be removed. This is in contrast to a FIFO (First In, First Out) data structure, in which the first item added is the first one to be removed.&lt;/li&gt;
&lt;li&gt;LIFO data structures are often used in computing and programming because they are simple to implement and can be manipulated quickly and efficiently. An example of a LIFO data structure is a stack, which is a list of items that are added and removed in a specific order. When an item is added to the top of a stack, it is said to be &amp;ldquo;pushed&amp;rdquo; onto the stack. When an item is removed from the top of the stack, it is said to be &amp;ldquo;popped&amp;rdquo; off the stack.&lt;/li&gt;
&lt;li&gt;LIFO data structures have a number of applications, including implementing undo/redo functions in software, evaluating mathematical expressions, and implementing memory allocators in operating systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;markov-chain&#34;&gt;Markov chain&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A Markov chain is a mathematical system that undergoes transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the system arrived at its current state, the possible future states are fixed. In other words, the probability of transitioning to any particular state is dependent only on the current state and time elapsed.&lt;/li&gt;
&lt;li&gt;A Markov chain can be represented as a directed graph, with the edges representing the probability of transitioning from one state to another. The nodes of the graph represent the states of the system, and the edges are labeled with the probabilities of transitioning between the states.&lt;/li&gt;
&lt;li&gt;Markov chains are used to model a wide variety of systems in which the future state of the system is dependent only on the current state, including processes that involve randomness, such as the spread of disease, the movement of financial markets, and the analysis of computer algorithms. They are also used in the study of animal behavior, linguistics, and other fields.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;queue&#34;&gt;Queue&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of a probability-based model, a queue is a system in which items arrive at a certain rate and are processed or served at a different rate. The items that arrive and are waiting to be processed form a queue.&lt;/li&gt;
&lt;li&gt;Queueing theory is a branch of mathematics that studies the behavior of queues and the systems that create them. It is used to model and analyze the performance of systems that involve waiting in line, such as call centers, computer networks, and manufacturing systems.&lt;/li&gt;
&lt;li&gt;In a queueing model, the arrival rate of items and the processing rate of the system are important factors that determine the behavior of the queue. If the arrival rate is higher than the processing rate, the queue will grow over time, leading to an increase in waiting time for items. If the processing rate is higher than the arrival rate, the queue will shrink over time.&lt;/li&gt;
&lt;li&gt;Queueing models can be used to analyze the performance of a system and to make predictions about the behavior of the queue under different conditions. They can also be used to identify bottlenecks in a system and to optimize the performance of the system by adjusting the arrival rate and the processing rate.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;service-rate&#34;&gt;Service rate&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In a probability model, the service rate refers to the rate at which a system is able to process or serve items. The service rate is an important factor that determines the behavior of a queue or other system in which items are waiting to be processed.&lt;/li&gt;
&lt;li&gt;In a queueing model, the service rate is typically represented as the average number of items that the system is able to process per unit of time. It is used to calculate the expected waiting time for items in the queue, as well as the probability of the queue being empty or full at a given time.&lt;/li&gt;
&lt;li&gt;The service rate is often influenced by factors such as the number of servers or processing units available, the efficiency of the processing units, and the complexity of the tasks being performed. By adjusting the service rate, it is possible to optimize the performance of a system and to reduce the waiting time for items in the queue.&lt;/li&gt;
&lt;li&gt;The service rate is typically contrasted with the arrival rate, which is the rate at which items arrive at the system and enter the queue. The relationship between the service rate and the arrival rate determines the behavior of the queue and the expected waiting time for items.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;simulation&#34;&gt;Simulation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Simulation is the process of creating a model of a real-world system or process, and using it to predict the behavior of the system over time. Simulations are used in a wide variety of fields, including engineering, computer science, economics, and the natural sciences, to study and analyze the behavior of complex systems.&lt;/li&gt;
&lt;li&gt;There are several types of simulations, including:
&lt;ul&gt;
&lt;li&gt;Discrete event simulation: This type of simulation models the behavior of systems that change state at discrete points in time, such as a manufacturing system or a computer network.&lt;/li&gt;
&lt;li&gt;Continuous simulation: This type of simulation models the behavior of systems that change continuously over time, such as a chemical reaction or a mechanical system.&lt;/li&gt;
&lt;li&gt;Monte Carlo simulation: This type of simulation uses random numbers to model the behavior of systems that involve uncertainty, such as financial markets or weather patterns.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Simulations can be used to study the behavior of a system under different conditions, to optimize the performance of a system, and to make predictions about the behavior of the system in the future. They are often used in conjunction with other analytical and mathematical techniques to study complex systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;steady-state&#34;&gt;Steady state&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In a system or process, the steady state is a condition in which the system is in a stable, equilibrium state and is not changing over time. In other words, the system has reached a state of balance and is no longer undergoing significant changes.&lt;/li&gt;
&lt;li&gt;The concept of steady state is used in a variety of fields, including physics, engineering, economics, and biology. In physics and engineering, the steady state is often used to describe systems that are in a state of equilibrium and are not undergoing any net change, such as a fluid flowing through a pipe at a constant rate. In economics, the steady state is often used to describe the long-term equilibrium of an economy, in which the growth rate of the economy is constant and there is no net increase in the capital stock.&lt;/li&gt;
&lt;li&gt;The concept of steady state is often contrasted with the concept of transient state, which refers to a temporary condition in which a system is changing and is not yet in a stable equilibrium. The process of reaching the steady state from a transient state is known as relaxation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;stochastic-simulation&#34;&gt;Stochastic simulation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stochastic simulation is a type of simulation that involves the use of random numbers or probabilities to model the behavior of a system or process. It is used to study systems that involve uncertainty or randomness, such as financial markets, weather patterns, and biological systems.&lt;/li&gt;
&lt;li&gt;In a stochastic simulation, the system or process being studied is represented by a set of rules or equations that describe how the system changes over time. These rules may include probabilistic elements, such as the probability of a certain event occurring or the probability distribution of certain variables. The simulation is then run by randomly generating values for the variables and using them to update the state of the system at each time step.&lt;/li&gt;
&lt;li&gt;Stochastic simulation is often used in conjunction with other analytical and mathematical techniques, such as statistical analysis and optimization, to study complex systems. It can be used to study the behavior of a system under different conditions, to optimize the performance of a system, and to make predictions about the behavior of the system in the future.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;transition-matrix&#34;&gt;Transition matrix&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A transition matrix is a matrix that is used to describe the transitions between different states in a Markov process. In a Markov process, a system moves from one state to another according to certain probabilistic rules, and the transition matrix specifies the probability of transitioning from one state to another.&lt;/li&gt;
&lt;li&gt;The elements of a transition matrix are the probabilities of transitioning between states. The rows of the matrix represent the starting states, and the columns represent the ending states. The element in the i-th row and j-th column of the matrix represents the probability of transitioning from the i-th state to the j-th state.&lt;/li&gt;
&lt;li&gt;Transition matrices are used in a variety of fields, including engineering, computer science, and economics, to model and analyze the behavior of systems that change over time. They are commonly used in the analysis of Markov processes, which are systems in which the future state of the system is determined only by the current state and the elapsed time.&lt;/li&gt;
&lt;li&gt;Transition matrices can be used to calculate the probability of reaching a particular state at a given time, to analyze the long-term behavior of a system, and to make predictions about the future behavior of the system. They are also used to identify patterns and trends in the system and to optimize the performance of the system.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;transition-probability&#34;&gt;Transition probability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A transition matrix is a matrix that is used to describe the transitions between different states in a Markov process. In a Markov process, a system moves from one state to another according to certain probabilistic rules, and the transition matrix specifies the probability of transitioning from one state to another.&lt;/li&gt;
&lt;li&gt;The elements of a transition matrix are the probabilities of transitioning between states. The rows of the matrix represent the starting states, and the columns represent the ending states. The element in the i-th row and j-th column of the matrix represents the probability of transitioning from the i-th state to the j-th state.&lt;/li&gt;
&lt;li&gt;Transition matrices are used in a variety of fields, including engineering, computer science, and economics, to model and analyze the behavior of systems that change over time. They are commonly used in the analysis of Markov processes, which are systems in which the future state of the system is determined only by the current state and the elapsed time.&lt;/li&gt;
&lt;li&gt;Transition matrices can be used to calculate the probability of reaching a particular state at a given time, to analyze the long-term behavior of a system, and to make predictions about the future behavior of the system. They are also used to identify patterns and trends in the system and to optimize the performance of the system.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;validation-of-simulation&#34;&gt;Validation (of simulation)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Validation is the process of evaluating a simulation model to determine whether it accurately represents the real-world system or process that it is intended to model. The goal of validation is to ensure that the simulation results are reliable and accurately reflect the behavior of the system being studied.&lt;/li&gt;
&lt;li&gt;There are several steps involved in validating a simulation model, including:
&lt;ul&gt;
&lt;li&gt;Defining the scope and objectives of the simulation&lt;/li&gt;
&lt;li&gt;Developing the simulation model using a suitable modeling approach&lt;/li&gt;
&lt;li&gt;Verifying the internal consistency and logic of the model&lt;/li&gt;
&lt;li&gt;Calibrating the model using real-world data&lt;/li&gt;
&lt;li&gt;Testing the model using a variety of inputs and scenarios&lt;/li&gt;
&lt;li&gt;Comparing the results of the simulation to real-world data or other sources of information&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Validation is an important step in the development of a simulation model, as it helps to ensure that the model is accurate and can be trusted to make reliable predictions about the behavior of the system being studied. It is also an ongoing process, as the model may need to be revised and re-validated as new information becomes available or the system being studied changes over time.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;probability-distributions&#34;&gt;Probability distributions&lt;/h2&gt;
&lt;h3 id=&#34;bernoulli-distribution&#34;&gt;Bernoulli distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Bernoulli distribution is a probability distribution that describes the outcome of a binary event, such as the toss of a coin or the outcome of a yes/no question. It is a discrete distribution, meaning that the random variable can only take on a finite number of values.&lt;/li&gt;
&lt;li&gt;In the Bernoulli distribution, there are only two possible outcomes: success (denoted by a value of 1) or failure (denoted by a value of 0). The probability of success is denoted by p, and the probability of failure is denoted by (1-p).&lt;/li&gt;
&lt;li&gt;The Bernoulli distribution is defined by a single parameter, p, which represents the probability of success. The probability mass function (PMF) of the Bernoulli distribution is given by:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;PMF(x) = p^x * (1-p)^(1-x)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where x is a value of 0 (failure) or 1 (success).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Bernoulli distribution is a special case of the binomial distribution, which describes the outcome of a series of independent binary events. It is often used to model the probability of success in situations where there are only two possible outcomes, such as the probability of winning a game or the probability of a coin landing heads.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bias&#34;&gt;Bias&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bias refers to a systematic error or deviation from the true value of a measurement or estimate. It can occur in various forms, such as:
&lt;ul&gt;
&lt;li&gt;Sampling bias: This occurs when the sample of data being analyzed is not representative of the population being studied.&lt;/li&gt;
&lt;li&gt;Measurement bias: This occurs when the measurement process is not accurate or reliable, leading to systematic errors in the measurements.&lt;/li&gt;
&lt;li&gt;Observational bias: This occurs when the observer or researcher&amp;rsquo;s expectations or preconceptions influence the results of an experiment or study.&lt;/li&gt;
&lt;li&gt;Confirmation bias: This occurs when the researcher is more likely to accept or seek out evidence that supports their hypothesis or preconceptions, and is less likely to consider evidence that contradicts their beliefs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Bias can have significant impacts on the accuracy and reliability of research and measurement, and it is important to try to minimize bias whenever possible. This can be done through careful design of experiments and studies, using random sampling and other techniques to ensure a representative sample, and using objective measurement techniques to minimize measurement bias.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;binomial-distribution&#34;&gt;Binomial distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The binomial distribution is a probability distribution that describes the outcome of a series of independent binary events, such as the toss of a coin or the outcome of a series of yes/no questions. It is a discrete distribution, meaning that the random variable can only take on a finite number of values.&lt;/li&gt;
&lt;li&gt;In the binomial distribution, there are only two possible outcomes for each event: success (denoted by a value of 1) or failure (denoted by a value of 0). The probability of success is denoted by p, and the probability of failure is denoted by (1-p). The number of events is denoted by n.&lt;/li&gt;
&lt;li&gt;The binomial distribution is defined by two parameters: n, the number of events, and p, the probability of success. The probability mass function (PMF) of the binomial distribution is given by:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;PMF(x) = (n choose x) * p^x * (1-p)^(n-x)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where x is the number of successes and (n choose x) is the binomial coefficient.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The binomial distribution is often used to model the probability of a certain number of successes in a series of independent events, such as the probability of flipping heads a certain number of times in a series of coin flips. It is a useful distribution for modeling situations where there are only two possible outcomes and the probability of success is constant for each event.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;distribution-fitting&#34;&gt;Distribution-fitting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Distribution fitting is the process of selecting a statistical distribution that best represents the data being analyzed. It is a common step in statistical analysis and is used to describe the distribution of a set of data and to make predictions about future observations.&lt;/li&gt;
&lt;li&gt;There are several methods for fitting a distribution to a set of data, including:
&lt;ul&gt;
&lt;li&gt;Visual inspection: This involves plotting the data and visually comparing it to the shape of known distributions to see which one is the best fit.&lt;/li&gt;
&lt;li&gt;Goodness-of-fit tests: These tests evaluate the fit of the data to a particular distribution by calculating a statistic, such as a p-value, which measures the probability of observing the data if the chosen distribution is true.&lt;/li&gt;
&lt;li&gt;Maximum likelihood estimation: This method estimates the parameters of a distribution that maximize the likelihood of observing the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;It is important to choose an appropriate distribution for the data being analyzed, as the choice of distribution can affect the accuracy of the results and the conclusions drawn from the data. In some cases, it may be necessary to use more than one distribution to adequately describe the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;exponential-distribution&#34;&gt;Exponential distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The exponential distribution is a probability distribution that describes the time between events in a Poisson process, which is a process in which events occur independently at a constant average rate. It is a continuous distribution, meaning that the random variable can take on any value within a given range.&lt;/li&gt;
&lt;li&gt;The exponential distribution is defined by a single parameter, lambda (λ), which represents the average rate at which events occur. The probability density function (PDF) of the exponential distribution is given by:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;PDF(x) = λ * e^(-λx)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where x is the time between events and e is the base of the natural logarithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The exponential distribution has the property that the time between events is exponentially distributed, meaning that the probability of an event occurring decreases exponentially as the time since the last event increases. This makes it a useful distribution for modeling processes in which the rate of occurrence decreases over time, such as the time between arrivals at a busy airport or the time between failures of a piece of equipment.&lt;/li&gt;
&lt;li&gt;The exponential distribution is often used in reliability engineering and other fields to model the time between failures or other events of interest. It is also related to the Poisson distribution, which is a discrete distribution that describes the number of events in a given time period.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;geometric-distribution&#34;&gt;Geometric distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The geometric distribution is a probability distribution that describes the number of Bernoulli trials needed to get a single success. It is a discrete distribution, meaning that the random variable can only take on a finite number of values.&lt;/li&gt;
&lt;li&gt;In the geometric distribution, there are two possible outcomes for each trial: success (denoted by a value of 1) or failure (denoted by a value of 0). The probability of success is denoted by p, and the probability of failure is denoted by (1-p).&lt;/li&gt;
&lt;li&gt;The geometric distribution is defined by a single parameter, p, which represents the probability of success. The probability mass function (PMF) of the geometric distribution is given by:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;PMF(x) = (1-p)^(x-1) * p
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where x is the number of trials.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The geometric distribution is often used to model the number of trials needed to get a single success in a series of independent events, such as the number of coin flips needed to get a heads or the number of times a die must be rolled to get a particular number. It is a useful distribution for modeling situations where there are only two possible outcomes and the probability of success decreases with each trial.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;iid&#34;&gt;IID&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;IID stands for &amp;ldquo;independent and identically distributed.&amp;rdquo; It is a term used to describe a sequence of random variables that are independent of each other and have the same probability distribution.&lt;/li&gt;
&lt;li&gt;In other words, IID random variables are uncorrelated and have the same statistical properties. This means that the value of one random variable in the sequence does not affect the value of any other random variable in the sequence, and that the probability of any given value occurring is the same for all variables in the sequence.&lt;/li&gt;
&lt;li&gt;IID random variables are often used in statistical analysis and probability theory, as they have a number of useful properties that make them easier to work with. For example, the expected value of the sum of IID random variables is equal to the sum of the expected values of the individual variables, which can simplify calculations.&lt;/li&gt;
&lt;li&gt;IID random variables are used in a variety of fields, including statistics, economics, engineering, and computer science. They are commonly used to model the behavior of systems that involve uncertainty or randomness, such as financial markets or communication networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;lower-tail&#34;&gt;Lower tail&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In a probability distribution, the lower tail refers to the portion of the distribution that is below a certain value or threshold. The lower tail is often defined relative to a particular value, such as the mean or median of the distribution, and it represents the probability that a random variable will take on a value that is less than this threshold.&lt;/li&gt;
&lt;li&gt;The lower tail of a distribution can be important in understanding the behavior of a random variable and in making predictions about its value. For example, in a financial context, the lower tail of a distribution may represent the probability of experiencing a significant loss or downturn.&lt;/li&gt;
&lt;li&gt;The lower tail of a distribution can be characterized by various statistics, such as the lower quartile, which is the value below which 25% of the observations fall, or the lower decile, which is the value below which 10% of the observations fall. These statistics can provide insight into the skewness of the distribution and the relative frequency of lower values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;memoryless-distribution&#34;&gt;Memoryless (distribution)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A memoryless distribution is a type of probability distribution that has the property that the conditional probability of an event occurring at a future time is independent of the time that has elapsed since the last event. This means that the probability of an event occurring at a given time is the same as the probability of it occurring at any other time, regardless of how much time has passed since the last event.&lt;/li&gt;
&lt;li&gt;An example of a memoryless distribution is the exponential distribution, which is often used to model the time between events in a Poisson process. The exponential distribution has the property that the probability of an event occurring at a given time is the same as the probability of it occurring at any other time, regardless of how much time has passed since the last event.&lt;/li&gt;
&lt;li&gt;Memoryless distributions are often used to model processes in which the probability of an event occurring does not depend on the time that has elapsed since the last event, such as the time between failures of a piece of equipment or the time between arrivals at a busy airport. They are useful for modeling systems in which the probability of an event occurring is constant over time.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;normal-distribution&#34;&gt;Normal distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The normal distribution, also known as the Gaussian distribution, is a continuous probability distribution that is widely used to model real-valued random variables. It is a symmetrical distribution with a bell-shaped curve and has a single peak, which is at the mean of the distribution.&lt;/li&gt;
&lt;li&gt;The normal distribution is defined by two parameters: the mean, which is the average value of the distribution, and the standard deviation, which is a measure of the spread of the distribution. The probability density function (PDF) of the normal distribution is given by:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;PDF(x) = (1 / (sqrt(2*π)σ)) * e^(-(x-μ)^2 / (2σ^2))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where x is the value of the random variable, μ is the mean of the distribution, σ is the standard deviation, and π is approximately 3.14.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The normal distribution is often used to model variables that are continuous and have a symmetrical distribution, such as height, weight, and IQ scores. It is a useful distribution because it has a number of desirable properties, such as being stable under linear transformations and having a simple analytical form. It is also commonly used in statistical analysis and probability theory.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;poisson-distribution&#34;&gt;Poisson distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Poisson distribution is a discrete probability distribution that describes the number of events that occur in a given time period, such as the number of phone calls received by a call center or the number of defects in a manufactured product. It is often used to model the number of times an event occurs in a fixed interval of time, space, or volume.&lt;/li&gt;
&lt;li&gt;The Poisson distribution is defined by a single parameter, lambda (λ), which represents the average rate at which events occur. The probability mass function (PMF) of the Poisson distribution is given by:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;PMF(x) = (λ^x * e^(-λ)) / x!
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where x is the number of events and e is the base of the natural logarithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Poisson distribution has the property that the probability of observing a given number of events decreases exponentially as the number of events increases. This makes it a useful distribution for modeling processes in which the rate of occurrence decreases as the number of events increases, such as the number of errors in a document or the number of vehicles arriving at a traffic light.&lt;/li&gt;
&lt;li&gt;The Poisson distribution is often used in fields such as engineering, economics, and operations research to model the occurrence of events over time or space. It is related to the exponential distribution, which is a continuous distribution that describes the time between events in a Poisson process.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;q-q-plot&#34;&gt;Q-Q plot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A Q-Q (quantile-quantile) plot is a graphical tool used to assess whether two data sets come from the same distribution. It is a scatter plot that plots the quantiles of one data set against the quantiles of another data set, and it is used to visualize the similarity or dissimilarity between the two distributions.&lt;/li&gt;
&lt;li&gt;To create a Q-Q plot, the data is first sorted in ascending order and then divided into equal-sized groups, called quantiles. The quantiles of each data set are then plotted against each other on the graph. If the two data sets come from the same distribution, the points on the Q-Q plot will fall along a straight line. If the distributions are different, the points will deviate from the line.&lt;/li&gt;
&lt;li&gt;Q-Q plots are often used in statistical analysis to compare the distributions of two data sets or to assess whether a data set follows a particular distribution. They are also useful for identifying outliers and assessing the normality of a data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tails&#34;&gt;Tail(s)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In a probability distribution, the tails refer to the portion of the distribution that is above or below a certain value or threshold. The tails of a distribution can be characterized by various statistics, such as the upper and lower quartiles or deciles, which are values above or below which a certain percentage of the observations fall.&lt;/li&gt;
&lt;li&gt;The tails of a distribution can be important in understanding the behavior of a random variable and in making predictions about its value. For example, in a financial context, the tails of a distribution may represent the probability of experiencing a significant gain or loss.&lt;/li&gt;
&lt;li&gt;The tails of a distribution can also be used to characterize the skewness of the distribution. A distribution with a long left tail (i.e., a tail that extends to the left of the mean) is said to be left-skewed, while a distribution with a long right tail is said to be right-skewed. A symmetrical distribution, on the other hand, has equal tails on both sides of the mean.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;upper-tail&#34;&gt;Upper tail&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In a probability distribution, the upper tail refers to the portion of the distribution that is above a certain value or threshold. The upper tail is often defined relative to a particular value, such as the mean or median of the distribution, and it represents the probability that a random variable will take on a value that is greater than this threshold.&lt;/li&gt;
&lt;li&gt;The upper tail of a distribution can be important in understanding the behavior of a random variable and in making predictions about its value. For example, in a financial context, the upper tail of a distribution may represent the probability of experiencing a significant gain or upturn.&lt;/li&gt;
&lt;li&gt;The upper tail of a distribution can be characterized by various statistics, such as the upper quartile, which is the value above which 25% of the observations fall, or the upper decile, which is the value above which 10% of the observations fall. These statistics can provide insight into the skewness of the distribution and the relative frequency of higher values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;weibull-distribution&#34;&gt;Weibull distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Weibull distribution is a continuous probability distribution that is often used to model the time it takes for a failure to occur in a system or the time between events in a process. It is a flexible distribution that can take on a variety of shapes, including a bell-shaped curve similar to the normal distribution, a curve with a long tail to the right (similar to the exponential distribution), or a curve with a long tail to the left (similar to the log-normal distribution).&lt;/li&gt;
&lt;li&gt;The Weibull distribution is defined by two parameters: alpha (α), which determines the shape of the distribution, and beta (β), which determines the scale of the distribution. The probability density function (PDF) of the Weibull distribution is given by:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;PDF(x) = (α / β) * (x / β)^(α-1) * e^(-(x/β)^α)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where x is the value of the random variable.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Weibull distribution is often used in reliability engineering and other fields to model the time to failure of a system or the time between events. It is a useful distribution because it can take on a variety of shapes, making it suitable for modeling a wide range of phenomena. It is also commonly used in statistical analysis and probability theory.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;regression&#34;&gt;Regression&lt;/h2&gt;
&lt;h3 id=&#34;adjusted-r-squaredadjusted-r2&#34;&gt;Adjusted R-squared/Adjusted R2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Adjusted R-squared is a statistic that attempts to adjust the R-squared value for the number of predictors in a regression model. It is often used to determine whether the addition of new predictors to a model significantly improves the model&amp;rsquo;s ability to predict the response variable.&lt;/li&gt;
&lt;li&gt;R-squared is a measure of how well a model fits the data. It is calculated as the proportion of the variance in the response variable that is explained by the model. However, R-squared can sometimes be artificially inflated when adding additional predictors to the model, even if those predictors do not significantly improve the model&amp;rsquo;s ability to predict the response.&lt;/li&gt;
&lt;li&gt;Adjusted R-squared is calculated by taking into account the number of predictors in the model and the sample size. It adjusts the R-squared value downward to account for the addition of predictors that do not significantly improve the model.&lt;/li&gt;
&lt;li&gt;In general, a higher adjusted R-squared value indicates a better fit for the model. However, it is important to consider other evaluation metrics in addition to adjusted R-squared when assessing the performance of a regression model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;area-under-curveauc&#34;&gt;Area under curve/AUC&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The area under the curve (AUC) is a measure of the performance of a binary classifier, such as a diagnostic test. It represents the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example.&lt;/li&gt;
&lt;li&gt;The AUC can be calculated from the receiver operating characteristic (ROC) curve, which plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) at various classification thresholds. The AUC is the area under the ROC curve.&lt;/li&gt;
&lt;li&gt;AUC values range from 0 to 1, with a higher value indicating a better performing classifier. An AUC of 0.5 indicates that the classifier is no better than random guessing, while an AUC of 1 indicates perfect classification.&lt;/li&gt;
&lt;li&gt;The AUC is a useful evaluation metric because it is independent of the classification threshold and is not sensitive to the imbalance in the class distribution. It is often used in medical research to evaluate the performance of diagnostic tests.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bayesian-regression&#34;&gt;Bayesian regression&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bayesian regression is a type of regression analysis that is based on Bayesian statistics. In Bayesian regression, model parameters are considered random variables, and a probability distribution is assigned to each parameter.&lt;/li&gt;
&lt;li&gt;In contrast to traditional regression, where the values of the model parameters are estimated using maximum likelihood estimation, Bayesian regression involves estimating the posterior distribution of the model parameters given the data and a prior distribution. This posterior distribution represents the updated belief about the model parameters after taking into account the observed data.&lt;/li&gt;
&lt;li&gt;One advantage of Bayesian regression is that it allows for the incorporation of prior knowledge or beliefs about the model parameters into the analysis. It also provides a full probability distribution for the model parameters, which can be useful for making predictions and for understanding the uncertainty in the estimates.&lt;/li&gt;
&lt;li&gt;Bayesian regression can be implemented using Markov Chain Monte Carlo (MCMC) techniques or variational inference methods. It is often used in situations where the number of predictors is large or when there is limited data available.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;box-cox-transformation&#34;&gt;Box-Cox transformation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Box-Cox transformation is a way to transform non-normal dependent variables into a normal shape. Normality is an assumption for many statistical techniques, so being able to transform a non-normal dependent variable into a normal shape can be useful for the purposes of analysis. The Box-Cox transformation is defined as:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Y = (X^lambda - 1) / lambda
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where X is the variable to be transformed, Y is the transformed variable, and lambda is a parameter that you can choose to optimize the transformation. If lambda is equal to 0, the transformation becomes the natural logarithm. If lambda is equal to 1, the transformation is the identity transformation (i.e., the variable is left unchanged).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Box-Cox transformation is often used in regression analysis, particularly when the dependent variable is not normal. It can be used to stabilize the variance of the dependent variable, improve the linearity of the model, and/or meet the assumptions of normality.&lt;/li&gt;
&lt;li&gt;It is important to note that the Box-Cox transformation is only appropriate for continuous variables. If you have a categorical variable, you should not use the Box-Cox transformation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;branching&#34;&gt;Branching&lt;/h3&gt;
&lt;p&gt;In the context of machine learning, branching can be used to create decision trees, which are a type of model used for classification and regression tasks. A decision tree is a flowchart-like tree structure where an internal node represents feature (an attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the input data by recursive partitioning.&lt;/p&gt;
&lt;p&gt;Here is an example of a decision tree in Python using the scikit-learn library:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;from sklearn import tree

# features (X) and labels (y)
X = [[0, 0], [1, 1]]
y = [0, 1]

# create the decision tree model
model = tree.DecisionTreeClassifier()

# train the model
model.fit(X, y)

# predict a label for a new sample
print(model.predict([[2., 2.]]))
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In the context of machine learning, branching can be used to create decision trees, which are a type of model used for classification and regression tasks. A decision tree is a flowchart-like tree structure where an internal node represents feature (an attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the input data by recursive partitioning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Here is an example of a decision tree in Python using the scikit-learn library:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;from sklearn import tree

# features (X) and labels (y)
X = [[0, 0], [1, 1]]
y = [0, 1]

# create the decision tree model
model = tree.DecisionTreeClassifier()

# train the model
model.fit(X, y)

# predict a label for a new sample
print(model.predict([[2., 2.]]))
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;In this example, the decision tree will learn to predict a label (0 or 1) based on the input features (X). The tree will create branches based on the conditions specified in the decision rules, and the leaves of the tree will contain the predicted labels. Decision trees are a simple and powerful tool for many machine learning tasks, and they can be used in a variety of applications such as recommendation systems, fraud detection, and image classification.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cart&#34;&gt;CART&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CART (Classification and Regression Trees) is a decision tree algorithm used for classification and regression tasks. It is a popular algorithm for building decision trees because it is simple to understand and implement, and it can handle both continuous and categorical variables.&lt;/li&gt;
&lt;li&gt;In a CART decision tree, the tree is built by selecting splits on the features that maximize the reduction in impurity. Impurity refers to the amount of uncertainty or randomness in the data. The goal of the algorithm is to create splits that reduce the impurity of the data as much as possible, resulting in purer leaves (i.e., leaves with a single class or value).&lt;/li&gt;
&lt;li&gt;Here is an example of a CART decision tree in Python using the scikit-learn library:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;from sklearn import tree

# features (X) and labels (y)
X = [[0, 0], [1, 1]]
y = [0, 1]

# create the CART model
model = tree.DecisionTreeClassifier(criterion=&amp;#39;gini&amp;#39;)

# train the model
model.fit(X, y)

# predict a label for a new sample
print(model.predict([[2., 2.]]))
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;In this example, the CART model will learn to predict a label (0 or 1) based on the input features (X). The tree will create splits based on the Gini impurity criterion, and the leaves of the tree will contain the predicted labels. CART is a widely used algorithm for building decision trees, and it is often used in a variety of applications such as recommendation systems, fraud detection, and image classification.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;classification-tree&#34;&gt;Classification tree&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A classification tree is a type of decision tree that is used for classification tasks. It is a flowchart-like tree structure where an internal node represents feature (an attribute), the branch represents a decision rule, and each leaf node represents the outcome.&lt;/li&gt;
&lt;li&gt;The topmost node in a classification tree is known as the root node. It learns to partition on the input data by recursive partitioning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;concordance-index&#34;&gt;Concordance index&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A concordance index (also known as the c-index) is a measure of the predictive ability of a binary classifier. It is calculated as the fraction of all pairs of samples (one positive, one negative) where the positive sample has a higher predicted probability of being positive than the negative sample. The c-index ranges from 0 to 1, where a value of 1 indicates perfect prediction and a value of 0 indicates no predictive ability. The c-index is often used to evaluate the performance of a classification model, particularly in the field of medical statistics.&lt;/li&gt;
&lt;li&gt;Here is an example of how to calculate the c-index in Python:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import numpy as np

# true labels (y_true) and predicted probabilities (y_pred)
y_true = [0, 1, 1, 0, 1]
y_pred = [0.1, 0.8, 0.7, 0.2, 0.9]

# calculate the c-index
c_index = 0
for i in range(len(y_true)):
    for j in range(i+1, len(y_true)):
        if (y_true[i] == 0 and y_true[j] == 1) or (y_true[i] == 1 and y_true[j] == 0):
            c_index += (y_pred[i] &amp;gt; y_pred[j]) + (y_pred[i] == y_pred[j])
print(c_index / (len(y_true) * (len(y_true) - 1) / 2))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;decision-tree&#34;&gt;Decision tree&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A decision tree is a flowchart-like tree structure that is used to make decisions based on conditions specified in the decision rules. It is a popular tool in machine learning and is often used for classification and regression tasks.&lt;/li&gt;
&lt;li&gt;In a decision tree, the tree is built by selecting splits on the features that maximize the reduction in impurity. Impurity refers to the amount of uncertainty or randomness in the data. The goal of the algorithm is to create splits that reduce the impurity of the data as much as possible, resulting in purer leaves (i.e., leaves with a single class or value).&lt;/li&gt;
&lt;li&gt;Here is an example of a decision tree in Python using the scikit-learn library:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;from sklearn import tree

# features (X) and labels (y)
X = [[0, 0], [1, 1]]
y = [0, 1]

# create the decision tree model
model = tree.DecisionTreeClassifier()

# train the model
model.fit(X, y)

# predict a label for a new sample
print(model.predict([[2., 2.]]))
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;In this example, the decision tree will learn to predict a label (0 or 1) based on the input features (X). The tree will create splits based on the decision rules, and the leaves of the tree will contain the predicted labels. Decision trees are a simple and powerful tool for many machine learning tasks, and they can be used in a variety of applications such as recommendation systems, fraud detection, and image classification.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;elastic-net&#34;&gt;Elastic net&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Elastic Net is a linear regression model that combines the penalties of both L1 (Lasso) and L2 (Ridge) regularization. It is trained with both L1 and L2 regularization, and the mixing parameter alpha determines the weighting between the two. When alpha=0, Elastic Net is equivalent to Ridge Regression, and when alpha=1, it is equivalent to Lasso Regression.&lt;/li&gt;
&lt;li&gt;The advantage of Elastic Net over Ridge Regression is that it is able to handle correlated features better, and the advantage over Lasso is that it does not require the features to be standardized.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;forest&#34;&gt;Forest&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of regression, a decision tree forest is a type of ensemble model that is made up of a collection of decision trees trained on different subsets of the training data. The individual decision trees in the forest make predictions based on the features in the data, and the predictions of the trees are combined to make the final prediction for the forest.&lt;/li&gt;
&lt;li&gt;There are several ways to combine the predictions of the individual trees in the forest. One common method is to take the mean of the predictions of all the trees in the forest. Another method is to have each tree vote on the final prediction, and the most popular prediction is chosen as the output of the forest.&lt;/li&gt;
&lt;li&gt;Decision tree forests are often used for regression tasks because they can handle high-dimensional data and are resistant to overfitting. They are also able to handle missing values in the data, which is a common problem in real-world datasets.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;interaction-term&#34;&gt;Interaction term&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An interaction term is a term in a statistical model that represents the effect of two variables on an outcome, rather than the effect of each variable individually. Interaction terms allow you to determine whether the relationship between two variables and the outcome is different from the individual relationships of each variable with the outcome.&lt;/li&gt;
&lt;li&gt;For example, let&amp;rsquo;s say you are studying the relationship between income, education, and happiness. You might include an interaction term in your model to test whether the relationship between income and happiness is different for people with different levels of education. If you find a significant interaction term, it suggests that the relationship between income and happiness is not the same for all levels of education.&lt;/li&gt;
&lt;li&gt;In a statistical model, interaction terms are included as additional predictors along with the main effects (individual variables). They are usually represented by the product of the two variables that are being interacted. For example, if you are including an interaction term between variables X and Y, it would be represented as X*Y in the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;𝑘-nearest-neighbor-regression&#34;&gt;𝑘-Nearest-Neighbor regression&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;k-Nearest Neighbor (k-NN) regression is a simple and easy-to-implement machine learning method used for regression tasks. In k-NN regression, the prediction for a given data point is based on the mean of the target values of the k nearest neighbors to that data point.&lt;/li&gt;
&lt;li&gt;To make a prediction for a new data point, the distance between that point and all the other points in the training set is calculated. The k points in the training set that are closest to the new point are then identified, and the mean of the target values of these k points is taken as the prediction for the new point.&lt;/li&gt;
&lt;li&gt;One of the main advantages of k-NN regression is that it is a non-parametric method, which means that it does not make any assumptions about the underlying functional form of the data. This makes it well-suited for working with complex, non-linear relationships in the data. However, k-NN regression can be computationally expensive and may not be suitable for very large datasets.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;knot&#34;&gt;Knot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of a spline, a knot is a point where the function changes curvature. A spline is a piecewise continuous curve that is used to approximate a set of data points, and knots are used to control the smoothness of the curve.&lt;/li&gt;
&lt;li&gt;For example, consider a set of data points that show the relationship between temperature and ice cream sales at a store. A spline with a single knot would be a simple curve that passes through all the data points, while a spline with multiple knots would have a more complex shape that is able to better capture the underlying pattern in the data. The position and number of knots in the spline can be chosen to trade off between smoothness and fit to the data.&lt;/li&gt;
&lt;li&gt;In general, splines are used to smooth noisy data or to fit a curve to a set of data points when a parametric form for the curve is not known. They are commonly used in regression and smoothing applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;lasso-regression&#34;&gt;Lasso regression&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Lasso regression is a linear regression method that uses L1 regularization to encourage sparsity in the model. L1 regularization is a form of regularization that adds a penalty term to the objective function of the model based on the absolute values of the model coefficients. The penalty term is controlled by a hyperparameter alpha, which determines the strength of the regularization.&lt;/li&gt;
&lt;li&gt;Lasso regression has the effect of driving some of the coefficients of the model to zero, effectively removing the corresponding features from the model. This can be useful for feature selection, as it allows you to identify the most important features in the data and remove the rest.&lt;/li&gt;
&lt;li&gt;Lasso regression is particularly well-suited for cases where there are a large number of features and only a few of them are truly relevant. It is also useful when the relationships between the features and the outcome are sparse, i.e., when most of the features have little or no effect on the outcome. However, Lasso regression can be sensitive to the scale of the features, and it is generally recommended to standardize the features before fitting a Lasso model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;leaf&#34;&gt;Leaf&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the context of a decision tree, a leaf is a terminal node that does not have any children. In a decision tree for regression, the value stored at a leaf node is the mean of the target values of the training examples that reach that leaf.&lt;/li&gt;
&lt;li&gt;For example, consider a decision tree for predicting the price of a house based on features such as the size of the house, the number of bedrooms, and the location. The tree might have a leaf node for houses with three bedrooms that are located in a certain neighborhood. The value stored at this leaf node would be the mean of the prices of all the houses with three bedrooms in that neighborhood in the training set.&lt;/li&gt;
&lt;li&gt;When making a prediction for a new house using the decision tree, the tree is traversed from the root to a leaf node based on the feature values of the new house. The value stored at the leaf node is then taken as the prediction for the house. Decision trees are often used for regression tasks because they are able to handle high-dimensional data and can handle missing values in the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;linear-regression&#34;&gt;Linear regression&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Linear regression is a statistical method used to model the linear relationship between a dependent variable and one or more independent variables. The goal of linear regression is to find the best-fitting line to a set of data points, where the best-fitting line is one that minimizes the sum of the squared differences between the predicted values and the true values.&lt;/li&gt;
&lt;li&gt;Linear regression models can be represented by the equation:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;y = b0 + b1x1 + b2x2 + ... + bn*xn
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where y is the dependent variable, x1, x2, &amp;hellip;, xn are the independent variables, and b0, b1, b2, &amp;hellip;, bn are the coefficients that represent the strength and direction of the relationship between each independent variable and the dependent variable. The coefficients are determined by fitting the model to the training data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear regression is a simple and widely-used method for modeling linear relationships in data. It is well-suited for cases where the relationship between the variables is well-approximated by a straight line. However, it is not capable of modeling more complex, non-linear relationships.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;logistic-regression&#34;&gt;Logistic regression&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Logistic regression is a statistical method used for classification tasks. It is a supervised learning algorithm that takes a set of input features and uses them to predict a binary outcome (0 or 1).&lt;/li&gt;
&lt;li&gt;The predictions of a logistic regression model are based on the probability of the positive class (class 1). The probability is computed using the logistic function, which maps any real-valued number to the range [0, 1]. The logistic function has the following form:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;p = 1 / (1 + e^(-z))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where p is the probability of the positive class and z is a linear combination of the input features and the model coefficients.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To make a prediction for a new data point, the model computes the probability of the positive class using the logistic function, and the predicted class is 1 if the probability is greater than or equal to 0.5 and 0 otherwise.&lt;/li&gt;
&lt;li&gt;Logistic regression is widely used in a variety of applications, including image classification, spam filtering, and predicting customer churn. It is simple to implement and efficient to train, and it can be extended to handle multi-class classification tasks by using one-vs-rest or one-vs-all approaches.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;logit-model&#34;&gt;Logit model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A logit model is a type of statistical model that is used for binary classification tasks. It is a type of generalized linear model that uses the logit function as the link function and the binary outcome as the dependent variable.&lt;/li&gt;
&lt;li&gt;The logit function is a transformation of the probability of the positive class (class 1) into the real line. It is defined as the natural logarithm of the odds ratio:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;logit(p) = ln(p / (1 - p))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where p is the probability of the positive class. The logit function maps any probability value in the range [0, 1] to the range (-infinity, infinity).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In a logit model, the predicted probability of the positive class is computed using the logit function and a linear combination of the input features and the model coefficients. The predicted class is then obtained by thresholding the probability at 0.5: class 1 if the probability is greater than or equal to 0.5 and class 0 otherwise.&lt;/li&gt;
&lt;li&gt;Logit models are widely used in a variety of applications, including image classification, spam filtering, and predicting customer churn. They are simple to implement and efficient to train, and they can be extended to handle multi-class classification tasks by using one-vs-rest or one-vs-all approaches.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mars&#34;&gt;MARS&lt;/h3&gt;
&lt;h3 id=&#34;multi-adaptive-regression-splines-mars&#34;&gt;Multi-adaptive regression splines (MARS)&lt;/h3&gt;
&lt;h3 id=&#34;p-value&#34;&gt;p-value&lt;/h3&gt;
&lt;h3 id=&#34;p-value-fishing&#34;&gt;p-value fishing&lt;/h3&gt;
&lt;h3 id=&#34;poisson-regression&#34;&gt;Poisson regression&lt;/h3&gt;
&lt;h3 id=&#34;pruning&#34;&gt;Pruning&lt;/h3&gt;
&lt;h3 id=&#34;pseudo-r-squaredpseudo-r2&#34;&gt;Pseudo-R-squared/Pseudo-R2&lt;/h3&gt;
&lt;h3 id=&#34;r-squaredr2&#34;&gt;R-squared/R2&lt;/h3&gt;
&lt;h3 id=&#34;random-forest&#34;&gt;Random forest&lt;/h3&gt;
&lt;h3 id=&#34;receiver-operating-characteristic-curve-roc-curve&#34;&gt;Receiver operating characteristic curve (ROC curve)&lt;/h3&gt;
&lt;h3 id=&#34;regression-1&#34;&gt;Regression&lt;/h3&gt;
&lt;h3 id=&#34;regression-splines&#34;&gt;Regression splines&lt;/h3&gt;
&lt;h3 id=&#34;regression-tree&#34;&gt;Regression tree&lt;/h3&gt;
&lt;h3 id=&#34;ridge-regression&#34;&gt;Ridge regression&lt;/h3&gt;
&lt;h3 id=&#34;roc-curve&#34;&gt;ROC curve&lt;/h3&gt;
&lt;h3 id=&#34;root&#34;&gt;Root&lt;/h3&gt;
&lt;h3 id=&#34;spline-regression&#34;&gt;Spline regression&lt;/h3&gt;
&lt;h3 id=&#34;transformation&#34;&gt;Transformation&lt;/h3&gt;
&lt;h3 id=&#34;tree&#34;&gt;Tree&lt;/h3&gt;
&lt;h2 id=&#34;time-series-models&#34;&gt;Time series models&lt;/h2&gt;
&lt;h3 id=&#34;additive-seasonality&#34;&gt;Additive seasonality&lt;/h3&gt;
&lt;h3 id=&#34;arima&#34;&gt;ARIMA&lt;/h3&gt;
&lt;h3 id=&#34;autoregression&#34;&gt;Autoregression&lt;/h3&gt;
&lt;h3 id=&#34;autoregressive-integrated-moving-average-arima&#34;&gt;Autoregressive integrated moving average (ARIMA)&lt;/h3&gt;
&lt;h3 id=&#34;differencing&#34;&gt;Differencing&lt;/h3&gt;
&lt;h3 id=&#34;double-exponential&#34;&gt;Double exponential&lt;/h3&gt;
&lt;h3 id=&#34;smoothing&#34;&gt;smoothing&lt;/h3&gt;
&lt;h3 id=&#34;exponential-smoothing&#34;&gt;Exponential smoothing&lt;/h3&gt;
&lt;h3 id=&#34;garch&#34;&gt;GARCH&lt;/h3&gt;
&lt;h3 id=&#34;generalized-autoregressive-conditional-heteroscedasticity-garch&#34;&gt;Generalized autoregressive conditional heteroscedasticity (GARCH)&lt;/h3&gt;
&lt;h3 id=&#34;holt-winters-method&#34;&gt;Holt-Winters method&lt;/h3&gt;
&lt;h3 id=&#34;moving-average&#34;&gt;Moving average&lt;/h3&gt;
&lt;h3 id=&#34;multiplicative-seasonality&#34;&gt;Multiplicative seasonality&lt;/h3&gt;
&lt;h3 id=&#34;seasonalitycycles&#34;&gt;Seasonality/cycles&lt;/h3&gt;
&lt;h3 id=&#34;seasonality-lengthcycle-length&#34;&gt;Seasonality length/cycle length&lt;/h3&gt;
&lt;h3 id=&#34;single-exponential-smoothing&#34;&gt;Single exponential smoothing&lt;/h3&gt;
&lt;h3 id=&#34;smoothing-1&#34;&gt;Smoothing&lt;/h3&gt;
&lt;h3 id=&#34;smoothing-constant&#34;&gt;Smoothing constant&lt;/h3&gt;
&lt;h3 id=&#34;stationary-process&#34;&gt;Stationary process&lt;/h3&gt;
&lt;h3 id=&#34;trend&#34;&gt;Trend&lt;/h3&gt;
&lt;h3 id=&#34;triple-exponential-smoothing&#34;&gt;Triple exponential smoothing&lt;/h3&gt;
&lt;h3 id=&#34;winters-method&#34;&gt;Winters&amp;rsquo; method&lt;/h3&gt;
&lt;h2 id=&#34;variable-selection&#34;&gt;Variable Selection&lt;/h2&gt;
&lt;h3 id=&#34;backward-elimination&#34;&gt;Backward elimination&lt;/h3&gt;
&lt;h3 id=&#34;elastic-net-1&#34;&gt;Elastic net&lt;/h3&gt;
&lt;h3 id=&#34;forward-selection&#34;&gt;Forward selection&lt;/h3&gt;
&lt;h3 id=&#34;lassolasso-regression&#34;&gt;Lasso/Lasso regression&lt;/h3&gt;
&lt;h3 id=&#34;overfitting&#34;&gt;Overfitting&lt;/h3&gt;
&lt;h3 id=&#34;regularization&#34;&gt;Regularization&lt;/h3&gt;
&lt;h3 id=&#34;ridge-regression-1&#34;&gt;Ridge regression&lt;/h3&gt;
&lt;h3 id=&#34;simplicity-of-a-model&#34;&gt;Simplicity (of a model)&lt;/h3&gt;
&lt;h3 id=&#34;stepwise-regression&#34;&gt;Stepwise regression&lt;/h3&gt;
&lt;h3 id=&#34;variable-selection-1&#34;&gt;Variable selection&lt;/h3&gt;
&lt;h2 id=&#34;misc&#34;&gt;Misc&lt;/h2&gt;
&lt;h3 id=&#34;1-norm&#34;&gt;1-norm&lt;/h3&gt;
&lt;h3 id=&#34;2-norm&#34;&gt;2-norm&lt;/h3&gt;
&lt;h3 id=&#34;convex-hull-of-a-set-of-points&#34;&gt;Convex hull (of a set of points)&lt;/h3&gt;
&lt;h3 id=&#34;descriptive-analytics&#34;&gt;Descriptive analytics&lt;/h3&gt;
&lt;h3 id=&#34;distance&#34;&gt;Distance&lt;/h3&gt;
&lt;h3 id=&#34;elbow-diagram&#34;&gt;Elbow diagram&lt;/h3&gt;
&lt;h3 id=&#34;error-per-data-point&#34;&gt;Error (per data point)&lt;/h3&gt;
&lt;h3 id=&#34;error-total-over-data-set&#34;&gt;Error (total over data set)&lt;/h3&gt;
&lt;h3 id=&#34;euclidian-distancestraight--line-distance&#34;&gt;Euclidian distance/straight- line distance&lt;/h3&gt;
&lt;h3 id=&#34;fitting&#34;&gt;Fitting&lt;/h3&gt;
&lt;h3 id=&#34;heteroscedasticity&#34;&gt;Heteroscedasticity&lt;/h3&gt;
&lt;h3 id=&#34;infinity-norm&#34;&gt;Infinity-norm&lt;/h3&gt;
&lt;h3 id=&#34;linear-combination&#34;&gt;Linear combination&lt;/h3&gt;
&lt;h3 id=&#34;manhattan-distance&#34;&gt;Manhattan distance&lt;/h3&gt;
&lt;h3 id=&#34;minkowski-distance-of-order-𝑝&#34;&gt;Minkowski distance (of order 𝑝)&lt;/h3&gt;
&lt;h3 id=&#34;model-mathematical&#34;&gt;Model (mathematical)&lt;/h3&gt;
&lt;h3 id=&#34;multiplier&#34;&gt;Multiplier&lt;/h3&gt;
&lt;h3 id=&#34;normdistance-norm&#34;&gt;Norm/distance norm&lt;/h3&gt;
&lt;h3 id=&#34;order-of-magnitude&#34;&gt;Order of magnitude&lt;/h3&gt;
&lt;h3 id=&#34;orthogonal&#34;&gt;Orthogonal&lt;/h3&gt;
&lt;h3 id=&#34;outlier&#34;&gt;Outlier&lt;/h3&gt;
&lt;h3 id=&#34;overfitting-1&#34;&gt;Overfitting&lt;/h3&gt;
&lt;h3 id=&#34;𝑝-norm&#34;&gt;𝑝-norm&lt;/h3&gt;
&lt;h3 id=&#34;parameter&#34;&gt;Parameter&lt;/h3&gt;
&lt;h3 id=&#34;perturbation&#34;&gt;Perturbation&lt;/h3&gt;
&lt;h3 id=&#34;prediction&#34;&gt;Prediction&lt;/h3&gt;
&lt;h3 id=&#34;predictive-analytics&#34;&gt;Predictive analytics&lt;/h3&gt;
&lt;h3 id=&#34;prescriptive-analytics&#34;&gt;Prescriptive analytics&lt;/h3&gt;
&lt;h3 id=&#34;rectilinear-distance&#34;&gt;Rectilinear distance&lt;/h3&gt;
&lt;h3 id=&#34;threshold&#34;&gt;Threshold&lt;/h3&gt;
&lt;h3 id=&#34;transformation-1&#34;&gt;Transformation&lt;/h3&gt;
</description>
    </item>
    
  </channel>
</rss>