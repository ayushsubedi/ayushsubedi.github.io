<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine-learning on Ayush Subedi</title>
    <link>https://ayushsubedi.github.io/categories/machine-learning/</link>
    <description>Recent content in machine-learning on Ayush Subedi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 01 Jan 2023 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ayushsubedi.github.io/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>AWS Certified ML - Specialty exam (MLS-C01) - 1. Data Engineering</title>
      <link>https://ayushsubedi.github.io/posts/aws_ml_speciality_data_engineering/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/aws_ml_speciality_data_engineering/</guid>
      <description>&lt;h1 id=&#34;data-engineering&#34;&gt;Data Engineering&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#s3&#34;&gt;S3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kinesis&#34;&gt;Kinesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#glue&#34;&gt;Glue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#redshift&#34;&gt;Redshift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rds&#34;&gt;RDS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dynamodb&#34;&gt;DynamoDB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#opensearch&#34;&gt;OpenSearch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aws-data-pipeline&#34;&gt;AWS Data Pipeline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aws-batch&#34;&gt;AWS Batch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aws-batch&#34;&gt;AWS DMS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-functions&#34;&gt;Step Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#efs&#34;&gt;EFS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ebs&#34;&gt;EBS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#emr&#34;&gt;EMR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#misc&#34;&gt;Misc&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This domain requires understanding of creating data repositories for machine learning, identification and implementation of data ingestion solution, and
identification and implementation of a data transformation solution.&lt;/p&gt;
&lt;p&gt;Data engineering is the process of building and maintaining the infrastructure and systems that are used to store, process, and analyze data. In the context of Amazon Web Services (AWS), data engineering involves the use of various AWS services and tools to build and operate data pipelines, data lakes, and other data processing systems.&lt;/p&gt;
&lt;p&gt;Some common AWS services and tools that are used in data engineering on AWS include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amazon S3: A fully managed object storage service that is used to store and retrieve data.&lt;/li&gt;
&lt;li&gt;Amazon EMR: A fully managed big data processing service that is used to process and analyze large datasets using Apache Hadoop and Apache Spark.&lt;/li&gt;
&lt;li&gt;AWS Glue: A fully managed extract, transform, and load (ETL) service that is used to move and transform data between data stores.&lt;/li&gt;
&lt;li&gt;Amazon Redshift: A fully managed data warehouse service that is used to store and analyze large amounts of data using SQL and business intelligence tools.&lt;/li&gt;
&lt;li&gt;Amazon RDS: A fully managed database service that is used to set up, operate, and scale relational databases in the cloud.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By using these and other AWS services, data engineers can build and maintain robust, scalable, and cost-effective data processing systems on the AWS Cloud.&lt;/p&gt;
&lt;h2 id=&#34;s3&#34;&gt;S3&lt;/h2&gt;
&lt;p&gt;Amazon S3 (Simple Storage Service) is a cloud storage service that allows you to store and retrieve data at any time, from anywhere on the web. It is designed to make web-scale computing easier for developers by providing a simple, highly scalable, and cost-effective way to store and retrieve any amount of data. With S3, you can store and retrieve any amount of data, at any time, from anywhere on the web. S3 is designed to provide 99.999999999% durability and scale past trillions of objects worldwide. It is used to store and retrieve any amount of data, at any time, from anywhere on the web. It is an object storage service that offers industry-leading scalability, data availability, security, and performance.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;foundational for machine learning projects since it is a cost effective solution for datasets storage&lt;/li&gt;
&lt;li&gt;object based storage, bucket name need to be globally unique, however the storage itself is unique to regions&lt;/li&gt;
&lt;li&gt;key is the full path of the file and even though it looks like there is a folder based heirarchy, that is not how it works&lt;/li&gt;
&lt;li&gt;you can have a very long file name, in the sense that the path (key) can be very long&lt;/li&gt;
&lt;li&gt;individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 TB. The largest object that can be uploaded in a single PUT is 5 GB.&lt;/li&gt;
&lt;li&gt;object tags can be added, helpful with classification and security lifecycle (these are key value pairs)&lt;/li&gt;
&lt;li&gt;decoupling of compute and storage side&lt;/li&gt;
&lt;li&gt;perfect use case of data lake, since it can store various formats of data (object storage)&lt;/li&gt;
&lt;li&gt;it is possible to partition the storage, which is helpful (speedy) when querying via athena. Kinesis partitions the data automatically.&lt;/li&gt;
&lt;li&gt;11 9&amp;rsquo;s durability (for all storage classes)&lt;/li&gt;
&lt;li&gt;availability differs between availability classes&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;storage-classes&#34;&gt;Storage classes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;S3, Standard / General Purpose: for frequently accessed data&lt;/li&gt;
&lt;li&gt;S3, Infrequent Access: lower cost than standard, for data accessed monthly, and requires milliseconds retrival, but there is a cost associated with retrival&lt;/li&gt;
&lt;li&gt;S3, Infrequent Access, One Zone, good for secondary copies of backup, or data that can be recreated, infrequent access for cost saving&lt;/li&gt;
&lt;li&gt;S3, Glacier Instant Retrival, price per storage + price per retrival, can access within milliseconds, for low cost storage for long-lived data&lt;/li&gt;
&lt;li&gt;S3, Glacier Flexible Retrival, expedited: 1-5 mins, standard: 3-5 hrs, bulk: 5-12 hrs (free), for long term low cost storage for backups and archives  with different retrival options&lt;/li&gt;
&lt;li&gt;S3, Glacier Deep Archive: lowest cost, 180 days of minimum storage, for rarely accessed archive data&lt;/li&gt;
&lt;li&gt;S3, Intelligent Tiering: move objects between tiers with monthly monitoring and auto-tiering fee&lt;/li&gt;
&lt;li&gt;It is possible to move objects between these storage classes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://d1.awsstatic.com/reInvent/re21-pdp-tier1/s3/Amazon-S3-Storage-Classes.pdf&#34;&gt;more info&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;lifecycle-rules&#34;&gt;Lifecycle rules&lt;/h3&gt;
&lt;p&gt;Amazon S3 Lifecycle rules allow you to define policies for how Amazon S3 stores objects. You can use Lifecycle rules to specify when objects transition to different storage classes, or when they expire and are deleted. This can help you reduce your storage costs by moving objects to lower-cost storage classes or deleting them when they are no longer needed. You can set up Lifecycle rules at the bucket level or at the object level (for individual objects or for groups of objects). You can also specify different rules for different prefixes or object tags.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transitioning objects between classes is possible&lt;/li&gt;
&lt;li&gt;Transition Actions can be used to configure objects to transition to another storage class&lt;/li&gt;
&lt;li&gt;Transition Actions can also be used for expiration, incomplete multi part uploads etc.&lt;/li&gt;
&lt;li&gt;Rules can be applied to buckets, specific paths of the project or also to tags&lt;/li&gt;
&lt;li&gt;Amazon S3 analytics works exclusively on S3 standard, and S3 IA, and provides analytics on usage&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;performance-chart&#34;&gt;Performance Chart&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/s3_storage_classes.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;encryption&#34;&gt;Encryption&lt;/h3&gt;
&lt;p&gt;Amazon S3 supports several encryption options to help users secure their data at rest. These options include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SSE-S3: This option uses server-side encryption with Amazon S3-managed keys. With this option, Amazon S3 encrypts the data as it is written to disks in its data centers and decrypts it when it is accessed.&lt;/li&gt;
&lt;li&gt;SSE-KMS: This option uses server-side encryption with AWS KMS-managed keys. With this option, users can create, rotate, and manage the keys used to encrypt their data.&lt;/li&gt;
&lt;li&gt;SSE-C: This option allows users to use their own encryption keys to encrypt their data. Users are responsible for securely managing their keys and rotating them as needed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Users can enable encryption when creating a new bucket or when uploading an object to an existing bucket. They can also enable encryption for all objects in an existing bucket by enabling bucket-level encryption.&lt;/p&gt;
&lt;h3 id=&#34;security-policy&#34;&gt;Security Policy&lt;/h3&gt;
&lt;p&gt;Amazon S3 bucket policies allow users to add additional security controls to their S3 buckets and objects. A bucket policy is a JSON document that defines the permissions for an S3 bucket. It can be used to grant permissions to other AWS accounts, or to grant public access to a bucket and its objects.&lt;/p&gt;
&lt;p&gt;With a bucket policy, users can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Grant read and write permissions to a specific AWS account for all objects in a bucket.&lt;/li&gt;
&lt;li&gt;Grant read-only permissions to the anonymous user for all objects in a bucket.&lt;/li&gt;
&lt;li&gt;Grant read and write permissions to a specific AWS account for all objects with a specific prefix (such as &amp;ldquo;private/&amp;rdquo;).&lt;/li&gt;
&lt;li&gt;Deny all access to a specific AWS account for all objects in a bucket.&lt;/li&gt;
&lt;li&gt;It is important for users to carefully consider the permissions they grant in their bucket policy, as it can have wide-ranging effects on the security of the bucket and its contents.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;misc&#34;&gt;Misc&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Amazon S3 VPC Endpoints allow users to access Amazon S3 from within their virtual private cloud (VPC) without the need for an Internet gateway, NAT device, or VPN connection. With VPC Endpoints, users can access S3 from their VPC over an optimized network path, reducing Internet traffic and improving performance.&lt;/li&gt;
&lt;li&gt;Users can create a VPC Endpoint for Amazon S3 in their VPC, and then configure their VPC security groups and IAM policies to allow access to the endpoint. They can then use the endpoint to access Amazon S3 using the Amazon S3 APIs or the AWS Management Console, just as they would over the Internet.&lt;/li&gt;
&lt;li&gt;VPC Endpoints for Amazon S3 are supported in all regions and are available in two types: Gateway Endpoints and Interface Endpoints. Gateway Endpoints are powered by a highly available network gateway, while Interface Endpoints are powered by a highly available Network Load Balancer. Users can choose the endpoint type that best meets their needs.&lt;/li&gt;
&lt;li&gt;Amazon S3 CloudTrail is a service that enables users to record API calls made to Amazon S3 and log the events to an Amazon S3 bucket. This allows users to track changes to their objects, buckets, and Amazon S3 configurations, and to identify and troubleshoot issues.&lt;/li&gt;
&lt;li&gt;With CloudTrail, users can:
&lt;ul&gt;
&lt;li&gt;Track changes to their Amazon S3 objects and bucket metadata.&lt;/li&gt;
&lt;li&gt;Determine who made a change and when it was made.&lt;/li&gt;
&lt;li&gt;Audit changes to their Amazon S3 bucket and object permissions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CloudTrail logs are stored in an Amazon S3 bucket that the user specifies, and they can be delivered to an Amazon CloudWatch Logs log group or an Amazon SNS topic. Users can use the CloudTrail logs to monitor their S3 resources and to ensure compliance with their policies.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kinesis&#34;&gt;Kinesis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Kinesis is a fully managed (alternative to Kafka), cloud-based service that enables users to process and analyze streaming data in real-time. With Kinesis, users can build custom applications that process and analyze data as it arrives, and they can scale these applications to process any volume of data, at any time.&lt;/li&gt;
&lt;li&gt;Kinesis consists of three main components:
&lt;ul&gt;
&lt;li&gt;Producers: Producers are sources of data that send data records to Kinesis streams.&lt;/li&gt;
&lt;li&gt;Kinesis streams: A Kinesis stream is a sequence of data records that are persisted for a set period of time. Users can create and delete streams, and they can specify the number of shards in a stream.&lt;/li&gt;
&lt;li&gt;Consumers: Consumers are applications that read and process data records from Kinesis streams.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Kinesis is designed to be highly available and durable, and it can automatically scale to handle increases in traffic.&lt;/li&gt;
&lt;li&gt;Users can use Kinesis to build custom applications that can process and analyze real-time data streams, and they can use the service to support a wide range of use cases, such as real-time analytics, fraud detection, and Internet of Things (IoT) applications.&lt;/li&gt;
&lt;li&gt;Data is replicated to at least 3 AZ&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kinesis-streams&#34;&gt;Kinesis Streams&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Kinesis Streams is a fully managed, cloud-based service that allows real-time processing of streaming data at high scale.&lt;/li&gt;
&lt;li&gt;It can continuously capture and store terabytes of data per hour from hundreds of thousands of sources, such as website clickstreams, financial transactions, social media feeds, IT logs, and location-tracking events.&lt;/li&gt;
&lt;li&gt;With Kinesis Streams, users can build custom applications that process or analyze the data as it arrives, or they can use the provided Kinesis Data Streams API to load the data into other AWS services, such as Amazon S3, Amazon Redshift, or Amazon Elasticsearch Service, for long-term storage and analysis.&lt;/li&gt;
&lt;li&gt;Streams are divided into shards and partitions&lt;/li&gt;
&lt;li&gt;The maximum throughput of a single shard 1 mb/seconds or 1000 messages/seconds&lt;/li&gt;
&lt;li&gt;Data retention: 24 hours by default. It can go up to 365 days. This is useful for reprocessing/replaying data&lt;/li&gt;
&lt;li&gt;Immutable, 1 mb in size&lt;/li&gt;
&lt;li&gt;Provisioned mode: choose number of shards and scale manually or using an API&lt;/li&gt;
&lt;li&gt;Each shard gets 1mb/s in, 2mb/s out&lt;/li&gt;
&lt;li&gt;On demand mode: each capacity provisioned is 4mb/s&lt;/li&gt;
&lt;li&gt;If you can plan capacity, use provisioned. however, use on demand if capacity is unknown&lt;/li&gt;
&lt;li&gt;Custom code for producer or consumer is possible&lt;/li&gt;
&lt;li&gt;Real time (200 ms latency, possible all the way up to 70ms)&lt;/li&gt;
&lt;li&gt;Automatic scaling with on-demand mode&lt;/li&gt;
&lt;li&gt;Multi consumers is possible from one source&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kinesis-analytics&#34;&gt;Kinesis Analytics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Kinesis Analytics is a fully managed, cloud-based service that allows users to process and analyze streaming data in real-time with SQL.&lt;/li&gt;
&lt;li&gt;With Kinesis Analytics, users can run ad-hoc queries on the data, or they can set up a SQL-based stream processing application to perform transformations on the data as it arrives. SQL or Apache Flink can be used here.&lt;/li&gt;
&lt;li&gt;The output of these queries and transformations can be fed back into Kinesis Streams for further processing, or it can be stored in other AWS services, such as Amazon S3 or Amazon Redshift, for long-term analysis.&lt;/li&gt;
&lt;li&gt;Select columns, continious metric generation, responsive analytics, etc.&lt;/li&gt;
&lt;li&gt;Serverless, scales automatically, pay for resouces consumed but expensive&lt;/li&gt;
&lt;li&gt;Schema discovery&lt;/li&gt;
&lt;li&gt;Lambda can be used for preprocessing&lt;/li&gt;
&lt;li&gt;Two machine learning algorithms:
&lt;ul&gt;
&lt;li&gt;Random cut forest for anomaly detection on numeric columns in a stream, uses recent data to compute the model. A random cut forest (RCF) is a machine learning algorithm that is used for anomaly detection in streaming data. It works by constructing a number of decision trees on randomly selected subsets of the data, and then comparing the score for each new data point to the scores of similar points in the trees. If the score for a new data point is significantly lower than the scores of similar points in the trees, it is considered to be an anomaly. The number of trees in the forest and the size of the subsets of data used to train each tree can be adjusted to control the sensitivity of the model. RCFs are particularly well-suited for detecting anomalies in large, high-dimensional datasets, and they are often used in conjunction with streaming data platforms, such as Amazon Kinesis Streams.&lt;/li&gt;
&lt;li&gt;Hotspots: A hotspots algorithm is a type of machine learning algorithm that is used to identify spatial clusters of events or observations in a dataset. These clusters, which are also known as hotspots, are areas in which the concentration of events or observations is significantly higher than the surrounding areas. Hotspots algorithms are often used in a variety of applications, such as crime mapping, disease surveillance, and marketing analysis. There are several different approaches to identifying hotspots, including spatial clustering methods, spatial scan statistics, and kernel density estimation. These methods can be applied to a variety of types of data, including point data, such as crime incidents or disease cases, and areal data, such as census tracts or zip codes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kinesis-firehose&#34;&gt;Kinesis Firehose&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Kinesis Firehose is a fully managed service&lt;/li&gt;
&lt;li&gt;makes it easy to load streaming data into data stores and analytics tools&lt;/li&gt;
&lt;li&gt;It can capture, transform, and load data streams into Amazon S3, Amazon Redshift, and Amazon Elasticsearch Service, Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards&lt;/li&gt;
&lt;li&gt;Kinesis Firehose is a simple and reliable way to load streaming data into data stores and analytics tools.&lt;/li&gt;
&lt;li&gt;most common is firehose reading from kinesis streams&lt;/li&gt;
&lt;li&gt;near realtime service because it batch writes&lt;/li&gt;
&lt;li&gt;data desitination can be s3, redshift, elastisearch, splunk, new relic, or http endpoint&lt;/li&gt;
&lt;li&gt;60 seconds latency minimum for non full batches&lt;/li&gt;
&lt;li&gt;data ingestion into redshift, s3, elasticsearch, splunk&lt;/li&gt;
&lt;li&gt;automatic scaling&lt;/li&gt;
&lt;li&gt;conversions from csv/json to parquet and orc and requires the use of glue&lt;/li&gt;
&lt;li&gt;and transformation through lambda csv to json is possible&lt;/li&gt;
&lt;li&gt;compression is possible&lt;/li&gt;
&lt;li&gt;automates scaling&lt;/li&gt;
&lt;li&gt;no data storage&lt;/li&gt;
&lt;li&gt;no replay capability&lt;/li&gt;
&lt;li&gt;it is a serverless transformation tool&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kinesis-video-streams&#34;&gt;Kinesis Video Streams&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Web Services (AWS) Kinesis Video Streams is a fully managed service that allows users to stream live video from connected devices to the cloud.&lt;/li&gt;
&lt;li&gt;This service is designed to make it easy to build applications that process and analyze live video streams, as well as store and transmit videos securely at scale.&lt;/li&gt;
&lt;li&gt;With Kinesis Video Streams, users can stream live video from millions of devices and easily build applications for real-time video analytics and machine learning.&lt;/li&gt;
&lt;li&gt;In addition, the service allows users to stream video directly to other AWS services, such as Amazon S3, Amazon Kinesis Data Streams, and Amazon Rekognition, for further processing and analysis.&lt;/li&gt;
&lt;li&gt;Producers: security camera, body-worn cam, aws deeplens, radar data, camera&lt;/li&gt;
&lt;li&gt;One producer per video stream&lt;/li&gt;
&lt;li&gt;Video playback capability&lt;/li&gt;
&lt;li&gt;Sagemaker, rekognition video, 1 hour to 10 years of storage&lt;/li&gt;
&lt;li&gt;Checkpointing via dynamodb, frames to Sagemaker for ML inference, publish to stream, lambda can be used for notification.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;glue&#34;&gt;Glue&lt;/h2&gt;
&lt;h3 id=&#34;glue-data-catalog-and-glue-data-crawlers&#34;&gt;Glue Data Catalog and Glue Data Crawlers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;AWS Glue Data Catalog is a fully managed, cloud-native metadata store that provides a central place to store, annotate, and share metadata across AWS services, applications, and tools.&lt;/li&gt;
&lt;li&gt;It makes it easy to discover and understand data, and facilitates the development of data-driven applications.&lt;/li&gt;
&lt;li&gt;With the Glue Data Catalog, users can create, maintain, and access metadata such as database and table definitions, column names and data types, and data lineage.&lt;/li&gt;
&lt;li&gt;The Glue Data Catalog is integrated with other AWS services such as Amazon Redshift, Amazon Athena, and Amazon EMR, and is accessible through the AWS Management Console, the AWS Glue API, and the AWS Glue ETL (extract, transform, and load) library.&lt;/li&gt;
&lt;li&gt;Schemas are versioned&lt;/li&gt;
&lt;li&gt;Glue crawlers help build the Catalog&lt;/li&gt;
&lt;li&gt;Glue will also extract the partitions, this is helpful for query optimization&lt;/li&gt;
&lt;li&gt;Glue Data Crawlers are a tool within the Amazon Glue service that allows users to extract metadata from their data stores and create table definitions in the Glue Data Catalog.&lt;/li&gt;
&lt;li&gt;This enables the creation of ETL jobs and development endpoints in Glue, which can be used to move and transform data.&lt;/li&gt;
&lt;li&gt;Glue Data Crawlers can connect to various data stores, including Amazon S3 and RDS, as well as any JDBC-compliant data store.&lt;/li&gt;
&lt;li&gt;Custom connectors for other data stores can also be created using the Glue ETL library. To use Glue Data Crawlers, a Glue ETL job or development endpoint must first be created, after which the Glue ETL library can be utilized for data movement and transformation tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;glue-etl&#34;&gt;Glue ETL&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Transform data, clean data, enrich data before doing analysis&lt;/li&gt;
&lt;li&gt;Generate ETL code in python or scala, you can modify the code&lt;/li&gt;
&lt;li&gt;Possible to provide your own spark or pyspark scripts&lt;/li&gt;
&lt;li&gt;Target can be S3, JDBC or in glue data catalog&lt;/li&gt;
&lt;li&gt;Fully managed, cost effective, pay only for the resources consumed&lt;/li&gt;
&lt;li&gt;Jobs are run on a serverless Spark platform&lt;/li&gt;
&lt;li&gt;Glue scheduler to schedule the jobs&lt;/li&gt;
&lt;li&gt;Glue triggers to automate job runs based on events&lt;/li&gt;
&lt;li&gt;Transformations can be bundled (drop, filter, join, map)&lt;/li&gt;
&lt;li&gt;Machine learning transformation (find matches, duplicates even when data do not match exactly, dedup)&lt;/li&gt;
&lt;li&gt;Any apache spark transformation is possible, and changing in format is possible.&lt;/li&gt;
&lt;li&gt;Multiple ways to create glue jobs including visual editors, python notebooks, python script, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;redshift&#34;&gt;Redshift&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Redshift is a fully managed data warehouse service offered by Amazon Web Services (AWS).&lt;/li&gt;
&lt;li&gt;It is designed to handle petabyte-scale data warehouses and make it easy to analyze data using SQL and business intelligence tools.&lt;/li&gt;
&lt;li&gt;Amazon Redshift is based on PostgreSQL, and it supports many of the same data types and functions as PostgreSQL.&lt;/li&gt;
&lt;li&gt;To use Amazon Redshift, users first need to set up a cluster of compute nodes. They can then load data into the cluster and perform SQL queries on the data. Amazon Redshift integrates with various data sources and destinations, including Amazon S3, Amazon EMR, and Amazon RDS.&lt;/li&gt;
&lt;li&gt;It also integrates with a variety of business intelligence tools, such as Quicksight, Tableau, Qlik, and MicroStrategy.&lt;/li&gt;
&lt;li&gt;Amazon Redshift offers a number of features to help users manage their data warehouses, including automatic data compression, data replication, and data security. It also provides a number of performance enhancements, such as columnar storage, data caching, and parallel query execution.&lt;/li&gt;
&lt;li&gt;Overall, Amazon Redshift is a powerful and scalable data warehouse solution for analyzing large datasets in the cloud.&lt;/li&gt;
&lt;li&gt;OLAP&lt;/li&gt;
&lt;li&gt;Uses SQL to analyze structured and semi-structured data across data warehouses, operational databases, and data lakes&lt;/li&gt;
&lt;li&gt;Redshift Spectrum can directly query from S3&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;rds&#34;&gt;RDS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Relational Database Service (RDS) is a fully managed database service offered by Amazon Web Services (AWS).&lt;/li&gt;
&lt;li&gt;It makes it easy to set up, operate, and scale a relational database in the cloud.&lt;/li&gt;
&lt;li&gt;Amazon RDS supports a variety of database engines, including MySQL, MariaDB, PostgreSQL, Oracle, and Microsoft SQL Server.&lt;/li&gt;
&lt;li&gt;With Amazon RDS, users can create and manage a database without the need to install and maintain database software.&lt;/li&gt;
&lt;li&gt;Amazon RDS handles tasks such as hardware provisioning, database setup, patching, and backups.&lt;/li&gt;
&lt;li&gt;It also provides features such as automated failover and read replicas to help users improve availability and scalability.&lt;/li&gt;
&lt;li&gt;Amazon RDS is a popular choice for applications that require a relational database, such as e-commerce, content management, and customer relationship management systems.&lt;/li&gt;
&lt;li&gt;It is particularly well-suited for use cases that require high availability and low latency, such as online transaction processing (OLTP).&lt;/li&gt;
&lt;li&gt;Must provision servers in advance&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;dynamodb&#34;&gt;DynamoDB&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon DynamoDB is a fully managed NoSQL database service offered by Amazon Web Services (AWS).&lt;/li&gt;
&lt;li&gt;It is designed to be scalable, fast, and flexible, making it a good choice for applications that need high performance and low latency.&lt;/li&gt;
&lt;li&gt;DynamoDB stores data in tables, and each table has a primary key that uniquely identifies each item. The primary key can be either a simple primary key (a single attribute) or a composite primary key (a combination of two or more attributes).&lt;/li&gt;
&lt;li&gt;DynamoDB supports both key-value and document data models, and it offers a number of powerful features, such as global secondary indexes, auto scaling, and stream-based data replication.&lt;/li&gt;
&lt;li&gt;DynamoDB is a popular choice for applications that need to store large amounts of data that is frequently read or written, such as online gaming, real-time analytics, and IoT applications.&lt;/li&gt;
&lt;li&gt;It is also well-suited for applications that need to scale rapidly, as it can automatically adjust capacity to meet changing demand.&lt;/li&gt;
&lt;li&gt;Useful to store ML model (or checkpoints)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;opensearch&#34;&gt;OpenSearch&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Previously ElasticSearch&lt;/li&gt;
&lt;li&gt;Amazon OpenSearch is a search service that makes it easy to build and run search applications.&lt;/li&gt;
&lt;li&gt;It is based on the open source Apache Lucene search engine, and it provides a number of features to help users build sophisticated search experiences, such as full-text search, faceted search, and hit highlighting.&lt;/li&gt;
&lt;li&gt;With Amazon OpenSearch, users can index and search large datasets, such as websites, documents, and logs.&lt;/li&gt;
&lt;li&gt;They can also customize the search experience by adding search criteria, filters, and facets, and by displaying search results in various formats.&lt;/li&gt;
&lt;li&gt;Amazon OpenSearch also provides analytics and monitoring capabilities to help users understand how their search applications are being used.&lt;/li&gt;
&lt;li&gt;Amazon OpenSearch is a flexible and scalable search solution that is well-suited for a wide range of applications, such as e-commerce, content management, and data analysis.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;aws-data-pipeline&#34;&gt;AWS Data Pipeline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Data Pipeline is a fully managed data processing service that helps users move and transform data between data stores.&lt;/li&gt;
&lt;li&gt;It is designed to be easy to use and highly reliable, and it can handle data processing tasks of any size.&lt;/li&gt;
&lt;li&gt;With Amazon Data Pipeline, users can create pipelines that move data between data stores, such as Amazon S3, Amazon RDS, and Amazon Redshift.&lt;/li&gt;
&lt;li&gt;They can also use Data Pipeline to transform data, such as by aggregating, filtering, or joining data from different sources. Data Pipeline supports a variety of data formats and sources, and it can be used to schedule and automate data processing tasks.&lt;/li&gt;
&lt;li&gt;Amazon Data Pipeline is a useful tool for a wide range of data processing tasks, such as data warehousing, ETL, and analytics. It is particularly well-suited for use cases that involve moving and transforming large amounts of data, as it can scale to handle data processing needs of any size.&lt;/li&gt;
&lt;li&gt;Data sources can be on premise&lt;/li&gt;
&lt;li&gt;Runs on EC2 but fully managed&lt;/li&gt;
&lt;li&gt;Orchestration service&lt;/li&gt;
&lt;li&gt;Glue is managed, serverless, spark focused, ETL focused, has catalog&lt;/li&gt;
&lt;li&gt;Data Pipeline is orchestation tool, and can do more&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;aws-batch&#34;&gt;AWS Batch&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;For any non-ETL batch is usually better than glue&lt;/li&gt;
&lt;li&gt;Amazon Web Services Batch is a fully managed batch processing service that makes it easy to run batch computing workloads on the AWS Cloud.&lt;/li&gt;
&lt;li&gt;It is designed to be scalable, fault-tolerant, and flexible, and it supports a wide range of workloads, such as machine learning, data processing, and scientific simulations.&lt;/li&gt;
&lt;li&gt;With AWS Batch, users can define batch computing workloads as &amp;ldquo;jobs&amp;rdquo; and &amp;ldquo;tasks,&amp;rdquo; and the service automatically provisions the required compute resources and executes the tasks.&lt;/li&gt;
&lt;li&gt;Users can specify the desired level of concurrency and resource allocation for their jobs, and AWS Batch will automatically scale up or down as needed.&lt;/li&gt;
&lt;li&gt;AWS Batch also integrates with other AWS services, such as Amazon S3 and Amazon ECS, to provide a complete batch processing solution.&lt;/li&gt;
&lt;li&gt;AWS Batch is a useful tool for organizations that need to run large-scale batch computing workloads, such as financial analysis, scientific simulations, and media processing.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;li&gt;Batch can be scheduled using cloudwatch, step functions&lt;/li&gt;
&lt;li&gt;Not just for ETL but absolutely anything at all&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;aws-dms&#34;&gt;AWS DMS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Web Services Database Migration Service (AWS DMS) is a fully managed service that makes it easy to migrate databases to the AWS Cloud.&lt;/li&gt;
&lt;li&gt;It is designed to be reliable, efficient, and flexible, and it supports a wide range of database platforms, including Oracle, MySQL, and Microsoft SQL Server.&lt;/li&gt;
&lt;li&gt;With AWS DMS, users can migrate their databases to the AWS Cloud with minimal downtime.&lt;/li&gt;
&lt;li&gt;The service handles tasks such as data extraction, transformation, and load, and it supports both one-time and ongoing migrations.&lt;/li&gt;
&lt;li&gt;AWS DMS also provides a number of features to help users manage their database migrations, such as change data capture, data transformation, and task scheduling.&lt;/li&gt;
&lt;li&gt;AWS DMS is a useful tool for organizations that want to migrate their databases to the cloud, or that need to replicate their databases across multiple regions for disaster recovery or other purposes.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;li&gt;Supports homogeneous migrations and heterogeneous migrations&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-functions&#34;&gt;Step Functions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Web Services Step Functions is a fully managed service that makes it easy to coordinate the various components of complex, distributed applications.&lt;/li&gt;
&lt;li&gt;It is based on the concepts of tasks and state machines, and it provides a visual workflow editor to help users design and manage their applications.&lt;/li&gt;
&lt;li&gt;With AWS Step Functions, users can define and execute workflows that coordinate multiple AWS services, such as AWS Lambda, Amazon ECS, and AWS Batch.&lt;/li&gt;
&lt;li&gt;The service automatically scales to meet the needs of the workflow, and it provides features such as error handling and retry logic to help users build resilient applications.&lt;/li&gt;
&lt;li&gt;AWS Step Functions is a useful tool for organizations that need to coordinate the various components of complex, distributed applications, such as data pipelines, machine learning workflows, and microservices architectures.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;li&gt;Audit of history of workflow&lt;/li&gt;
&lt;li&gt;Allows waiting&lt;/li&gt;
&lt;li&gt;Maximum execution time of 1 year&lt;/li&gt;
&lt;li&gt;Can be used to train/tune a ML model&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;efs&#34;&gt;EFS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Elastic File System (EFS) is a fully managed, cloud-native file storage service that makes it easy to store and access files from multiple Amazon Elastic Compute Cloud (EC2) instances.&lt;/li&gt;
&lt;li&gt;It is designed to be scalable, highly available, and easy to use, and it supports the Network File System (NFS) protocol.&lt;/li&gt;
&lt;li&gt;With AWS EFS, users can create file systems and store files in them, and they can access the files from multiple EC2 instances at the same time.&lt;/li&gt;
&lt;li&gt;EFS automatically scales up or down as needed to meet the storage and performance needs of the applications, and it provides features such as file system access control and data durability to help users manage their file storage.&lt;/li&gt;
&lt;li&gt;AWS EFS is a useful tool for organizations that need to store and access files from multiple EC2 instances, such as web servers, application servers, and development environments.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ebs&#34;&gt;EBS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Elastic Block Store (EBS) is a fully managed, cloud-native block storage service that makes it easy to store and access data from Amazon Elastic Compute Cloud (EC2) instances.&lt;/li&gt;
&lt;li&gt;It is designed to be scalable, highly available, and easy to use, and it supports a variety of storage types and performance levels.&lt;/li&gt;
&lt;li&gt;With AWS EBS, users can create storage volumes and attach them to EC2 instances, and they can use the volumes to store and access data.&lt;/li&gt;
&lt;li&gt;EBS provides a number of features to help users manage their storage, such as snapshotting, data replication, and encryption.&lt;/li&gt;
&lt;li&gt;It also supports a variety of storage types, including SSD-backed volumes for high performance and HDD-backed volumes for lower cost.&lt;/li&gt;
&lt;li&gt;AWS EBS is a useful tool for organizations that need to store and access data from EC2 instances, such as databases, file systems, and applications.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;li&gt;EBS volumes are attached to specific EC2 instances, and they scale with the needs of the applications running on those instances.&lt;/li&gt;
&lt;li&gt;EFS file systems, on the other hand, can be accessed concurrently by multiple EC2 instances, and they scale automatically to meet the needs of the workload.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;emr&#34;&gt;EMR&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Elastic MapReduce (EMR) is a fully managed, cloud-native big data processing service that makes it easy to process large amounts of data using Apache Hadoop and Apache Spark.&lt;/li&gt;
&lt;li&gt;It is designed to be scalable, highly available, and easy to use, and it integrates with a variety of data sources and destinations.&lt;/li&gt;
&lt;li&gt;With AWS EMR, users can create clusters of Amazon Elastic Compute Cloud (EC2) instances and use them to process and analyze large datasets stored in Amazon S3 or other data stores.&lt;/li&gt;
&lt;li&gt;EMR supports a variety of big data processing frameworks, including Hadoop, Spark, and Flink, and it provides tools to help users manage and monitor their clusters.&lt;/li&gt;
&lt;li&gt;AWS EMR is a useful tool for organizations that need to process and analyze large amounts of data, such as log files, scientific simulations, and machine learning models.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;misc-1&#34;&gt;Misc&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;AWS DataSync: for data migrations from on-premises to AWS storage services&lt;/li&gt;
&lt;li&gt;MQTT: IOT protocol, Standard messaging protocol&lt;/li&gt;
&lt;li&gt;Apache Spark, Apache Hive, Apache Hadoop, and Apache Pig are all open-source technologies that are used for data processing and analysis. However, they are designed for different purposes and have different strengths and weaknesses.
&lt;ul&gt;
&lt;li&gt;Apache Spark is a fast, in-memory data processing engine that is used for real-time data processing and analytics. It is particularly well-suited for use cases that require fast processing times, such as streaming data and interactive data exploration.&lt;/li&gt;
&lt;li&gt;Apache Hive is a data warehousing and SQL-like query language that is used to process and analyze large datasets stored in the Hadoop Distributed File System (HDFS). It is particularly well-suited for use cases that involve complex data transformations and aggregations.&lt;/li&gt;
&lt;li&gt;Apache Hadoop is a distributed computing platform that is used to store and process large amounts of data. It is composed of several modules, including HDFS for storing data, YARN for resource management, and MapReduce for parallel data processing. Hadoop is a popular choice for batch processing and offline data analysis.&lt;/li&gt;
&lt;li&gt;Apache Pig is a high-level data processing language that is used to write and execute MapReduce jobs on Apache Hadoop. It is particularly well-suited for use cases that involve complex data transformations and complex data structures.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>AWS Certified ML - Specialty exam (MLS-C01) - 2. Exploratory Data Analysis</title>
      <link>https://ayushsubedi.github.io/posts/aws_ml_speciality_eda/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/aws_ml_speciality_eda/</guid>
      <description>&lt;h1 id=&#34;exploratory-data-analysis&#34;&gt;Exploratory Data Analysis&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#python-in-data-science-and-machine-learning&#34;&gt;Python in data science and machine learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-data&#34;&gt;Types of Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-distributions&#34;&gt;Data Distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trends-and-seasonality&#34;&gt;Trends and seasonality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#athena&#34;&gt;Athena&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quicksight&#34;&gt;Quicksight&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-visualization&#34;&gt;Types of visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#emr&#34;&gt;EMR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hadoop&#34;&gt;Hadoop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#apache-spark&#34;&gt;Apache Spark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#emr-notebooks,-security-and-instance-types&#34;&gt;EMR Notebooks, Security and Instance Types&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feature-engineering&#34;&gt;Feature Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#imputing-missing-data&#34;&gt;Imputing Missing Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#unbalanced-data&#34;&gt;Unbalanced Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#handling-outliers&#34;&gt;Handling Outliers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#binning,-transoforming,-encoding,-scaling,-and-shuffling&#34;&gt;Binning, Transoforming, Encoding, Scaling, and Shuffling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-sagemaker-ground-truth-and-label-generation&#34;&gt;Amazon Sagemaker Ground Truth and Label Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#misc&#34;&gt;Misc&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This section requires understanding of sanitizing and preparing data for modeling, performing feature engineering, and analyzing and visualizing data for machine learning.&lt;/p&gt;
&lt;p&gt;Exploratory Data Analysis (EDA) is a process of analyzing and summarizing a dataset in order to understand its structure and relationships. In the context of Amazon Web Services (AWS), EDA is often performed on large datasets that are stored in AWS storage services such as Amazon S3 or Amazon EBS.&lt;/p&gt;
&lt;p&gt;To perform EDA on AWS, users can use various tools and services provided by AWS. For example, users can use Amazon Elastic MapReduce (EMR) to process and analyze large datasets using tools such as Apache Spark or Hive. Users can also use Amazon Athena to query datasets stored in Amazon S3 using SQL.&lt;/p&gt;
&lt;p&gt;In addition to these tools, users can also use various AWS services and libraries to visualize and explore the data. For example, users can use Amazon QuickSight to create interactive charts and dashboards, or use libraries such as pandas and matplotlib to create custom visualizations.&lt;/p&gt;
&lt;p&gt;Overall, EDA on AWS involves using a combination of tools and services to understand the structure and relationships within a dataset, and to gain insights that can inform further analysis and decision making.&lt;/p&gt;
&lt;h2 id=&#34;python-in-data-science-and-machine-learning&#34;&gt;Python in data science and machine learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Python code will not be tested in the exam.&lt;/li&gt;
&lt;li&gt;Python is a popular language for data exploration, analysis, and machine learning. It has a number of useful libraries for loading, manipulating, and visualizing data, as well as for building and training machine learning models.&lt;/li&gt;
&lt;li&gt;For data exploration and visualization, some popular libraries include pandas, numpy, and matplotlib. Pandas is a library for working with tabular data, numpy is a library for working with numerical data, and matplotlib is a library for creating charts and plots.&lt;/li&gt;
&lt;li&gt;For machine learning, some popular libraries include scikit-learn, tensorflow, and pytorch. These libraries include a wide range of tools for tasks such as classification, regression, clustering, and deep learning.&lt;/li&gt;
&lt;li&gt;Overall, Python is a powerful and flexible language for data analysis and machine learning, and is widely used in the field.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;types-of-data&#34;&gt;Types of Data&lt;/h2&gt;
&lt;p&gt;There are many different types of data, and the type of data can often influence the analysis and techniques used to understand it. Some common types of data include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Numeric data: This includes data that is represented as numbers, such as integers or floating point values.&lt;/li&gt;
&lt;li&gt;Categorical data: This includes data that consists of categories or groups, such as gender or eye color.&lt;/li&gt;
&lt;li&gt;Ordinal data: This is similar to categorical data, but the categories have a natural ordering, such as low, medium, and high.&lt;/li&gt;
&lt;li&gt;Binary data: This is data that has only two categories, such as true/false or 0/1.&lt;/li&gt;
&lt;li&gt;Time series data: This is data that is collected over time, such as daily stock prices or monthly sales figures.&lt;/li&gt;
&lt;li&gt;Text data: This is data that is represented as text, such as emails or social media posts.&lt;/li&gt;
&lt;li&gt;Image data: This is data that is represented as images, such as photographs or videos.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-distributions&#34;&gt;Data Distributions&lt;/h2&gt;
&lt;h3 id=&#34;normal-distribution&#34;&gt;Normal Distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The normal distribution, also known as the Gaussian distribution, is a continuous probability distribution that is defined by a symmetrical bell-shaped curve.&lt;/li&gt;
&lt;li&gt;It is one of the most widely used and well-known probability distributions in statistics, and is commonly used to model real-valued random variables.&lt;/li&gt;
&lt;li&gt;The normal distribution is completely defined by its mean and standard deviation.&lt;/li&gt;
&lt;li&gt;The mean is the center of the distribution and determines the location of the peak of the curve.&lt;/li&gt;
&lt;li&gt;The standard deviation is a measure of the spread of the distribution and determines the width of the curve.&lt;/li&gt;
&lt;li&gt;A larger standard deviation means that the data is more spread out, while a smaller standard deviation means that the data is more concentrated around the mean.&lt;/li&gt;
&lt;li&gt;The normal distribution has a number of useful properties. For example, the empirical rule states that for a normal distribution, approximately 68% of the data lies within one standard deviation of the mean, 95% of the data lies within two standard deviations of the mean, and 99.7% of the data lies within three standard deviations of the mean.&lt;/li&gt;
&lt;li&gt;This means that if a dataset follows a normal distribution, a large percentage of the data will be concentrated around the mean. Overall, the normal distribution is a widely used and important distribution in statistics, and is often used to model real-valued data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;probability-mass-function-discrete-data-type&#34;&gt;Probability Mass function (discrete data type)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A probability mass function (PMF) is a function that gives the probability of a discrete random variable taking on a particular value. For a random variable X, the PMF is denoted as f(x), and it is defined as the probability that X takes on the value x.&lt;/li&gt;
&lt;li&gt;The PMF is a useful tool for describing the probability distribution of a discrete random variable. It specifies the probability of each possible outcome, and can be used to calculate various statistical quantities such as the mean, variance, and skewness of the distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bernoulli-discrete-data-type&#34;&gt;Bernoulli (discrete data type)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Bernoulli distribution is a discrete probability distribution that models the probability of a binary outcome, such as the result of a coin flip or a yes/no question. It is defined by a single parameter p, which represents the probability of success (the probability of the outcome being &amp;ldquo;yes&amp;rdquo;).&lt;/li&gt;
&lt;li&gt;The Bernoulli distribution is a special case of the binomial distribution, where the number of trials is fixed at n=1. In other words, it models a single binary event, such as the flip of a coin.&lt;/li&gt;
&lt;li&gt;The probability mass function (PMF) of a Bernoulli-distributed random variable X is given by the following formula:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(x) = p^x * (1-p)^(1-x)&lt;/p&gt;
&lt;p&gt;where x is the outcome (0 for &amp;ldquo;no&amp;rdquo; and 1 for &amp;ldquo;yes&amp;rdquo;), and p is the probability of success.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Bernoulli distribution has a mean of p and a variance of p(1-p). It is a simple but widely used distribution, and is often used as a building block for more complex models.&lt;/li&gt;
&lt;li&gt;Overall, the Bernoulli distribution is a useful tool for modeling the probability of a binary outcome, and is widely used in a variety of applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;binomial-discrete-data-type&#34;&gt;Binomial (discrete data type)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The binomial distribution is a discrete probability distribution that is used to model the probability of a certain number of successes in a fixed number of independent trials. It is defined by two parameters: the number of trials (n) and the probability of success in each trial (p).&lt;/li&gt;
&lt;li&gt;The binomial distribution can be used to model a wide variety of situations, such as the probability of flipping a coin and getting a certain number of heads in a row, or the probability of a certain number of defects occurring in a batch of products.&lt;/li&gt;
&lt;li&gt;The probability mass function (PMF) of a binomial-distributed random variable X is given by the following formula:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(x) = (n choose x) * p^x * (1-p)^(n-x)&lt;/p&gt;
&lt;p&gt;where x is the number of successes, n is the number of trials, p is the probability of success in each trial, and &amp;ldquo;choose&amp;rdquo; represents the binomial coefficient.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The binomial distribution has a number of useful properties, such as the fact that the mean and variance can be easily calculated from the parameters n and p. It is also a limiting distribution, meaning that it can be used to approximate other distributions under certain conditions.&lt;/li&gt;
&lt;li&gt;Overall, the binomial distribution is a useful tool for modeling the probability of a certain number of successes in a fixed number of independent trials, and is widely used in a variety of applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;geometric-distribution&#34;&gt;Geometric Distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The geometric distribution is a discrete probability distribution that models the number of Bernoulli trials (i.e., a series of independent &amp;ldquo;success-failure&amp;rdquo; experiments) needed to get a success. It is defined by a single parameter p, which is the probability of success on each trial.&lt;/li&gt;
&lt;li&gt;The probability mass function (PMF) of the geometric distribution is given by:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(k) = (1 - p)^(k-1) * p&lt;/p&gt;
&lt;p&gt;where k is the number of trials needed to get a success and p is the probability of success on each trial.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The geometric distribution has several characteristics:
&lt;ul&gt;
&lt;li&gt;It is a one-sided distribution, which means that it is defined only for k &amp;gt; 0.&lt;/li&gt;
&lt;li&gt;It is a discrete distribution, which means that it is defined for a specific set of values rather than for a continuous range of values.&lt;/li&gt;
&lt;li&gt;It has a mean of 1/p, which is the expected number of trials needed to get a success.&lt;/li&gt;
&lt;li&gt;The geometric distribution is often used in modeling the number of trials needed to get a success, such as the number of ads that need to be shown before a customer clicks on one, or the number of patients that need to be treated before a certain medical condition is cured. It is also used in reliability engineering to model the number of failures before a unit fails.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;poission-discrete-data-type&#34;&gt;Poission (discrete data type)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Poisson distribution is a discrete probability distribution that is used to model the number of times an event occurs within a certain period of time or space. It is commonly used to model events that occur randomly and independently, such as the number of customers arriving at a store or the number of defects in a manufactured product.&lt;/li&gt;
&lt;li&gt;The Poisson distribution is defined by a single parameter, called the rate parameter or the mean rate of occurrence. This parameter is denoted as lambda () and represents the average number of times the event occurs per unit of time or space.&lt;/li&gt;
&lt;li&gt;The probability mass function (PMF) of a Poisson-distributed random variable X is given by the following formula:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(x) = (^x * e^(-)) / x!&lt;/p&gt;
&lt;p&gt;where x is the number of times the event occurs,  is the rate parameter, and e is the base of the natural logarithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Poisson distribution has a number of useful properties, such as the fact that the mean and variance are equal to the rate parameter . It is also a limiting distribution, meaning that it can be used to approximate other distributions under certain conditions.&lt;/li&gt;
&lt;li&gt;Overall, the Poisson distribution is a useful tool for modeling the number of times an event occurs within a certain period of time or space, and is widely used in a variety of applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;exponential-distribution&#34;&gt;Exponential Distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The exponential distribution is a continuous probability distribution that describes the time between events in a Poisson process, which is a process in which events occur continuously and independently at a constant average rate. It is defined by a single parameter  (lambda), which is the rate at which the events occur.&lt;/li&gt;
&lt;li&gt;The probability density function (PDF) of the exponential distribution is given by:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(x) =  * e^(-x)&lt;/p&gt;
&lt;p&gt;where x is the time between events and e is the base of the natural logarithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The exponential distribution has several characteristics:
&lt;ul&gt;
&lt;li&gt;It is a memoryless distribution, which means that the probability of an event occurring at time t+x, given that it has not occurred by time t, is the same as the probability of the event occurring at time x.&lt;/li&gt;
&lt;li&gt;It has a constant hazard rate, which means that the probability of an event occurring at any given time is constant.&lt;/li&gt;
&lt;li&gt;It is a one-sided distribution, which means that it is defined only for x &amp;gt; 0.&lt;/li&gt;
&lt;li&gt;The exponential distribution is often used in modeling the time between failures of equipment, the time between arrivals at a service facility, and the time between phone calls at a call center. It is also used in survival analysis to model the time until an event occurs, such as death or failure.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;weibull-distribution&#34;&gt;Weibull Distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The Weibull distribution is a continuous probability distribution that is often used to model time-to-failure data in reliability engineering. It is defined by two parameters: shape and scale.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The probability density function (PDF) of the Weibull distribution is given by:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(x) = (shape/scale) * (x/scale)^(shape-1) * e^(-(x/scale)^shape)&lt;/p&gt;
&lt;p&gt;where x is the time to failure, shape is the shape parameter, and scale is the scale parameter.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Weibull distribution has several characteristics:
&lt;ul&gt;
&lt;li&gt;It is a one-sided distribution, which means that it is defined only for x &amp;gt; 0.&lt;/li&gt;
&lt;li&gt;It has a shape parameter that controls the shape of the curve. If the shape parameter is less than 1, the curve is &amp;ldquo;skewed&amp;rdquo; to the right, meaning that it has a longer tail on the right side. - If the shape parameter is greater than 1, the curve is &amp;ldquo;skewed&amp;rdquo; to the left, meaning that it has a longer tail on the left side. If the shape parameter is equal to 1, the curve is symmetrical.&lt;/li&gt;
&lt;li&gt;It has a scale parameter that controls the spread of the curve. If the scale parameter is large, the curve is spread out and has a longer tail. If the scale parameter is small, the curve is more concentrated and has a shorter tail.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The Weibull distribution is often used in reliability engineering to model the time until failure of a component or system. It is also used in other fields, such as meteorology, to model wind speed and in economics to model stock returns.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;trends-and-seasonality&#34;&gt;Trends and seasonality&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Trends and seasonality are two common patterns that can occur in time series data.&lt;/li&gt;
&lt;li&gt;A trend is a long-term increase or decrease in the data. It can be either linear, meaning that the data increases or decreases at a constant rate, or nonlinear, meaning that the rate of change varies over time. Trends can be caused by various factors such as changes in consumer demand, economic conditions, or technological innovations.&lt;/li&gt;
&lt;li&gt;Seasonality is a pattern that repeats over a specific time period, such as annually or monthly. It can be caused by factors such as weather patterns, holidays, or consumer behavior.&lt;/li&gt;
&lt;li&gt;Both trends and seasonality can have important implications for forecasting and decision making. For example, if a company sees a trend of increasing sales, it may decide to ramp up production or hire more staff. If a company sees seasonal fluctuations in demand, it may need to adjust its inventory or staffing levels accordingly.&lt;/li&gt;
&lt;li&gt;To analyze trends and seasonality in time series data, various techniques can be used such as smoothing methods, decomposition methods, and autoregressive models. It is important to correctly identify and account for these patterns in order to make accurate forecasts and informed decisions.&lt;/li&gt;
&lt;li&gt;Additive time series data is characterized by a constant trend and constant seasonality over time. This means that the trend and seasonality do not change, and the data can be modeled using a linear function. The relationship between the data and the trend and seasonality components can be represented as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Data = Trend + Seasonality + Noise&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiplicative time series data, on the other hand, is characterized by a varying trend and varying seasonality. This means that the trend and seasonality change over time, and the data cannot be modeled using a linear function. The relationship between the data and the trend and seasonality components can be represented as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Data = Trend * Seasonality * Noise&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It is important to correctly identify whether a time series is additive or multiplicative, as this can influence the choice of modeling techniques and the interpretation of the results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Also this is possible:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Additive trend and additive seasonality&lt;/li&gt;
&lt;li&gt;Additive trend and multiplicative seasonality&lt;/li&gt;
&lt;li&gt;Multiplicative trend and additive seasonality&lt;/li&gt;
&lt;li&gt;Multiplicative trend and multiplicative seasonality&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;athena&#34;&gt;Athena&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Athena is a serverless, interactive query service that allows users to analyze data stored in Amazon S3 using SQL.&lt;/li&gt;
&lt;li&gt;It is designed to be fast and easy to use, and can be used to analyze data from a wide variety of sources such as logs, streaming data, and data lakes.&lt;/li&gt;
&lt;li&gt;To use Athena, users first define a data schema by creating tables that point to the data stored in Amazon S3.&lt;/li&gt;
&lt;li&gt;They can then use SQL to query the data and analyze it using various functions and aggregations. Athena supports a wide range of SQL functions and data types, and users can also use custom user-defined functions (UDFs) to extend its capabilities.&lt;/li&gt;
&lt;li&gt;It is also highly scalable, and can handle queries on large datasets with minimal performance degradation.&lt;/li&gt;
&lt;li&gt;Presto under the hood&lt;/li&gt;
&lt;li&gt;Supports multiple formats&lt;/li&gt;
&lt;li&gt;Unstructured, semi structured or structured&lt;/li&gt;
&lt;li&gt;Ad hoc queries, querying data before loading to Redshift, analyze Cloudtrail, integration with Jupyter, Zepplin, Integration with quicksight, integration with ODBC, JDBC&lt;/li&gt;
&lt;li&gt;AWS Glue datalog can extract the schema for Athena to use&lt;/li&gt;
&lt;li&gt;Pay as you go, inexpensive, converting to columner saves a lot of money, Glue and S3 have their own charges&lt;/li&gt;
&lt;li&gt;IAM policies, encryption is possible, TLS is possible&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;quicksight&#34;&gt;Quicksight&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon QuickSight is a business intelligence and data visualization service provided by Amazon Web Services (AWS).&lt;/li&gt;
&lt;li&gt;It allows users to create interactive dashboards and charts to visualize and analyze data from a wide variety of sources.&lt;/li&gt;
&lt;li&gt;To use QuickSight, users first need to connect it to their data sources, which can include data stored in Amazon S3, Amazon Redshift, Amazon RDS, and other AWS data stores, as well as external data sources such as spreadsheets and databases.&lt;/li&gt;
&lt;li&gt;Once the data is connected, users can use QuickSight&amp;rsquo;s visual interface to create charts, graphs, and other visualizations to explore and analyze the data.&lt;/li&gt;
&lt;li&gt;QuickSight offers a range of features and tools to help users analyze and understand their data.&lt;/li&gt;
&lt;li&gt;These include built-in analytics functions, support for custom SQL queries, and the ability to share and collaborate on dashboards with other users.&lt;/li&gt;
&lt;li&gt;Overall, Amazon QuickSight is a powerful and easy-to-use tool for creating interactive dashboards and visualizations, and is widely used in a variety of applications such as business intelligence, data exploration, and data reporting.&lt;/li&gt;
&lt;li&gt;Ad-hoc analysis&lt;/li&gt;
&lt;li&gt;Can do calculated columns etc.&lt;/li&gt;
&lt;li&gt;SPICE: Super Fast Parallel, In memory Calculation engine 10 gb&lt;/li&gt;
&lt;li&gt;Quicksight is quick because of SPICE&lt;/li&gt;
&lt;li&gt;Quicksights machine learning insights: Anomaly detection using Random cut forest, Forecasting and auto narratives (not too mature).&lt;/li&gt;
&lt;li&gt;Multifactor authentication&lt;/li&gt;
&lt;li&gt;Works with vpc, and provides row-level security&lt;/li&gt;
&lt;li&gt;Users defined via IAM or email signup&lt;/li&gt;
&lt;li&gt;AugoGraph feature selects the best graph for the respective data type&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;types-of-visualization&#34;&gt;Types of visualization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bar charts: These are used to compare categories or groups of data. They can be vertical or horizontal, and can be used to show both quantitative and categorical data.&lt;/li&gt;
&lt;li&gt;Line charts: These are used to show trends over time or other continuous variables. They can be used to show multiple data series on the same chart.&lt;/li&gt;
&lt;li&gt;Scatter plots: These are used to show the relationship between two numeric variables. They can be used to show correlations, patterns, and trends in the data.&lt;/li&gt;
&lt;li&gt;Pie charts: These are used to show proportions or percentages. They are most commonly used to show how a whole is divided into parts.&lt;/li&gt;
&lt;li&gt;Histograms: These are used to show the distribution of a continuous variable. They show the frequency or density of data points within different ranges or bins.&lt;/li&gt;
&lt;li&gt;Box plots: These are used to show the distribution and spread of a continuous variable. They show the minimum, first quartile, median, third quartile, and maximum values of the data.&lt;/li&gt;
&lt;li&gt;Heatmaps: These are used to show patterns and trends in data organized in a grid. They use color to represent the data, with warmer colors indicating higher values and cooler colors indicating lower values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;emr&#34;&gt;EMR&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Elastic MapReduce (EMR) is a fully managed, cloud-native big data processing service that makes it easy to process large amounts of data using Apache Hadoop and Apache Spark.&lt;/li&gt;
&lt;li&gt;It is designed to be scalable, highly available, and easy to use, and it integrates with a variety of data sources and destinations.&lt;/li&gt;
&lt;li&gt;With AWS EMR, users can create clusters of Amazon Elastic Compute Cloud (EC2) instances and use them to process and analyze large datasets stored in Amazon S3 or other data stores.&lt;/li&gt;
&lt;li&gt;EMR supports a variety of big data processing frameworks, including Hadoop, Spark, and Flink, and it provides tools to help users manage and monitor their clusters.&lt;/li&gt;
&lt;li&gt;AWS EMR is a useful tool for organizations that need to process and analyze large amounts of data, such as log files, scientific simulations, and machine learning models.&lt;/li&gt;
&lt;li&gt;It is fully managed, so users do not need to worry about infrastructure or maintenance.&lt;/li&gt;
&lt;li&gt;Provides notebooks&lt;/li&gt;
&lt;li&gt;Master nodes (manages the cluster), Core node (holds HDFS data and run tasks), Task node (only runs tasks)&lt;/li&gt;
&lt;li&gt;HDFS is epimerical&lt;/li&gt;
&lt;li&gt;Transient cluster vs Long running cluster&lt;/li&gt;
&lt;li&gt;IAM configure permissions&lt;/li&gt;
&lt;li&gt;CloudTrail: audit requests&lt;/li&gt;
&lt;li&gt;Data Pipeline: schedule and start clusters&lt;/li&gt;
&lt;li&gt;EMRFS: access s3 as if it were HDFS, uses DynamoDB to track consistency&lt;/li&gt;
&lt;li&gt;EBS for HDFS is also possible&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;hadoop&#34;&gt;Hadoop&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Apache Hadoop is an open-source software framework for storing and processing large amounts of data in a distributed computing environment.&lt;/li&gt;
&lt;li&gt;It is designed to handle data that is too large or complex for traditional database systems, and can process and analyze data in parallel across a large number of servers.&lt;/li&gt;
&lt;li&gt;Hadoop consists of two main components: the Hadoop Distributed File System (HDFS) and the MapReduce programming model.&lt;/li&gt;
&lt;li&gt;HDFS is a distributed file system that stores data across a large number of servers, and MapReduce is a programming model that allows developers to write programs that can process and analyze large amounts of data in parallel.&lt;/li&gt;
&lt;li&gt;Hadoop is commonly used for tasks such as data analysis, machine learning, and log processing. It is also often used in conjunction with other tools and technologies such as Apache Spark, Apache Hive, and Apache Pig to build more complex data processing pipelines.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;apache-spark&#34;&gt;Apache Spark&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Apache Spark is an open-source, distributed computing system that is designed for fast and flexible data processing.&lt;/li&gt;
&lt;li&gt;It is a popular choice for tasks such as data analytics, machine learning, and real-time stream processing.&lt;/li&gt;
&lt;li&gt;Spark is built on top of the Hadoop distributed file system (HDFS) and is designed to be highly scalable and efficient.&lt;/li&gt;
&lt;li&gt;It can process and analyze data in parallel across a large number of servers, and supports a wide range of programming languages including Python, Java, R, and Scala.&lt;/li&gt;
&lt;li&gt;One of the main benefits of Spark is its ability to process data in memory, which allows it to be much faster than other distributed computing systems that rely on disk-based storage.&lt;/li&gt;
&lt;li&gt;It also includes a number of useful libraries and tools for tasks such as machine learning, graph processing, and stream processing.&lt;/li&gt;
&lt;li&gt;in memory caching, DAGs&lt;/li&gt;
&lt;li&gt;Batch processing and real time analytics, graph processing, machine learning&lt;/li&gt;
&lt;li&gt;Spark context, cluster manager via spark or yarn, executors&lt;/li&gt;
&lt;li&gt;Spark core&lt;/li&gt;
&lt;li&gt;Spark RDD, DataFrames and Datasets are built on top of RDD, and they are most commonly used at the moment&lt;/li&gt;
&lt;li&gt;Spark Streaming is possible (works in mini batches). Unbounded database table&lt;/li&gt;
&lt;li&gt;MlLib (distributed machine learning)&lt;/li&gt;
&lt;li&gt;Graphx (distributed graph processing)&lt;/li&gt;
&lt;li&gt;Zepplin can run spark code interactively, and can also use charts/plots&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;spark-mllib&#34;&gt;Spark MLlib&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MLlib is a machine learning library for Apache Spark. It is designed to provide scalable and efficient machine learning algorithms that can be used on big data.&lt;/li&gt;
&lt;li&gt;MLlib includes a wide range of machine learning algorithms and utility functions, including algorithms for classification, regression, clustering, collaborative filtering, and dimensionality reduction.&lt;/li&gt;
&lt;li&gt;It also includes tools for feature engineering, such as feature extraction, transformation, and selection.&lt;/li&gt;
&lt;li&gt;MLlib is designed to be easy to use, and includes APIs for several programming languages including Python, Java, R, and Scala.&lt;/li&gt;
&lt;li&gt;It is also designed to be highly scalable, and can be used to build machine learning models on large datasets distributed across multiple servers.&lt;/li&gt;
&lt;li&gt;Classification: logistic regression and naive bayes&lt;/li&gt;
&lt;li&gt;Regression&lt;/li&gt;
&lt;li&gt;Decision trees&lt;/li&gt;
&lt;li&gt;Recommendation engine (ALS)&lt;/li&gt;
&lt;li&gt;Clustering (K-means)&lt;/li&gt;
&lt;li&gt;LDA (topic modeling)&lt;/li&gt;
&lt;li&gt;ML workflow utilities (pipelines, feature transformation, persistence)&lt;/li&gt;
&lt;li&gt;SVD, PCA and statistics&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;emr-notebooks-security-and-instance-types&#34;&gt;EMR Notebooks, Security and Instance Types&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon EMR Notebooks is a service that allows users to create and manage Jupyter notebooks on Amazon Elastic MapReduce (EMR) clusters. EMR is a cloud-based big data processing service, and Jupyter notebooks are interactive, web-based documents that can contain code, text, and visualizations.&lt;/li&gt;
&lt;li&gt;EMR Notebooks provides a simple and flexible way to analyze and visualize data stored in Amazon S3 or other data stores using a variety of tools and libraries such as Apache Spark, Python, and R. Users can create and edit notebooks using a web-based editor, and can also use notebooks to run and debug code, create visualizations, and collaborate with other users.&lt;/li&gt;
&lt;li&gt;EMR Notebooks is fully integrated with EMR, which means that users can easily spin up and down EMR clusters to process and analyze large datasets, and can also access other EMR features such as security and data access controls.&lt;/li&gt;
&lt;li&gt;Similar concept to Zeppelin, with more AWS integration&lt;/li&gt;
&lt;li&gt;Notebooks backed up to s3&lt;/li&gt;
&lt;li&gt;Provision clusters from the notebooks&lt;/li&gt;
&lt;li&gt;Hosted inside a vpc&lt;/li&gt;
&lt;li&gt;Accessed only via aws console&lt;/li&gt;
&lt;li&gt;IAM policies, Kerberos (a computer-network authentication protocol that works on the basis of tickets to allow nodes communicating over a non-secure network to prove their identity to one another in a secure manner), SSH, IAM roles&lt;/li&gt;
&lt;li&gt;Spot instances are good choice for task nodes, only use on core or master if you are testing or very cost sensitive, however, you are risking partial data loss&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;feature-engineering&#34;&gt;Feature Engineering&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Feature engineering is the process of creating new features or transforming existing features in a dataset in order to improve the performance of a machine learning model. It is a crucial step in the machine learning process, and can have a significant impact on the model&amp;rsquo;s accuracy and effectiveness.&lt;/li&gt;
&lt;li&gt;There are many different techniques that can be used in feature engineering, including:
&lt;ul&gt;
&lt;li&gt;Feature selection: This involves selecting a subset of the most relevant features from a dataset to use in a model. This can help to reduce overfitting, improve model interpretability, and reduce training time.&lt;/li&gt;
&lt;li&gt;Feature extraction: This involves creating new features from existing data by combining or transforming the original features. For example, a new feature could be created by taking the square root of an existing feature.&lt;/li&gt;
&lt;li&gt;Feature transformation: This involves transforming the scale or distribution of a feature in order to improve model performance. For example, data may need to be normalized or standardized in order to be used in some models.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;imputing-missing-data&#34;&gt;Imputing Missing Data&lt;/h2&gt;
&lt;p&gt;There are several ways to impute (or fill in) missing data, and the best method will depend on the specific dataset and the nature of the missing data. Some common methods for imputing missing data include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mean imputation: This involves replacing missing values with the mean (or average) of the non-missing values. This is a simple method that can be useful for numerical data, but can be biased if the data has a skewed distribution.&lt;/li&gt;
&lt;li&gt;Median imputation: This is similar to mean imputation, but uses the median (or middle value) instead of the mean. It can be less affected by outliers than mean imputation and may be a better choice for skewed data.&lt;/li&gt;
&lt;li&gt;Mode imputation: This involves replacing missing values with the most frequent (or mode) value in the dataset. It is often used for categorical data.&lt;/li&gt;
&lt;li&gt;Regression imputation: This involves using a regression model to predict the missing values based on the other available features. It can be a more powerful method, but requires a good understanding of the relationships between the features and the target variable.&lt;/li&gt;
&lt;li&gt;Nearest Neighbors (k-NN) imputation is a method for imputing missing data that uses the k-NN algorithm to fill in missing values based on the values of the nearest neighbors. It is a simple and intuitive method that is often used in machine learning and data analysis. To use k-NN imputation, the first step is to identify the nearest neighbors of the data point with missing values. This is typically done using a distance measure such as Euclidean distance, and the number of neighbors (k) is a user-defined parameter. Once the nearest neighbors have been identified, the missing values can be imputed by averaging the values of the neighbors. k-NN imputation can be a useful method for filling in missing data, especially when the data is highly correlated and the relationships between the features are well understood. However, it can be sensitive to the choice of k, and may not always produce the best results.&lt;/li&gt;
&lt;li&gt;Dropping NA&amp;rsquo;s: This method can be useful in some cases, such as when the number of missing values is small and removing them does not significantly affect the overall size of the dataset. However, it can also lead to a loss of information and may not be appropriate if the missing values are prevalent or if they are likely to be informative.&lt;/li&gt;
&lt;li&gt;MICE (multiple imputation by chained equations): Multiple imputation by chained equations (MICE) is a method for imputing missing data that involves creating multiple imputed datasets and combining them to produce a final result. It is a more advanced method that can be more robust and accurate than other imputation methods, especially when the data has a complex structure and the relationships between the features are not well understood. The final result is produced by combining the imputed datasets using appropriate statistical methods, such as averaging or weighted averaging. MICE can be a powerful method for imputing missing data, but it can also be time-consuming and requires a good understanding of statistical modeling.&lt;/li&gt;
&lt;li&gt;Categorical is usually not trivial&lt;/li&gt;
&lt;li&gt;Just get more data (if possible)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;unbalanced-data&#34;&gt;Unbalanced Data&lt;/h2&gt;
&lt;p&gt;There are several approaches for handling imbalanced data in machine learning, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Balancing the data: This can be done by oversampling the minority class or undersampling the majority class.&lt;/li&gt;
&lt;li&gt;Using a different evaluation metric: Instead of using accuracy, try using precision, recall, or F1 score, which are more sensitive to imbalanced data.&lt;/li&gt;
&lt;li&gt;Adjusting the class weight: Some algorithms allow you to adjust the weight of each class, which can be used to give more importance to the minority class.&lt;/li&gt;
&lt;li&gt;Using a different algorithm: Some algorithms are more robust to imbalanced data than others. For example, tree-based algorithms like random forests and gradient boosting tend to perform better on imbalanced data than algorithms like logistic regression.&lt;/li&gt;
&lt;li&gt;Using data augmentation: If you are working with image data, you can use data augmentation techniques to generate additional minority class examples.&lt;/li&gt;
&lt;li&gt;Anomaly detection: If the minority class represents anomalies or rare events, you can treat the problem as an anomaly detection problem rather than a classification problem.&lt;/li&gt;
&lt;li&gt;Synthetic minority oversampling technique (SMOTE): This is a popular method for oversampling the minority class by synthesizing new examples rather than simply replicating existing ones.&lt;/li&gt;
&lt;li&gt;Cost-sensitive learning: In this approach, the cost of misclassifying examples from the minority class is higher than the cost of misclassifying examples from the majority class.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;handling-outliers&#34;&gt;Handling Outliers&lt;/h2&gt;
&lt;p&gt;There are several ways to identify outliers in a dataset:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Visualization: One of the easiest ways to identify outliers is to plot the data using a box plot or histogram. Outliers will typically be located outside the &amp;ldquo;whiskers&amp;rdquo; of the box plot or outside the range of the histogram.&lt;/li&gt;
&lt;li&gt;Statistical tests: You can use statistical tests to identify outliers in a dataset. For example, you can use the Z-score method to identify outliers by calculating the distance of each data point from the mean in terms of standard deviations. Data points with a Z-score greater than a certain threshold (such as 3 or 4) can be considered outliers.&lt;/li&gt;
&lt;li&gt;Data transformation: Transforming the data, such as taking the log of the data, can make outliers more obvious.&lt;/li&gt;
&lt;li&gt;Anomaly detection algorithms: There are also machine learning algorithms specifically designed for detecting anomalies or outliers in a dataset. These algorithms include density-based methods, distance-based methods, and model-based methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are several approaches for handling outliers in a dataset:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ignore the outlier: This is an option if the outlier does not have a significant impact on the model or if the outlier is the result of a data entry error.&lt;/li&gt;
&lt;li&gt;Drop the outlier: This is an option if the outlier is not representative of the population being studied and if the outlier has a significant impact on the model.&lt;/li&gt;
&lt;li&gt;Transform the data: Some algorithms are more robust to outliers, such as decision trees and random forests. Transforming the data, such as using the log transformation, can also make the model more robust to outliers.&lt;/li&gt;
&lt;li&gt;Use robust models: Some models, such as linear regression with the Huber loss function, are less sensitive to outliers than other models.&lt;/li&gt;
&lt;li&gt;Anomaly detection: If the outlier represents an anomaly or rare event, you can treat the problem as an anomaly detection problem rather than a classification or regression problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;binning-transforming-encoding-scaling-and-shuffling&#34;&gt;Binning, Transforming, Encoding, Scaling, and Shuffling&lt;/h2&gt;
&lt;h3 id=&#34;binning&#34;&gt;Binning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Binning is a process of transforming numerical data into categorical data by dividing the data into a set of bins or intervals. This can be useful for reducing the number of unique values in a dataset, which can make it easier to visualize and analyze the data.&lt;/li&gt;
&lt;li&gt;For example, if you have a dataset with a large range of numerical values, you could use binning to group the values into a smaller set of intervals. This would allow you to plot the data on a histogram or bar chart, which would be more informative than a scatter plot.&lt;/li&gt;
&lt;li&gt;There are several ways to determine the size and number of bins to use for binning data, including:
&lt;ul&gt;
&lt;li&gt;Fixed width bins: In this approach, you specify the size of the bins and the data is divided into intervals of that size.&lt;/li&gt;
&lt;li&gt;Fixed number of bins: In this approach, you specify the number of bins and the data is divided into that number of intervals.&lt;/li&gt;
&lt;li&gt;Optimal bin width: In this approach, the optimal bin width is determined using a statistical method, such as the Scott&amp;rsquo;s normal reference rule or the Freedman-Diaconis rule.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;transforming&#34;&gt;Transforming&lt;/h3&gt;
&lt;p&gt;Data transformation is a process of converting data from one format or representation to another. This can be useful for several reasons, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data cleaning: Data transformation can be used to fix or remove errors or inconsistencies in the data.&lt;/li&gt;
&lt;li&gt;Data preparation: Data transformation can be used to prepare the data for analysis by formatting the data in a specific way or creating new variables.&lt;/li&gt;
&lt;li&gt;Data reduction: Data transformation can be used to reduce the size or complexity of the data, such as by aggregating the data or removing unnecessary variables.&lt;/li&gt;
&lt;li&gt;Data normalization: Data transformation can be used to scale the data to a common range, such as by normalizing the data to have a mean of 0 and a standard deviation of 1.&lt;/li&gt;
&lt;li&gt;Data transformation can also be used to make the data more amenable to a specific algorithm or technique, such as by binning numerical data or encoding categorical data.&lt;/li&gt;
&lt;li&gt;There are many different types of data transformation techniques, including scaling, centering, normalization, aggregation, imputation, and encoding. The appropriate transformation technique will depend on the specific characteristics of the data and the goals of the analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;encoding&#34;&gt;Encoding&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Encoding is the process of converting data from one format into another, often for the purpose of efficient storage or transmission. In the context of machine learning, encoding is often used to convert categorical data, which can&amp;rsquo;t be represented as numerical values, into numerical form.&lt;/li&gt;
&lt;li&gt;There are several types of encoding techniques that can be used, including:
&lt;ul&gt;
&lt;li&gt;One-hot encoding: This technique converts each categorical value into a new binary column, with a value of 1 indicating the presence of the categorical value and a value of 0 indicating its absence.&lt;/li&gt;
&lt;li&gt;Label encoding: This technique converts each categorical value into a numerical value, such as an integer. However, this can lead to problems if the numerical values are interpreted as having a meaningful order.&lt;/li&gt;
&lt;li&gt;Count encoding: This technique encodes the categorical values by the count of each value in the dataset.&lt;/li&gt;
&lt;li&gt;Binary encoding: This technique encodes the categorical values as binary code.&lt;/li&gt;
&lt;li&gt;Target encoding: This technique encodes the categorical values using the mean of the target variable for each value.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;scaling-and-normalizing&#34;&gt;Scaling and normalizing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Scaling and normalization are techniques used to transform variables so that they have a comparable scale. This can be useful for a variety of reasons, such as:&lt;/li&gt;
&lt;li&gt;Some machine learning algorithms are sensitive to the scale of the input variables, and can perform poorly if the variables are on a different scale. Scaling the variables to the same scale can improve the performance of these algorithms.&lt;/li&gt;
&lt;li&gt;Scaling the variables can also make it easier to compare the magnitude of the variables.&lt;/li&gt;
&lt;li&gt;There are several ways to scale and normalize data:
&lt;ul&gt;
&lt;li&gt;Min-max scaling: This scales the variables to a specific range, such as 0-1 or -1 to 1.&lt;/li&gt;
&lt;li&gt;Standardization: This scales the variables so that they have a mean of 0 and a standard deviation of 1.&lt;/li&gt;
&lt;li&gt;Normalization: This scales the variables so that they have a unit norm (a length of 1).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;shuffling&#34;&gt;Shuffling&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Shuffling is the process of randomly rearranging the rows of a dataset. This can be useful for several reasons:&lt;/li&gt;
&lt;li&gt;Machine learning algorithms often expect the data to be in a random order. Shuffling the data before training a model can help ensure that the model is not biased by the order of the data.&lt;/li&gt;
&lt;li&gt;Shuffling the data can also help ensure that the training and test sets are representative of the overall dataset. If the data is not shuffled and the rows are ordered in a certain way, the training and test sets may not be representative of the overall dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tokenization&#34;&gt;Tokenization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements, known as tokens. Tokenization is an important preprocessing step for many natural language processing (NLP) tasks, such as text classification and information retrieval.&lt;/li&gt;
&lt;li&gt;There are several approaches to tokenization, including:
&lt;ul&gt;
&lt;li&gt;Word tokenization: This involves dividing the text into words.&lt;/li&gt;
&lt;li&gt;Sentence tokenization: This involves dividing the text into sentences.&lt;/li&gt;
&lt;li&gt;Word-level tokenization: This involves dividing the text into words and punctuation, such as &amp;ldquo;don&amp;rsquo;t&amp;rdquo; being split into &amp;ldquo;do&amp;rdquo; and &amp;ldquo;n&amp;rsquo;t.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;N-gram tokenization: This involves dividing the text into contiguous sequences of n items, such as bigrams (pairs of words) or trigrams (triplets of words).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;stemming-and-lemmatization&#34;&gt;Stemming and lemmatization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stemming and lemmatization are techniques used to normalize words to their base form, known as a stem or lemma. These techniques are often used as a preprocessing step for natural language processing (NLP) tasks, such as text classification and information retrieval.&lt;/li&gt;
&lt;li&gt;Stemming involves removing the suffixes from a word to obtain the root form of the word. For example, the stem of the word &amp;ldquo;jumping&amp;rdquo; might be &amp;ldquo;jump,&amp;rdquo; and the stem of the word &amp;ldquo;stemmer,&amp;rdquo; might be &amp;ldquo;stem.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Lemmatization, on the other hand, involves determining the base form of a word based on its part of speech and meaning. For example, the lemma of the word &amp;ldquo;was&amp;rdquo; might be &amp;ldquo;be,&amp;rdquo; and the lemma of the word &amp;ldquo;better&amp;rdquo; might be &amp;ldquo;good.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Stemming and lemmatization can be useful for reducing the dimensionality of the data and improving the performance of NLP models, but they can also remove some of the context and meaning of the words.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-sagemaker-ground-truth-and-label-generation&#34;&gt;Amazon Sagemaker Ground Truth and Label Generation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon SageMaker Ground Truth is a fully managed data labeling service that allows users to build highly accurate training datasets for machine learning.&lt;/li&gt;
&lt;li&gt;It offers a variety of labeling methods, including human labeling, active learning, and automatic labeling.&lt;/li&gt;
&lt;li&gt;SageMaker Ground Truth also includes workflows for common data labeling tasks, such as image classification, object detection, and semantic segmentation.&lt;/li&gt;
&lt;li&gt;It also provides tools for managing and tracking the data labeling process, including the ability to set up labeling jobs, track their progress, and review the results.&lt;/li&gt;
&lt;li&gt;This helps users ensure that their data is labeled accurately and efficiently.&lt;/li&gt;
&lt;li&gt;Ambiguous data is sent to humans&lt;/li&gt;
&lt;li&gt;Mechanical Turk: Amazon Mechanical Turk (MTurk) is a cloud platform that enables organizations to use a network of human workers to perform tasks that are typically difficult or time-consuming for computers to perform. These tasks, known as Human Intelligence Tasks (HITs), can include data labeling, transcription, image annotation, and many other types of work.&lt;/li&gt;
&lt;li&gt;Amazon SageMaker Ground Truth Plus helps you to create high-quality training datasets without having to build labeling applications or manage a labeling workforce.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;misc&#34;&gt;Misc&lt;/h2&gt;
&lt;h3 id=&#34;synthetic-features&#34;&gt;Synthetic Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Synthetic features are artificially created features that are derived from existing features in a dataset. They are often used to improve the performance of machine learning models by providing additional information that may not be present in the original features. Synthetic features can be created using various techniques, such as combining or transforming existing features, or by applying statistical or mathematical operations to the data.&lt;/li&gt;
&lt;li&gt;One example of a synthetic feature is a polynomial feature, which is created by taking the product of a feature with itself or with other features. Polynomial features can capture nonlinear relationships between features and the target variable, and can improve the performance of linear models on nonlinear problems. Other examples of synthetic features include interactions between features, binned features, and dummy variables.&lt;/li&gt;
&lt;li&gt;Synthetic features can be useful for improving the performance of machine learning models, especially when the original features are not sufficient for making accurate predictions. However, care should be taken when creating synthetic features, as adding too many of them can lead to overfitting and degrade model performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;stop-words&#34;&gt;Stop words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stop words are common words that are typically filtered out before natural language processing (NLP) tasks, such as text classification or text mining, because they do not provide meaningful information. Examples of stop words include words like &amp;ldquo;a,&amp;rdquo; &amp;ldquo;an,&amp;rdquo; &amp;ldquo;the,&amp;rdquo; &amp;ldquo;and,&amp;rdquo; and &amp;ldquo;but.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;There are several ways to identify and handle stop words:
&lt;ul&gt;
&lt;li&gt;Use a list of stop words: Many NLP libraries and frameworks, such as NLTK and scikit-learn, include a pre-defined list of stop words that can be used to filter out common words.&lt;/li&gt;
&lt;li&gt;Identify stop words using term frequency-inverse document frequency (TF-IDF): This method involves calculating the importance of each word in a document or corpus and filtering out the least important words, which are often stop words.&lt;/li&gt;
&lt;li&gt;Customize the stop word list: You can customize the list of stop words based on the specific needs of your task. For example, if you are working with domain-specific language, you may need to add domain-specific words to the stop word list.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;To handle stop words, you can simply filter them out of the dataset before performing the NLP task. This can be done by comparing the words in the dataset to the stop word list and removing any words that are on the list.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s important to carefully consider whether or not to filter out stop words, as they can sometimes provide important context or meaning. For example, in the phrase &amp;ldquo;not good,&amp;rdquo; the word &amp;ldquo;not&amp;rdquo; is a stop word that changes the meaning of the phrase.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tf-idf&#34;&gt;TF-IDF&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Term Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure that is used to evaluate the importance of a word in a document or corpus. The importance of a word is determined by its frequency in the document and in the corpus as a whole.&lt;/li&gt;
&lt;li&gt;TF-IDF is calculated as the product of the term frequency (TF) and the inverse document frequency (IDF). The term frequency is the number of times a word appears in the document, and the inverse document frequency is the logarithm of the number of documents in the corpus divided by the number of documents that contain the word.&lt;/li&gt;
&lt;li&gt;TF-IDF is often used as a weighting factor in information retrieval and text mining, and can be useful for tasks such as document classification, clustering, and keyword extraction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;correlation&#34;&gt;Correlation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Correlation is a statistical measure that indicates the strength and direction of a linear relationship between two variables. A positive correlation indicates that as one variable increases, the other variable also increases. A negative correlation indicates that as one variable increases, the other variable decreases.&lt;/li&gt;
&lt;li&gt;The correlation coefficient, denoted by &amp;ldquo;r,&amp;rdquo; is a measure of the strength of the relationship between the variables. It ranges from -1 to 1, where -1 indicates a strong negative correlation, 0 indicates no correlation, and 1 indicates a strong positive correlation.&lt;/li&gt;
&lt;li&gt;Correlation can be useful for understanding the relationship between two variables and predicting one variable based on the other. However, it&amp;rsquo;s important to remember that correlation does not necessarily imply causation, meaning that a correlation between two variables does not necessarily mean that one variable causes the other.&lt;/li&gt;
&lt;li&gt;There are several methods for calculating the correlation between variables, including Pearson&amp;rsquo;s correlation coefficient, Spearman&amp;rsquo;s rank correlation coefficient, and Kendall&amp;rsquo;s tau. The appropriate method will depend on the characteristics of the data and the goals of the analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;p-value&#34;&gt;p-value&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The p-value is a statistical measure that is used to assess the significance of a hypothesis test. It is the probability of obtaining a test statistic at least as extreme as the one observed, assuming that the null hypothesis is true.&lt;/li&gt;
&lt;li&gt;The null hypothesis is a statement that there is no statistical relationship between the variables being tested. The alternative hypothesis is the opposite of the null hypothesis and states that there is a statistical relationship between the variables.&lt;/li&gt;
&lt;li&gt;To interpret the p-value, you compare it to a significance level, which is a predetermined cutoff value. If the p-value is less than the significance level, you can reject the null hypothesis and conclude that there is a statistical relationship between the variables. If the p-value is greater than the significance level, you fail to reject the null hypothesis and cannot conclude that there is a statistical relationship between the variables.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s important to carefully consider the appropriate significance level and the limitations of the p-value, as it can be affected by factors such as sample size and the distribution of the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;elbow-plot&#34;&gt;Elbow plot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An elbow plot is a graphical method used to determine the appropriate number of clusters to use in a cluster analysis. It plots the within-cluster sum of squared distances (WCSS) for each possible number of clusters, and the number of clusters is chosen at the &amp;ldquo;elbow&amp;rdquo; point, where the change in WCSS begins to level off.&lt;/li&gt;
&lt;li&gt;To create an elbow plot, you first perform a cluster analysis for a range of possible number of clusters, and then plot the WCSS for each number of clusters. The WCSS can be calculated as the sum of the squared distance between each point and its cluster centroid.&lt;/li&gt;
&lt;li&gt;The &amp;ldquo;elbow&amp;rdquo; point is generally considered to be the point where the WCSS starts to decrease more slowly, indicating that adding more clusters is not significantly improving the fit of the model.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s important to carefully consider the limitations of the elbow method, as it can be affected by the shape of the data and may not always clearly identify the appropriate number of clusters. Other methods, such as the silhouette method, can also be used to determine the number of clusters in a dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;summary-statistics&#34;&gt;Summary Statistics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Summary statistics are quantitative measures that describe and summarize a dataset. They provide a quick and easy way to get a sense of the characteristics and patterns in the data.&lt;/li&gt;
&lt;li&gt;Some common summary statistics include:&lt;/li&gt;
&lt;li&gt;Mean: The mean is the arithmetic average of the data. It is calculated by summing all the values and dividing by the number of values.&lt;/li&gt;
&lt;li&gt;Median: The median is the middle value of the data when it is sorted in ascending order. It is a measure of central tendency that is resistant to outliers.&lt;/li&gt;
&lt;li&gt;Mode: The mode is the most frequent value in the data.&lt;/li&gt;
&lt;li&gt;Range: The range is the difference between the maximum and minimum values in the data.&lt;/li&gt;
&lt;li&gt;Variance: The variance is a measure of the spread or dispersion of the data. It is calculated as the sum of the squared differences between each value and the mean, divided by the number of values.&lt;/li&gt;
&lt;li&gt;Standard deviation: The standard deviation is the square root of the variance. It is a measure of the spread of the data that is in the same units as the original data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;distance-norms&#34;&gt;Distance Norms&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Distance norms, also known as metrics, are functions that define a distance between two points in a space. These functions take two points as inputs and return a non-negative number that represents the distance between them. Different distance norms can be used depending on the characteristics of the data and the requirements of the application.&lt;/li&gt;
&lt;li&gt;Some common distance norms include:
&lt;ul&gt;
&lt;li&gt;Euclidean distance: This is the most common distance norm and is based on the Pythagorean theorem. It is defined as the square root of the sum of the squares of the differences between the coordinates of the two points.&lt;/li&gt;
&lt;li&gt;Manhattan distance: This distance norm is based on the sum of the absolute differences of the coordinates of the two points. It is also known as the &amp;ldquo;taxi cab&amp;rdquo; distance because it represents the distance a taxi cab would need to travel to get from one point to the other.&lt;/li&gt;
&lt;li&gt;Minkowski distance: This is a generalization of the Euclidean and Manhattan distances. It is defined as the sum of the absolute differences of the coordinates of the two points, raised to a power. The value of the power determines whether the distance is more similar to the Euclidean or Manhattan distance.&lt;/li&gt;
&lt;li&gt;Cosine distance: This distance norm is based on the cosine similarity between two vectors. It is often used in text analysis to measure the similarity between documents.&lt;/li&gt;
&lt;li&gt;Jaccard distance: This distance norm is based on the Jaccard similarity coefficient and is often used to compare the similarity of sets. It is defined as the size of the intersection of the sets divided by the size of the union of the sets.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;qq-plots&#34;&gt;QQ plots&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A Q-Q plot (short for &amp;ldquo;quantile-quantile plot&amp;rdquo;) is a graphical way to compare two probability distributions by plotting their quantiles against each other. It is a plot of the sorted data against an idealized distribution with a uniform distribution. The purpose of a Q-Q plot is to check whether two datasets come from the same distribution.&lt;/li&gt;
&lt;li&gt;To create a Q-Q plot, you first need to specify the distribution that you want to use as the reference distribution. Then, you sort both datasets and plot the quantiles of one dataset against the quantiles of the other dataset. If the two datasets come from the same distribution, the points in the Q-Q plot will lie approximately on a straight line. If the points do not lie on a straight line, it suggests that the two datasets come from different distributions.&lt;/li&gt;
&lt;li&gt;Q-Q plots are often used to check whether a dataset follows a particular distribution, such as a normal distribution. They are also useful for comparing datasets to see whether they come from the same distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;parametric-and-non-parametric-test&#34;&gt;Parametric and Non-Parametric test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A parametric test is a statistical test that assumes that the data comes from a population with a known probability distribution, such as a normal distribution. The test uses parameters of the distribution (such as the mean and standard deviation) to make statistical inferences about the population.&lt;/li&gt;
&lt;li&gt;Parametric tests are based on the assumption that the data follows a particular probability distribution, and they are generally more powerful (i.e., able to detect differences with smaller sample sizes) than nonparametric tests, which do not make any assumptions about the distribution of the data. However, if the assumption of a known distribution is not met, the results of a parametric test may be less reliable.&lt;/li&gt;
&lt;li&gt;Examples of parametric tests include the t-test, the ANOVA test, and the linear regression analysis. These tests are commonly used to compare means, variances, and relationships between variables.&lt;/li&gt;
&lt;li&gt;Nonparametric tests, on the other hand, do not assume that the data comes from a particular distribution and are more robust to departures from normality. Examples of nonparametric tests include the Wilcoxon rank-sum test and the Kruskal-Wallis test.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;markov-chain-model&#34;&gt;Markov Chain model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A Markov chain is a mathematical system that undergoes transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the system arrived at its current state, the possible future states are fixed.&lt;/li&gt;
&lt;li&gt;A Markov chain is often represented by a state transition diagram, which shows all the possible states that the system can be in, and the transitions between these states. The transitions are governed by transition probabilities, which specify the probability of moving from one state to another.&lt;/li&gt;
&lt;li&gt;Markov chains have many applications in various fields, including economics, computer science, and physics. They are used to model systems that change over time and have a finite number of states. Some examples of systems that can be modeled using Markov chains include:
&lt;ul&gt;
&lt;li&gt;The behavior of a customer moving through a website, where the states represent the different pages on the website and the transitions represent the clicks that the customer makes to move from one page to another.&lt;/li&gt;
&lt;li&gt;The weather, where the states represent different weather conditions (such as sunny, cloudy, or rainy) and the transitions represent the probability of the weather changing from one condition to another.&lt;/li&gt;
&lt;li&gt;The movement of a particle through a lattice, where the states represent the different positions that the particle can occupy and the transitions represent the probability of the particle moving from one position to another.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;optimization&#34;&gt;Optimization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Optimization is the process of finding the best solution to a problem, given certain constraints. In mathematics, optimization problems involve finding the maximum or minimum value of a function, subject to certain constraints. These constraints can be equality constraints, which specify that a certain relationship must hold among the variables, or inequality constraints, which specify that a certain relationship must not hold among the variables.&lt;/li&gt;
&lt;li&gt;There are many different methods for solving optimization problems, including gradient descent, the simplex method, and the interior point method. The choice of method depends on the specific problem and the desired properties of the solution.&lt;/li&gt;
&lt;li&gt;Optimization is used in many fields, including engineering, economics, and machine learning. Some examples of optimization problems include:
&lt;ul&gt;
&lt;li&gt;Finding the optimal allocation of resources, such as the allocation of capital to different investments, or the allocation of production capacity to different products.&lt;/li&gt;
&lt;li&gt;Finding the optimal design of a system, such as the design of an aircraft or the design of a supply chain.&lt;/li&gt;
&lt;li&gt;Finding the optimal parameters of a machine learning model, such as the weights of a neural network or the regularization parameters of a linear model.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;objective-function&#34;&gt;Objective Function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An objective function is a mathematical function that is used to represent the goal of an optimization problem. The goal of the optimization problem is to find the values of the variables that either maximize or minimize the objective function, subject to certain constraints.&lt;/li&gt;
&lt;li&gt;The objective function is also known as the cost function, the loss function, or the criterion function. It is a measure of how well a given solution meets the requirements of the problem. In general, the objective function is a scalar-valued function, meaning that it maps a vector of variables to a single scalar value.&lt;/li&gt;
&lt;li&gt;The objective function is an important component of an optimization problem, as it defines the goal of the optimization and determines the solution of the problem. The objective function is often defined in terms of the decision variables (the variables that are being optimized) and the parameters of the problem (the constants that define the problem).&lt;/li&gt;
&lt;li&gt;For example, in a linear programming problem, the objective function is a linear function that represents the cost or profit associated with a particular allocation of resources. In a nonlinear programming problem, the objective function is a nonlinear function that represents the cost or performance of a system.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;exponential-smoothing-for-outlier-detection&#34;&gt;Exponential smoothing for outlier detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Exponential smoothing is a time series forecasting method that is used to predict future values based on historical data. It is based on the idea of giving more weight to more recent observations and less weight to observations that are further in the past.&lt;/li&gt;
&lt;li&gt;Exponential smoothing can be used for outlier detection by analyzing the residuals (i.e., the differences between the predicted values and the actual values) of the time series. If the residuals contain outliers (i.e., values that are significantly different from the majority of the residuals), it may indicate that there is something unusual or unexpected happening in the time series.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;box-cox-transformation&#34;&gt;Box-Cox Transformation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Box-Cox transformation is a way to transform non-normal dependent variables into a normal shape. It is named after George Box and David Cox, who introduced the transformation in 1964. The transformation is defined as:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Y = (X^L - 1) / L&lt;/p&gt;
&lt;p&gt;where X is the variable to be transformed, Y is the transformed variable, and L is a parameter that needs to be estimated. When L = 0, the Box-Cox transformation becomes the log transformation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Box-Cox transformation can be useful in regression analysis, ANOVA, and other statistical tests when the assumptions of normality are not met. It can also be useful in improving the interpretability of the model by making the relationship between the dependent and independent variables more linear.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pca-for-visualization-and-eda&#34;&gt;PCA for visualization and EDA&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Principal component analysis (PCA) is a statistical technique that is used to reduce the dimensionality of a data set.&lt;/li&gt;
&lt;li&gt;It does this by identifying the directions in which the data vary the most, and then projecting the data onto a lower-dimensional space.&lt;/li&gt;
&lt;li&gt;This can be useful for visualization, as it can allow you to plot high-dimensional data in a 2D or 3D space.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>AWS Certified ML - Specialty exam (MLS-C01) - 3a. Modeling Pre-requisite</title>
      <link>https://ayushsubedi.github.io/posts/aws_ml_speciality_modeling_prereq/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/aws_ml_speciality_modeling_prereq/</guid>
      <description>&lt;h1 id=&#34;modeling-pre-reqs&#34;&gt;Modeling Pre-reqs&lt;/h1&gt;
&lt;h2 id=&#34;basic-machine-learning&#34;&gt;Basic Machine Learning&lt;/h2&gt;
&lt;h2 id=&#34;confusion-matrix&#34;&gt;Confusion Matrix&lt;/h2&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;h2 id=&#34;design-of-experiments&#34;&gt;Design of Experiments&lt;/h2&gt;
&lt;h2 id=&#34;game-theory&#34;&gt;Game Theory&lt;/h2&gt;
&lt;h2 id=&#34;model-quality&#34;&gt;Model Quality&lt;/h2&gt;
&lt;h2 id=&#34;non-parametric-tests&#34;&gt;Non-Parametric Tests&lt;/h2&gt;
&lt;h2 id=&#34;optimization&#34;&gt;Optimization&lt;/h2&gt;
&lt;h2 id=&#34;probability-based-models&#34;&gt;Probability based models&lt;/h2&gt;
&lt;h2 id=&#34;probability-distributions&#34;&gt;Probability distributions&lt;/h2&gt;
&lt;h2 id=&#34;regression&#34;&gt;Regression&lt;/h2&gt;
&lt;h2 id=&#34;time-series-models&#34;&gt;Time series models&lt;/h2&gt;
&lt;h2 id=&#34;variable-selection&#34;&gt;Variable Selection&lt;/h2&gt;
&lt;h2 id=&#34;other-topics&#34;&gt;Other Topics&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>AWS Certified ML - Specialty exam (MLS-C01) - 3b. Modeling</title>
      <link>https://ayushsubedi.github.io/posts/aws_ml_speciality_modeling/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/aws_ml_speciality_modeling/</guid>
      <description>&lt;h1 id=&#34;modeling&#34;&gt;Modeling&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#activation-functions&#34;&gt;Activation Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#convolutional-neural-network&#34;&gt;Convolutional Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#recurrent-neural-networks&#34;&gt;Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modern-nlp-with-bert-and-gpt-and-transfer-learning&#34;&gt;Modern NLP with BERT and GPT, and Transfer Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deep-learning-on-ec2-and-emr&#34;&gt;Deep Learning on EC2 and EMR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tuning-neural-networks&#34;&gt;Tuning Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regularization-techniques-for-neural-networks-dropout-early-stopping&#34;&gt;Regularization Techniques for Neural Networks (Dropout, Early Stopping)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#l1-and-l2-regularization&#34;&gt;L1 and L2 Regularization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#grief-with-gradients-the-vanishing-gradient-problem&#34;&gt;Grief with Gradients The Vanishing Gradient problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-confusion-matrix&#34;&gt;The Confusion Matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#precision-recall-f1-auc-and-more&#34;&gt;Precision, Recall, F1, AUC, and more&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ensemble-methods-bagging-and-boosting&#34;&gt;Ensemble Methods Bagging and Boosting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introducing-amazon-sagemaker&#34;&gt;Introducing Amazon SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-learner-in-sagemaker&#34;&gt;Linear Learner in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#xgboost-in-sagemaker&#34;&gt;XGBoost in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#seq2seq-in-sagemaker&#34;&gt;Seq2Seq in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deepar-in-sagemaker&#34;&gt;DeepAR in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#blazingtext-in-sagemaker&#34;&gt;BlazingText in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#object2vec-in-sagemaker&#34;&gt;Object2Vec in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#object-detection-in-sagemaker&#34;&gt;Object Detection in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#image-classification-in-sagemaker&#34;&gt;Image Classification in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#semantic-segmentation-in-sagemaker&#34;&gt;Semantic Segmentation in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-cut-forest-in-sagemaker&#34;&gt;Random Cut Forest in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#neural-topic-model-in-sagemaker&#34;&gt;Neural Topic Model in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#latent-dirichlet-allocation-lda-in-sagemaker&#34;&gt;Latent Dirichlet Allocation (LDA) in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-nearest-neighbors-knn-in-sagemaker&#34;&gt;K-Nearest-Neighbors (KNN) in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-means-clustering-in-sagemaker&#34;&gt;K-Means Clustering in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#principal-component-analysis-pca-in-sagemaker&#34;&gt;Principal Component Analysis (PCA) in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#factorization-machines-in-sagemaker&#34;&gt;Factorization Machines in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ip-insights-in-sagemaker&#34;&gt;IP Insights in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reinforcement-learning-in-sagemaker&#34;&gt;Reinforcement Learning in SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#automatic-model-tuning&#34;&gt;Automatic Model Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#apache-spark-with-sagemaker&#34;&gt;Apache Spark with SageMaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-studio-and-sagemaker-experiments&#34;&gt;SageMaker Studio, and SageMaker Experiments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-debugger&#34;&gt;SageMaker Debugger&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-autopilot-automl&#34;&gt;SageMaker Autopilot / AutoML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-model-monitor&#34;&gt;SageMaker Model Monitor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-recent-features-jumpstart-data-wrangler-features-store-edge-manager&#34;&gt;Other recent features (JumpStart, Data Wrangler, Features Store, Edge Manager)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-canvas&#34;&gt;SageMaker Canvas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bias-measures-in-sagemaker-canvas&#34;&gt;Bias Measures in SageMaker Canvas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-training-compiler&#34;&gt;SageMaker Training Compiler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-comprehend&#34;&gt;Amazon Comprehend&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-translate&#34;&gt;Amazon Translate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-transcribe&#34;&gt;Amazon Transcribe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-polly&#34;&gt;Amazon Polly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-rekognition&#34;&gt;Amazon Rekognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-forecast&#34;&gt;Amazon Forecast&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-forecast-algorithms&#34;&gt;Amazon Forecast Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-lex&#34;&gt;Amazon Lex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-personalize&#34;&gt;Amazon Personalize&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lightning-round-textract-deeplens-deepracher-lookout-and-monitron&#34;&gt;Lightning round! TexTract, DeepLens, DeepRacher, Lookout, and Monitron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#torchserve-aws-neuron-and-aws-panorama&#34;&gt;TorchServe, AWS Neuron, and AWS Panorama&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deep-composer-fraud-detection-codeguru-and-contact-lens&#34;&gt;Deep Composer, Fraud Detection, CodeGuru, and Contact Lens&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#amazon-kendra-and-amazon-augmented-ai-a2i&#34;&gt;Amazon Kendra and Amazon Augmented AI (A2I)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This section covers framing business problems as machine learning problems, selecting the appropriate model(s) for a given machine learning problem, training machine learning models, performing hyperparameter optimization, and evaluate machine learning models.&lt;/p&gt;
&lt;h2 id=&#34;deeplearning-frameworks&#34;&gt;Deeplearning Frameworks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Tensorflow/Keras (Google)&lt;/li&gt;
&lt;li&gt;PyTorch (Meta)&lt;/li&gt;
&lt;li&gt;MXNet (Apache, and therefore AWS leans towards this)&lt;/li&gt;
&lt;li&gt;Scikit-Learn (for simple DL)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;activation-functions&#34;&gt;Activation Functions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Apply a non linear transformation&lt;/li&gt;
&lt;li&gt;Given the input, what should by output be&lt;/li&gt;
&lt;li&gt;Can be applied in between layers, or in the output layer&lt;/li&gt;
&lt;li&gt;Step Function, Sigmoid, TanH, ReLU, Leaky ReLU&lt;/li&gt;
&lt;li&gt;Binary Step Function is either on or off, cannot handle multiple classification, vertical slopes do not work with calculus&lt;/li&gt;
&lt;li&gt;Sigmoid: 0 to 1&lt;/li&gt;
&lt;li&gt;TanH: -1 to 1&lt;/li&gt;
&lt;li&gt;For Sigmoid and TanH there is a vanishing gradient problem (value changes slowly for high or low value)&lt;/li&gt;
&lt;li&gt;Sigmoid and TanH are computationally expensive&lt;/li&gt;
&lt;li&gt;ReLu: fast to compute, for inputs that are zero or negative, it is a linear function (dying relu problem)&lt;/li&gt;
&lt;li&gt;Leaky ReLU solves this&lt;/li&gt;
&lt;li&gt;Parametric ReLU, slope in the negative part is learned via backpropagation, complicated&lt;/li&gt;
&lt;li&gt;Exponential Linear Unit (ELU)&lt;/li&gt;
&lt;li&gt;Maxout: usually not worth the effort&lt;/li&gt;
&lt;li&gt;Softmax: usually the final layer of a classification model&lt;/li&gt;
&lt;li&gt;RNN&amp;rsquo;s do well with Tanh&lt;/li&gt;
&lt;li&gt;Sigmoid if more that one classification is required for the same thing&lt;/li&gt;
&lt;li&gt;For everything else, start with ReLU&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;convolutional-neural-network&#34;&gt;Convolutional Neural Network&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;CNN vs MLP (Multilayer perceptron)&lt;/li&gt;
&lt;li&gt;They have convolutional layers&lt;/li&gt;
&lt;li&gt;Some filters may detect edges, lines, shapes etc. and deeper layers can detect objects&lt;/li&gt;
&lt;li&gt;Feature location invariant, Shift Invariant, Space Invariant Artificial Neural Networks&lt;/li&gt;
&lt;li&gt;Image and video recognition, recommender systems, image classification, image segmentations,&lt;/li&gt;
&lt;li&gt;Machine translation, Sentence Classification, Sentiment analysis&lt;/li&gt;
&lt;li&gt;AlexNet, LeNet, GoogLeNet, ResNet as an example&lt;/li&gt;
&lt;li&gt;source data must be of appropriate dimensions&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;recurrent-neural-networks&#34;&gt;Recurrent Neural Networks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;deals with sequences in time (predict stock prices, understand words in a sentence, translation etc)&lt;/li&gt;
&lt;li&gt;time series data, sequence of arbitrary length&lt;/li&gt;
&lt;li&gt;captions for images, order matters&lt;/li&gt;
&lt;li&gt;structure and context is relevant&lt;/li&gt;
&lt;li&gt;machine generated music&lt;/li&gt;
&lt;li&gt;past behaviour of neuron impacts the future&lt;/li&gt;
&lt;li&gt;Sequence to Sequence: predict stock prices based on series of historic data&lt;/li&gt;
&lt;li&gt;Sequence to vector: words in a sentence to sentiment&lt;/li&gt;
&lt;li&gt;Vector to sequence: create captions from an image&lt;/li&gt;
&lt;li&gt;Encoder -&amp;gt; Decoder: Sequence -&amp;gt; vector -&amp;gt; sequence, machine translation&lt;/li&gt;
&lt;li&gt;Backpropogation through time&lt;/li&gt;
&lt;li&gt;Ends up looking like a really really deep neural network&lt;/li&gt;
&lt;li&gt;Therefore, we use truncated backpropagation through time&lt;/li&gt;
&lt;li&gt;State from earlier time steps get diluted over time, Long Short-Term memory cell LSTM cell&lt;/li&gt;
&lt;li&gt;GRU cell: Gated Recurrent Unit, Simplified LSTM which performs almost as well&lt;/li&gt;
&lt;li&gt;Traning RNN&amp;rsquo;s is hard, very sensitive to topologies, choice of hyperparameters, very resource intensive, a wrong choice can lead to a RNN that does not converge at all.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;modern-nlp-with-bert-and-gpt-and-transfer-learning&#34;&gt;Modern NLP with BERT and GPT, and Transfer Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Transformer deep learning architectures&lt;/li&gt;
&lt;li&gt;BERT, RoBERTa, T5, GPT2, GPT3, etc&lt;/li&gt;
&lt;li&gt;DistilBERT: uses knowledge distillation to reduce model size by 40%&lt;/li&gt;
&lt;li&gt;BERT: Bi-directional Encoder Representations from Transformers&lt;/li&gt;
&lt;li&gt;GPT: Generative Pre-trained Transformer&lt;/li&gt;
&lt;li&gt;Transfer Learning&lt;/li&gt;
&lt;li&gt;Model zoos: hugging face offer pre trained models to start with&lt;/li&gt;
&lt;li&gt;Hugging face DLC (deep learning containers)&lt;/li&gt;
&lt;li&gt;Transfer Learning, retrain=True vs False&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;deep-learning-on-ec2emr&#34;&gt;Deep Learning on EC2/EMR&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;EMR supports Apache MXNet and GPU instance types&lt;/li&gt;
&lt;li&gt;Appropriate instance types for deep learning&lt;/li&gt;
&lt;li&gt;P3, P2, G3&lt;/li&gt;
&lt;li&gt;Deep Learning AMI&amp;rsquo;s&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tuning-neural-networks&#34;&gt;Tuning Neural Networks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Neural nets are trained by gradient descent or sth similar&lt;/li&gt;
&lt;li&gt;We start at some random point, and sample different solutions seeking to minimize some cost functions, over many epochs&lt;/li&gt;
&lt;li&gt;how far apart these samples are is the learning rate&lt;/li&gt;
&lt;li&gt;learning rate is an example of a hyperparameter&lt;/li&gt;
&lt;li&gt;batch size is also a hyperparameter, smaller batch size can work out of local minima&lt;/li&gt;
&lt;li&gt;small batch size tend to not get stuck in local minima&lt;/li&gt;
&lt;li&gt;large batch sizes can converge on the wrong solution at random&lt;/li&gt;
&lt;li&gt;large learning rates can overshoot the correct solution&lt;/li&gt;
&lt;li&gt;small learning rates increate training time&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;regularization-techniques-for-neural-networks-dropout-early-stopping&#34;&gt;Regularization Techniques for Neural Networks (Dropout, Early Stopping)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Regularization helps with avoiding overfitting&lt;/li&gt;
&lt;li&gt;build simple model, dropout, early stopping can also help with avoiding overfitting&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;l1-and-l2-regularization&#34;&gt;L1 and L2 Regularization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;L1: sum of abs value of weights: perform feature selection, computationally inefficient, sparse output&lt;/li&gt;
&lt;li&gt;L2: sum of square of weights, all features considered but weighted, computationally efficient, dence output&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;grief-with-gradients-the-vanishing-gradient-problem&#34;&gt;Grief with Gradients The Vanishing Gradient problem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;vanishing gradient propogate to deeper layer&lt;/li&gt;
&lt;li&gt;slope is approaching zero&lt;/li&gt;
&lt;li&gt;it could be the local miminum or global where the convergence is happening&lt;/li&gt;
&lt;li&gt;long short term memory RNN can be used&lt;/li&gt;
&lt;li&gt;resnet also helps with vanishing gradient problem&lt;/li&gt;
&lt;li&gt;better activation function (relu is a good choice)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-confusion-matrix&#34;&gt;The Confusion Matrix&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;sometimes accuracy does not tell the whole story&lt;/li&gt;
&lt;li&gt;TP, TN, FP, FN&lt;/li&gt;
&lt;li&gt;Confusion matrix shows this&lt;/li&gt;
&lt;li&gt;multi class confusion matrix: heatmap&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;precision-recall-f1-auc-and-more&#34;&gt;Precision, Recall, F1, AUC, and more&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Precision/Correct Positives/Percent of relevant results: when you are a lot about false positives: TP/(TP+FP)&lt;/li&gt;
&lt;li&gt;Recall/Sensitivity/True Positive Rate:  TP/(TP + FN): when you care about false negatives&lt;/li&gt;
&lt;li&gt;F1 score: harmonic mean of Precision and Recall&lt;/li&gt;
&lt;li&gt;Specificity: TN/(TN+FP)&lt;/li&gt;
&lt;li&gt;RMSE, AMSE, etc.&lt;/li&gt;
&lt;li&gt;ROC curve: Receiver Operating Characteristic Curve: Plot of true positive rate (recall) vs false positive rate at various threshold setting.&lt;/li&gt;
&lt;li&gt;AUC curve: area under the ROC curve.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ensemble-methods-bagging-and-boosting&#34;&gt;Ensemble Methods Bagging and Boosting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bagging: Generate N new training sets by random sampling with replacement, each resampled model can be trained in parallel&lt;/li&gt;
&lt;li&gt;Boosting: Observations are weighted, training is sequential&lt;/li&gt;
&lt;li&gt;XGBoost is the latest hotness, boosting generally yields better accuracy, bagging avoids overfitting, bagging is easier to parallelize&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introducing-amazon-sagemaker&#34;&gt;Introducing Amazon SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;built to handle the entire machine learning workflow&lt;/li&gt;
&lt;li&gt;deploy model, evaluate results in production, fetch, clean and prepare data, train and evaluate a model&lt;/li&gt;
&lt;li&gt;training data will be in s3, sagemakaker docker EC2 for inference&lt;/li&gt;
&lt;li&gt;spins as many hosts, spins as many endpoints&lt;/li&gt;
&lt;li&gt;Sagemaker notebook: notebook instance on EC2, has access to s3, scikit learn, spark, tensorflow, ability to deploy trained models for making predictions at scale&lt;/li&gt;
&lt;li&gt;hyperparameter tuning from notebook&lt;/li&gt;
&lt;li&gt;Sagemaker console&lt;/li&gt;
&lt;li&gt;Data comes from S3, ideal format is RecordIO/Protobuf/csv&lt;/li&gt;
&lt;li&gt;Can also ingest from Athena, EMR, Redshift, Amazon Keyspaces DB&lt;/li&gt;
&lt;li&gt;Apache Spark integrates with Sagemaker&lt;/li&gt;
&lt;li&gt;Scikit learn, numpy, pandas all work&lt;/li&gt;
&lt;li&gt;Create training job&lt;/li&gt;
&lt;li&gt;save your trained model to s3&lt;/li&gt;
&lt;li&gt;can be deployed using persistent endpoint for making individual predictions on demand&lt;/li&gt;
&lt;li&gt;or batch transform to get prediction for and entire dataset&lt;/li&gt;
&lt;li&gt;inference pipelines&lt;/li&gt;
&lt;li&gt;sagemaker neo for deploying to edge devices&lt;/li&gt;
&lt;li&gt;elastic inference for accelerating deep learning models&lt;/li&gt;
&lt;li&gt;automatic scaling of endpoints as needed&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;linear-learner-in-sagemaker&#34;&gt;Linear Learner in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Linear learer can handle both classification and regression&lt;/li&gt;
&lt;li&gt;can do classification using Linear Learner threshold&lt;/li&gt;
&lt;li&gt;as long as a line will fit&lt;/li&gt;
&lt;li&gt;RecordIO wrapped protobuf float32, or csv (first column assumed to be the label)&lt;/li&gt;
&lt;li&gt;File or pipe mode both supported&lt;/li&gt;
&lt;li&gt;pipe mode will be more efficient&lt;/li&gt;
&lt;li&gt;if s3 is taking to long to train, pipe is a simple optimization&lt;/li&gt;
&lt;li&gt;training data should be normalized&lt;/li&gt;
&lt;li&gt;input data should be shuffled&lt;/li&gt;
&lt;li&gt;uses SGD&lt;/li&gt;
&lt;li&gt;multiple models are optimized in parallel&lt;/li&gt;
&lt;li&gt;tune l1, l2 regularization&lt;/li&gt;
&lt;li&gt;balance multiclass weights: give each class equal importance in loss functions&lt;/li&gt;
&lt;li&gt;learning rate, mini batch size, l1 regualization&lt;/li&gt;
&lt;li&gt;multi gpu does not help&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;xgboost-in-sagemaker&#34;&gt;XGBoost in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;eXtreme gradient boosting&lt;/li&gt;
&lt;li&gt;boosted group of decision trees&lt;/li&gt;
&lt;li&gt;gradient descent&lt;/li&gt;
&lt;li&gt;winning a lot of kaggle competitions&lt;/li&gt;
&lt;li&gt;fast&lt;/li&gt;
&lt;li&gt;classification/regression&lt;/li&gt;
&lt;li&gt;CSV/libsvm/recordIO-protobuf/parquet&lt;/li&gt;
&lt;li&gt;models are searilized/deserialized with pickle&lt;/li&gt;
&lt;li&gt;can use as a framework withing notebooks&lt;/li&gt;
&lt;li&gt;or as a built in sagemaker algorithm&lt;/li&gt;
&lt;li&gt;subsample (prevent overfitting)&lt;/li&gt;
&lt;li&gt;ETA (step size shrinkage, prevents overfitting)&lt;/li&gt;
&lt;li&gt;Gamma (minimul loss reduction to create a partition)&lt;/li&gt;
&lt;li&gt;Alpha (L1 regularization term, larger = more conservative)&lt;/li&gt;
&lt;li&gt;Lambda (L2 regularization term, larger = more conservative)&lt;/li&gt;
&lt;li&gt;eval_metric: Optimize on AUC, example: if you care about false positives more than accuracy&lt;/li&gt;
&lt;li&gt;scale_pos_weight: adjusts balance of positive and negative weights, helpful for unbalanced classes&lt;/li&gt;
&lt;li&gt;max_depth : too high may overfit&lt;/li&gt;
&lt;li&gt;Xgboost with cpu: M5 is a good choice (optimize for memory and not compute)&lt;/li&gt;
&lt;li&gt;Xgboost with gpu: tree_method hyperparameter: gpu_hist, cheaper and faster, P3 is good choice&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;seq2seq-in-sagemaker&#34;&gt;Seq2Seq in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;sequence to sequence (example machine translation, text summarization, speech to text)&lt;/li&gt;
&lt;li&gt;implemented with RNN&amp;rsquo;s and CNN&amp;rsquo;s with attention&lt;/li&gt;
&lt;li&gt;RecordIO-Protobuf tokens must be integers&lt;/li&gt;
&lt;li&gt;start with tokenized text files&lt;/li&gt;
&lt;li&gt;convert to protobuf using sample code&lt;/li&gt;
&lt;li&gt;must provide training data, validation data and vocabulary files&lt;/li&gt;
&lt;li&gt;training machine translation can take days, pretrained models are available&lt;/li&gt;
&lt;li&gt;public training datasets are avaialable for specific translation tasks&lt;/li&gt;
&lt;li&gt;batch_size, optimizer_type, learning_rate, num_layers_encoder, num_layers_decoder, can optimize on accuracy, bleu score (compares against multiple reference translations), perplexity (cross-entropy)&lt;/li&gt;
&lt;li&gt;cannot be parallelized&lt;/li&gt;
&lt;li&gt;can only use gpu instance&lt;/li&gt;
&lt;li&gt;can use multi gpu within an instance machine&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;deepar-in-sagemaker&#34;&gt;DeepAR in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Forecasting one dimensional time series data&lt;/li&gt;
&lt;li&gt;uses rnn&amp;rsquo;s&lt;/li&gt;
&lt;li&gt;allows you to train the same model over several related time series&lt;/li&gt;
&lt;li&gt;finds frequencies and seasonality&lt;/li&gt;
&lt;li&gt;json lines format, Gzip or Parquet&lt;/li&gt;
&lt;li&gt;each record must contain, start and target&lt;/li&gt;
&lt;li&gt;each record can contain dynamic features and categorical features&lt;/li&gt;
&lt;li&gt;always include entire time series for training, testing and inference&lt;/li&gt;
&lt;li&gt;use entire dataset as test set&lt;/li&gt;
&lt;li&gt;do not use very large values for prediction (&amp;gt;400)&lt;/li&gt;
&lt;li&gt;train on many time series&lt;/li&gt;
&lt;li&gt;contect length, epochs, mini batch size, learning rate, num cells&lt;/li&gt;
&lt;li&gt;can use cpu or gpu&lt;/li&gt;
&lt;li&gt;single or multi machine&lt;/li&gt;
&lt;li&gt;cpu only for inferene&lt;/li&gt;
&lt;li&gt;may need larger instances for tuning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;blazingtext-in-sagemaker&#34;&gt;BlazingText in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Text classification: predict labels for a sentence, useful in web searches, information retrieal, supervised&lt;/li&gt;
&lt;li&gt;Word2vec: creates a vector representation of workds&lt;/li&gt;
&lt;li&gt;semantically similar words are represented by vectors close to each otehr&lt;/li&gt;
&lt;li&gt;this is called a word embedding&lt;/li&gt;
&lt;li&gt;it is useful for nlp, but is not an nlp algorithm itself&lt;/li&gt;
&lt;li&gt;it only works on individual words, not sentences or documents&lt;/li&gt;
&lt;li&gt;for supervised mode, one sentence per line, first word in the sentence is the string &lt;em&gt;label&lt;/em&gt; followed by the label&lt;/li&gt;
&lt;li&gt;Also, &amp;ldquo;augmented manifest text format&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Word3vec just wants a text file with one training sentence per line&lt;/li&gt;
&lt;li&gt;There are multiple modes:&lt;/li&gt;
&lt;li&gt;Cbow (Continuous Bag of Words)&lt;/li&gt;
&lt;li&gt;Skip-gram&lt;/li&gt;
&lt;li&gt;Batch skip-gram (Distributed computation over many CPU nodes)&lt;/li&gt;
&lt;li&gt;Word2vec: mode, learning rate, window size, verctor dim, negative samples&lt;/li&gt;
&lt;li&gt;Text classification: epochs, learning rate, word ngrams, vector dim&lt;/li&gt;
&lt;li&gt;For cbow and skipgram, recommend a single ml.p3.2xlarge, any single CPU or single GPU instance will work&lt;/li&gt;
&lt;li&gt;for batch_skipgram, can use single or multiple CPU instances&lt;/li&gt;
&lt;li&gt;for text classification C5 recommended if less than 2GB training data, for larger datasets use a single GPU instance ml.p2.xlarge or ml.p3.2xlarge&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;object2vec-in-sagemaker&#34;&gt;Object2Vec in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;creates low-dimensional dense embeddings of high-dimensional objects&lt;/li&gt;
&lt;li&gt;compute nearest neighbors of objects&lt;/li&gt;
&lt;li&gt;visualize clusters&lt;/li&gt;
&lt;li&gt;genre prediction&lt;/li&gt;
&lt;li&gt;recommendations&lt;/li&gt;
&lt;li&gt;data must be tokenized into integers&lt;/li&gt;
&lt;li&gt;training data consists of pairs of tokens and or sequenses of tokens&lt;/li&gt;
&lt;li&gt;process data into json lines and shuffle it&lt;/li&gt;
&lt;li&gt;train with two input channels, two encoders, and a comparator&lt;/li&gt;
&lt;li&gt;encoder choices: average-pooled embeddings, cnn&amp;rsquo;s, bidirectional lstm&lt;/li&gt;
&lt;li&gt;comparator is followed by feed-fowrard neural network&lt;/li&gt;
&lt;li&gt;usual suspect: dropout, early stopping, epochs, learning rate, bbatch size, layers, activation function, optimizer, weight decay&lt;/li&gt;
&lt;li&gt;Enc1_network, enc2_network&lt;/li&gt;
&lt;li&gt;instance types: can only train on a single machine (cpu or gpu, multi-gpu ok)&lt;/li&gt;
&lt;li&gt;inference: use ml.p2.2xlarge&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;object-detection-in-sagemaker&#34;&gt;Object Detection in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;identify all objects in an image with bounding box&lt;/li&gt;
&lt;li&gt;detects and classifies objects with a single deep neural network&lt;/li&gt;
&lt;li&gt;classes are accompanied by confidence scores&lt;/li&gt;
&lt;li&gt;can train from scratch, or use pretrained models based on imagenet&lt;/li&gt;
&lt;li&gt;recodrio or image format&lt;/li&gt;
&lt;li&gt;with image format, supply a json file for annotation data for each image&lt;/li&gt;
&lt;li&gt;takes and image input, outputs all instances of objects in teh imagte with categories and confidence scores&lt;/li&gt;
&lt;li&gt;uses cnn with single shot multibox detector ssd algorithm, the base being vgg-16 or resnet-50&lt;/li&gt;
&lt;li&gt;transfer learning mode/incrementatl training: use pretrained model for the base network instead of random inintial weights&lt;/li&gt;
&lt;li&gt;uses flip, rescale, and jitter internally to avoid overfitting&lt;/li&gt;
&lt;li&gt;mini batch size, learning rate, optimizer&lt;/li&gt;
&lt;li&gt;gpu instances for training&lt;/li&gt;
&lt;li&gt;multi gpu multi machines&lt;/li&gt;
&lt;li&gt;for inference cpu is enough&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;image-classification-in-sagemaker&#34;&gt;Image Classification in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;assign one or more labels to an image&lt;/li&gt;
&lt;li&gt;does not tell you where objects are&lt;/li&gt;
&lt;li&gt;mxnet recordio (not protobuf)&lt;/li&gt;
&lt;li&gt;raw jpg or png&lt;/li&gt;
&lt;li&gt;.lst files to associate image index and class&lt;/li&gt;
&lt;li&gt;augmented manifest image format enables pipe mode&lt;/li&gt;
&lt;li&gt;resnet cnn under the hood&lt;/li&gt;
&lt;li&gt;full training mode&lt;/li&gt;
&lt;li&gt;transfer learning mode&lt;/li&gt;
&lt;li&gt;default image is 224 224 3&lt;/li&gt;
&lt;li&gt;bbatch size, learning rate, optimizer&lt;/li&gt;
&lt;li&gt;weight decay, beta 1, beta 2, eps, gamma&lt;/li&gt;
&lt;li&gt;gpu instance fr training&lt;/li&gt;
&lt;li&gt;cpu or gpu for inference&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;semantic-segmentation-in-sagemaker&#34;&gt;Semantic Segmentation in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;pixel level object classificaion&lt;/li&gt;
&lt;li&gt;different from image classification&lt;/li&gt;
&lt;li&gt;useful for self driving vehicles, medical imaging, robot sensing&lt;/li&gt;
&lt;li&gt;produces a semantic mask&lt;/li&gt;
&lt;li&gt;jpg or img with annotations&lt;/li&gt;
&lt;li&gt;augmented manifest image format supported for pipe mode&lt;/li&gt;
&lt;li&gt;jpg images accepted for inference&lt;/li&gt;
&lt;li&gt;mxnet gluon and gluon cv&lt;/li&gt;
&lt;li&gt;fully convolution network, pyramid scene parsing, deeplabv3&lt;/li&gt;
&lt;li&gt;resnet50, renet101, both rained on imagenet&lt;/li&gt;
&lt;li&gt;incremental training, or scratch&lt;/li&gt;
&lt;li&gt;epochs, learning rate, batch size, optimizer, algorithm, backbone&lt;/li&gt;
&lt;li&gt;only gpu for training (p2 or p3), and only on one maching&lt;/li&gt;
&lt;li&gt;cpu or gpu for inference&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;random-cut-forest-in-sagemaker&#34;&gt;Random Cut Forest in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;anomaly detection&lt;/li&gt;
&lt;li&gt;unsupervised&lt;/li&gt;
&lt;li&gt;detect unexpected spikes in time series data&lt;/li&gt;
&lt;li&gt;breaks in periodicity&lt;/li&gt;
&lt;li&gt;unclassifiable data points&lt;/li&gt;
&lt;li&gt;assigns and anamoly score to each data points&lt;/li&gt;
&lt;li&gt;recordio protobuf or csv&lt;/li&gt;
&lt;li&gt;can use file or pipe mode on either&lt;/li&gt;
&lt;li&gt;optional test channel for computation&lt;/li&gt;
&lt;li&gt;creates a forest of trees where each tree is a partition of the training data, looks at expected change in complexity of the tree as a result of adding a point into it&lt;/li&gt;
&lt;li&gt;data is sampled randomly and then trained&lt;/li&gt;
&lt;li&gt;rcf shows up in kinesis analytics as well, it can work on streaming data as well.&lt;/li&gt;
&lt;li&gt;num_trees, num_samples_per_tree (should be chosen such that 1/num_samples_per_tree approximates the ratio of anomalous to normal data)&lt;/li&gt;
&lt;li&gt;does not take advantage of gpu&lt;/li&gt;
&lt;li&gt;ml.c5.xl for inference&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;neural-topic-model-in-sagemaker&#34;&gt;Neural Topic Model in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;organize documents into topics&lt;/li&gt;
&lt;li&gt;classify or summarize documents based on topics&lt;/li&gt;
&lt;li&gt;it is not just tf/idf&lt;/li&gt;
&lt;li&gt;unsupervised: algorithm is neural variational inference&lt;/li&gt;
&lt;li&gt;four data channels, train, validation, test and auxiliary&lt;/li&gt;
&lt;li&gt;record io or csv&lt;/li&gt;
&lt;li&gt;words muyst be tokenized into integers&lt;/li&gt;
&lt;li&gt;file or pipe mode&lt;/li&gt;
&lt;li&gt;you define how many topics you want, these topics are latent representation based on top ranking words&lt;/li&gt;
&lt;li&gt;one of two modelling algorithms sagemaker offers&lt;/li&gt;
&lt;li&gt;batch size, num_topics&lt;/li&gt;
&lt;li&gt;gpu or cpu&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;latent-dirichlet-allocation-lda-in-sagemaker&#34;&gt;Latent Dirichlet Allocation (LDA) in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;latent dirichlet allocation&lt;/li&gt;
&lt;li&gt;another topic modeling algorithm but not based on deep learning&lt;/li&gt;
&lt;li&gt;unsupervised: topics are unlabeled, they are just grouping of documents with a shared subseet of words&lt;/li&gt;
&lt;li&gt;can be used for other purposes as well&lt;/li&gt;
&lt;li&gt;train channel, optional test channel&lt;/li&gt;
&lt;li&gt;protobuf or csv&lt;/li&gt;
&lt;li&gt;each document has counts for every word in vocabulary&lt;/li&gt;
&lt;li&gt;pipe mode: only supported with proto&lt;/li&gt;
&lt;li&gt;unsupervised, generates however many topics you specify&lt;/li&gt;
&lt;li&gt;per-word log likelyhood&lt;/li&gt;
&lt;li&gt;num_topics, alpha0&lt;/li&gt;
&lt;li&gt;cpu single instance, cannot parallelize&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;k-nearest-neighbors-knn-in-sagemaker&#34;&gt;K-Nearest-Neighbors (KNN) in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;simple classification or regression algorithm&lt;/li&gt;
&lt;li&gt;classification: k closest points&lt;/li&gt;
&lt;li&gt;regression: average values&lt;/li&gt;
&lt;li&gt;train channel, test channel emits accuracy or MSE&lt;/li&gt;
&lt;li&gt;protobuf or csv (first column is label)&lt;/li&gt;
&lt;li&gt;data is sampled, sagemaker includes dimensionality reduction stage, build an index for looking up neighbors, serialize the model, query the model for given K&lt;/li&gt;
&lt;li&gt;hyperparameter K, sample_size&lt;/li&gt;
&lt;li&gt;cpu or gpu&lt;/li&gt;
&lt;li&gt;cpu or gpu for inference&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;k-means-clustering-in-sagemaker&#34;&gt;K-Means Clustering in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;unsupervised clustering&lt;/li&gt;
&lt;li&gt;divide data into k groups, where members of a group are as similar as possible to each other&lt;/li&gt;
&lt;li&gt;web scale k means clustering&lt;/li&gt;
&lt;li&gt;training input: train channel, train in shardedbys3key and testing: fullyreplicated&lt;/li&gt;
&lt;li&gt;recordio or csv&lt;/li&gt;
&lt;li&gt;file or pipemode&lt;/li&gt;
&lt;li&gt;every ovservation is mapped to n-dimensional space&lt;/li&gt;
&lt;li&gt;works to optimize the center of k clusters&lt;/li&gt;
&lt;li&gt;algorithm: k means++ tries to make initial clusters far away, lloyd&amp;rsquo;s method&lt;/li&gt;
&lt;li&gt;mini_batch_size, extra_center_factor, init_method&lt;/li&gt;
&lt;li&gt;cpu or gpu, but cpu recommended&lt;/li&gt;
&lt;li&gt;only one gpu per instance used on gpu&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;principal-component-analysis-pca-in-sagemaker&#34;&gt;Principal Component Analysis (PCA) in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;dimensionality reduction&lt;/li&gt;
&lt;li&gt;unsupervised&lt;/li&gt;
&lt;li&gt;covariance matrix is created, then SVD&lt;/li&gt;
&lt;li&gt;two modesL regular: for sparse matrix, randomized: for large number of observations and features&lt;/li&gt;
&lt;li&gt;algorithm_mode and subtract_mean&lt;/li&gt;
&lt;li&gt;gpu or cpu&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;factorization-machines-in-sagemaker&#34;&gt;Factorization Machines in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;dealing with sparse data&lt;/li&gt;
&lt;li&gt;item recommendations&lt;/li&gt;
&lt;li&gt;supervised: classification or regression&lt;/li&gt;
&lt;li&gt;limited to pair-wise interactions&lt;/li&gt;
&lt;li&gt;protobuf with float32&lt;/li&gt;
&lt;li&gt;bias, factors, and linear terms&lt;/li&gt;
&lt;li&gt;cpu or gpu, cpu recommended&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ip-insights-in-sagemaker&#34;&gt;IP Insights in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;finding fishy behaviour&lt;/li&gt;
&lt;li&gt;unsupervised learning of ip address&lt;/li&gt;
&lt;li&gt;identifies suspicious behaviour from ip addresses&lt;/li&gt;
&lt;li&gt;user names, account ids, not need to pre process&lt;/li&gt;
&lt;li&gt;training channel, optional validation (computes auc score)&lt;/li&gt;
&lt;li&gt;csv only (entity, ips)&lt;/li&gt;
&lt;li&gt;neural network to learn latent vector representations of entities and ip addresses&lt;/li&gt;
&lt;li&gt;entities are hashed and embedded&lt;/li&gt;
&lt;li&gt;automatically generates negative samples during training by randomly pairing entities and ips&lt;/li&gt;
&lt;li&gt;num_entity vectors, vector_dim, epochs, learning rate, batch size&lt;/li&gt;
&lt;li&gt;cpu or gpu&lt;/li&gt;
&lt;li&gt;gpu recommended&lt;/li&gt;
&lt;li&gt;multiple gpu can be used withing an instance&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reinforcement-learning-in-sagemaker&#34;&gt;Reinforcement Learning in SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;agent and environment&lt;/li&gt;
&lt;li&gt;supply chain management, hvac systems, industrial robots, dialog systems, autonomous vehicles&lt;/li&gt;
&lt;li&gt;yields fast on-line performance once the space has been explored&lt;/li&gt;
&lt;li&gt;Q learning: environment, actions, state/action part&lt;/li&gt;
&lt;li&gt;uses a deep learning framework with tensorflow and mxnet&lt;/li&gt;
&lt;li&gt;supports intel coach and ray rllib toolkits&lt;/li&gt;
&lt;li&gt;custom, open-source or commercial environments supported&lt;/li&gt;
&lt;li&gt;can distribute trining and environment rollout&lt;/li&gt;
&lt;li&gt;multi core and multi instance&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;automatic-model-tuning&#34;&gt;Automatic Model Tuning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;define the hyperparameters you care about&lt;/li&gt;
&lt;li&gt;sagemaker spins up a hyperparameter tuning job that trains as many combinations as you will allow&lt;/li&gt;
&lt;li&gt;it learns as it goes, so it does not have to try every possible combination&lt;/li&gt;
&lt;li&gt;intelligent&lt;/li&gt;
&lt;li&gt;do not optimize too many hyperparameters at once&lt;/li&gt;
&lt;li&gt;limit your ranges to as samall range&lt;/li&gt;
&lt;li&gt;use logarithmic scales&lt;/li&gt;
&lt;li&gt;do not run too many training jobs concurently&lt;/li&gt;
&lt;li&gt;make sure training jobs running on multiple instance report the correct objective metric in the end&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;apache-spark-with-sagemaker&#34;&gt;Apache Spark with SageMaker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;apache spark allows for preprocessing and also has mllib&lt;/li&gt;
&lt;li&gt;combination of sagemaker and spark is possible&lt;/li&gt;
&lt;li&gt;preprocess with spark, and instead of using mllib, you can use sagemaker estimator, you can use kmeans, pca, xgboost&lt;/li&gt;
&lt;li&gt;sagemakermodel, can be used to make inferences&lt;/li&gt;
&lt;li&gt;connect notebook to a remote emr&lt;/li&gt;
&lt;li&gt;fit, transform in sagemaker&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-studio-and-sagemaker-experiments&#34;&gt;SageMaker Studio, and SageMaker Experiments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;visual ide&lt;/li&gt;
&lt;li&gt;sagemaker notebooks&lt;/li&gt;
&lt;li&gt;sagemaker experiments&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-debugger&#34;&gt;SageMaker Debugger&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;saves internal model state at periodical intervals&lt;/li&gt;
&lt;li&gt;gradients/tensors over time is saved&lt;/li&gt;
&lt;li&gt;define rules for detecting unwanted conditions while training&lt;/li&gt;
&lt;li&gt;a debug job is run for each rule&lt;/li&gt;
&lt;li&gt;logs and fires a cloudwatch event when the rule is hit&lt;/li&gt;
&lt;li&gt;sagemaker studio debugger dashboards&lt;/li&gt;
&lt;li&gt;auto generated training reports&lt;/li&gt;
&lt;li&gt;built in rules: monitor system bottlenecks, profile model framework operations, debug model parameters&lt;/li&gt;
&lt;li&gt;supported framewords and algorithms: tensorflow, pytorch, mxnet, xgboost, sagemaker generic estimator&lt;/li&gt;
&lt;li&gt;debugger api&amp;rsquo;s available in github&lt;/li&gt;
&lt;li&gt;smdebug is the library&lt;/li&gt;
&lt;li&gt;Sagemaker debugger insights dashboard&lt;/li&gt;
&lt;li&gt;profiler report, hardware system metrics, framework metrics&lt;/li&gt;
&lt;li&gt;built in actions to receive notifications or stop training&lt;/li&gt;
&lt;li&gt;profiling system resource usage and training&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-autopilot--automl&#34;&gt;SageMaker Autopilot / AutoML&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;automates algorithm selection, data preprocessing, model tuning&lt;/li&gt;
&lt;li&gt;it does all the trial and error for you&lt;/li&gt;
&lt;li&gt;automl&lt;/li&gt;
&lt;li&gt;automatic model creation&lt;/li&gt;
&lt;li&gt;model leaderboard&lt;/li&gt;
&lt;li&gt;ranks&lt;/li&gt;
&lt;li&gt;can add in human guidance&lt;/li&gt;
&lt;li&gt;human in the loop&lt;/li&gt;
&lt;li&gt;with or without code in sagemaker studio&lt;/li&gt;
&lt;li&gt;problem types: binary/multiclass classification&lt;/li&gt;
&lt;li&gt;linear learner, xgboost, mlp&lt;/li&gt;
&lt;li&gt;data must be tabular csv&lt;/li&gt;
&lt;li&gt;autopilot explainability&lt;/li&gt;
&lt;li&gt;integrates with sagemaker clarify&lt;/li&gt;
&lt;li&gt;transparency on how models arrive at predictions&lt;/li&gt;
&lt;li&gt;feature attributions: uses shap baselines/shapley values, research from cooperative game theory, assigns each feature an importance value for a give prediction&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-model-monitor&#34;&gt;SageMaker Model Monitor&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;get alery on quality deviations on your deployed models via cloudwatch&lt;/li&gt;
&lt;li&gt;visualize data drift&lt;/li&gt;
&lt;li&gt;detect anomalies and outliers&lt;/li&gt;
&lt;li&gt;detect new features&lt;/li&gt;
&lt;li&gt;no code required&lt;/li&gt;
&lt;li&gt;data is stored in s3, monitoring jobs are scheduled via a monitoring schedule, metrics are emitted to cloudwatch, integrates with quicksight, tensorboard etc.&lt;/li&gt;
&lt;li&gt;drift in statistical properties of the features&lt;/li&gt;
&lt;li&gt;drift in model quality&lt;/li&gt;
&lt;li&gt;bias drift&lt;/li&gt;
&lt;li&gt;feature attribution drift&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;other-recent-features-jumpstart-data-wrangler-features-store-edge-manager&#34;&gt;Other recent features (JumpStart, Data Wrangler, Features Store, Edge Manager)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;jumpstart: one click models and algorithms from model zoos: 150 open source models in nlp, object detection, image classification etc&lt;/li&gt;
&lt;li&gt;data wrangler: import transform analayze and export data withing sagemaker studio&lt;/li&gt;
&lt;li&gt;feature studio: find, discover and share features in studio:online and offline modes&lt;/li&gt;
&lt;li&gt;sagemaker edge manager: software agent for edge devices, models optimized with agemaker neo, collects and samples data for monitoring, labeling and retraining&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-canvas&#34;&gt;SageMaker Canvas&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;no code machine learning for business analysts&lt;/li&gt;
&lt;li&gt;upload csv data, select a column to predict, build it and make predictions&lt;/li&gt;
&lt;li&gt;can also join datasets&lt;/li&gt;
&lt;li&gt;classification or regressions&lt;/li&gt;
&lt;li&gt;automatic data cleaning, missing values, outlier and duplicates&lt;/li&gt;
&lt;li&gt;share models and datasets with sagemaker studio&lt;/li&gt;
&lt;li&gt;import from redshift is possible&lt;/li&gt;
&lt;li&gt;time series must be enabled via IAM&lt;/li&gt;
&lt;li&gt;vpc&lt;/li&gt;
&lt;li&gt;a little expensive&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;bias-measures-in-sagemaker-clarify&#34;&gt;Bias Measures in SageMaker Clarify&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;class imbalance&lt;/li&gt;
&lt;li&gt;difference in proportions of labels&lt;/li&gt;
&lt;li&gt;kullback-leibler divergence, jensen-shannon divergence&lt;/li&gt;
&lt;li&gt;lp-norm&lt;/li&gt;
&lt;li&gt;total variation distance&lt;/li&gt;
&lt;li&gt;kolmogorov-smirnov&lt;/li&gt;
&lt;li&gt;conditional demographic disparity&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-training-compiler&#34;&gt;SageMaker Training Compiler&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;integrates into AWS deep learning containers&lt;/li&gt;
&lt;li&gt;compile and optimize training jobs on gpu&lt;/li&gt;
&lt;li&gt;can accelerate training up to 50%&lt;/li&gt;
&lt;li&gt;converts models into hardware-optimized instructions&lt;/li&gt;
&lt;li&gt;tested with hugging face transformers library, or bring your own model&lt;/li&gt;
&lt;li&gt;ensure gpu instance are used in ml.p3, ml.p4&lt;/li&gt;
&lt;li&gt;pytorch models must use pytorch xla&amp;rsquo;s model save function&lt;/li&gt;
&lt;li&gt;enable dubug flask in compiler_config parameter to enable debugging&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-comprehend&#34;&gt;Amazon Comprehend&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;nlp and text analytics&lt;/li&gt;
&lt;li&gt;input social media, emails, web pages, documents, transcripts, medical records (comprehend medical)&lt;/li&gt;
&lt;li&gt;extract key phrases, entities, sentiment, language, syntax, topics, and document classifications&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-translate&#34;&gt;Amazon Translate&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;translates text&lt;/li&gt;
&lt;li&gt;uses deep learning&lt;/li&gt;
&lt;li&gt;supports custom terminology for proper names&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-transcribe&#34;&gt;Amazon Transcribe&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;speech to text&lt;/li&gt;
&lt;li&gt;speaker identification&lt;/li&gt;
&lt;li&gt;channel identification&lt;/li&gt;
&lt;li&gt;language identification&lt;/li&gt;
&lt;li&gt;custom vocabularies&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-polly&#34;&gt;Amazon Polly&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;text to speech&lt;/li&gt;
&lt;li&gt;polly is parrot&lt;/li&gt;
&lt;li&gt;lexicons&lt;/li&gt;
&lt;li&gt;ssml (speech synthesis markup language)&lt;/li&gt;
&lt;li&gt;speech marks&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-rekognition&#34;&gt;Amazon Rekognition&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;compute vision&lt;/li&gt;
&lt;li&gt;object and scene detection&lt;/li&gt;
&lt;li&gt;image moderation, facial analysis, celebrity recognition, face comparison, text in image, video analysis&lt;/li&gt;
&lt;li&gt;kinesis video stream h.264 encoded, 5-30 fps&lt;/li&gt;
&lt;li&gt;can use lambda to trigger image analysis upon upload&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-forecast&#34;&gt;Amazon Forecast&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;fully managed service to deliver highly accurate forecasts with ml&lt;/li&gt;
&lt;li&gt;automl chooses the best model for your time series data&lt;/li&gt;
&lt;li&gt;arima, deepar, ets, npts, prophet&lt;/li&gt;
&lt;li&gt;works with any time series&lt;/li&gt;
&lt;li&gt;inventory planning, financial planning, resource planning, based on dataset groups, predictors and forecasts&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-forecast-algorithms&#34;&gt;Amazon Forecast Algorithms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;cnnqr: convolutional neural network quantile regression, best for large datasets with hundreds of time series, accepts related historical time series data and metadata&lt;/li&gt;
&lt;li&gt;deepar+ : recurrent neural network, best for large datasets, accepts related forward-looking time series and metadata&lt;/li&gt;
&lt;li&gt;prophet: additive model with non linear trends and seasonality&lt;/li&gt;
&lt;li&gt;npts: non parametric time series: good for sparse data&lt;/li&gt;
&lt;li&gt;arima: simple datasets&lt;/li&gt;
&lt;li&gt;ets: exponential smoothing&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-lex&#34;&gt;Amazon Lex&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;chatbot engine&lt;/li&gt;
&lt;li&gt;lambda to fulfill intent from text&lt;/li&gt;
&lt;li&gt;can deploy to aws mobile sdk, facebook messenger, slack, twilio&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-personalize&#34;&gt;Amazon Personalize&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;fully managed recommendation engine&lt;/li&gt;
&lt;li&gt;api access: feed in data, provide schema in avro, javascript or sdk, get recommendations, get personalized ranking&lt;/li&gt;
&lt;li&gt;real time or batch recommendations&lt;/li&gt;
&lt;li&gt;recommendations for new users and new items&lt;/li&gt;
&lt;li&gt;contextual recommendations&lt;/li&gt;
&lt;li&gt;similar items&lt;/li&gt;
&lt;li&gt;datasets, recipes, solutions, compaignhs&lt;/li&gt;
&lt;li&gt;hidden_dimensions, bptt, recency_mask, min/max_user_history_length_percentile, exploration_weight, exploration_item_age_cut_off&lt;/li&gt;
&lt;li&gt;necessary to maintain recency&lt;/li&gt;
&lt;li&gt;bucket policy&lt;/li&gt;
&lt;li&gt;data ingestion: per gb, training per training hour, inference per tps-hour, batch recommendations: per user or per item&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lightning-round-textract-deeplens-deepracher-lookout-and-monitron&#34;&gt;Lightning round! TexTract, DeepLens, DeepRacher, Lookout, and Monitron&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;TexTract: ocr with forms, fields, tables support&lt;/li&gt;
&lt;li&gt;DeepLens: deep learning enabled video camera, integrated with rekognition, sagemaker, polly, tensorflow, mxnet, caffe&lt;/li&gt;
&lt;li&gt;DeepRacer: reinforcement learning powered 1/18 scale race car&lt;/li&gt;
&lt;li&gt;Lookout: equipment, metrics and vision: detect defects in silicon wafers, circuit boards etc.&lt;/li&gt;
&lt;li&gt;Monitron: end to end system for monitoring equipment and predictive maintenance&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;torchserve-aws-neuron-and-aws-panorama&#34;&gt;TorchServe, AWS Neuron, and AWS Panorama&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;TorchServe: model serving framework for pytorch&lt;/li&gt;
&lt;li&gt;AWS Neuron: ml inferentia chip, Ec2 inf1 instance type&lt;/li&gt;
&lt;li&gt;Panorama: computer vision at the edge&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;deep-composer-fraud-detection-codeguru-and-contact-lens&#34;&gt;Deep Composer, Fraud Detection, CodeGuru, and Contact Lens&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;DeepComposer: ai powered keyboard&lt;/li&gt;
&lt;li&gt;fraud detection: upload your own data&lt;/li&gt;
&lt;li&gt;Codeguru: automated code reviews, finds lines of code that hurt performance&lt;/li&gt;
&lt;li&gt;contact lens: for customer support call centers, ingests audio, sentiment analysis&lt;/li&gt;
&lt;li&gt;finds utterances that correlate with successful calls&lt;/li&gt;
&lt;li&gt;categorize calls automatically&lt;/li&gt;
&lt;li&gt;measure talk speed and interruptions&lt;/li&gt;
&lt;li&gt;theme detection: discovers emerging issues&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;amazon-kendra-and-amazon-augmented-ai-a2i&#34;&gt;Amazon Kendra and Amazon Augmented AI (A2I)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Enterprise search with natural languate&lt;/li&gt;
&lt;li&gt;combines data from sharepoint, intranet, sharing services, jdbc, s4 into one searchable repo&lt;/li&gt;
&lt;li&gt;ml powered, uses thumbs up/down&lt;/li&gt;
&lt;li&gt;relevance tuning, boost strength of document freshness&lt;/li&gt;
&lt;li&gt;Kendra: Alexa&amp;rsquo;s sister&lt;/li&gt;
&lt;li&gt;AugmentedAI: human review of ml predictions, mechanical turk workforce or vendors&lt;/li&gt;
&lt;li&gt;integrated into textract and rekognition&lt;/li&gt;
&lt;li&gt;integrates with sagemaker&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>AWS Certified ML - Specialty exam (MLS-C01) - 4. Machine Learning Implementation and Operations</title>
      <link>https://ayushsubedi.github.io/posts/aws_ml_speciality_ml/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/aws_ml_speciality_ml/</guid>
      <description>&lt;h1 id=&#34;machine-learning-implementation-and-operations&#34;&gt;Machine Learning Implementation and Operations&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#section-intro-machine-learning-implementation-and-operations&#34;&gt;Section Intro: Machine Learning Implementation and Operations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemakers-inner-details-and-production-variants&#34;&gt;SageMaker&amp;rsquo;s Inner Details and Production Variants&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-on-the-edge-sagemaker-neo-and-iot-greengrass&#34;&gt;SageMaker On the Edge: SageMaker Neo and IoT Greengrass&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-security-encryption-at-rest-and-in-transit&#34;&gt;SageMaker Security: Encryption at Rest and In Transit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-security-vpcs-iam-logging-and-monitoring&#34;&gt;SageMaker Security: VPC&amp;rsquo;s, IAM, Logging, and Monitoring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-resource-management-instance-types-and-spot-training&#34;&gt;SageMaker Resource Management: Instance Types and Spot Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-resource-management-elastic-inference-automatic-scaling-azs&#34;&gt;SageMaker Resource Management: Elastic Inference, Automatic Scaling, AZ&amp;rsquo;s&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-serverless-inference-and-inference-recommender&#34;&gt;SageMaker Serverless Inference and Inference Recommender&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sagemaker-inference-pipelines&#34;&gt;SageMaker Inference Pipelines&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This section covers building machine learning solutions for performance, availability, scalability, resiliency, and fault tolerance, recommending and implementing the appropriate machine learning services and features for a given problem, applying basic AWS security practices to machine learning solutions and deploying and operationalizing machine learning solutions.&lt;/p&gt;
&lt;h2 id=&#34;section-intro-machine-learning-implementation-and-operations&#34;&gt;Section Intro: Machine Learning Implementation and Operations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;scaling, productionalization and security&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemakers-inner-details-and-production-variants&#34;&gt;SageMaker&amp;rsquo;s Inner Details and Production Variants&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;all models in agemaker are hosted in docker containers&lt;/li&gt;
&lt;li&gt;the docker container is registered with ECR&lt;/li&gt;
&lt;li&gt;pre built deep learning, scikit learn and spark ml&lt;/li&gt;
&lt;li&gt;pre built tensorflow, mxnet, chainer, pytorch etc. horovod or parameter server is a way to distribute tensorflow training&lt;/li&gt;
&lt;li&gt;you can also use any script or algorithm within sagemaker&lt;/li&gt;
&lt;li&gt;the containers are isolated and contain all dependencies&lt;/li&gt;
&lt;li&gt;Dockerfile structure&lt;/li&gt;
&lt;li&gt;using your own image&lt;/li&gt;
&lt;li&gt;you can test muliple models on live traffic using Production Variants (Roll out variant weights)&lt;/li&gt;
&lt;li&gt;A/B test is posible&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-on-the-edge-sagemaker-neo-and-iot-greengrass&#34;&gt;SageMaker On the Edge: SageMaker Neo and IoT Greengrass&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Train once, run anywhere&lt;/li&gt;
&lt;li&gt;supports multiple architecture&lt;/li&gt;
&lt;li&gt;optimizes code for specific devices&lt;/li&gt;
&lt;li&gt;consists of a compiler and a runtime&lt;/li&gt;
&lt;li&gt;Neo compiled models can be deployed to an https endpoint, must be the same instance type used for compilation&lt;/li&gt;
&lt;li&gt;or you can deploy to iot greengrass&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-security-encryption-at-rest-and-in-transit&#34;&gt;SageMaker Security: Encryption at Rest and In Transit&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;IAM&lt;/li&gt;
&lt;li&gt;MFA&lt;/li&gt;
&lt;li&gt;SSL/TLS&lt;/li&gt;
&lt;li&gt;Cloudtrail to log API and user activity&lt;/li&gt;
&lt;li&gt;encryption&lt;/li&gt;
&lt;li&gt;AWS key mangement service is accepted by notebooks and all sagemaker jobs&lt;/li&gt;
&lt;li&gt;s3 can be encrypted as well&lt;/li&gt;
&lt;li&gt;All traffic supports TLS/SSL&lt;/li&gt;
&lt;li&gt;inter node training communication may be optionally encrypted&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-security-vpcs-iam-logging-and-monitoring&#34;&gt;SageMaker Security: VPC&amp;rsquo;s, IAM, Logging, and Monitoring&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;training jobs run in VPC&lt;/li&gt;
&lt;li&gt;private VPC&lt;/li&gt;
&lt;li&gt;s3 vpc endpoints&lt;/li&gt;
&lt;li&gt;IAM user permissions for CreateTrainingJob, CreateModel, CreateEndpointConfig, CreateTransformJob, CreateHyperParameterTuningJob, CreateNotebookInstance, UpdateNotebookInstance&lt;/li&gt;
&lt;li&gt;Predefined policies for AmazonSageMakerReadOnly, AmazonSageMakerFullAccess, AdministratorAccess, DataScientist&lt;/li&gt;
&lt;li&gt;cloudwatch can log, monitor and alarm on invocations and latency of endpoints, health of instance nodes, ground truth (active workers)&lt;/li&gt;
&lt;li&gt;cloudtrail records actions from users, roles, and services within Sagemaker: log files are delivered to s3 for auditing purposes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-resource-management-instance-types-and-spot-training&#34;&gt;SageMaker Resource Management: Instance Types and Spot Training&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;depends but usually gpu for training and cpu for inference&lt;/li&gt;
&lt;li&gt;EC2 spot training: checkpoints to s3 so training can resume&lt;/li&gt;
&lt;li&gt;can increate training time&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-resource-management-elastic-inference-automatic-scaling-azs&#34;&gt;SageMaker Resource Management: Elastic Inference, Automatic Scaling, AZ&amp;rsquo;s&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;accelerates deep learning inference at a fraction of a cost&lt;/li&gt;
&lt;li&gt;EI accelerator may be added alongside a CPU instance&lt;/li&gt;
&lt;li&gt;EI accelerators can also be applied to notebooks&lt;/li&gt;
&lt;li&gt;works with tensorflow, pytorch, mxnet, onnx may be used to export models to mxnet&lt;/li&gt;
&lt;li&gt;works with custom containers built with El-enabled Tensorflow, Pytorch or mxnet&lt;/li&gt;
&lt;li&gt;works with image classification and object detection built in algorithms&lt;/li&gt;
&lt;li&gt;Automatic scaling: can be used to define target metrics, min or max capacity, cooldown periods, works with cloudwatch, dynamically adjusts number of instances for a production variant, load test your configuration before using it.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-serverless-inference-and-inference-recommender&#34;&gt;SageMaker Serverless Inference and Inference Recommender&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;specify your container, memory requirement, concurrency requirements&lt;/li&gt;
&lt;li&gt;underlying capacity is automatically provisioned and scaled&lt;/li&gt;
&lt;li&gt;good for infrequent or unpredictable traffic, will scale down to zero when there are no requests&lt;/li&gt;
&lt;li&gt;chared based on usage&lt;/li&gt;
&lt;li&gt;monitor via cloudwatch&lt;/li&gt;
&lt;li&gt;Inference Recommender  recommends what instance type and configuration for your model&lt;/li&gt;
&lt;li&gt;automates load testing and model tuning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sagemaker-inference-pipelines&#34;&gt;SageMaker Inference Pipelines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;linear sequence of 2-15 containers&lt;/li&gt;
&lt;li&gt;any combination of pre-trained built-in algorithms or your own algorithms in Docker&lt;/li&gt;
&lt;li&gt;combine pre-processing, predictions, post-processing&lt;/li&gt;
&lt;li&gt;Spark ML and scikit-learn&lt;/li&gt;
&lt;li&gt;chaining multiple inference containers into a pipeline of results&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Term Frequecy Inverse Document Frequency (TFIDF)</title>
      <link>https://ayushsubedi.github.io/posts/tfidf/</link>
      <pubDate>Sun, 25 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/tfidf/</guid>
      <description>&lt;h1 id=&#34;wikipedia-search-using-tfidf&#34;&gt;Wikipedia search using TFIDF&lt;/h1&gt;
&lt;h2 id=&#34;term-frequecy-inverse-document-frequency&#34;&gt;Term Frequecy Inverse Document Frequency&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://ayushsubedi.github.io/img/tfidf.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;please, call, the, number, below, do, not, us, please call, call the, the number, number below, please do, do not, not call, call us&lt;/p&gt;
&lt;p&gt;dimension = [2, 16]&lt;/p&gt;
&lt;h1 id=&#34;example-of-unigram-tfidf&#34;&gt;Example of unigram TFIDF&lt;/h1&gt;
&lt;h2 id=&#34;imports&#34;&gt;Imports&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; pd
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; os
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pyspark
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyspark.sql &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; SparkSession
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyspark.sql.types &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyspark.sql.functions &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; udf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyspark.ml.feature &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; HashingTF, IDF, Tokenizer
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;sparksession&#34;&gt;SparkSession&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;spark &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SparkSession&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;builder \
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;appName(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tfidf&amp;#39;&lt;/span&gt;)\
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;config(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;spark.jars&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;../jars/snowflake-jdbc-3.13.6.jar, ../jars/spark-snowflake_2.12-2.9.0-spark_3.1.jar&amp;#39;&lt;/span&gt;) \
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;getOrCreate()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;spark&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sparkContext&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;setLogLevel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;WARN&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;22/12/27 13:35:58 WARN Utils: Your hostname, SPMBP136.local resolves to a loopback address: 127.0.0.1; using 192.168.0.101 instead (on interface en6)
22/12/27 13:35:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
22/12/27 13:35:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable


Setting default log level to &amp;quot;WARN&amp;quot;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;file_path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;../datasets/wiki.csv&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wiki &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; spark&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;csv&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;option(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;header&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(file_path)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wiki&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;+---+--------------------+-------------------+--------------------+
| ID|               Title|               Time|            Document|
+---+--------------------+-------------------+--------------------+
| 12|           Anarchism|2008-12-30 06:23:05|&amp;quot;Anarchism (somet...|
| 25|              Autism|2008-12-24 20:41:05|&amp;quot;Autism is a brai...|
| 39|              Albedo|2008-12-29 18:19:09|&amp;quot;The albedo of an...|
|290|                   A|2008-12-27 04:33:16|&amp;quot;The letter A is ...|
|303|             Alabama|2008-12-29 08:15:47|&amp;quot;Alabama (formall...|
|305|            Achilles|2008-12-30 06:18:01|&amp;quot;thumb\n\nIn Gree...|
|307|     Abraham Lincoln|2008-12-28 20:18:23|&amp;quot;Abraham Lincoln ...|
|308|           Aristotle|2008-12-29 23:54:48|&amp;quot;Aristotle (Greek...|
|309|An American in Paris|2008-09-27 19:29:28|&amp;quot;An American in P...|
|324|       Academy Award|2008-12-28 17:50:43|&amp;quot;The Academy Awar...|
|330|             Actrius|2008-05-23 15:24:32|Actrius (Actresse...|
|332|     Animalia (book)|2008-12-18 11:12:34|thumbAnimalia (IS...|
|334|International Ato...|2008-11-21 22:40:20|International Ato...|
|336|            Altruism|2008-12-27 03:57:17|&amp;quot;Altruism is self...|
|339|            Ayn Rand|2008-12-30 08:03:06|&amp;quot;Ayn Rand (,   M...|
|340|        Alain Connes|2008-09-03 13:41:39|Alain Connes (bor...|
|344|          Allan Dwan|2008-11-14 05:28:58|Allan Dwan (April...|
|358|             Algeria|2008-12-29 02:54:36|&amp;quot;Algeria (, al-Ja...|
|359|List of character...|2008-12-23 20:20:21|&amp;quot;This is a list o...|
|569|        Anthropology|2008-12-28 23:04:30|&amp;quot;Anthropology (, ...|
+---+--------------------+-------------------+--------------------+
only showing top 20 rows
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wiki&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;filter(wiki&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Document&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isNull())&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;1
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wiki &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; wiki&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;filter(&lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt;wiki&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Document&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isNull())
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wiki&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;+---+--------------------+-------------------+--------------------+
| ID|               Title|               Time|            Document|
+---+--------------------+-------------------+--------------------+
| 12|           Anarchism|2008-12-30 06:23:05|&amp;quot;Anarchism (somet...|
| 25|              Autism|2008-12-24 20:41:05|&amp;quot;Autism is a brai...|
| 39|              Albedo|2008-12-29 18:19:09|&amp;quot;The albedo of an...|
|290|                   A|2008-12-27 04:33:16|&amp;quot;The letter A is ...|
|303|             Alabama|2008-12-29 08:15:47|&amp;quot;Alabama (formall...|
|305|            Achilles|2008-12-30 06:18:01|&amp;quot;thumb\n\nIn Gree...|
|307|     Abraham Lincoln|2008-12-28 20:18:23|&amp;quot;Abraham Lincoln ...|
|308|           Aristotle|2008-12-29 23:54:48|&amp;quot;Aristotle (Greek...|
|309|An American in Paris|2008-09-27 19:29:28|&amp;quot;An American in P...|
|324|       Academy Award|2008-12-28 17:50:43|&amp;quot;The Academy Awar...|
|330|             Actrius|2008-05-23 15:24:32|Actrius (Actresse...|
|332|     Animalia (book)|2008-12-18 11:12:34|thumbAnimalia (IS...|
|334|International Ato...|2008-11-21 22:40:20|International Ato...|
|336|            Altruism|2008-12-27 03:57:17|&amp;quot;Altruism is self...|
|339|            Ayn Rand|2008-12-30 08:03:06|&amp;quot;Ayn Rand (,   M...|
|340|        Alain Connes|2008-09-03 13:41:39|Alain Connes (bor...|
|344|          Allan Dwan|2008-11-14 05:28:58|Allan Dwan (April...|
|358|             Algeria|2008-12-29 02:54:36|&amp;quot;Algeria (, al-Ja...|
|359|List of character...|2008-12-23 20:20:21|&amp;quot;This is a list o...|
|569|        Anthropology|2008-12-28 23:04:30|&amp;quot;Anthropology (, ...|
+---+--------------------+-------------------+--------------------+
only showing top 20 rows
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tokenizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Tokenizer(inputCol&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Document&amp;#34;&lt;/span&gt;, outputCol&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;words&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wordsData &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform(wiki)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wordsData&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;+---+--------------------+-------------------+--------------------+--------------------+
| ID|               Title|               Time|            Document|               words|
+---+--------------------+-------------------+--------------------+--------------------+
| 12|           Anarchism|2008-12-30 06:23:05|&amp;quot;Anarchism (somet...|[&amp;quot;anarchism, (som...|
| 25|              Autism|2008-12-24 20:41:05|&amp;quot;Autism is a brai...|[&amp;quot;autism, is, a, ...|
| 39|              Albedo|2008-12-29 18:19:09|&amp;quot;The albedo of an...|[&amp;quot;the, albedo, of...|
|290|                   A|2008-12-27 04:33:16|&amp;quot;The letter A is ...|[&amp;quot;the, letter, a,...|
|303|             Alabama|2008-12-29 08:15:47|&amp;quot;Alabama (formall...|[&amp;quot;alabama, (forma...|
|305|            Achilles|2008-12-30 06:18:01|&amp;quot;thumb\n\nIn Gree...|[&amp;quot;thumb\n\nin, gr...|
|307|     Abraham Lincoln|2008-12-28 20:18:23|&amp;quot;Abraham Lincoln ...|[&amp;quot;abraham, lincol...|
|308|           Aristotle|2008-12-29 23:54:48|&amp;quot;Aristotle (Greek...|[&amp;quot;aristotle, (gre...|
|309|An American in Paris|2008-09-27 19:29:28|&amp;quot;An American in P...|[&amp;quot;an, american, i...|
|324|       Academy Award|2008-12-28 17:50:43|&amp;quot;The Academy Awar...|[&amp;quot;the, academy, a...|
|330|             Actrius|2008-05-23 15:24:32|Actrius (Actresse...|[actrius, (actres...|
|332|     Animalia (book)|2008-12-18 11:12:34|thumbAnimalia (IS...|[thumbanimalia, (...|
|334|International Ato...|2008-11-21 22:40:20|International Ato...|[international, a...|
|336|            Altruism|2008-12-27 03:57:17|&amp;quot;Altruism is self...|[&amp;quot;altruism, is, s...|
|339|            Ayn Rand|2008-12-30 08:03:06|&amp;quot;Ayn Rand (,   M...|[&amp;quot;ayn, rand, (,, ...|
|340|        Alain Connes|2008-09-03 13:41:39|Alain Connes (bor...|[alain, connes, (...|
|344|          Allan Dwan|2008-11-14 05:28:58|Allan Dwan (April...|[allan, dwan, (ap...|
|358|             Algeria|2008-12-29 02:54:36|&amp;quot;Algeria (, al-Ja...|[&amp;quot;algeria, (,, al...|
|359|List of character...|2008-12-23 20:20:21|&amp;quot;This is a list o...|[&amp;quot;this, is, a, li...|
|569|        Anthropology|2008-12-28 23:04:30|&amp;quot;Anthropology (, ...|[&amp;quot;anthropology, (...|
+---+--------------------+-------------------+--------------------+--------------------+
only showing top 20 rows
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hashingTF &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; HashingTF(inputCol&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;words&amp;#34;&lt;/span&gt;, outputCol&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rawFeatures&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;featuredData &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; hashingTF&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform(wordsData)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;featuredData&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;+---+--------------------+-------------------+--------------------+--------------------+--------------------+
| ID|               Title|               Time|            Document|               words|         rawFeatures|
+---+--------------------+-------------------+--------------------+--------------------+--------------------+
| 12|           Anarchism|2008-12-30 06:23:05|&amp;quot;Anarchism (somet...|[&amp;quot;anarchism, (som...|(262144,[15157,27...|
| 25|              Autism|2008-12-24 20:41:05|&amp;quot;Autism is a brai...|[&amp;quot;autism, is, a, ...|(262144,[15,1546,...|
| 39|              Albedo|2008-12-29 18:19:09|&amp;quot;The albedo of an...|[&amp;quot;the, albedo, of...|(262144,[7853,240...|
|290|                   A|2008-12-27 04:33:16|&amp;quot;The letter A is ...|[&amp;quot;the, letter, a,...|(262144,[6037,942...|
|303|             Alabama|2008-12-29 08:15:47|&amp;quot;Alabama (formall...|[&amp;quot;alabama, (forma...|(262144,[1797,256...|
|305|            Achilles|2008-12-30 06:18:01|&amp;quot;thumb\n\nIn Gree...|[&amp;quot;thumb\n\nin, gr...|(262144,[10758,16...|
|307|     Abraham Lincoln|2008-12-28 20:18:23|&amp;quot;Abraham Lincoln ...|[&amp;quot;abraham, lincol...|(262144,[2564,460...|
|308|           Aristotle|2008-12-29 23:54:48|&amp;quot;Aristotle (Greek...|[&amp;quot;aristotle, (gre...|(262144,[2767,356...|
|309|An American in Paris|2008-09-27 19:29:28|&amp;quot;An American in P...|[&amp;quot;an, american, i...|(262144,[2366,670...|
|324|       Academy Award|2008-12-28 17:50:43|&amp;quot;The Academy Awar...|[&amp;quot;the, academy, a...|(262144,[2931,328...|
|330|             Actrius|2008-05-23 15:24:32|Actrius (Actresse...|[actrius, (actres...|(262144,[6558,674...|
|332|     Animalia (book)|2008-12-18 11:12:34|thumbAnimalia (IS...|[thumbanimalia, (...|(262144,[2284,609...|
|334|International Ato...|2008-11-21 22:40:20|International Ato...|[international, a...|(262144,[847,925,...|
|336|            Altruism|2008-12-27 03:57:17|&amp;quot;Altruism is self...|[&amp;quot;altruism, is, s...|(262144,[5675,680...|
|339|            Ayn Rand|2008-12-30 08:03:06|&amp;quot;Ayn Rand (,   M...|[&amp;quot;ayn, rand, (,, ...|(262144,[528,1091...|
|340|        Alain Connes|2008-09-03 13:41:39|Alain Connes (bor...|[alain, connes, (...|(262144,[154,1595...|
|344|          Allan Dwan|2008-11-14 05:28:58|Allan Dwan (April...|[allan, dwan, (ap...|(262144,[1578,181...|
|358|             Algeria|2008-12-29 02:54:36|&amp;quot;Algeria (, al-Ja...|[&amp;quot;algeria, (,, al...|(262144,[3852,492...|
|359|List of character...|2008-12-23 20:20:21|&amp;quot;This is a list o...|[&amp;quot;this, is, a, li...|(262144,[14376,19...|
|569|        Anthropology|2008-12-28 23:04:30|&amp;quot;Anthropology (, ...|[&amp;quot;anthropology, (...|(262144,[57138,10...|
+---+--------------------+-------------------+--------------------+--------------------+--------------------+
only showing top 20 rows
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;idf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; IDF(inputCol&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rawFeatures&amp;#34;&lt;/span&gt;, outputCol&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;features&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;idfModel &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; idf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(featuredData)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rescaledData &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; idfModel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform(featuredData)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rescaledData&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;22/12/27 13:36:11 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB
+---+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+
| ID|               Title|               Time|            Document|               words|         rawFeatures|            features|
+---+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+
| 12|           Anarchism|2008-12-30 06:23:05|&amp;quot;Anarchism (somet...|[&amp;quot;anarchism, (som...|(262144,[15157,27...|(262144,[15157,27...|
| 25|              Autism|2008-12-24 20:41:05|&amp;quot;Autism is a brai...|[&amp;quot;autism, is, a, ...|(262144,[15,1546,...|(262144,[15,1546,...|
| 39|              Albedo|2008-12-29 18:19:09|&amp;quot;The albedo of an...|[&amp;quot;the, albedo, of...|(262144,[7853,240...|(262144,[7853,240...|
|290|                   A|2008-12-27 04:33:16|&amp;quot;The letter A is ...|[&amp;quot;the, letter, a,...|(262144,[6037,942...|(262144,[6037,942...|
|303|             Alabama|2008-12-29 08:15:47|&amp;quot;Alabama (formall...|[&amp;quot;alabama, (forma...|(262144,[1797,256...|(262144,[1797,256...|
|305|            Achilles|2008-12-30 06:18:01|&amp;quot;thumb\n\nIn Gree...|[&amp;quot;thumb\n\nin, gr...|(262144,[10758,16...|(262144,[10758,16...|
|307|     Abraham Lincoln|2008-12-28 20:18:23|&amp;quot;Abraham Lincoln ...|[&amp;quot;abraham, lincol...|(262144,[2564,460...|(262144,[2564,460...|
|308|           Aristotle|2008-12-29 23:54:48|&amp;quot;Aristotle (Greek...|[&amp;quot;aristotle, (gre...|(262144,[2767,356...|(262144,[2767,356...|
|309|An American in Paris|2008-09-27 19:29:28|&amp;quot;An American in P...|[&amp;quot;an, american, i...|(262144,[2366,670...|(262144,[2366,670...|
|324|       Academy Award|2008-12-28 17:50:43|&amp;quot;The Academy Awar...|[&amp;quot;the, academy, a...|(262144,[2931,328...|(262144,[2931,328...|
|330|             Actrius|2008-05-23 15:24:32|Actrius (Actresse...|[actrius, (actres...|(262144,[6558,674...|(262144,[6558,674...|
|332|     Animalia (book)|2008-12-18 11:12:34|thumbAnimalia (IS...|[thumbanimalia, (...|(262144,[2284,609...|(262144,[2284,609...|
|334|International Ato...|2008-11-21 22:40:20|International Ato...|[international, a...|(262144,[847,925,...|(262144,[847,925,...|
|336|            Altruism|2008-12-27 03:57:17|&amp;quot;Altruism is self...|[&amp;quot;altruism, is, s...|(262144,[5675,680...|(262144,[5675,680...|
|339|            Ayn Rand|2008-12-30 08:03:06|&amp;quot;Ayn Rand (,   M...|[&amp;quot;ayn, rand, (,, ...|(262144,[528,1091...|(262144,[528,1091...|
|340|        Alain Connes|2008-09-03 13:41:39|Alain Connes (bor...|[alain, connes, (...|(262144,[154,1595...|(262144,[154,1595...|
|344|          Allan Dwan|2008-11-14 05:28:58|Allan Dwan (April...|[allan, dwan, (ap...|(262144,[1578,181...|(262144,[1578,181...|
|358|             Algeria|2008-12-29 02:54:36|&amp;quot;Algeria (, al-Ja...|[&amp;quot;algeria, (,, al...|(262144,[3852,492...|(262144,[3852,492...|
|359|List of character...|2008-12-23 20:20:21|&amp;quot;This is a list o...|[&amp;quot;this, is, a, li...|(262144,[14376,19...|(262144,[14376,19...|
|569|        Anthropology|2008-12-28 23:04:30|&amp;quot;Anthropology (, ...|[&amp;quot;anthropology, (...|(262144,[57138,10...|(262144,[57138,10...|
+---+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+
only showing top 20 rows
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;search&#34;&gt;Search&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;search_article&lt;/span&gt;(keyword):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# get the hash val from keyword&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    schema &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; StructType([StructField(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;words&amp;#34;&lt;/span&gt;, ArrayType(StringType()))])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    temp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; spark&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;createDataFrame(([[[keyword]]]), schema)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;toDF(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;words&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    temp_unhashed &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; hashingTF&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform(temp)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;select(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rawFeatures&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;collect()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    val &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int(temp_unhashed[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rawFeatures&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;indices[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;#&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    termExtractor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; udf(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x:float(x[val]), FloatType())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    final &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rescaledData&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;withColumn(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;score&amp;#39;&lt;/span&gt;, termExtractor(rescaledData&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;features))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    final &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; final&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;filter(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;score&amp;gt;0&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;orderBy(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;score&amp;#34;&lt;/span&gt;, ascending&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; final&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;select(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ID&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Title&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;score&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;search_article(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mystery&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;22/12/27 13:36:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB


[Stage 11:===================&amp;gt;                                      (1 + 2) / 3]

+----+--------------------+--------+
|  ID|               Title|   score|
+----+--------------------+--------+
| 984|     Agatha Christie|5.521461|
| 986|          The Plague|5.521461|
|1307|The Alan Parsons ...|5.521461|
+----+--------------------+--------+
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;search_article(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;comic&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;22/12/27 13:36:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB
+----+--------------------+----------+
|  ID|               Title|     score|
+----+--------------------+----------+
| 931|The Amazing Spide...|14.4849415|
|2101|             Asterix|  9.656628|
|1549|             Agathon|  9.656628|
|2023|           Aeschylus|  9.656628|
|1028|        Aristophanes|  9.656628|
|1614|              Alexis|  4.828314|
|1784|  Athenian democracy|  4.828314|
+----+--------------------+----------+
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;search_article(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;revolution&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;22/12/27 13:36:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB
+----+--------------------+---------+
|  ID|               Title|    score|
+----+--------------------+---------+
|1973| American Revolution|12.052151|
|2273|            AFC Ajax|4.0173836|
| 339|            Ayn Rand|4.0173836|
| 572|Agricultural science|4.0173836|
| 771|American Revoluti...|4.0173836|
| 915|       Andrey Markov|4.0173836|
| 930|       Alvin Toffler|4.0173836|
|1030|     Austrian School|4.0173836|
|1057|      Anatole France|4.0173836|
|1192| Artistic revolution|4.0173836|
|1316|      Annales School|4.0173836|
|1676|Alfonso XII of Spain|4.0173836|
|1363|  Andr-Marie Ampre|4.0173836|
|2075|  Aircraft hijacking|4.0173836|
|1784|  Athenian democracy|4.0173836|
|1844|          Archimedes|4.0173836|
|2070|Act of Settlement...|4.0173836|
+----+--------------------+---------+
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;search_article(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;football&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;22/12/27 13:36:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB
+----+--------------------+---------+
|  ID|               Title|    score|
+----+--------------------+---------+
|2273|            AFC Ajax|54.596165|
|2357|American Football...|46.196754|
|2174|        Arsenal F.C.|29.397936|
|2358|           A.S. Roma| 25.19823|
|2102|   Arizona Cardinals|20.998526|
|2103|     Atlanta Falcons| 16.79882|
| 615|American Football...| 16.79882|
| 925|Alumni Athletic Club|12.599115|
|2289|  AZ (football club)| 4.199705|
|2310|       Arthur Miller| 4.199705|
|1797|                Acre| 4.199705|
|2363|Alessandro Scarlatti| 4.199705|
|2382|               Aalen| 4.199705|
|1016|       Achill Island| 4.199705|
+----+--------------------+---------+
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;search_article(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;emirates&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;22/12/27 13:36:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB
+----+------------+--------+
|  ID|       Title|   score|
+----+------------+--------+
|2174|Arsenal F.C.|6.214608|
+----+------------+--------+
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;search_article(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;the&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;22/12/27 13:36:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB
+----+--------------------+---------+
|  ID|               Title|    score|
+----+--------------------+---------+
|1854| Geography of Africa|56.093544|
|2273|            AFC Ajax|43.326492|
|2023|           Aeschylus|41.968296|
|1216|              Athens|30.287798|
| 717|             Alberta|26.213207|
|2358|           A.S. Roma|23.904272|
| 841|      Attila the Hun|23.360992|
|1285|Geography of Alabama|23.089354|
|2338|Rise and Fall of ...|21.323696|
|1440|       Abydos, Egypt|19.150581|
| 904|           Aluminium| 18.87894|
|1905|              Ambush|18.199842|
|1962|  Apparent magnitude|17.928204|
|1557|Agrippina the You...|17.792383|
|1613|  Alexios I Komnenos|17.792383|
|1234|     Acoustic theory|17.520744|
|2064|      Antonio Canova|15.619268|
|1686| Alfonso V of Aragon| 15.07599|
|1451|APL (programming ...| 15.07599|
|2274|Arthur Stanley Ed...| 14.80435|
+----+--------------------+---------+
only showing top 20 rows
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Flight delay prediction and exploration in the United States</title>
      <link>https://ayushsubedi.github.io/posts/airlines_delay/</link>
      <pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ayushsubedi.github.io/posts/airlines_delay/</guid>
      <description>&lt;h1 id=&#34;flight-delay-prediction-and-exploration-in-the-us&#34;&gt;Flight delay prediction and exploration in the US&lt;/h1&gt;
&lt;h3 id=&#34;as-a-part-of-georgia-tech-omsa-cse-6242-data-and-visual-analytics&#34;&gt;As a part of Georgia Tech OMSA, CSE 6242: Data and Visual Analytics&lt;/h3&gt;
&lt;p&gt;Poster Presentation&lt;/p&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;420&#34; src=&#34;https://www.youtube.com/embed/ChbIK66ka1A&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;Poster&lt;/p&gt;
&lt;iframe width=&#34;100%&#34; height =&#34;1024&#34; src=&#34;https://ayushsubedi.github.io/pdfs/Poster.pdf#toolbar=0&#34;&gt;&lt;/iframe&gt;
&lt;hr/&gt;
&lt;p&gt;Final Report&lt;/p&gt;
&lt;iframe width=&#34;100%&#34; height =&#34;1024&#34; src=&#34;https://ayushsubedi.github.io/pdfs/cse6242_airlines.pdf#toolbar=0&#34;&gt;&lt;/iframe&gt;
</description>
    </item>
    
  </channel>
</rss>